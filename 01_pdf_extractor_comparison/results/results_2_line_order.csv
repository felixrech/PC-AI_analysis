id,user_type,filename,n_pages,page,source_id,source,task,content,result
pdfminer_2665582_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665582.pdf,3,2,2665582,attachments/2665582.pdf#page=2,Is the line order correct?,"<pre>SHERPA  (Shaping  the  ethical  dimensions  of  smart  information  systems  (SIS)  –  a  European 
perspective) is a project that focuses on ethical and human rights aspects of smart information 
systems (artificial intelligence and big data analytics).  
Regulatory  governance  systems  are  a  key  part  of  the  SHERPA  Final  Recommendations,1  which 
include a call for creating an EU regulatory framework2 and establishing an EU Agency for AI.3 
SHERPA believes a robust, mandatory legal framework at the EU level is needed to ensure that 
ethical issues and human rights concerns related to AI are adequately addressed. There are many 
specific  elements  of  the  SHERPA  recommendations  that  appear  in  the  proposed  text  of  the 
Regulation,  like  red  lines  for  some  AI  applications,  mandatory  requirements  for  high-risk  AI 
systems, and the creation of a centralised body. However, the draft text does not do enough to 
protect  fundamental  rights,  often  lacks  conceptual  clarity,  and  leaves  many  questions 
unanswered 
For more information, please visit: https://www.project-sherpa.eu 
Risk-based approach 
The proposed Regulation adopts a four-tiered risk-based approach, where AI systems are subject 
to different rules depending on the level of risk. While one purpose of the regulatory framework 
is to guarantee safety and fundamental rights of EU citizens, the risk-based approach adopted by 
the Commission may not be sufficient.  There is no reference to the EU Agency for Fundamental 
Rights, nor are there provisions on complaint and redress mechanisms available to those whose 
rights are violated by AI systems. Furthermore, the proposed regulation has a somewhat binary 
approach,  failing  to  adequately  take  into  account  impacts  across  the  spectrum  of  risk.  Most 
mandatory requirements apply only to high-risk systems; by comparison, low-risk AI systems are 
only subject to transparency requirements and minimal-risk AI systems have no requirements. 
Definition of AI 
SHERPA  recommended  that  AI  be  clearly  defined  in  each  use  context  with  regard  to  relevant 
issues. While it is a challenge to precisely define AI, definitions used in the proposed Regulation 
are  often  overly  broad  and  too  open  to  interpretation.  Additionally,  despite  attempts  to  be 
“technology neutral and as future proof as possible”, the proposed definition of AI is linked to 
‘software’, leaving out potential future developments of AI. The proposal takes into account the 
difficulty of defining AI by moving the definition into an appendix which is subject to review and 
revision. While this is reasonable in light of the problematic nature of the term AI, it does add to 
uncertainty about the future scope of the Regulation.  
Red lines 
Under  the  proposal’s  risk-based  approach,  AI  systems  that  pose  the  highest  level  of  risk  to 
fundamental  rights  and  safety  are  prohibited.  The  short  list  of  banned  AI  systems  –  only  four 
categories – includes social scoring and AI that subliminally manipulates human behaviour in a 
harmful  way.  While  SHERPA  welcomes  the  explicit  inclusion  of  red  lines  in  the  regulatory 
framework, the short list is incomplete and has many loopholes.  For example, use of remote, 
1 https://www.project-sherpa.eu/recommendations/ 
2 https://www.project-sherpa.eu/regulatory-framework/ 
3 https://www.project-sherpa.eu/european-agency-for-ai/ 
2 
This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. 786641 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
tika_2665527_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665527.pdf,5,2,2665527,attachments/2665527.pdf#page=2,Is the line order correct?,"<pre>I. Introduction


Moje Państwo Foundation (“Foundation”) is an organization working for the


development of democracy, open and transparent public authority and civic engagement.


By exercising the right of access to public information and the right to re-use public


sector information, the Foundation collects publicly available data sets and makes them


available to citizens through the Foundation's services.


Due to the fact that the context of the consultations run by the European Commission


from 26th April to 6th August 2021 are related to many relevant aspects for the


Foundation’s activity, we present our position in regards to the Proposal for a Regulation


Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) (“the


AI Act” or “the European Commission’s Proposal”) below – with particular emphasis


on the use of artificial intelligence systems (“AI systems”) by public institutions.


II. General remarks


We appreciate the work of the European Commission undertaken to create a regulation


that aims to ensure that artificial intelligence is safe, lawful and in line with EU


fundamental rights. The European Commission’s Proposal contains many valuable


solutions creating a legal framework for artificial intelligence.


From the perspective of our Foundation, the AI Act should – to a greater extent


– ensure transparency of the public sector in relation to the use of artificial


intelligence systems.


AI systems can be used by public institutions in many key areas, such as, for example,


health protection, education, social services, the judiciary and the economy. As a


consequence, these systems can have a very broad and varied impact and can affect


many different social groups, including those particularly vulnerable to discrimination.


The use of AI systems by the state in many situations may lead to shaping the scope of


rights and obligations of citizens by these systems.


The European Commission’s Proposal presupposes measures to regulate the use of AI


systems in relation to public authorities - for example, the proposal prohibits AI-based


social scoring for general purposes done by public authorities; the use of ‘real time’


remote biometric identification systems in publicly accessible spaces for the purpose of


law enforcement is also prohibited unless certain limited exceptions apply; Annex III


1</pre>",POSITIVE
PyPDF2_2665345_14,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665345.pdf,17,14,2665345,attachments/2665345.pdf#page=14,Is the line order correct?,"<pre>Es ist deshalb zu definieren, wann man noch von Daten ohne Personenbezug reden kann. (siehe dazu 
auch https://www.akeuropa.eu/de/evaluation -der-datenschutz -grundverordnung -dsgvo )  
Die DSGVO enthält allgemeine Grundsätze, die die vielen Rechtskonflikte zwischen Geheimhaltungs - 
und V erwertungsinteressen nicht unmittelbar lösen können. Unzulässige Verarbeitungspraktiken aus-
zuforschen und rechtlich richtig zu würdigen, überfordert nicht nur KonsumentInnen, sondern aufwands-
bedingt zunehmend auch die Aufsichtsbehörden. Dies schadet der Re chtssicherheit und mindert das 
Vertrauen in die Vorteile von KI. Die Ausstattung der Behörden entspricht nicht dem Bedarf, um rasch, 
sorgfältig, technikkundig und investigativ den vielfältigen Aufsichtsaufgaben nachzukommen. Die Ver-
lagerung von einer ex -ante Prüfpflicht in sensiblen Fällen zu einer nachträglichen Aufarbeitung von 
Rechtsverletzungen samt Schadenersatzansprüchen eröffnet schwerwiegende Schutzlücken, wenn 
Rechtsdurchsetzung nicht rasch und reibungslos funktioniert.  
 
Verbot von Anwendungen, bei  denen die „Accountability“ an Grenzen stößt:  
Mit der Selbstlernfähigkeit der Systeme können Softwareentwickler oft selbst nicht mehr nachvollziehen, 
welchen logischen Weg Algorithmen einschlagen. Entscheidet KI aber selbst darüber, welche Daten sie 
für we lchen Zweck nutzt, widerspricht dies fundamental dem Rechtsgrundsatz der „Accountability“ (Zu-
rechnung, Verantwortung, Haftung), und kollidiert auch mit der Pflicht, im Erhebungszeitpunkt bereits 
den genauen Verwendungszweck der Daten anzugeben  
Alle Entsche idungen, Produkte und Dienste die auf Algorithmen basieren, müssen erklär - und überprüf-
bar bleiben. KonsumentInnen dürfen angesichts einer Vielzahl an Beteiligten (Entwickler, Hersteller, 
Anwender, Dienstleister) nicht zum Spielball unklarer Verantwortlich keiten werden. Sie sollen im Sinne 
einer Solidarhaftung Unterlassungs - und Schadenersatzansprüche gegen jeden Beteiligten in der Wert-
schöpfungskette richten können (mit anbieterseitigen Regressmöglichkeiten).  
 
Einbindung der Betroffenen :  
Daten - und Privatsphärenschutz sollten wirtschaftlichen Interessen grundsätzlich vorgehen. Wie verhält 
es sich aber, wenn Eingriffe in diese Rechte mit lebenswichtigen Interessen einzelner Personen, von 
Gruppen oder der Gesamtgesellschaft begründet werden?  Interessenskollisionen sind vorprogram-
miert, sobald KI -Anwendungen im Gesundheitssektor Verbesserung bei der Erkennung, Behandlung 
und Heilung von Krankheiten oder im sicherheitspolizeilichen Einsatz eine bessere Kriminalitätspräven-
tion bzw -aufklärung ve rsprechen. Der Preis für diesen (potentiellen) Fortschritt ist hoch: Interessen von 
großen Bevölkerungsteilen können damit gefährdet werden. Vor diesem Hintergrund braucht es für die 
Mehrzahl an KI -Anwendungen, die Grundrechte berühren, eine ex ante -Genehm igung durch ein unab-
hängiges Gremium. In dieses sind neben Datenschutzbehörden und Technikexperten auch VertreterIn-
nen der jeweils betroffenen Gruppen (ArbeitnehmerInnen, KonsumentInnen, PatientInnen, etc) mitein-
zubeziehen. Denn auch bei der Klärung von Re chtsfragen wird sorgfältig zwischen verschiedenen Inte-
ressen, Verhältnismäßigkeiten, Werten etc. abzuwägen sein. Diese Entscheidungen können abhängig 
von der jeweiligen Betroffenheit und dem jeweiligen weltanschaulichen Hintergrund sehr verschieden 
ausfall en. Die gesellschaftliche Akzeptanz von Entscheidungen für oder gegen einzelne KI -Anwendun-
gen und flankierende Auflagen fällt höher aus, wenn bei der Zusammensetzung des Entscheidungsgre-
miums auf eine breite Beteiligung aller betroffenen Gruppen geachtet w ird.   
 
Produkthaftungsregeln aktualisieren : 
Die Produkthaftungs -RL aus dem Jahr 1985 kennt für digitale Trends wie KI keine Antworten. Eine 
überarbeitete RL muss auf alle materiellen und nicht materiellen Sachen, digitale Dienstleistungen und 
digitalen In halte anwendbar sein und sollte deshalb auch Cybersicherheitsrisiken, mangelnde Software-
updates und unzureichende DSGVO -Konformität zu den „Defekten “ eines Produktes zählen. Ebenso 
Schäden, die durch die Fähigkeit selbst zu lernen und autonome Entscheidung en zu treffen oder durch 
einen Missbrauch der verwendeten Daten entstehen. Ausgezeichnete Detailvorschläge für die Überar-
beitung der Produkthaftungs -RL enthält das BEUC -Positionspapier „ Product Liability 2.0 “  
 </pre>",POSITIVE
pdfminer_2665616_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665616.pdf,3,3,2665616,attachments/2665616.pdf#page=3,Is the line order correct?,"<pre>14. In order to make these provisions effective, FNA calls for greater market vigilance by a dedicated service being empowered to taking 
emergency measures to restore competition. In 2010, the European Economic and Social Committee (EESC) recommended this continuous 
market surveillance to ensure consumer’s choice: “fair competition is the best way to promote  economic efficiency, consumer choice and 
safety in the car repair market. The actual choice should be analysed regularly. If the analysis concludes that there is market distortion, any 
corrective measures to be introduced should be assessed.7” 
15.  National  competition  authorities  do  need  more  complete  powers,  in  order  to  give  consumers  the  best  possible  protection  from 
anticompetitive behaviours, as stressed by Commissioner Margrethe VESTAGER’s statement.8 They need the financial and human resources 
to collect and go through the evidence. 
Federation of Craft Businesses in the automotive sector and in mobility services (FNA)                                                    August 2021 
7 Information Report of the European Economic and Social Committee (EESC) INT / 501 of 6 September 2010, paragraph 1.1 
 Collision damage claims management: how to ensure the consumer's freedom of choice and security? INT/501 – CESE 395/2010 fin EN/o-FR/NT/nm 
8 Statement 17/726 by Commissioner VESTAGER on Commission proposal to make national competition  authorities even more effective  enforcers for the 
benefit of jobs and growth, 22 March 2017 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
pdfminer_2662780_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2662780.pdf,4,1,2662780,attachments/2662780.pdf#page=1,Is the line order correct?,"<pre>Education International 
Internationale de l'Education 
Internacional de la Educación 
http://www.ei-ie.org 
EUROPEAN REGION- 
ETUCE 
President 
Larry FLANAGAN 
Vice-Presidents 
Odile CORDELIER  
Andreas KELLER 
Trudy KERPERIEN 
Dorte LANGE 
Galina MERKULOVA  
Branimir STRUKELJ  
Boulevard Bischoffsheim, 15 
1000 Brussels, Belgium 
Tel +32 2 224 06 91/92 
Fax +32 2 224 06 94 
secretariat@csee-etuce.org 
http://www.csee-etuce.org 
European Director 
Susan FLOCKEN 
Treasurer 
Joan DONEGAN 
ETUCE  
European Trade Union Committee for Education 
EI European Region  
ETUCE position on the EU Regulation on Artificial Intelligence 
(Adopted by the ETUCE Bureau on 7 June 2021) 
Background: 
On 21 April 2021, the European Commission published a proposal for a “Regulation on a 
European Approach for Artificial intelligence” (the AI Regulation). With this proposal, the 
European  Commission  follows  up  on  its  White  Paper  on  Artificial  Intelligence  (February 
2020), based on the results of a broad consultation process to which ETUCE contributed. 
The aim of the initiative is to establish the first EU legal framework regulating the entire 
lifecycle of the use of Artificial Intelligence (AI) in all sectors, including education.  
The AI Regulation classifies the use of Artificial Intelligence in various sectors based on the 
risk  that  the  AI  tools  have  on  the  health  and  safety  and  the  fundamental  rights  of 
individuals. Concerning education, the proposal considers the use of Artificial Intelligence 
tools in education as high-risk as potentially harmful to the right to education and training 
as  well  as  the  right  not  to  be  discriminated  in  education.  For  high-risk  sectors,  the  AI 
Regulation establishes stricter horizontal legal requirements to which AI tools must comply 
before being authorised on the market. These include risk management system during the 
entire lifecycle of the AI system.     
Following  the  publication  of  the  proposal,  on  26  April  2021,  the  European  Commission 
issued  a  public  consultation  that  will  run  until  20  July  2021,  accompanied  by  an  impact 
assessment report.  
The following text is the ETUCE response to the public consultation bringing the perspective 
of teachers, academics and other education personnel on the sections of the AI Regulation 
that touch upon the education sector.  
ETUCE reply: 
ETUCE  welcomes  the  publication  of  the  AI  Regulation  as  it  sets  the  ground  for  the  first 
comprehensive EU regulation on Artificial Intelligence to ensure a controlled development 
of AI tools in education and address the risks connected to their use by teachers, academic, 
other education personnel and students. While ETUCE recognises the potential of digital 
technologies and Artificial Intelligence tools to bring about improvements in education, it 
also  underlines  the  numerous  ethical  concerns  related  to  their  trustworthiness,  data 
privacy,  accountability,  transparency  and  their  impact  on  equality  and  inclusion  in 
education. ETUCE underlines that further research at national and European level is needed 
to assess and address the risks connected to the use of Artificial Intelligence in education 
with constant and meaningful consultation with education social partners.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
tika_2665590_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665590.pdf,1,1,2665590,attachments/2665590.pdf#page=1,Is the line order correct?,"<pre>Putting startups at the heart of AI innovation - CroAI’s opinion on the European
Commission’s Artificial Intelligence Act


In April 2021, the European Commission (EC) published its much-awaited Artificial Intelligence
Act (AIA), the first global attempt to establish a legal framework for a technology that, as the AIA
states, carries both benefits and risks to humans and society. The Croatian AI Association
(CroAI) welcomes the EC’s efforts to set its own approach to AI, as it previously did with privacy.
However, our main concern is that the AIA does not adequately address the needs of start-ups,
who are the main drivers of innovation.


CroAI believes that the AIA must be an enabler of AI innovation and strongly stand behind
startu-ps, especially during prototyping and testing while pursuing a product-market fit. We
therefore advocate for the AIA to include unequivocal support for innovators by mandating the
following measures:


1. Startups are allowed to create their own sandboxes on a case-by-case basis rather than
a one-size-fits all approach, due to the unique nature of each test.


2. Start-ups will follow a Code of conduct. The Code of conduct will help them mitigate risks
while testing through tools such as limiting the number of test users, human oversight,
purchasing insurance, transparency, and accountability.


3. While in their sandboxes, startups do not need to involve supervisory authorities. Still,
they are accountable for complying with the Code of conduct.


4. When leaving their sandboxes, which means that they have found a product-market fit,
startups need to invest in fully complying with AIA rules and regulations, which will make
much more sense at that time in a product’s development cycle.


The principal concern founders and investors have when thinking about doing AI in Europe is
the cost and unpredictability of complying with the AIA. They see it as an unnecessary risk and
a burden that they can easily avoid by moving a start-up to some other innovation hub in the
world. CroAI believes that by integrating these measures into the AIA, the EC will address most
of those concerns and make the EU an excellent choice for AI innovation.


CroAI is at the European Commission’s disposal to elaborate more on the reasoning behind this
proposal and how to get it to life.</pre>",POSITIVE
tika_2665616_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665616.pdf,3,3,2665616,attachments/2665616.pdf#page=3,Is the line order correct?,"<pre>14. In order to make these provisions effective, FNA calls for greater market vigilance by a dedicated service being empowered to taking 


emergency measures to restore competition. In 2010, the European Economic and Social Committee (EESC) recommended this continuous 


market surveillance to ensure consumer’s choice: “fair competition is the best way to promote economic efficiency, consumer choice and 


safety in the car repair market. The actual choice should be analysed regularly. If the analysis concludes that there is market distortion, any 


corrective measures to be introduced should be assessed.7” 


15. National competition authorities do need more complete powers, in order to give consumers the best possible protection from 


anticompetitive behaviours, as stressed by Commissioner Margrethe VESTAGER’s statement.8 They need the financial and human resources 


to collect and go through the evidence. 


Federation of Craft Businesses in the automotive sector and in mobility services (FNA)                                                    August 2021 


 
7 Information Report of the European Economic and Social Committee (EESC) INT / 501 of 6 September 2010, paragraph 1.1 
 Collision damage claims management: how to ensure the consumer's freedom of choice and security? INT/501 – CESE 395/2010 fin EN/o-FR/NT/nm 
8 Statement 17/726 by Commissioner VESTAGER on Commission proposal to make national competition authorities even more effective enforcers for the 
benefit of jobs and growth, 22 March 2017</pre>",POSITIVE
fitz_2662901_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662901.pdf,10,2,2662901,attachments/2662901.pdf#page=2,Is the line order correct?,"<pre> 
 
EUROPEAN ASSOCIATION OF CO-OPERATIVE BANKS 
The Co-operative Difference : Sustainability, Proximity, Governance 
 
2 
 
Introduction 
 
The European Association of Co-operative Banks (EACB) is happy to contribute to the discussion 
on the Artificial Intelligence (AI) legislative proposal. 
 
The EACB recognises that the AI proposal is the Commission’s first ever legal framework on the 
matter, which addresses the risks of AI and aims to position Europe to play a leading role globally. 
It should be recognised that this is a risky bet. If European values were not ultimately adopted 
on an international scale, European companies would be at a disadvantage compared to non-
European players active in less restrictive regulatory environments. 
 
We believe that the European Commission, the European Parliament and the Council should 
remain vigilant to ensure that European players are not unduly constrained in their prospect of 
developing innovative AI solutions compared to international competitors. 
 
We would like to highlight the following points: 
 
• 
The EACB welcomes the Commission’s risk-based approach as basis for a proportionate 
legal text. The Commission suggests a risk-pyramid approach: the higher the risk (for 
users) using AI system, the more additional measures. 
 
• 
We appreciate the technology-neutral and future-proof definition of AI, recognising that 
AI is a “fast evolving family of technologies” that is constantly developing. Nevertheless, 
combining the definition of artificial intelligence system together with the techniques and 
approaches of Annex I of the proposal, we observe that the scope of the Regulation is 
becoming quite wide as it also includes rule-based approaches. 
 
• 
We believe it is of paramount importance to make sure that the AI proposal will not add 
new and burdensome requirements for the banking sector and create conflicts and 
overlaps with existing rules: e.g., sector-specific regulation (CRD, CRR). 
 
• 
We particularly value the Commission’s human-centric perspective in designing AI rules: 
o 
The responsibility for an action or a decision still lies with a human being; 
o 
Actions and decisions of an AI system have to be traceable and understandable by 
humans using it; and 
o 
Actions and decisions of an AI system can always be changed/corrected by a human 
being (human oversight). 
 
• 
We understand that the Regulation’s intention is to protect the safety and fundamental 
rights of EU citizens, thus that the requirements for high-risk AI systems are only targeted 
at AI applications that could possibly pose risks to natural persons. 
 
• 
Generally, some provisions of the Regulation contain somewhat vague wording, e.g., the 
definitions provided for “remote biometric identification system” and “user”. Moreover, we 
believe that the definition of ‘developer’ and ‘end user’ are missing from the legal text. 
These points should be further clarified in order to guarantee legal certainty for providers, 
developers and users of AI systems. 
</pre>",POSITIVE
pdfminer_2665205_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665205.pdf,4,3,2665205,attachments/2665205.pdf#page=3,Is the line order correct?,"<pre>preserving the professional and pedagogical autonomy and academic freedom of teachers 
and academics. 
Transparency and AI literacy and CPD of teachers on AI: 
ETUCE welcomes that the proposal of AI Regulation requires that users of AI tools (who 
include students, teachers, academic and education staff for the education sector) must be 
adequately  informed  about  the  intended  purpose,  level  of  accuracy,  residual  risks  of  AI 
tools. Nevertheless, ETUCE highlights that providing information is not sufficient to ensure 
the transparency of the AI tools when users miss the adequate digital skills and data and AI 
literacy to interpret it. Therefore, it is of utmost importance to improve the importance of 
digital skills, AI literacy and data literacy in educational curricula and raise awareness on 
the  risks  related  to  the  use  of  AI  tools  in  education.  It  is  also  essential  to  ensure  that 
infrastructures of education institutions are adequately equipped for digital education as 
well as to provide  equal access to digital  technologies and ICT  tools to all teachers and 
students, with particular attention to the most disadvantaged groups. To these purposes, 
sustainable  public  investment  should  be  provided  by  national  governments  and  the 
European Commission should provide financial support through European funding such as 
Horizon Europe, Digital Europe and in the framework of National Recovery and Resilience 
Facility. 
While the AI Regulation blandly mention to the possibility of providing users with training 
on Artificial Intelligence, ETUCE emphasises that it is crucial that sustainable public funding 
are provided at national and European level to ensure that teachers, trainers, academics 
and other education personnel receive up-to-date and free of charge continuous training 
and professional development on the use of AI tools in accordance with their professional 
needs.  
EdTech expansion and issues of intellectual property rights, data privacy of teachers:   
ETUCE points out that the development of the use of Artificial Intelligence in education has 
been  accompanied  by  the  expansion  of  Ed-tech  companies  that  are  progressively 
increasing  their  influence  in  the  education  sector,  especially  under  the  pressure  of 
emergency  online  teaching  and  learning  during  the COVID-19  pandemic.  ETUCE  reminds 
that education is a human right and public good whose value needs to be protected. ETUCE 
calls for further public responsibility from national governments that should not limit their 
scope to regulating the EdTech sector and should develop and implement public platforms 
for online teaching and learning to protect the public value of education. In addition, public 
platforms should be implemented in full respect of professional autonomy of teachers and 
education personnel as well as academic freedom and autonomy of education institutions, 
without  creating  pressure on teachers and education personnel  regarding the education 
material and pedagogical methods they use. It is also essential to protect the accountability 
and  transparency  in  the  governance  of  public  education  systems  from  the  influence  of 
private and commercial interests and actors.  
3 
 
 
 
 
</pre>",POSITIVE
PyPDF2_2665452_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665452.pdf,4,2,2665452,attachments/2665452.pdf#page=2,Is the line order correct?,"<pre> 
    Press release  
Date   12 October 2020  
Subject   BMW Group code of ethics for artificial intelligence.  
Page   2 
 
  
 
Corporate Communications  
Seven principles covering the development and application of artificial 
intelligence at the BMW Group:  
• Human agency and oversight.  
The BMW Group implements appropriate human monitoring of decisions made by 
AI applications and considers possible ways that humans can overrule algorithmic 
decisions.  
• T echnical robustness and safety.  
The BMW Group aims to develop robust AI applications and observes the applicable 
safety standards d esigned to decrease the risk of unintended consequences and 
errors.  
• Privacy and data governance.   
The BMW Group extends its state -of-the-art data privacy and data security 
measures to cover storage and processing in AI applications.  
• Transparency.   
The BMW Gr oup aims for explainability of AI applications and open communication 
where respective technologies are used.  
• Diversity, non -discrimination and fairness.  
The BMW Group respects human dignity and therefore sets out to build fair AI 
applications. This includ es preventing non -compliance by AI applications.  
• Environmental and societal well -being.  
The BMW Group is committed to developing and using AI applications that promote 
the well -being of customers, employees and partners. This aligns with the BMW 
Group’s go als in the areas of human rights and sustainability, which includes climate 
change and environmental protection.  </pre>",POSITIVE
tika_2662603_1,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662603.pdf,9,1,2662603,attachments/2662603.pdf#page=1,Is the line order correct?,"<pre>An Assessment of the AI Regulation Proposed by
the European Commission


Patrick Glauner


Abstract In April 2021, the European Commission published a proposed regulation
on AI. It intends to create a uniform legal framework for AI within the European
Union (EU). In this chapter, we analyze and assess the proposal. We show that the
proposed regulation is actually not needed due to existing regulations. We also argue
that the proposal clearly poses the risk of overregulation. As a consequence, this
would make the use or development of AI applications in safety-critical application
areas, such as in healthcare, almost impossible in the EU. This would also likely
further strengthen Chinese and US corporations in their technology leadership. Our
assessment is based on the oral evidence we gave in May 2021 to the joint session
of the European Union affairs committees of the German federal parliament and the
French National Assembly.


1 Introduction


Artificial intelligence (AI) aims to automate human decision-making behavior and
is therefore also considered the next phase of the industrial revolution. We have
previously reviewed the state of the art and challenges ofAI applications in healthcare
(Glauner, 2021a). This book provides a forecast of howAI and other technologies are
likelty to skyrocket healthcare in the foreseeable future. The European Commission
published proposed regulation in April 2021 (European Commission, 2021) that
intends to create a uniform legal framework for AI within the European Union (EU).
The proposal particularly addresses safety-critical applications, a category that also


Patrick Glauner
Deggendorf Institute of Technology, Deggendorf, Germany e-mail: patrick@glauner.info


Preprint. To appear in the 2022 Springer book “The Future Circle of Healthcare: AI, 3D
Printing, Longevity, Ethics, and Uncertainty Mitigation"" edited by Sepehr Ehsani, Patrick Glauner,
Philipp Plugmann and Florian M. Thieringer.


1</pre>",POSITIVE
tika_2663391_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663391.pdf,2,1,2663391,attachments/2663391.pdf#page=1,Is the line order correct?,"<pre>Velázquez, 64-66, 2ª planta - 28001 MADRID  
www.asnef.com  -  asnef@asnef.com 


CIF: G28516003 


Página 1 





Madrid, 30 de julio de 2021 





La Asociación Nacional de Establecimientos Financieros de Crédito (ASNEF), 


representada por su Secretario General, D. Ignacio Pla Vidal, y debidamente inscrita en el 


Registro de Transparencia de la Unión Europea con el nº 11218815591-29, presenta las 


siguientes observaciones dentro del plazo de consulta pública abierta en relación con: 





Propuesta De Reglamento Del Parlamento Europeo Y Del Consejo Para El Establecimiento De 


Reglas Armonizadas Sobre Inteligencia Artificial (Ley De Inteligencia Artificial) 


[COM(2021)206] 








1. En primer lugar, con relación al considerando 37, la propuesta de Reglamento menciona 


lo siguiente: 


“Habida cuenta del alcance sumamente limitado de su impacto y de las escasas 


alternativas disponibles en el mercado, conviene dejar exentos a los sistemas de IA 


destinados a evaluar la solvencia y la calificación crediticia cuando los pongan en servicio 


proveedores a pequeña escala para su propio uso.”  


Consideramos de suma importancia que se especifique el significado de ""proveedores a 


pequeña escala” y “para su propio uso” en el contexto de esta excepción a la consideración 


como riesgo alto y a la aplicación de los consiguientes requisitos y obligaciones. 


2. En segundo lugar, rogamos que se especifique cómo deberán cumplir aquellos sistemas 


ya en producción que, según esta regulación, son de alto riesgo. 


3. Por último, solicitamos que se incluya un amplio periodo transitorio para su 


aplicabilidad, al objeto de que las entidades que ya están utilizando Inteligencia artificial para la 


evaluación de la solvencia y credit score puedan adaptarse a los nuevos requisitos. 








 



http://www.asnef.com/

mailto:asnef@asnef.com</pre>",NEUTRAL
pdfminer_2665600_2,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665600.pdf,6,2,2665600,attachments/2665600.pdf#page=2,Is the line order correct?,"<pre>1. 
Introduction 
The technical complexity, pervasive penetration and multifaceted social implications of the use of artificial 
intelligence (“AI”) in every-day life of EU citizens  unequivocally necessitate the introduction of clear and 
sensible rules for development, deployment and use of AI. The endeavors in this respect of the European 
Union institutions, and of the European Commission in particular, are laudable. The strive to shape those 
rules  in  a  way  that  promotes  innovation  and  the  uptake  of  AI  in  the  European  Union  (the  “EU”)  is  also 
praiseworthy.  
However, any future regulation of AI should also be introspective, smart and fair. Introspective in the sense 
that  it  should  learn  from  and  avoid  deficiencies  and  past  mistakes  in  other  relevant  areas  of  business 
regulation  (e.g.  data  protection,  antitrust).  Smart  in  the  sense  that  it  should  have  built  in  already  now  a 
conceptual design and a toolkit that would allow addressing issues arising from the advance in AI (e.g. the 
emergence of strong and general AI). Fair in the sense that a future regulation of AI should – while creating 
a favourable business environment – also account and cater for the interests of society at large, including 
end users and citizens more generally. 
This position paper focuses on analysis of the proposal for a regulation of the European Parliament and the 
Council  laying  down  harmonized  rules  on  artificial  intelligence  (Artificial  Intelligence  Act)  (the  “Draft  AI 
Regulation” or the “Draft”) and recommendations  on the conceptual design of and certain fundamental 
rules under the Draft with view to the need of an introspective, smart and fair legal framework for design, 
development, deployment and use of AI in the EU.  
2.  Risk-based approach and ethical/human rights backbone 
The  Draft  AI  Regulation  retains  the  risk-based  approach  and  the  ethical/human  rights  backbone  initially 
contemplated  in  the  European  Commission’s  White  Paper  on  AI1  and  the  recommendations  of  the 
European Parliament of October 2020,2 while building upon regulation in other areas (e.g. data protection, 
consumer  protection,  standardization)  to  address  the  more  specific  technical  aspects  of  AI  design, 
development,  deployment  and  use.  This  approach  generally  makes  sense  given  how  contextually 
dependent  the  deployment  and  use  of  AI  are  and  how  quickly  the  technology  evolves.  Too  legalistic 
requirements might result in under- or overregulation.  
However, clearer emphasis needs to be put in the final text of the Artificial Intelligence Act on outcomes, 
rather than on formal course of action. Art. 13 to Art. 15 of the Draft already require attainment of certain 
overall outcomes (transparency, accuracy, etc.). Yet, other obligations under Chapters 2 and 3 of Title III 
presuppose mere process-like actions (e.g. documenting, record-keeping, risk management based on step-
by-step actions) in order to demonstrate compliance. If the emphasis in this framework does not firmly and 
eloquently  lie  with  ultimate  outcomes  –  and  that  various  processes  are  one  of  the  means  to  that  end  – 
compliance  would  morph  into  a  “box-ticking”-  and  “window-dressing”-type  of  adherence  to  the  formal 
requirements under the Artificial Intelligence Act, as is the case with similar rather “technical” requirements 
under the European Union Genera Data Protection Regulation.3 
3.  Prohibited use of AI 
The explicit bans under Art. 5 of the Draft AI Regulation are a welcome development in an attempt to limit 
the use of AI to manipulative and/or privacy-invasive ends.  
However, the proposed language and approach have some deficiencies, as set out below.  
3.1  AI employing subliminal techniques or exploiting vulnerabilities 
Page 1 of 5 
 
 
 
</pre>",POSITIVE
PyPDF2_2665627_6,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665627.pdf,11,6,2665627,attachments/2665627.pdf#page=6,Is the line order correct?,"<pre>From our perspective we wonder if there is evidence that regul atory sandboxes are an 
effective strategy for facilitating innovation. We would like to see a more elaborate report on 
the consequences of the regulation in terms of its effect on innovation, specifically in relation 
to regulatory sandboxes and SMEs.  
 
Another way forward would be for an SME to partner with a larger actor with the ability to 
take the full administrative work onboard. The deal could have an impact on IP rights and 
revenue streams which in turn could mean that the regulation serves to protect  the established 
actors on the market. The benefit for SMEs would be that they can focus on their innovative 
ideas and make the administration of the AI system a question of commercial contracts 
instead of a negotiation with national authorities. The latte r demands another set of 
competences that not all SMEs have made a priority to recruit.  
 
The proposed regulation mentions another possibility in Article 17.2 that states that the 
quality management system ensuring compliance with the regulation should be i n relation to 
the size of the organization. This could be used by SMEs instead of regulatory sandboxes or 
difficult negotiations with large and established actors. But it could also be used by large 
organisations that want to reduce their administration by  relating the quality management 
system to the size of the unit developing the AI system, not the whole organization.  
 
Regulatory sandboxes is a topic we will return to from the perspective of public governance 
further down in our response.  
 
Responsibiliti es in a system -of-systems  
In this section we will focus on AI systems and their development as systems -of-systems, but 
also on AI systems  as component s in other systems  and what effects the proposed regulation 
can have in terms of responsibilities among an d between actors.  
 
System boundaries  
A challenge for actors with the proposed regulation is that artefacts and systems developed 
for other purposes can be used for future development of high -risk AI systems and therefore 
covered by the regulation (Annex IV .2). It could be services and platforms providing data 
regarding road conditions [5] or the placement of wastewater  wells [6] (depending on the 
interpretation of the definition in Annex I they might be high -risk AI systems or mere data 
sources). If the sup plied data is used for developing AI systems which can be used for 
managing vital infrastructure like roads and water supply (Annex III) they will be part of the 
technology chain behind the AI system and need to be administered as such.   
 
The same goes fo r developers of models, such as digital twins, if they are used for developing 
AI systems that are classified as high -risk systems. Providers of data regarding populations 
face the same uncertainty if the data covers for instance taxable income, number of residents 
at a specific address or fluctuation of property prices since they can be used to determine 
strategies or decisions for social benefits or targeted interventions against social exclusion 
(Annex III.5 and III.8). In the l ong run all providers of digital artefacts face the probability 
that their service, data or technology will be used for developing high -risk AI and therefore 
need to have the right documentation to conform with the regulation and avoid fines . 
 
At the same time there are political initiatives that promote public authorities and actors to 
facilitate data sharing  as well as a need for digital simulations and models for societal and 
business planning. These artefacts could be high -risk AI systems in themselves or become </pre>",POSITIVE
PyPDF2_2665488_5,company,../24212003_requirements_for_artificial_intelligence/attachments/2665488.pdf,5,5,2665488,attachments/2665488.pdf#page=5,Is the line order correct?,"<pre> 
 
5 / 5 Bei Rückfragen oder Anmerkungen stehen Ihnen zur Verfügung:  
 
Simon Kessel  
Referent für Digitales und Mobilität  
VKU -Europabüro  
 
Telefon: +32 2 740 16 -55 
E-Mail: Kessel@vku.de   </pre>",POSITIVE
PyPDF2_2665648_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665648.pdf,8,1,2665648,attachments/2665648.pdf#page=1,Is the line order correct?,<pre>C H A I  P o s i t i o n  P a p e r  o n  t h e  E U  A r t i ,NEGATIVE
fitz_2663366_9,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663366.pdf,13,9,2663366,attachments/2663366.pdf#page=9,Is the line order correct?,"<pre> 
 
 
29 July 2021 
 
 
 
9 
set nutrition labels,” that can potentially provide a snapshot of how a system was developed and how 
it performs. These tools, however, are still being developed. There are no clear standards for such 
documentation efforts, nor have they yet been demonstrated to be practical at a meaningful scale.  
 
7. 
Conformity assessments  
 
 Self-assessments – CIPL believes that relying on organisations’ self-assessments for most high-risk AI 
systems strikes a good balance between innovation and the preservation of fundamental rights. 
Requiring prior approval for all high-risk AI systems would be harmful to innovation (especially as the 
Commission notes that expertise for AI auditing is only now being accumulated). Many organisations 
have already established self-assessment processes in the context of GDPR and will be able to leverage 
them for high-risk AI assessment purposes.  
 No double reporting – CIPL underlines that it would be overly burdensome for all products that require 
conformity assessments under legislation listed in Annex II to also be subject to additional AI-specific 
requirements set out in the AI Act. Some video conferencing products’ endpoints, for example, require 
conformity assessments under the Radio Equipment Directive listed in Annex II of the AI Act. The use 
of AI in such systems is intended to enhance collaboration between employees and is therefore 
unlikely to be high-risk. Although the infrastructure with conformity assessment bodies is well-
established and efficient, it will be essential to make sure that there is no double reporting requirement 
and that these bodies are properly equipped to deal with this new role.  
8. 
Regulatory Sandboxes 
 
 More clarity – CIPL welcomes the inclusion of a legal basis for regulatory sandboxes in the AI Act. The 
relevant provisions should be underpinned by clear and unambiguous rules for those making use of 
sandboxes, including sufficient guidance to regulators about their operation, and the need for 
consistency in approaches. In particular, the provisions should address how insights and learnings 
obtained from the regulatory sandbox exercises can inform the policy making process of the AI Act 
(including modifications to the Annexes), as well as its enforcement.  
 
 Incentives - The AI Act needs to more clearly lay out the incentives for organisations to join sandboxes 
and the outcomes they can expect.8 CIPL encourages the Commission and the relevant regulators to 
work with industry to: (1) think about the specific functioning of sandboxes; (2) set them up in a way 
that truly helps companies to drive innovation in a protected environment to unearth learnings for all 
stakeholders involved; and (3) enable sandboxes to reach beyond SMEs and be made more inclusive. 9 
                                                 
8 See CIPL Paper Regulatory Sandboxes in Data Protection – Constructive Engagement and Innovative Regulation in 
Practice. 
9 The AI Act currently prioritises regulatory sandboxes to small-scale providers and start-ups. The possible impact on 
a level playing field needs to be assessed as sandboxes can only take in a number of applications at any given time, 
and this is likely to be far smaller a number than the amount of AI innovations being developed in the market place.  
</pre>",POSITIVE
tika_2665502_8,other,../24212003_requirements_for_artificial_intelligence/attachments/2665502.pdf,10,8,2665502,attachments/2665502.pdf#page=8,Is the line order correct?,"<pre>8 


authorities and – due to unequal investments in the various member states’ authorities – citizen 
protection is not at the same level in each EU country.  


Also in the context of this regulation, it should be considered that too much emphasis on the 
national level can lead to a risk of unequal implementation in different member states, at different 
speeds and potentially different interpretations. Belgium has, for instance, been lagging behind 
with regard to the implementation of the GDPR; this delay may affect innovation and a European 
level playing field, and the same risks to happen in the field of AI. Strong coordination at the 
European level will hence be crucial; also given the fact that many AI systems may be used 
transnationally and may be imported from third party countries. Moreover, given the importance 
of the risks attached to the use of AI as set out in this regulation, it will be essential that these 
authorities receive proper funding (and a sufficiently skilled workforce – which may be difficult in 
this field) so that they can provide adequate guidance for organizations and ensure a high level 
of citizen protection. 


The issue of different implementation speeds will also affect the creation of codes of conduct that 
can be voluntarily applied to AI systems other than high-risk systems. If it is assumed that the 
creation of a code of conduct is roughly the same effort and cost for any sector or member state, 
this absolute cost will mean that there may be more codes of conducts for sectors and member 
states with a higher turnover. Smaller member states with smaller markets will thus likely have 
less means to create these codes of conduct. This is another reason why a common European 
approach would be preferential. 


In this regard, the obligation to appoint an authorized representative established in the European 
Union in case an importer cannot be identified (recital 56 and article 25) is welcomed. Building on 
the experience with the GDPR, it is crucial to allow all organizations in charge of the 
implementation and enforcement of the regulation – be it the national competent authorities 
(NCAs), market surveillance authorities or other bodies – to be able to conduct all necessary steps 
towards the authorized representative, independently of where in the European Union it is 
established. Related to this point, it is essential that several NCAs can oversee the notified bodies 
and technical services performing the conformity assessment. In other words, not only the NCA 
of the country in which the notified bodies and technical services are established, but also NCA 
from other European member states should be able to do so, especially to ensure protection in 
case a specific NCA would be too under-resourced. 


Furthermore, while currently not foreseen in the proposed regulation, citizens should be provided 
with measures for redress and a right to file a complaint with national authorities, since this will 
not only help closing the protection gap of the proposal, but it can also help national authorities to 
assess and establish potential breaches of the regulation. In this way, public and private 
enforcement can be more complementary, and citizens will have a more active role in ensuring 
the protection of their rights. In the same line of thought, the link between the GDPR and this 
regulation should be highlighted. 


More than the fact that the Proposal does not, currently, foresee any mechanisms through which 
citizens can file a complaint, it seems to ignore the rights of citizens altogether. Despite the fact 
that the Recitals make numerous references to protecting “health, safety and fundamental rights,” 
the conceptual structure of the proposal is built on existing market surveillance schemes known 
from product safety legislation. The proposal seems to combine two concepts that are</pre>",POSITIVE
pdfminer_2665432_6,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665432.pdf,7,6,2665432,attachments/2665432.pdf#page=6,Is the line order correct?,"<pre>Furthermore,  the  proposal  does  not  include  the  possibility  of  harmed  consumers  to  be 
represented by an NGO, including consumer organisations, in the exercise of their rights. 
An  article  similar  to  Art  80  GDPR  (Representation  of  data  subjects)  or  Article  68  of  the 
proposal for a Digital Services Act (Representation) should be introduced. In the context 
of the AI Act, this representation shall not be subject to a mandate. 
Also, to enable collective redress actions, this proposal should be included in the Annex of 
the Representative Actions Directive26. A similar provision was included in the Commission’s 
proposal for a Digital Services Act.27 
6. The  governance  and  enforcement  structure  must  be  clear  and  ensure 
an  effective  and  consistent  application  of  the  rules  at  European  and 
national level 
The  proposed  governance  and  enforcement  structure  mainly  rests  at  national  level  and 
raises issues in relation to the obligations, competences and powers of the different actors 
involved28 and the different processes envisaged.  
For example:  
-  The  need  to  ensure  a  coherent  and  coordinated  EU-wide  enforcement.  While  the 
Commission plays a central role if a national supervisory authority notifies its intention 
to adopt measures against an AI system in its territory29, the Commission has no powers 
to proactively take the lead in case of inaction by national authorities. The autonomy 
and powers of the European AI Board30, comprised of high-level representatives of the 
national  supervisory  authorities  and  chaired  by  the  Commission,  also  seem  quite 
limited.  
-  There is a need to clarify potential overlaps with existing bodies such as the European 
Data Protection Board, as well as the role of Data Protection Authorities – a view that 
other stakeholders also share31. 
- 
It is also important to ensure a common approach when it comes to the cooperation 
between  the  different  competent  authorities  and  supervisory  authorities  at  national 
level, to ensure effective and swift enforcement as well as to avoid different approaches 
by Member States. 
-  Given  the  lack  of  redress  mechanisms  for  consumers  in  the  proposal,  there  is  no 
authority that is entrusted with dealing with consumer complaints in case of breaches 
of the regulation. 
26 Directive (EU) 2020/1828 on representative actions for the protection of the collective interests of consumers 
and repealing Directive 2009/22/EC; 
27 See Article 72 of the Proposal for a Regulation on a Single Market For Digital Services (Digital Services Act) 
and amending Directive 2000/31/EC;  
28  Namely:  the  European  Artificial  Intelligence  Board  (EAIB),  the  national  competent  authorities,  the  national 
supervisory  authority,  the  Commission,  the  European  Data  Protection  Supervisor  (EDPS)  and  the  AI  system 
providers); 
29 See Articles 65 (5) and 66 of the proposal; 
30 See Articles 56 – 58 of the proposal; 
31  See,  for  example,  Access  Now:  https://www.accessnow.org/eu-minimal-steps-to-regulate-harmful-ai-
systems/. 
6 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
tika_2665488_5,company,../24212003_requirements_for_artificial_intelligence/attachments/2665488.pdf,5,5,2665488,attachments/2665488.pdf#page=5,Is the line order correct?,"<pre>5 / 5 


Bei Rückfragen oder Anmerkungen stehen Ihnen zur Verfügung: 





Simon Kessel  


Referent für Digitales und Mobilität 


VKU-Europabüro 





Telefon: +32 2 740 16-55 
E-Mail: Kessel@vku.de  



mailto:Kessel@vku.de</pre>",NEUTRAL
fitz_2665170_5,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665170.pdf,12,5,2665170,attachments/2665170.pdf#page=5,Is the line order correct?,"<pre> 
 
 
Hub France IA – Groupe de Travail Banques et Auditabilité 
 
p 5/ 12  
Il serait nécessaire d’élaborer une analyse d’impact sur les coûts (et la perte de bénéfice des secteurs « à haut 
risque ») plus complète que celle indiquée au §3.3. 
Recommandation 3 : dans les domaines à haut risque, préciser deux sous-catégories de système IA à haut risque 
permettant de différencier deux niveaux de mise en œuvre de la conformité. Préciser les impacts économiques 
complets des secteurs à haut risque concernés. 
Par ailleurs, la définition en extension du périmètre de tels systèmes pourrait faire apparaître un risque juridique 
si l’usage visé n’apparait pas dans la liste au moment de la mise sur le marché, mais que celle-ci est ensuite 
modifiée. L’Article 7 (Modifications de l’annexe III) semble indiquer que la liste des domaines de l’annexe III 
(points 1 à 8) ne peut pas être modifiée : 
« La Commission est habilitée à adopter des actes délégués conformément à l’article 73 afin de mettre 
à jour la liste figurant à l’annexe III en y ajoutant des systèmes d’IA à haut risque lorsque les deux 
conditions suivantes sont remplies: 
(a) 
les systèmes d’IA sont destinés à être utilisés dans l’un des domaines énumérés à l’annexe III, 
points 1 à 8; 
(b) 
les systèmes d’IA présentent un risque de préjudice pour la santé et la sécurité, ou un risque 
d’incidence négative sur les droits fondamentaux, qui, eu égard à sa gravité et à sa probabilité 
d’occurrence, est équivalent ou supérieur au risque de préjudice ou d’incidence négative que présentent 
les systèmes d’IA à haut risque déjà visés à l’annexe III » 
Un usage ne figurant pas dans la liste des « domaines énumérés à l’annexe III » ne peut donc pas y être intégré 
ensuite. Cependant s’il y figure et que la liste de l’Annexe III est donc modifiée, comment les acteurs seront-ils 
notifiés ? la mise en conformité sera-t-elle être rétroactive ? dans quels délais ? 
De même, si nous restons dans la perspective d’un établissement des risques ex ante fondé sur des cas d’usage, 
quels sont les processus qui seront mis en place afin de réévaluer au fil du temps le niveau de risque des cas 
d’usage ? Le recours aux « actes délégués » est très imprécis et leur périmètre devrait être très clairement 
encadré. 
Recommandation 4 : préciser les règles de modification de l’Annexe III pour les systèmes IA à haut risque par le 
biais d’actes délégués. 
PROBLEMES CONCRETS POUR L’IMPLEMENTATION DE LA MISE EN CONFORMITE 
Nous avons identifié de nombreux cas où l’implémentation pratique de la mise en conformité semble 
extrêmement coûteuse, voire impossible. Notre crainte repose principalement sur la mise en place d’une 
réglementation qui sera vue comme un frein à l’innovation permise par l’IA, compte tenu de ces contraintes de 
mise en conformité. Par ailleurs, nous ne voyons pas comment il serait possible d’appliquer la mise en conformité 
sur certains systèmes déjà en production mais pour lesquels les exigences requises ne peuvent être 
rétroactivement mises en œuvre. Pour chaque contrainte de mise en conformité devrait être conduite une 
analyse d’impact sur l’innovation. 
Nous listons ici quelques cas qui nous apparaissent critiques.  
MISES A JOUR FREQUENTES 
 
Une fois qu’une solution comprenant un composant d’apprentissage a été mise sur le marché, elle doit être 
monitorée pour contrôler ses performances. Habituellement, les données dérivent peu à peu et leur distribution 
</pre>",POSITIVE
PyPDF2_2662901_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662901.pdf,10,2,2662901,attachments/2662901.pdf#page=2,Is the line order correct?,"<pre>  
EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS  
The Co -operative Difference :  Sustainability, Pro ximity , Governance  
 
2 
 
Introduction  
 
The European Association of Co -operative Banks (EACB) is happy to contribute to the discussion 
on the Artificial Intelligence (AI) legislative proposal.  
 
The EACB recognises  that the AI proposal is the Commission’s first ever legal framework on the 
matter, which addresses the risks of AI and aims to position Europe to play a leading role globally.  
It should be recogni sed that this is a risky bet. If European values were not u ltimately adopted 
on an international scale, European companies would be at a disadvantage compared to non-
European players active in less restrictive regulatory environments . 
 
We believe that the European Commission, the European Parliament and the Counci l should 
remain vigilant to ensure that European players are not unduly  constrained in their prospect of 
developing innovative AI solutions compared to international competitors . 
 
We would like to highlight the following points:  
 
• The EACB welcome s the Commission’s risk -based approach as basis for  a proportionate 
legal text.  The Commission suggests a risk -pyramid  approach : the higher the risk (for 
users) using AI system, the more additional measures.  
 
• We appreciate  the technology -neutral and future -proof definition of AI, recognising that 
AI is a “fast evolving family of technologies” that is  constantly developing . Nevertheless, 
combining the definition of artificial intelligence system together with the techniques and 
approaches of Annex I of the proposal, we observe that the scope of the Regulation is 
becoming quite wide as it also includes rule -based approaches.  
 
• We believe it is of paramount importance to make sure that the AI proposal will not add 
new and burdensome requirements for the banking sector and create conflicts and 
overlaps with existing rules: e.g., sector -specific regulation (CRD, CRR).  
 
• We particularly value the Commission’s human -centric perspective in designing AI rules : 
o The responsibility for an action or a decision still lies with a human bein g; 
o Actions and decisions of an AI system have to be traceable and understandable by 
humans using it ; and  
o Actions and decisions of an AI  system  can always be changed/corrected by a human 
being  (human oversight ). 
 
• We understand that the Regulation’s intention is to protect the safety and fundamental 
rights of EU citizens, thus that the requirements for high -risk AI systems are only targeted 
at AI applications that could possibly pose risks to natural persons.  
 
• Generally, some provisions of the Regulation contain somewhat vague wording, e.g. , the 
definition s provided for “remote biometric identification system”  and “user” . Moreover, we 
believe that the definition of ‘developer’ and ‘end user’ are missing from the legal text.  
These points should be further clarified in order to guarantee legal certainty for providers , 
developers  and users of AI systems.  </pre>",POSITIVE
fitz_2665314_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2665314.pdf,6,2,2665314,attachments/2665314.pdf#page=2,Is the line order correct?,"<pre> 
 
 
Side 2 
 
 
We agree that under such a risk-based approach, most applications should fall outside of 
the scope of the regulation. 
4 PROHIBITED AI PRACTICES  
The Norwegian government supports the Commission's proposal for AI practices that 
shall be prohibited. However, we have a concern regarding the Article 5 – 1 (a) stating 
that an AI system that deploys subliminal techniques is prohibited if it is likely to cause 
physical or psychological harm. In our opinion, such manipulative practices are often 
used in cases that cause financial harm to people, which may in turn lead to 
psychological harm. We would propose that the prohibition of AI systems that deploy 
subliminal techniques is expanded to also include financial harm.  
 
Regarding Article 5 – 1 (c) the Norwegian Government supports the suggestion of 
prohibiting AI systems for evaluation or classification of the worthiness of natural persons 
where such use of AI-systems may lead to unjustified or disproportionate treatment of 
individuals. However, it should be carefully considered whether the terms 'unjustified or 
disproportionate' will provide sufficient safeguards against unwanted use of this type of 
AI-systems. The Norwegian government is concerned that this, in some cases, could 
allow for exclusion of individuals from the use of certain fundamental services that should 
be available to all, for instance health and care services.  
5 HIGH RISK AI SYSTEMS 
The Norwegian Government supports the overall approach to high risk AI systems in the 
Commission's proposal. We support the proposal for an EU database for high-risk AI 
systems. 
Determining borderline cases 
The term 'high risk system' is defined in article 6 with reference to lists in Annexes II and 
III. The Commission will be empowered to adopt delegated acts to update these lists. 
Deciding whether a system qualifies as a high-risk AI-system may require difficult 
considerations. It is our view that it may be necessary to adopt a procedure to determine 
borderline cases. We propose that such a procedure could be modelled after Article 4 in 
Regulation 2017/745 on medical devices. In addition, in order to ensure the best possible 
end user protection, we propose to include in the regulation that: 'In cases of doubt, 
where, taking into account all its characteristics, an ""AI system"" may be considered a 
""high-risk AI system"" the provisions of Title III, Chapter 2 of the Regulation apply'. Such 
provisions have been used in other EU regulations.  
Autonomous shipping 
Norway is at the forefront in the development of autonomous vessels and hosts a number 
of test sites. We recognise that safety systems for the navigation of such vessels are 
included in 'High-risk AI systems' through the inclusion of Directive 2014/90/EU in the list 
in Annex II. However, we see a growing activity around AI based systems intended for 
decision support to increase safety in harbours and fairways. This includes AI systems for 
</pre>",POSITIVE
tika_2663341_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663341.pdf,4,1,2663341,attachments/2663341.pdf#page=1,Is the line order correct?,"<pre>About the GFII: created in 1979, the GFII, the French organization of information professionals, is 
a unique association in the data landscape that brings together private and public data producers 
and re-users, such as the French Ministry of Interior, INPI, IGN, Total, BNP Paribas, Roquette frères, 
Saint-Gobain, Wolters-Kluwer, Elsevier, Françis Lefebvre-Dalloz group, Altarès. It gathers lawyers, 
engineers, data experts, compliance officers... 
 
The GFII aims to promote the economy of data, that means the recognition of the costs necessary 
for their manufacture, maintenance, development and dissemination in an assumed commercial 
environment, which does not exclude free of charge data sharing, but which puts more emphasis 
on the interoperability of data, their valorisation and their reusability. The 6 working groups 
produce positions papers and white papers in order to help shaping the future of data policy in 
France and in the EU by offering a balanced and economically sustainable point of view about data 
and IA. The GFII promotes a sustainable and ethical use of data and works closely with its members 
for offering the most expert and efficient feedback about the implementation of data policies. GFII 
members may be AI systems providers, users or both. 





REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING 


DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL 


INTELLIGENCE ACT) 


First of all, thanks to the European Commission to enable the GFII to answer this consultation on the 


draft regulation on AI. 


 1) Readability of the text 


The draft is rather complex to understand; understanding difficulties may then generate difficulties for 


being compliant, especially for SMEs and start up.  


 a) We invite then the legislator: 


- to amend the structure of the document by separating: 


- requirements dedicated to AI systems defined in the article 6 (1) / annex II and possibly 
Annex III (1) 
- requirements dedicated to AI systems defined in the article 6 (2) 
- requirements dedicated to AI systems defined in the annex III (6 to 8) 


- to define the requirements from the AI provider point of view, ie the operator that will have to comply 


with the future regulation, with a logical ie process oriented redactional architecture, step by step,</pre>",POSITIVE
fitz_2660610_7,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2660610.pdf,8,7,2660610,attachments/2660610.pdf#page=7,Is the line order correct?,"<pre>3.2.2
Art. 9.
Purpose limitation can prevent eﬀective testing of the AI. Therefore, the fol-
lowing change to Art. 9(6) is suggested: Testing procedures shall be suitable to
achieve the intended purpose of the AI system. It is not surprising that ”in-
tended purpose” is the model of intention, but since the Regulation goes into
”actual use” in for example the prohibitions, the Commission should consider
whether to change the wording for the Regulation, regardless of its connection
to the product legislation in the EU in general10.
3.2.3
Art. 10.
Two choices are worth taking noticing in this article. Art. 10(2)(a) uses the
term ”relevant”, which poses the question as to whether there are ”irrelevant”
design choices. Irrelevant design choices can still aﬀect the AI, so why not just
write design choices? There is little reason not to exclude ”relevant” here.
Art. 10(5) does not acknowledge the diﬀerent ways which bias can be pre-
vented. It could do so to prevent the creation of the expectation that personal
data must be used in any way. This can be considered, but otherwise the section
is very agreeable.
3.2.4
Art. 14.
Human oversight is not the only solution to the dangers of AI, and while most
of the article is worded appropriately, there is one detail that could be changed.
Art. 14(2) mentions that ”human oversight shall aim”, but choosing ”shall” is
not appropriate, the human oversight must ”aim”. The use of ”shall” conﬂicts
with the rest of the article, and it is suggested that it is excluded, purely in Art.
14(2).
3.2.5
Art. 15.
Why is ”state-of-the-art” not used to describe the state of the (cyber)security
of the AI? Because of the consequences of failure of the defences when it comes
to AI, it is suggested that Art. 15(4), second sentence is changed to the follow-
ing: The technical solutions aimed at ensuring the cybersecurity of high-risk AI
systems shall be state-of-the-art.
This, combined with existing guidance on the subject matter, should guar-
antee a higher level of defences, which is more than adequate for such potentially
dangerous tools.
3.2.6
Art. 41.
Art. 41(3) seems either redundant (in that the high-risk AI may fulﬁll it re-
gardless of this paragraph) or allows for circumvention of fulﬁllment of Chapter
10This could be achieved with a dedicated article on how intention is perceived diﬀerently
when it comes to AI than other products, but this suggestion is not included here.
</pre>",POSITIVE
PyPDF2_2665170_5,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665170.pdf,12,5,2665170,attachments/2665170.pdf#page=5,Is the line order correct?,"<pre>  
 Hub France IA – Groupe de Travail Banques et Auditabilité  p 5/ 12  
Il serait nécessaire d’élaborer une analyse d’impact sur les coûts (et la perte de bénéfice des secteurs « à haut risque ») plus complète que celle indiquée au §3.3. Recommandation 3 : dans les domaines à haut risque, préciser deux sous-catégories de système IA à haut risque permettant de différencier deux niveaux de mise en œuvre de la conformité. Préciser les impacts économiques complets des secteurs à haut risque concernés. Par ailleurs, la définition en extension du périmètre de tels systèmes pourrait faire apparaître un risque juridique si l’usage visé n’apparait pas dans la liste au moment de la mise sur le marché, mais que celle-ci est ensuite modifiée. L’Article 7 (Modifications de l’annexe III) semble indiquer que la liste des domaines de l’annexe III (points 1 à 8) ne peut pas être modifiée : « La Commission est habilitée à adopter des actes délégués conformément à l’article 73 afin de mettre à jour la liste figurant à l’annexe III en y ajoutant des systèmes d’IA à haut risque lorsque les deux conditions suivantes sont remplies: (a) les systèmes d’IA sont destinés à être utilisés dans l’un des domaines énumérés à l’annexe III, points 1 à 8; (b) les systèmes d’IA présentent un risque de préjudice pour la santé et la sécurité, ou un risque d’incidence négative sur les droits fondamentaux, qui, eu égard à sa gravité et à sa probabilité d’occurrence, est équivalent ou supérieur au risque de préjudice ou d’incidence négative que présentent les systèmes d’IA à haut risque déjà visés à l’annexe III » Un usage ne figurant pas dans la liste des « domaines énumérés à l’annexe III » ne peut donc pas y être intégré ensuite. Cependant s’il y figure et que la liste de l’Annexe III est donc modifiée, comment les acteurs seront-ils notifiés ? la mise en conformité sera-t-elle être rétroactive ? dans quels délais ? De même, si nous restons dans la perspective d’un établissement des risques ex ante fondé sur des cas d’usage, quels sont les processus qui seront mis en place afin de réévaluer au fil du temps le niveau de risque des cas d’usage ? Le recours aux « actes délégués » est très imprécis et leur périmètre devrait être très clairement encadré. Recommandation 4 : préciser les règles de modification de l’Annexe III pour les systèmes IA à haut risque par le biais d’actes délégués. PROBLEMES CONCRETS POUR L’IMPLEMENTATION DE LA MISE EN CONFORMITE Nous avons identifié de nombreux cas où l’implémentation pratique de la mise en conformité semble extrêmement coûteuse, voire impossible. Notre crainte repose principalement sur la mise en place d’une réglementation qui sera vue comme un frein à l’innovation permise par l’IA, compte tenu de ces contraintes de mise en conformité. Par ailleurs, nous ne voyons pas comment il serait possible d’appliquer la mise en conformité sur certains systèmes déjà en production mais pour lesquels les exigences requises ne peuvent être rétroactivement mises en œuvre. Pour chaque contrainte de mise en conformité devrait être conduite une analyse d’impact sur l’innovation. Nous listons ici quelques cas qui nous apparaissent critiques.  MISES A JOUR FREQUENTES  Une fois qu’une solution comprenant un composant d’apprentissage a été mise sur le marché, elle doit être monitorée pour contrôler ses performances. Habituellement, les données dérivent peu à peu et leur distribution </pre>",NEUTRAL
pdfminer_2662381_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662381.pdf,4,4,2662381,attachments/2662381.pdf#page=4,Is the line order correct?,"<pre>• 
mogelijk ook meerdere lidstaten bevoegd zijn ten aanzien van één 
provider? 
Is jurisdictie verbonden aan de vestigingsplaats van de provider, 
producent of gebruiker? Welke lidstaat is bevoegd om tot handhaving over 
te gaan, is nog onvoldoende scherp. Meer duiding of aanscherping is 
gewenst.  
6.  Coördinatie tussen toezichthouders 
•  Gedeelde verantwoordelijkheden betekent dat in de verschillende fasen 
van het toezicht de bevoegde autoriteit zaakkennis moet overdragen of 
dat een organisatie te maken krijgt met meerdere toezichthouders. Hoe 
wordt informatiedeling en coördinatie van toezichtsactiviteiten voorzien? 
En wat is de juridische basis? 
•  Daarnaast vraagt effectief markttoezicht in Europa om naast nationaal 
toezicht een Europees netwerk op te richten waar aan gezamenlijke 
toezichtsactiviteiten, onderzoeken en handhaving wordt gewerkt.   
7.  Rechtsbescherming burgers  
Er is onvoldoende voorzien in de mogelijkheid voor burgers om individuele 
problemen met AI te rapporteren, waardoor de bescherming tekortschiet. Op 
basis van de AI Act kijkt een toezichthouder naar het AI-systeem en de werking in 
het geheel, niet naar de gevolgen voor een individueel geval. Om de burgers 
voldoende rechtsbescherming te bieden moeten zij kunnen aangeven dat zij zijn 
benadeeld door de toepassing van AI.   
Het maatschappelijk vertrouwen in AI-toepassingen is afhankelijk van 
rechtsbescherming, effectief toezicht, transparante markttoegang en 
standaardisatie.  
i NISD. Directive (EU) 2016/1148 of the European Parliament and of the Council of 
6 July 2016 concerning measures for a high common level of security of network 
and information systems across the Union. 
ii eIDAS. VERORDENING (EU) Nr. 910/2014 VAN HET EUROPEES PARLEMENT EN DE 
RAAD van 23 juli 2014 betreffende elektronische identificatie en vertrouwensdiensten 
voor elektronische transacties in de interne markt. 
iii RED. Directive 2014/53/EU of the European Parliament and of the Council of 16 
April 2014 on the harmonisation of the laws of the Member States relating to the 
making available on the market of radio equipment. 
iv CSA. VERORDENING (EU) 2019/881 VAN HET EUROPEES PARLEMENT EN DE RAAD 
van 17 april 2019 zake ENISA (het Agentschap van de Europese Unie voor 
cyberbeveiliging), en inzake de certificering van de cyberbeveiliging van informatie- 
en communicatietechnologie. 
Pagina 4 van 4 
 
  
 
 
  
 
  
 
 
 
 
 
</pre>",POSITIVE
tika_2665205_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665205.pdf,4,3,2665205,attachments/2665205.pdf#page=3,Is the line order correct?,"<pre>3 


preserving the professional and pedagogical autonomy and academic freedom of teachers 


and academics. 





Transparency and AI literacy and CPD of teachers on AI: 


ETUCE welcomes that the proposal of AI Regulation requires that users of AI tools (who 


include students, teachers, academic and education staff for the education sector) must be 


adequately informed about the intended purpose, level of accuracy, residual risks of AI 


tools. Nevertheless, ETUCE highlights that providing information is not sufficient to ensure 


the transparency of the AI tools when users miss the adequate digital skills and data and AI 


literacy to interpret it. Therefore, it is of utmost importance to improve the importance of 


digital skills, AI literacy and data literacy in educational curricula and raise awareness on 


the risks related to the use of AI tools in education. It is also essential to ensure that 


infrastructures of education institutions are adequately equipped for digital education as 


well as to provide equal access to digital technologies and ICT tools to all teachers and 


students, with particular attention to the most disadvantaged groups. To these purposes, 


sustainable public investment should be provided by national governments and the 


European Commission should provide financial support through European funding such as 


Horizon Europe, Digital Europe and in the framework of National Recovery and Resilience 


Facility. 


While the AI Regulation blandly mention to the possibility of providing users with training 


on Artificial Intelligence, ETUCE emphasises that it is crucial that sustainable public funding 


are provided at national and European level to ensure that teachers, trainers, academics 


and other education personnel receive up-to-date and free of charge continuous training 


and professional development on the use of AI tools in accordance with their professional 


needs.  





EdTech expansion and issues of intellectual property rights, data privacy of teachers:   


ETUCE points out that the development of the use of Artificial Intelligence in education has 


been accompanied by the expansion of Ed-tech companies that are progressively 


increasing their influence in the education sector, especially under the pressure of 


emergency online teaching and learning during the COVID-19 pandemic. ETUCE reminds 


that education is a human right and public good whose value needs to be protected. ETUCE 


calls for further public responsibility from national governments that should not limit their 


scope to regulating the EdTech sector and should develop and implement public platforms 


for online teaching and learning to protect the public value of education. In addition, public 


platforms should be implemented in full respect of professional autonomy of teachers and 


education personnel as well as academic freedom and autonomy of education institutions, 


without creating pressure on teachers and education personnel regarding the education 


material and pedagogical methods they use. It is also essential to protect the accountability 


and transparency in the governance of public education systems from the influence of 


private and commercial interests and actors.</pre>",POSITIVE
pdfminer_2665502_8,other,../24212003_requirements_for_artificial_intelligence/attachments/2665502.pdf,10,8,2665502,attachments/2665502.pdf#page=8,Is the line order correct?,"<pre>authorities and – due to unequal investments in the various member states’ authorities – citizen 
protection is not at the same level in each EU country.  
Also  in  the  context  of  this  regulation,  it  should  be  considered  that  too  much  emphasis  on  the 
national level can lead to a risk of unequal implementation in different member states, at different 
speeds and potentially different interpretations. Belgium has, for instance, been lagging behind 
with regard to the implementation of the GDPR; this delay may affect innovation and a European 
level  playing  field,  and  the  same  risks  to  happen  in  the  field  of  AI.  Strong  coordination  at  the 
European  level  will  hence  be  crucial;  also  given  the  fact  that  many  AI  systems  may  be  used 
transnationally and may be imported from third party countries. Moreover, given the importance 
of the risks attached to the use of AI as set out in this regulation, it will be essential that these 
authorities receive proper funding (and a sufficiently skilled workforce – which may be difficult in 
this field) so that they can provide adequate guidance for organizations and ensure a high level 
of citizen protection. 
The issue of different implementation speeds will also affect the creation of codes of conduct that 
can be voluntarily applied to AI systems other than high-risk systems. If it is assumed that the 
creation of a code of conduct is roughly the same effort and cost for any sector or member state, 
this absolute cost will mean that there may be more codes of conducts for sectors and member 
states with a higher turnover. Smaller member states with smaller markets will thus likely have 
less means to create these codes of conduct. This is another reason why a common European 
approach would be preferential. 
In this regard, the obligation to appoint an authorized representative established in the European 
Union in case an importer cannot be identified (recital 56 and article 25) is welcomed. Building on 
the  experience  with  the  GDPR,  it  is  crucial  to  allow  all  organizations  in  charge  of  the 
implementation  and  enforcement  of  the  regulation  –  be  it  the  national  competent  authorities 
(NCAs), market surveillance authorities or other bodies – to be able to conduct all necessary steps 
towards  the  authorized  representative,  independently  of  where  in  the  European  Union  it  is 
established. Related to this point, it is essential that several NCAs can oversee the notified bodies 
and technical services performing the conformity assessment. In other words, not only the NCA 
of the country in which the notified bodies and technical services are established, but also NCA 
from other European member states should be able to do so, especially to ensure protection in 
case a specific NCA would be too under-resourced. 
Furthermore, while currently not foreseen in the proposed regulation, citizens should be provided 
with measures for redress and a right to file a complaint with national authorities, since this will 
not only help closing the protection gap of the proposal, but it can also help national authorities to 
assess  and  establish  potential  breaches  of  the  regulation.  In  this  way,  public  and  private 
enforcement can be more complementary, and citizens will have a more active role in ensuring 
the protection of their rights. In the same line of thought, the link between the GDPR and this 
regulation should be highlighted. 
More than the fact that the Proposal does not, currently, foresee any mechanisms through which 
citizens can file a complaint, it seems to ignore the rights of citizens altogether. Despite the fact 
that the Recitals make numerous references to protecting “health, safety and fundamental rights,” 
the conceptual structure of the proposal is built on existing market surveillance schemes known 
from  product  safety  legislation.  The  proposal  seems  to  combine  two  concepts  that  are 
8 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
tika_2660134_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660134.pdf,4,2,2660134,attachments/2660134.pdf#page=2,Is the line order correct?,"<pre>Real Decreto 1828/1999, de 3 de diciembre, es un registro jurídico de titularidades y gravámenes 


sobre bienes muebles, donde rigen y se aplican los principios hipotecarios reguladores del 


Registro de la Propiedad y Mercantil, que tiene por objeto la inscripción de actos y contratos 


sobre bienes muebles, previo control de su legalidad, proporcionando publicidad y 


transparencia a la propiedad y a las cargas o gravámenes que sobre bienes muebles se pudieran 


establecer. 


El Registro de Bienes Muebles está integrado, por razón de su objeto, por las siguientes 


secciones: (i) Sección de buques y aeronaves; (ii) Sección de automóviles y otros vehículos a 


motor; (iii) Sección de maquinaria industrial, establecimientos mercantiles y bienes de equipo; 


(iv) Sección de garantías reales; (v) Sección de otros bines muebles registrables y (vi) Sección de 


condiciones generales de la contratación. 


La inscripción en dichos registros jurídicos nacionales elevaría el nivel de protección jurídica de 


los ciudadanos que tienen relación con los sistemas de inteligencia artificial, algoritmos y demás 


elementos relacionados con este tipo de tecnología ya que dicha inscripción se produciría tras 


el cumplimiento de los requerimientos de publicidad, transparencia y seguridad jurídica 


prescritos en la legislación hipotecaria, especialmente el control de legalidad de la operación, 


exigido por el artículo 72 de la Ley de Hipoteca Mobiliaria y prenda sin desplazamiento. Esta 


normativa hipotecaria garantiza la oponibilidad erga omnes y presunción de exactitud, 


legitimación y fe pública de los derechos inscritos sobre activos mobiliarios en los registros 


jurídicos de bienes muebles. La transparencia y seguridad jurídica de las titularidades y cargas 


de dichos activos está también garantizada mediante su publicidad, profesionalmente 


responsable y respetuosa con las exigencias de la normativa de protección de datos, que se 


puede obtener por canales telemáticos existentes en los mencionados registros públicos 


conforme a lo establecido por el artículo 78 de la Ley de Hipoteca Mobiliaria y Prenda sin 


desplazamiento.  


Dicho incremento de la seguridad jurídica llevaría consigo asociado el fortalecimiento de las 


posibilidades de financiación de sus creadores, inversores o titulares ofreciendo dichos activos 


como garantía inscrita en los registros jurídicos correspondientes.  


Los usuarios de estos sistemas o algoritmos podrían identificar perfectamente quiénes serían 


los responsables en caso de mal funcionamiento de los mismos no teniendo que seguir un largo 


camino hasta su identificación. Siendo, por tanto, posible la fácil asignación a un titular de la 


responsabilidad por mal funcionamiento.  


En relación a dicha concreción de titularidad para la reclamación de responsabilidades en la 


Resolución de 2017 antes citada se va un paso más y se indica como recomendación que se 


podría dotar a los robots inteligentes de “una personalidad jurídica específica” de modo que 


los “robots autónomos más complejos puedan ser considerados personas electrónicas 


responsables de reparar los daños que puedan causar y posiblemente aplicar la personalidad 


electrónica a aquellos supuestos en los que los robots tomen decisiones autónomas inteligentes 


o interactúen con terceros de forma independiente.”  


La inscripción en el registro de bienes muebles correspondiente al lugar donde el titular de los 


sistemas de inteligencia artificial, algoritmos y demás elementos tenga su centro de actividades, 


su residencia habitual o su centro de administración, o alternativamente, donde tuviera una 


relevante conexión con el sistema empleado por causa del archivo en una base de datos pública 


nacional,  podría además vincularse a través de la referencia, vía artículo 75 LHMPSD, con la 


inscripción correspondiente del titular (generalmente una sociedad) en el Registro Mercantil</pre>",POSITIVE
tika_2665624_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2665624.pdf,11,4,2665624,attachments/2665624.pdf#page=4,Is the line order correct?,"<pre>johner-institut.de   


 
Johner Institut GmbH 


Reichenaustraße 1 


78467 Konstanz 


T +49 7531 94500-20 


info@johner-institut.de 


monitoring” or “serious 


incident 


A device is considered a high-


risk AI system, if the following 


two conditions are met (article 


6):  


(a) the AI system is intended to 


be used as a safety component 


of a product, or is itself a 


product, covered by the Union 


harmonization legislation listed 


in Annex II; 


(b) the product whose safety 


component is the AI system, or 


the AI system itself as a 


product, is required to 


undergo a third-party 


conformity assessment with a 


view to the placing on the 


market or putting into service 


of that product pursuant to the 


Union harmonization 


legislation listed in Annex II.  


Medical devices are covered 


by the regulations listed in 


Annex II, because the MDR 


and IVDR are mentioned. To 


fulfill the MDR, medical 


devices class IIa and higher 


must undergo a conformity 


assessment procedure.  





Does this mean all software 


as medical device using AI is 


considered to be a high-risk 


product? 





MDR rule 11 classifies 


software, independent of risk, 


in most cases in class IIa or 


higher. Thus, the extensive 


requirements for high-risk 


products would apply for 


medical devices. The negative 


effects of rule 11 would be 


amplified by the AI act. 


Recital (31) states: The 


classification of an AI system as 


high-risk pursuant to this 


Regulation should not 


necessarily mean that the 


product whose safety 


component is the AI system, or 


the AI system itself as a 


product, is considered ‘high-


risk’ under the criteria 


established in the relevant 


Union harmonization 


legislation that applies to the 


product. This is notably the 


case for Regulation (EU) 


2017/745 of the European 


Parliament and of the 


Council47 and Regulation (EU) 


2017/746 of the European 


Parliament and of the 


Council48, where a third-party 


conformity assessment is 


provided for medium-risk and 


high-risk products. 


→ This should be considered in 


the AI act 


In article 10 the AI act requires  


„training, validation, and 


testing data sets shall be 


relevant, representative, free of 


errors and complete. “ 


Real-world data is rarely “free 


of error” and “complete”. It is 


also unclear what “complete” 


means. Do all datasets need 


to be available (whatever this 


means) or is the complete 


data of one dataset required? 


This requirement should be 


annulled. More suitable seems 


the requirement, that 


manufactures must define 


quality standards and verify 


their compliance. Another 


possible requirement could be 


the claim that the definition of 


the quality standards has to be 


risk-based.  


Further, definitions i.a. “correct” 


are needed.</pre>",NEUTRAL
tika_2662226_7,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2662226.pdf,8,7,2662226,attachments/2662226.pdf#page=7,Is the line order correct?,"<pre>Raising standards for consumers 


ANEC-DIGITAL-2021-G-071 – July 2021   7 


be complementary to strict safety measures and should not exonerate manufacturers 


from ensuring that products do not present a risk to consumers.  


However, the inherent information asymmetry associated with AI or an 


evolving/machine learning system, makes the function of information different from 


information linked to traditional, non-AI products (e.g. Ecolabel) where the 


technological content of the product is “static”. The information is not very helpful if the 


behaviour of products changes over time but the information stays the same. One 


reason more for us to seriously wonder about the inclusion of emotion recognition 


system or a biometric categorisation system and ‘deep fake’ in the level of low-risk AI 


systems, especially as consumers will not benefit from the right to opt-out of the 


system.  


6 | Information sharing and market surveillance 





Market surveillance authorities should have sufficient resources to enforce the AI 


requirements. We stress the need to ensure national supervisory authorities have the 


financial, technical and technological means to carry out their mission. The 


possibility of imposing mandatory inspection fees – as done in Food Safety legislation- 


should be explored. The proceedings of the fines should be used to finance the market 


surveillance activities.  


6.1 Reporting of serious incidents and of malfunctioning (art.62) 


We think that serious incident or any malfunctioning of AI having an impact on 


consumer safety should also be reported. We refer to our long-lasting call for a pan 


European accidents and injuries database in order to assess whether a product is 


posing a high risk for consumers, with the aim of achieving a high quality, 


representative and up-to-date data sample for the entire Single Market5. 


6.2 Procedure for dealing with AI systems presenting a risk at national level (art.65) 


The precautionary principle allows market surveillance authorities to take temporary 


and preventive measures in the absence of a definitive proof of harm to consumers or 


the environment. As such, we think that this fundamental principle should be present 


in the AI Act which is dealing with new technologies and unforeseen effects. 


In current market surveillance practice, legal obstacles prevent an exchange of 


information in both the harmonised and non-harmonised areas about dangerous 


products with other countries/jurisdictions. It is therefore important that the AI Act 


provides for a strengthening of international cooperation by allowing the exchange 


of information beyond confidentiality rules. 


ENDS 





 
5 European consumer safety needs solid injury data, ANEC-EuroSafe position paper, November 2020</pre>",POSITIVE
pdfminer_2665480_38,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665480.pdf,64,38,2665480,attachments/2665480.pdf#page=38,Is the line order correct?,"<pre>Despite not relying on personal data of natural persons, the fundamental rights implications of 
these systems are important because they are used to determine who can be subject to increased 
police intervention (based on geographical location), how these interventions occur, and with 
what frequency. Used for law enforcement purposes, without due care, resource optimisation 
systems  may  contribute  to  over-policing  and  surveillance  of  specific  geographical  locations 
caused by data ‘feedback loops,’58 and in doing so, may further exacerbate existing problems 
with  systemic  discrimination  arising  from  historical  racial  and  socio-economic  biases  in 
existing policing datasets (as geographical location is often a proxy for race or economic class), 
with little opportunity for redress, and no transparency for affected individuals. Drawing from 
Philip  Alston  speaking  about  the  SyRI  welfare  fraud  detection  system  in  the  Netherlands, 
targeting entire neighbourhoods as suspect and “subject to special scrutiny” with a combination 
of digital and physical methods threatens the very essence of privacy, by contributing to general 
unease,  potential  prejudice,  and  chilling  effects  on  behaviour.59  It  matters  little  whether  the 
personal data of natural persons is implicated here. 
Were the final text of the Regulation therefore to exclude resource optimisation systems, this 
could amount to a significant failure to recognise the systemic social assumptions that are built 
into AI systems – as socio-technical systems which mediate social institutions and structures – 
which  by  virtue  of  their  implementation,  further  mediate  the  enjoyment  of  individuals’ 
fundamental  rights,  including  respect  for  their  human  dignity,  equality,  liberty  and  other 
freedoms.  We  therefore  recommend  that  geospatial  AI  systems  are  included  under  Annex 
III(6), and that reference is made to AI systems which affect the distribution of law enforcement 
resources.    
e)  The requirements that high-risk AI systems must comply with need to be strengthened and 
clarified 
Our final remarks on the proposed regulatory framework for high-risk AI systems concerns the 
strength and clarity of the requirements for such systems. While vague language – which might 
cause legal uncertainty and a weak protection against AI’s adverse effects – can be found under 
several  requirements  for  high-risk  systems,  we  highlight  a  few  questions  and  concerns 
regarding three requirements in particular: those pertaining to data governance, transparency 
and human oversight.  
Data governance obligations  
The first requirement which raises questions is ‘data governance.’ Firstly, the large discretion 
for providers of high-risk AI systems mentioned earlier is also reflected in the requirements for 
data  governance.  Article  10  of  the  Proposal,  dealing  with  requirements  of  data  quality  and 
governance,  also  lets  the  term  ‘appropriate’  do  some  heavy  lifting.  Indeed,  it  requires  that 
training, validation and testing data sets shall be subject to ‘appropriate’ data governance and 
management practices, that the data sets shall have ‘appropriate’ statistical properties, and that 
the processing of special categories of data to avoid the risk of bias is carried out subject to 
‘appropriate’  safeguards  for  fundamental  rights.  While  the  Article  can  be  commended  for 
specifying  the  minimal  considerations  that  should  be  taken  into  account  for  the  data 
management  process  to  be  considered  ‘appropriate,’  it  leaves  open  what  constitutes  an 
58   The  Law  Society  of  England  and  Wales,  “Algorithms  in  the  Criminal  Justice  System,”  June  4,  2019, 
https://www.lawsociety.org.uk/en/topics/research/algorithm-use-in-the-criminal-justice-system-report, 35. 
59   Philip  Alston,  “Brief  by  the  United  Nations  Special  Rapporteur  on  extreme  poverty  and  human  rights as 
Amicus Curiae in the case of NJCM c.s./De Staat der Nederlanden (SyRI) before the District Court of the 
Hague 
2019, 
https://www.ohchr.org/Documents/Issues/Poverty/Amicusfinalversionsigned.pdf, 29. 
number:  C/9/550982/HA  ZA 
18/388),”  OHCHR, 
September 
(case 
26, 
33 
 
   
 
</pre>",NEGATIVE
pdfminer_2665425_8,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665425.pdf,27,8,2665425,attachments/2665425.pdf#page=8,Is the line order correct?,"<pre>8 l 27 
Verbraucherzentrale Bundesverband e.V. 
Artificial intelligence needs real world regulation 
Legislators must complement Article 3 with a definition for non-professional users 
of AI systems. This must entail people using AI systems in their capacity as con-
sumers and citizens. It must also consider consumers who are affected by AI sys-
tems employed by professional users. 
1.3 Article 3 (34) – Emotion recognition system 
The definition of ‘emotion recognition systems’ in Art. 3 (34) is too narrow. The defini-
tion relies on the definition of biometric data as defined in Art. 3 (33), which itself is 
taken over from the General Data Protection Regulation (GDPR).6 It holds that bio-
metric data must “allow or confirm the unique identification of that natural person.” As a 
consequence ‘emotion recognition systems’ that do not rely on data allowing the unique 
identification of a natural person, will fall out of the scope of the AIA. However, vzbv 
holds that these types of systems should also fall under the AIA’s scope.  
This could include systems that rely only on the analysis of clicking, typing and cursor 
movement data for example. Also, for an AI system supporting a retail salesperson in a 
shop, it is not important to know the identity of a potential customer entering the shop. 
The AI system can provide the shop personnel with valuable real time personality/emo-
tion analysis data, based on the customer behaviour. For example inferences from 
measures on the relative tone/height, rhythm, and the speed of a voice, but not the 
voice itself. 
  The definition of ‘emotion recognition systems’ in Art. 3 (34) should not refer to bio-
metric data but to personal data. Otherwise, there is a significant risk for circumven-
tion of the legislation. 
2. THE SCOPE IS TOO NARROW PART I: NEGLECT OF ECONOMIC HARMS AND 
VIOLATIONS OF CONSUMER RIGHTS 
In general, the scope of the proposed AIA is too narrow and the legislation does not fo-
cus on consumers. The European Commission’s proposal focuses on problems of 
(product-)safety, health and fundamental rights linked to the use of AI systems. It 
mostly deals with high risks to people in their capacity as citizens and employees, ne-
glecting that AI systems can lead to significant economic/financial welfare losses for 
consumers or to violations of consumers’ rights. 
2.1 AI applications with large economic/financial impact or effects on consumer 
rights must be regarded as high-risk  
The European Commission’s proposal sees high-risks of AI systems nearly exclusively 
in the areas of (product-)safety, health and fundamental rights. The draft AIA focuses 
on mitigating risks to people in their capacity as citizens, patients, employees and stu-
dents (“education”). However, most AI systems in these areas are already subject to 
European legislation. Therefore, in practices, it can be doubted that consumers will 
benefit much from the draft AIA in these areas. 
___________________________________________________________________________________________ 
6 Compare Art. 4 (14): GDPR: European Parliament: EU General Data Protection Regulation (GDPR) Regulation (EU) 
2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with 
regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC 
(General Data Protection Regulation, OJ L 119, 4.5.2016, 2016. 
 
 
</pre>",POSITIVE
PyPDF2_2660134_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660134.pdf,4,2,2660134,attachments/2660134.pdf#page=2,Is the line order correct?,"<pre>Real Decreto 1828/1999, de 3 de diciembre, es un registro jurídico de titularidades y gravámenes 
sobre bienes muebles, donde rigen y se aplican los principios hipotecarios reguladores del 
Registro de la Propiedad y Mercantil, que tiene por  objeto la inscripción de actos y contratos 
sobre bienes muebles, previo control de su legalidad, proporcionando publicidad y 
transparencia a la propiedad y a las cargas o gravámenes que sobre bienes muebles se pudieran 
establecer . 
El Registro de Bienes Mu ebles está integrado, por razón de su objeto, por las siguientes 
secciones: (i) Sección de buques y aeronaves; (ii) Sección de automóviles y otros vehículos a 
motor; (iii) Sección de maquinaria industrial, establecimientos mercantiles y bienes de equipo; 
(iv) Sección de garantías reales; (v) Sección de otros bines muebles registrables y (vi) Sección de 
condiciones generales de la contratación.  
La inscripción en dichos registros jurídicos nacionales elevaría el nivel de protección  jurídica  de 
los ciudadanos  que tienen relación con los sistemas de inteligencia artificial, algoritmos y demás 
elementos relacionados con este tipo de tecnología ya que dicha inscripción se produciría tras 
el cumplimiento de los requerimientos de publicidad, transparencia y segurida d jurídica  
prescritos en la legislación hipotecaria, especialmente el control de legalidad de la operación, 
exigido por el artículo 72 de la Ley de Hipoteca Mobiliaria y prenda sin desplazamiento. Esta 
normativa hipotecaria garantiza la oponibilidad erga o mnes y presunción de exactitud, 
legitimación y fe pública de los derechos inscritos sobre activos mobiliarios en los registros  
jurídicos de bienes muebles . La transparencia y seguridad jurídica de las titularidades y cargas 
de dichos activos está también g arantizada mediante su publicidad, profesionalmente 
responsable y respetuosa con las exigencias de la normativa de protección de datos, que se 
puede obtener por canales telemáticos existentes en los mencionados registros públicos 
conforme a lo establecido por el artículo 78 de la Ley de Hipoteca Mobiliaria y Prenda sin 
desplazamiento.  
Dicho incremento de la seguridad jurídica llevaría consigo asociado  el fortalecimiento de las 
posibilidades de financiación  de sus creadores, inversores o titulares ofreciendo dichos activos 
como garantía  inscrita en los registros jurídicos correspondientes.  
Los usuarios de estos sistemas o algoritmos podrían identificar perfectamente quiénes serían 
los responsables en cas o de mal funcionamiento de los mismos  no teniendo que seguir un largo 
camino hasta su identificación. Siendo , por tanto , posible la fácil asignación a un titular de la 
responsabilidad  por mal funcionamiento.  
En relación a dicha concreción de titularidad par a la reclamación de responsabilidades en la 
Resolución de 2017 antes citada se va un paso más y se indica como  recomendaci ón que se 
podría  dotar a los robots inteligentes de “ una personalidad jurídica específica ” de modo que 
los “ robots autónomos más compl ejos puedan ser considerados personas electrónicas 
responsables de reparar los daños que puedan causar y posiblemente aplicar la personalidad 
electrónica a aquellos supuestos en los que los robots tomen decisiones autónomas inteligentes 
o interactúen con t erceros de forma independiente. ”  
La inscripción en el registro de bienes muebles correspondiente al lugar donde el titular de los 
sistemas de inteligencia artificial , algoritmos y demás elementos  tenga su centro de actividades, 
su residencia habitual o su  centro de administración , o alternativamente, donde tuviera una 
relevante conexión con el sistema empleado por causa de l archivo en una base de datos pública 
nacional ,  podría además vincularse a través de la referencia, vía artículo 75 LHMPSD, con la 
inscripción correspondiente del titular (generalmente una sociedad) en el Registro Mercantil  </pre>",POSITIVE
tika_2665425_8,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665425.pdf,27,8,2665425,attachments/2665425.pdf#page=8,Is the line order correct?,"<pre>Artificial intelligence needs real world regulation 8 l 27 


Verbraucherzentrale Bundesverband e.V. 


Legislators must complement Article 3 with a definition for non-professional users 


of AI systems. This must entail people using AI systems in their capacity as con-


sumers and citizens. It must also consider consumers who are affected by AI sys-


tems employed by professional users. 


1.3 Article 3 (34) – Emotion recognition system 


The definition of ‘emotion recognition systems’ in Art. 3 (34) is too narrow. The defini-


tion relies on the definition of biometric data as defined in Art. 3 (33), which itself is 


taken over from the General Data Protection Regulation (GDPR).6 It holds that bio-


metric data must “allow or confirm the unique identification of that natural person.” As a 


consequence ‘emotion recognition systems’ that do not rely on data allowing the unique 


identification of a natural person, will fall out of the scope of the AIA. However, vzbv 


holds that these types of systems should also fall under the AIA’s scope.  


This could include systems that rely only on the analysis of clicking, typing and cursor 


movement data for example. Also, for an AI system supporting a retail salesperson in a 


shop, it is not important to know the identity of a potential customer entering the shop. 


The AI system can provide the shop personnel with valuable real time personality/emo-


tion analysis data, based on the customer behaviour. For example inferences from 


measures on the relative tone/height, rhythm, and the speed of a voice, but not the 


voice itself. 


 The definition of ‘emotion recognition systems’ in Art. 3 (34) should not refer to bio-


metric data but to personal data. Otherwise, there is a significant risk for circumven-


tion of the legislation. 


2. THE SCOPE IS TOO NARROW PART I: NEGLECT OF ECONOMIC HARMS AND 


VIOLATIONS OF CONSUMER RIGHTS 


In general, the scope of the proposed AIA is too narrow and the legislation does not fo-


cus on consumers. The European Commission’s proposal focuses on problems of 


(product-)safety, health and fundamental rights linked to the use of AI systems. It 


mostly deals with high risks to people in their capacity as citizens and employees, ne-


glecting that AI systems can lead to significant economic/financial welfare losses for 


consumers or to violations of consumers’ rights. 


2.1 AI applications with large economic/financial impact or effects on consumer 


rights must be regarded as high-risk  


The European Commission’s proposal sees high-risks of AI systems nearly exclusively 


in the areas of (product-)safety, health and fundamental rights. The draft AIA focuses 


on mitigating risks to people in their capacity as citizens, patients, employees and stu-


dents (“education”). However, most AI systems in these areas are already subject to 


European legislation. Therefore, in practices, it can be doubted that consumers will 


benefit much from the draft AIA in these areas. 


___________________________________________________________________________________________ 


6 Compare Art. 4 (14): GDPR: European Parliament: EU General Data Protection Regulation (GDPR) Regulation (EU) 


2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with 


regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC 


(General Data Protection Regulation, OJ L 119, 4.5.2016, 2016.</pre>",POSITIVE
pdfminer_2660134_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660134.pdf,4,2,2660134,attachments/2660134.pdf#page=2,Is the line order correct?,"<pre>Real Decreto 1828/1999, de 3 de diciembre, es un registro jurídico de titularidades y gravámenes 
sobre  bienes  muebles,  donde  rigen  y  se  aplican  los  principios  hipotecarios  reguladores  del 
Registro de la Propiedad y Mercantil, que tiene por objeto la inscripción de actos y contratos 
legalidad,  proporcionando  publicidad  y 
sobre  bienes  muebles,  previo  control  de  su 
transparencia a la propiedad y a las cargas o gravámenes que sobre bienes muebles se pudieran 
establecer. 
El  Registro  de  Bienes  Muebles  está  integrado,  por  razón  de  su  objeto,  por  las  siguientes 
secciones: (i)  Sección de  buques  y aeronaves; (ii) Sección de  automóviles  y otros vehículos a 
motor; (iii) Sección de maquinaria industrial, establecimientos mercantiles y bienes de equipo; 
(iv) Sección de garantías reales; (v) Sección de otros bines muebles registrables y (vi) Sección de 
condiciones generales de la contratación. 
La inscripción en dichos registros jurídicos nacionales elevaría el nivel de protección jurídica de 
los ciudadanos que tienen relación con los sistemas de inteligencia artificial, algoritmos y demás 
elementos relacionados con este tipo de tecnología ya que dicha inscripción se produciría tras 
el  cumplimiento  de  los  requerimientos  de  publicidad,  transparencia  y  seguridad  jurídica 
prescritos en la legislación hipotecaria, especialmente el control de legalidad de la operación, 
exigido por el artículo 72 de la Ley de Hipoteca Mobiliaria y prenda sin desplazamiento. Esta 
normativa  hipotecaria  garantiza  la  oponibilidad  erga  omnes  y  presunción  de  exactitud, 
legitimación  y  fe  pública  de  los  derechos  inscritos  sobre  activos  mobiliarios  en  los  registros 
jurídicos de bienes muebles. La transparencia y seguridad jurídica de las titularidades y cargas 
de  dichos  activos  está  también  garantizada  mediante  su  publicidad,  profesionalmente 
responsable y  respetuosa con  las  exigencias  de  la  normativa  de  protección  de  datos,  que  se 
puede  obtener  por  canales  telemáticos  existentes  en  los  mencionados  registros  públicos 
conforme  a  lo  establecido  por  el  artículo  78  de  la  Ley  de  Hipoteca  Mobiliaria  y  Prenda  sin 
desplazamiento.  
Dicho incremento de  la seguridad jurídica llevaría consigo  asociado  el  fortalecimiento de  las 
posibilidades de financiación de sus creadores, inversores o titulares ofreciendo dichos activos 
como garantía inscrita en los registros jurídicos correspondientes.  
Los usuarios de estos sistemas o algoritmos podrían identificar perfectamente quiénes serían 
los responsables en caso de mal funcionamiento de los mismos no teniendo que seguir un largo 
camino hasta su identificación. Siendo, por tanto, posible la fácil asignación a un titular de la 
responsabilidad por mal funcionamiento.  
En  relación  a  dicha  concreción  de  titularidad  para  la  reclamación  de  responsabilidades  en  la 
Resolución de 2017 antes citada se va un paso más y se  indica como  recomendación que  se 
podría dotar a los robots inteligentes de “una personalidad jurídica específica” de modo que 
los  “robots  autónomos  más  complejos  puedan  ser  considerados  personas  electrónicas 
responsables de reparar los  daños que puedan causar y posiblemente aplicar la personalidad 
electrónica a aquellos supuestos en los que los robots tomen decisiones autónomas inteligentes 
o interactúen con terceros de forma independiente.”  
La inscripción en el registro de bienes muebles correspondiente al lugar donde el titular de los 
sistemas de inteligencia artificial, algoritmos y demás elementos tenga su centro de actividades, 
su  residencia  habitual o su  centro  de  administración,  o  alternativamente,  donde  tuviera  una 
relevante conexión con el sistema empleado por causa del archivo en una base de datos pública 
nacional,  podría además vincularse a través de la referencia, vía artículo 75 LHMPSD, con la 
inscripción correspondiente del titular (generalmente una sociedad) en el Registro Mercantil 
</pre>",NEGATIVE
tika_2665469_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665469.pdf,3,3,2665469,attachments/2665469.pdf#page=3,Is the line order correct?,"<pre>V1.0 5. august 2021 KMD  Side 3 af 3 











2.3 Sandboxes and data spaces 


Another requirement is the so-called AI regulatory sandboxes (Title V Article 53) for testing 


and validating AI systems before placement on the market.  





It is unclear whether these sandbox environments will be provided by member states’ 


authorities themselves or the European Data Protection Supervisor, or if the task will be 


outsourced to private companies. The same goes for the European common data spaces 


(Recital 45). KMD suggests a further clarification in the proposal. 








3 Clarification of various terms and concepts 





3.1 High-risk classification 


As for the protection of public interest in high-risk AI systems, the proposal states that 


common normative standards should be established (Recital 13). KMD suggests a clarification 


of whether these common normative standards are part of the report on AI standards2. 





The requirements regarding accuracy, consistency, robustness, appropriateness etc. in high-


risk AI systems (Title III Article 15) furthermore lacks specification. 





Title XII Article 83(2) regards the application of the regulation to high-risk AI systems already 


on the market. It states that these solutions will not be subject to the new regulation unless 


there are significant changes to design or purpose. KMD suggests a further elaboration on 


what constitutes a “significant change”. 





3.2 Placing on the market and putting into service 


The difference between “placing on the market” and “putting into service” (Title 1 Article 


3(9,11)) is subtle but regards first time a solution is made available to the market respectively 


first use. This raises a question about retrained AI systems for a new problem dataset. A 


clarification of whether using retrained AI systems are considered “first use” would be 


beneficial. 














2 Cf. JRC Publications Repository - AI Standardisation Landscape: state of play and link to the EC proposal for an AI regulatory 


framework (europa.eu) 



https://publications.jrc.ec.europa.eu/repository/handle/JRC125952

https://publications.jrc.ec.europa.eu/repository/handle/JRC125952</pre>",POSITIVE
fitz_2665600_2,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665600.pdf,6,2,2665600,attachments/2665600.pdf#page=2,Is the line order correct?,"<pre>Page 1 of 5 
 
1. Introduction 
The technical complexity, pervasive penetration and multifaceted social implications of the use of artificial 
intelligence (“AI”) in every-day life of EU citizens unequivocally necessitate the introduction of clear and 
sensible rules for development, deployment and use of AI. The endeavors in this respect of the European 
Union institutions, and of the European Commission in particular, are laudable. The strive to shape those 
rules in a way that promotes innovation and the uptake of AI in the European Union (the “EU”) is also 
praiseworthy.  
However, any future regulation of AI should also be introspective, smart and fair. Introspective in the sense 
that it should learn from and avoid deficiencies and past mistakes in other relevant areas of business 
regulation (e.g. data protection, antitrust). Smart in the sense that it should have built in already now a 
conceptual design and a toolkit that would allow addressing issues arising from the advance in AI (e.g. the 
emergence of strong and general AI). Fair in the sense that a future regulation of AI should – while creating 
a favourable business environment – also account and cater for the interests of society at large, including 
end users and citizens more generally. 
This position paper focuses on analysis of the proposal for a regulation of the European Parliament and the 
Council laying down harmonized rules on artificial intelligence (Artificial Intelligence Act) (the “Draft AI 
Regulation” or the “Draft”) and recommendations on the conceptual design of and certain fundamental 
rules under the Draft with view to the need of an introspective, smart and fair legal framework for design, 
development, deployment and use of AI in the EU.  
 
2. Risk-based approach and ethical/human rights backbone 
The Draft AI Regulation retains the risk-based approach and the ethical/human rights backbone initially 
contemplated in the European Commission’s White Paper on AI1 and the recommendations of the 
European Parliament of October 2020,2 while building upon regulation in other areas (e.g. data protection, 
consumer protection, standardization) to address the more specific technical aspects of AI design, 
development, deployment and use. This approach generally makes sense given how contextually 
dependent the deployment and use of AI are and how quickly the technology evolves. Too legalistic 
requirements might result in under- or overregulation.  
However, clearer emphasis needs to be put in the final text of the Artificial Intelligence Act on outcomes, 
rather than on formal course of action. Art. 13 to Art. 15 of the Draft already require attainment of certain 
overall outcomes (transparency, accuracy, etc.). Yet, other obligations under Chapters 2 and 3 of Title III 
presuppose mere process-like actions (e.g. documenting, record-keeping, risk management based on step-
by-step actions) in order to demonstrate compliance. If the emphasis in this framework does not firmly and 
eloquently lie with ultimate outcomes – and that various processes are one of the means to that end – 
compliance would morph into a “box-ticking”- and “window-dressing”-type of adherence to the formal 
requirements under the Artificial Intelligence Act, as is the case with similar rather “technical” requirements 
under the European Union Genera Data Protection Regulation.3 
 
3. Prohibited use of AI 
The explicit bans under Art. 5 of the Draft AI Regulation are a welcome development in an attempt to limit 
the use of AI to manipulative and/or privacy-invasive ends.  
However, the proposed language and approach have some deficiencies, as set out below.  
3.1 AI employing subliminal techniques or exploiting vulnerabilities 
</pre>",POSITIVE
pdfminer_2665170_5,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665170.pdf,12,5,2665170,attachments/2665170.pdf#page=5,Is the line order correct?,"<pre>Il serait nécessaire d’élaborer une analyse d’impact sur les coûts (et la perte de bénéfice des secteurs « à haut 
risque ») plus complète que celle indiquée au §3.3. 
Recommandation 3 : dans les domaines à haut risque, préciser deux sous-catégories de système IA à haut risque 
permettant de différencier deux niveaux de mise en œuvre de la conformité. Préciser les impacts économiques 
complets des secteurs à haut risque concernés. 
Par ailleurs, la définition en extension du périmètre de tels systèmes pourrait faire apparaître un risque juridique 
si l’usage visé n’apparait pas dans la liste au moment de la mise sur le marché, mais que celle-ci est ensuite 
modifiée.  L’Article  7  (Modifications  de  l’annexe  III)  semble  indiquer  que  la  liste  des  domaines  de  l’annexe  III 
(points 1 à 8) ne peut pas être modifiée : 
les systèmes d’IA sont destinés à être utilisés dans l’un des domaines énumérés à l’annexe III, 
« La Commission est habilitée à adopter des actes délégués conformément à l’article 73 afin de mettre 
à  jour  la  liste  figurant  à  l’annexe  III  en  y  ajoutant  des  systèmes  d’IA  à  haut  risque  lorsque  les  deux 
conditions suivantes sont remplies: 
(a) 
points 1 à 8; 
les systèmes d’IA présentent un risque de préjudice pour la santé et la sécurité, ou un risque 
(b) 
d’incidence  négative  sur  les  droits  fondamentaux,  qui,  eu  égard  à  sa  gravité  et  à  sa  probabilité 
d’occurrence, est équivalent ou supérieur au risque de préjudice ou d’incidence négative que présentent 
les systèmes d’IA à haut risque déjà visés à l’annexe III » 
Un usage ne figurant pas dans la liste des « domaines énumérés à l’annexe III » ne peut donc pas y être intégré 
ensuite. Cependant s’il y figure et que la liste de l’Annexe III est donc modifiée, comment les acteurs seront-ils 
notifiés ? la mise en conformité sera-t-elle être rétroactive ? dans quels délais ? 
De même, si nous restons dans la perspective d’un établissement des risques ex ante fondé sur des cas d’usage, 
quels sont les processus qui seront mis en place afin de réévaluer au fil du temps le niveau de risque des cas 
d’usage ?  Le  recours  aux  « actes  délégués »  est  très  imprécis  et  leur  périmètre  devrait  être  très  clairement 
encadré. 
Recommandation 4 : préciser les règles de modification de l’Annexe III pour les systèmes IA à haut risque par le 
biais d’actes délégués. 
PROBLEMES CONCRETS POUR L’IMPLEMENTATION DE LA MISE EN CONFORMITE 
Nous  avons  identifié  de  nombreux  cas  où  l’implémentation  pratique  de  la  mise  en  conformité  semble 
extrêmement  coûteuse,  voire  impossible.  Notre  crainte  repose  principalement  sur  la  mise  en  place  d’une 
réglementation qui sera vue comme un frein à l’innovation permise par l’IA, compte tenu de ces contraintes de 
mise en conformité. Par ailleurs, nous ne voyons pas comment il serait possible d’appliquer la mise en conformité 
sur  certains  systèmes  déjà  en  production  mais  pour  lesquels  les  exigences  requises  ne  peuvent  être 
rétroactivement  mises  en  œuvre.  Pour  chaque  contrainte  de  mise  en  conformité  devrait  être  conduite  une 
analyse d’impact sur l’innovation. 
Nous listons ici quelques cas qui nous apparaissent critiques.  
MISES A JOUR FREQUENTES 
Une  fois  qu’une  solution comprenant  un  composant  d’apprentissage  a  été  mise  sur  le  marché, elle  doit  être 
monitorée pour contrôler ses performances. Habituellement, les données dérivent peu à peu et leur distribution 
Hub France IA – Groupe de Travail Banques et Auditabilité 
p 5/ 12  
 
 
 
 
 
</pre>",NEGATIVE
tika_2665397_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665397.pdf,15,1,2665397,attachments/2665397.pdf#page=1,Is the line order correct?,"<pre>Commentary to the Commission’s proposal for the “AI Act” – 
Response to selected issues 
 


 Centre for Commercial Law, School of Law, University of Aberdeen 
 





This response is provided by a working group of the Centre for Commercial Law (CCL) at the 


University of Aberdeen. The working group consists of Dr Péter Cserne, Dr Rossana Ducato, 


and Dr Patricia Živković, and the response incorporates comments by Prof Abbe Brown, Dr 


Irène Couzigou, Dr Georgios Leontidis, Prof Nir Oren, Dr Clare Sutherland, Dr Paula Sweeney, 


Dr Burcu Yüksel Ripley. 


The analysis provided in this response contains a preliminary analysis of selected issues.  


The views and opinions reported in this response are submitted on behalf of the Centre for 


Commercial Law and do not necessarily express the position of the School of Psychology, 


School of Divinity, History, Philosophy & Art History, and the School of Natural and Computing 


Science.  





Table of Contents 
1. Market integration, market regulation and fundamental rights .................................... 1 


2. The scope of application ................................................................................................ 2 


2.1. Focus on the pre-market stage ................................................................................... 2 


2.2. The extra-territorial effect ........................................................................................... 3 


3. The risk-based approach ................................................................................................ 4 


3.1. Prohibited AI practices ........................................................................................... 4 


3.2. High risk AI systems ................................................................................................ 8 


4. Measures in support of innovation .............................................................................. 11 


5. A place for the “AI subject” .......................................................................................... 13 


6. Plain language and beyond .......................................................................................... 14</pre>",POSITIVE
tika_2665648_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665648.pdf,8,1,2665648,attachments/2665648.pdf#page=1,Is the line order correct?,"<pre>CHAI Position Paper on the EU Arti�cial IntelligenceAct
University of California, Berkeley


The Center for Human-Compatible AI (CHAI) is a research group based at UC Berkeley, with
academic a�liates at a variety of other universities. CHAI’s goal is to develop the conceptual and
technical wherewithal to reorient the general thrust of AI research towards provably bene�cial systems.
CHAI is led by Prof. Stuart Russell.


Recommendations
Make the regulation f uture-proof  and prepare for higher-risksystems


Update the regulation to address increasingly generalized AI systems that have multiple
purposes, such as OpenAI’s Generative Pre-trained Transformer 3 and DeepMind’s AlphaFold
system
Article 3(13) “reasonably foreseeable misuse” and “interaction with other systems”: Explicitly
consider the issue of high-risk and societal-scale consequences stemming from the interaction
of many low-risk systems
Article 6 classi�cation rules for high-risk systems: Include recommender systems in the
classi�cation rules for high-risk systems
Article 6 classi�cation rules for high-risk systems: Include the requirements to document
perceptual inputs, action outputs, objectives, and the operational environment for high-risk
systems
Article 6 classi�cation rules for high-risk systems: Include the requirements to document
time-of-sale properties of systems
List of high-risk categories in Annex III: Add categories


Protect people from psychological harm
Article 5 (1) (a) on psychological manipulation: Consider expanding the current de�nition of
“subliminal techniques beyond a person’s consciousness”
Section 3.5 of the Explanatory Memorandum: Include the protection of mental integrity in
the Explanatory Memorandum
Article 53 (AI regulatory sandboxes): Recognize the limit of sandboxes


1</pre>",POSITIVE
pdfminer_2660610_7,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2660610.pdf,8,7,2660610,attachments/2660610.pdf#page=7,Is the line order correct?,"<pre>3.2.2 Art. 9.
Purpose limitation can prevent eﬀective testing of the AI. Therefore, the fol-
lowing change to Art. 9(6) is suggested: Testing procedures shall be suitable to
achieve the intended purpose of the AI system. It is not surprising that ”in-
tended purpose” is the model of intention, but since the Regulation goes into
”actual use” in for example the prohibitions, the Commission should consider
whether to change the wording for the Regulation, regardless of its connection
to the product legislation in the EU in general10.
3.2.3 Art. 10.
Two choices are worth taking noticing in this article. Art. 10(2)(a) uses the
term ”relevant”, which poses the question as to whether there are ”irrelevant”
design choices. Irrelevant design choices can still aﬀect the AI, so why not just
write design choices? There is little reason not to exclude ”relevant” here.
Art. 10(5) does not acknowledge the diﬀerent ways which bias can be pre-
vented. It could do so to prevent the creation of the expectation that personal
data must be used in any way. This can be considered, but otherwise the section
is very agreeable.
3.2.4 Art. 14.
Human oversight is not the only solution to the dangers of AI, and while most
of the article is worded appropriately, there is one detail that could be changed.
Art. 14(2) mentions that ”human oversight shall aim”, but choosing ”shall” is
not appropriate, the human oversight must ”aim”. The use of ”shall” conﬂicts
with the rest of the article, and it is suggested that it is excluded, purely in Art.
14(2).
3.2.5 Art. 15.
Why is ”state-of-the-art” not used to describe the state of the (cyber)security
of the AI? Because of the consequences of failure of the defences when it comes
to AI, it is suggested that Art. 15(4), second sentence is changed to the follow-
ing: The technical solutions aimed at ensuring the cybersecurity of high-risk AI
systems shall be state-of-the-art.
This, combined with existing guidance on the subject matter, should guar-
antee a higher level of defences, which is more than adequate for such potentially
dangerous tools.
3.2.6 Art. 41.
Art. 41(3) seems either redundant (in that the high-risk AI may fulﬁll it re-
gardless of this paragraph) or allows for circumvention of fulﬁllment of Chapter
10This could be achieved with a dedicated article on how intention is perceived diﬀerently
when it comes to AI than other products, but this suggestion is not included here.
</pre>",POSITIVE
pdfminer_2661333_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2661333.pdf,2,2,2661333,attachments/2661333.pdf#page=2,Is the line order correct?,"<pre>2 
and innovation on the other. Thus, it is central that the proposal highlights the use of so-called 
regulatory sandboxes. It is an important part in promoting innovation and streamline regulatory 
compliance for future AI systems. Test and experimental environments, as well as regulatory 
sandboxes, are central to the development of reliable AI.  
For AI to contribute with desired effects, data needs to be made available. It is with the help of large 
amounts of data that AI can be used, for example, in healthcare diagnoses and contribute to 
preventive health. A prerequisite is the possibility to share and use data in a secured manner. 
Furthermore, data needs to be of high quality. 
In the proposal, parts of the activities under regional responsibility falls under areas of possible 
high-risk AI applications. It is therefore important that the right conditions are created to ensure that 
the public administrations have the right tools to make an adequate risk assessment of often 
complex AI value chains. It is of great importance that the consequences of the proposal for both 
public and private activities are analysed in detail. The possible increased administrative burden for 
regions and municipalities also needs to be analysed. 
Furthermore, digital skills and competences are key factors. A prerequisite for sustainable 
introduction and application of trusted AI systems is the understanding and commitment to AI 
development. The need for digital skills is growing but there is simultaneously a shortage of digital 
skills and excellence in many parts of the EU, for example in Sweden. This is a challenge, not least 
for the development of AI. The proposal for a new AI regulation can be expected to further increase 
the need for digital competence in various businesses and industries. Resources need to be invested 
in skills development in both public and private sectors. Investments in lifelong learning, efforts to 
attract international talent and improved measures to match the supply and demand in the labour 
market are needed in the EU. Region Västra Götaland therefore welcomes the European 
Commission's new coordinated plan on AI. It is important to work strategically on measures for 
skills supply, digital skills and increased investment, and to make use of the already existing 
structures at local, regional, national and European level. Regions have long experience of 
supporting and collaborating with actors in different ecosystems and infrastructures at regional 
level. These already existing forms of cooperation and ecosystems should be considered in the 
future work in the field of digitalisation and AI.
Region Västra Götaland  
Region Västra Götaland, governed by democratically elected politicians, has around 50 000 employees and is in 
addition to regional development also responsible for providing health care and public transport for all 
inhabitants in Västra Götaland. Västra Götaland is home to 1.7 million inhabitants. As a large procuring 
organisation and employer, Region Västra Götaland has the ambition to act as a forerunner within sustainable 
development and as test bed for new ideas and innovations. Together with the 49 municipalities, trade and 
industry, organisations and academia, we drive development with Västra Götaland’s best interests as objective. 
Region Västra Götaland’s position paper on EU AI Act 
| 
 2021-07-055 
 
 
 
 
 
</pre>",POSITIVE
tika_2662901_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662901.pdf,10,2,2662901,attachments/2662901.pdf#page=2,Is the line order correct?,"<pre>EUROPEAN ASSOCIATION OF CO-OPERATIVE BANKS 
The Co-operative Difference : Sustainability, Proximity, Governance 





2 
 


Introduction 





The European Association of Co-operative Banks (EACB) is happy to contribute to the discussion 


on the Artificial Intelligence (AI) legislative proposal. 





The EACB recognises that the AI proposal is the Commission’s first ever legal framework on the 


matter, which addresses the risks of AI and aims to position Europe to play a leading role globally. 


It should be recognised that this is a risky bet. If European values were not ultimately adopted 


on an international scale, European companies would be at a disadvantage compared to non-


European players active in less restrictive regulatory environments. 





We believe that the European Commission, the European Parliament and the Council should 


remain vigilant to ensure that European players are not unduly constrained in their prospect of 


developing innovative AI solutions compared to international competitors. 





We would like to highlight the following points: 





• The EACB welcomes the Commission’s risk-based approach as basis for a proportionate 


legal text. The Commission suggests a risk-pyramid approach: the higher the risk (for 


users) using AI system, the more additional measures. 





• We appreciate the technology-neutral and future-proof definition of AI, recognising that 


AI is a “fast evolving family of technologies” that is constantly developing. Nevertheless, 


combining the definition of artificial intelligence system together with the techniques and 


approaches of Annex I of the proposal, we observe that the scope of the Regulation is 


becoming quite wide as it also includes rule-based approaches. 





• We believe it is of paramount importance to make sure that the AI proposal will not add 


new and burdensome requirements for the banking sector and create conflicts and 


overlaps with existing rules: e.g., sector-specific regulation (CRD, CRR). 





• We particularly value the Commission’s human-centric perspective in designing AI rules: 


o The responsibility for an action or a decision still lies with a human being; 


o Actions and decisions of an AI system have to be traceable and understandable by 


humans using it; and 


o Actions and decisions of an AI system can always be changed/corrected by a human 


being (human oversight). 





• We understand that the Regulation’s intention is to protect the safety and fundamental 


rights of EU citizens, thus that the requirements for high-risk AI systems are only targeted 


at AI applications that could possibly pose risks to natural persons. 





• Generally, some provisions of the Regulation contain somewhat vague wording, e.g., the 


definitions provided for “remote biometric identification system” and “user”. Moreover, we 


believe that the definition of ‘developer’ and ‘end user’ are missing from the legal text. 


These points should be further clarified in order to guarantee legal certainty for providers, 


developers and users of AI systems.</pre>",POSITIVE
tika_2661333_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2661333.pdf,2,2,2661333,attachments/2661333.pdf#page=2,Is the line order correct?,"<pre>Region Västra Götaland’s position paper on EU AI Act |  2021-07-055 


2 


and innovation on the other. Thus, it is central that the proposal highlights the use of so-called 


regulatory sandboxes. It is an important part in promoting innovation and streamline regulatory 


compliance for future AI systems. Test and experimental environments, as well as regulatory 


sandboxes, are central to the development of reliable AI.  


For AI to contribute with desired effects, data needs to be made available. It is with the help of large 


amounts of data that AI can be used, for example, in healthcare diagnoses and contribute to 


preventive health. A prerequisite is the possibility to share and use data in a secured manner. 


Furthermore, data needs to be of high quality. 


In the proposal, parts of the activities under regional responsibility falls under areas of possible 


high-risk AI applications. It is therefore important that the right conditions are created to ensure that 


the public administrations have the right tools to make an adequate risk assessment of often 


complex AI value chains. It is of great importance that the consequences of the proposal for both 


public and private activities are analysed in detail. The possible increased administrative burden for 


regions and municipalities also needs to be analysed. 


Furthermore, digital skills and competences are key factors. A prerequisite for sustainable 


introduction and application of trusted AI systems is the understanding and commitment to AI 


development. The need for digital skills is growing but there is simultaneously a shortage of digital 


skills and excellence in many parts of the EU, for example in Sweden. This is a challenge, not least 


for the development of AI. The proposal for a new AI regulation can be expected to further increase 


the need for digital competence in various businesses and industries. Resources need to be invested 


in skills development in both public and private sectors. Investments in lifelong learning, efforts to 


attract international talent and improved measures to match the supply and demand in the labour 


market are needed in the EU. Region Västra Götaland therefore welcomes the European 


Commission's new coordinated plan on AI. It is important to work strategically on measures for 


skills supply, digital skills and increased investment, and to make use of the already existing 


structures at local, regional, national and European level. Regions have long experience of 


supporting and collaborating with actors in different ecosystems and infrastructures at regional 


level. These already existing forms of cooperation and ecosystems should be considered in the 


future work in the field of digitalisation and AI.





Region Västra Götaland  
Region Västra Götaland, governed by democratically elected politicians, has around 50 000 employees and is in 
addition to regional development also responsible for providing health care and public transport for all 
inhabitants in Västra Götaland. Västra Götaland is home to 1.7 million inhabitants. As a large procuring 
organisation and employer, Region Västra Götaland has the ambition to act as a forerunner within sustainable 
development and as test bed for new ideas and innovations. Together with the 49 municipalities, trade and 
industry, organisations and academia, we drive development with Västra Götaland’s best interests as objective.</pre>",POSITIVE
PyPDF2_2665504_10,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665504.pdf,11,10,2665504,attachments/2665504.pdf#page=10,Is the line order correct?,"<pre>(d) to determine the safety and compatibility with potential recipients;
(e) to predict treatment response or reactions;
(f) to define or monitoring therapeutic measures.
Later on, in Chapter II of that regulation, EU regulates more specifically the requirements 
regarding performance, design and manufacture. Here, all articles apply also to what would be
called “ AI” systems, given the previous definition. But some extra specificities are added, in 
point 16: 
16.   Electronic programmable systems — devices that incorporate electronic programmable 
systems and software that are devices in themselves
16.1.   Devices that incorporate electronic programmable systems, including software, or 
software that are devices in themselves, shall be designed to ensure repeatability, reliability 
and performance in line with their intended use. In the event of a single fault condition, 
appropriate means shall be adopted to eliminate or reduce as far as possible consequent risks
or impairment of performance.
16.2.   For devices that incorporate software or for software that are devices in themselves, the 
software shall be developed and manufactured in accordance with the state of the art taking 
into account the principles of development life cycle, risk management, including information 
security, verification and validation.
16.3.   Software referred to in this Section that is intended to be used in combination with 
mobile computing platforms shall be designed and manufactured taking into account the 
specific features of the mobile platform (e.g. size and contrast ratio of the screen) and the 
external factors related to their use (varying environment as regards level of light or noise).
16.4.   Manufacturers shall set out minimum requirements concerning hardware, IT networks 
characteristics and IT security measures, including protection against unauthorised access, 
necessary to run the software as intended.
We see here that the article regulated important aspects also pursued by the proposal of 
regulation on AI: security, availability, performance qualification, … It is not the aim of this note 
to dive more into IVD regulation, but the interested reader can verify that many other aspects 
of interest in the project of regulation on AI are properly dealt within that regulation, without 
once entering into the details of the technologies (which makes it stronger, more future-proof).
Proceeding this way is also a guarantee that any manufacturer of a specific kind of product, 
service, application will manage properly with the necessary requirements. It will also 
guarantee there is no conflict between an application-specific regulation and a technology-
specific regulation. 
Copyright @T.Helleputte – 2021
Page 10/11</pre>",POSITIVE
pdfminer_2665648_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665648.pdf,8,1,2665648,attachments/2665648.pdf#page=1,Is the line order correct?,<pre>CHAI Position Paper on the EU Arti,NEGATIVE
tika_2665627_6,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665627.pdf,11,6,2665627,attachments/2665627.pdf#page=6,Is the line order correct?,"<pre>From our perspective we wonder if there is evidence that regulatory sandboxes are an 


effective strategy for facilitating innovation. We would like to see a more elaborate report on 


the consequences of the regulation in terms of its effect on innovation, specifically in relation 


to regulatory sandboxes and SMEs.  





Another way forward would be for an SME to partner with a larger actor with the ability to 


take the full administrative work onboard. The deal could have an impact on IP rights and 


revenue streams which in turn could mean that the regulation serves to protect the established 


actors on the market. The benefit for SMEs would be that they can focus on their innovative 


ideas and make the administration of the AI system a question of commercial contracts 


instead of a negotiation with national authorities. The latter demands another set of 


competences that not all SMEs have made a priority to recruit. 





The proposed regulation mentions another possibility in Article 17.2 that states that the 


quality management system ensuring compliance with the regulation should be in relation to 


the size of the organization. This could be used by SMEs instead of regulatory sandboxes or 


difficult negotiations with large and established actors. But it could also be used by large 


organisations that want to reduce their administration by relating the quality management 


system to the size of the unit developing the AI system, not the whole organization. 





Regulatory sandboxes is a topic we will return to from the perspective of public governance 


further down in our response. 





Responsibilities in a system-of-systems 
In this section we will focus on AI systems and their development as systems-of-systems, but 


also on AI systems as components in other systems and what effects the proposed regulation 


can have in terms of responsibilities among and between actors. 





System boundaries 
A challenge for actors with the proposed regulation is that artefacts and systems developed 


for other purposes can be used for future development of high-risk AI systems and therefore 


covered by the regulation (Annex IV.2). It could be services and platforms providing data 


regarding road conditions [5] or the placement of wastewater wells [6] (depending on the 


interpretation of the definition in Annex I they might be high-risk AI systems or mere data 


sources). If the supplied data is used for developing AI systems which can be used for 


managing vital infrastructure like roads and water supply (Annex III) they will be part of the 


technology chain behind the AI system and need to be administered as such.   





The same goes for developers of models, such as digital twins, if they are used for developing 


AI systems that are classified as high-risk systems. Providers of data regarding populations 


face the same uncertainty if the data covers for instance taxable income, number of residents 


at a specific address or fluctuation of property prices since they can be used to determine 


strategies or decisions for social benefits or targeted interventions against social exclusion 


(Annex III.5 and III.8). In the long run all providers of digital artefacts face the probability 


that their service, data or technology will be used for developing high-risk AI and therefore 


need to have the right documentation to conform with the regulation and avoid fines. 





At the same time there are political initiatives that promote public authorities and actors to 


facilitate data sharing as well as a need for digital simulations and models for societal and 


business planning. These artefacts could be high-risk AI systems in themselves or become</pre>",POSITIVE
pdfminer_2662182_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662182.pdf,5,3,2662182,attachments/2662182.pdf#page=3,Is the line order correct?,"<pre>parezca real ha sido elaborado por medios automáticos, en nuestra opinión, también se 
debe  exigir  la  divulgación  de  qué  contenido  concreto  ha  sido  empleado  para  la 
elaboración  de  ese  nuevo  contenido.  Ello  es  indispensable  para  garantizar  que  los 
creadores y otros titulares de derechos sean remunerados por la utilización de sus obras 
o prestaciones.  
Por otro lado, se exige la regulación de  un  sistema de  responsabilidad para aquellos 
casos  en  los  que  sistemas  autónomos  generen  contenido  que  vulnere  DPI.  Si  el 
resultado generado por un sistema autónomo transforma o reproduce contenido ajeno 
¿ante quién deben reclamar los titulares de derechos afectados?. La falta de respuesta 
legal a esta cuestión no puede dejar desamparados a los creadores. 
En este sentido, en la Resolución del Parlamento Europeo, de 20 de octubre de 2020, 
con recomendaciones destinadas a la Comisión sobre un régimen de responsabilidad 
civil en materia de inteligencia artificial (2020/2014(INL)) se considera que es necesario 
realizar adaptaciones específicas y coordinadas de los regímenes de responsabilidad 
civil para evitar situaciones en las que personas que sufran un daño o un menoscabo a 
su patrimonio por el empleo de sistemas de IA acaben sin indemnización. 
SEGUNDO: CONTENIDO CREADO POR SISTEMAS DE INTELIGENCIA ARTIFICIAL  
Buena parte de la doctrina se ha volcado en el planteamiento del reto que supone la 
protección de los resultados o contenido obtenido mediante sistemas de IA. Mientras 
que el empleo de sistemas de IA por parte de un creador como una mera herramienta 
en la creación no debe suponer ningún reto ni variación a la legislación existente, sí que 
plantea dudas la protección de los resultados de aquellos sistemas que puedan llegar a 
crear de forma autónoma.  
En primer lugar, queremos destacar que la posible protección de los contenidos creados 
por sistemas autónomos no debe suponer un menoscabo a los intereses o derechos de 
los  creadores  humanos.  Las  creaciones  obtenidas  por  máquinas  no  deben  llegar  a 
competir  ni  a  sustituir  a  las  creaciones  humanas.  Es  altamente  probable  que  las 
creaciones  obtenidas  de  forma  autónoma  por  sistemas  artificiales  lleguen  a  ser 
monopolio de unas pocas empresas tecnológicas, por lo que se ha de evitar que estas 
3 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
fitz_2665590_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665590.pdf,1,1,2665590,attachments/2665590.pdf#page=1,Is the line order correct?,"<pre>Putting startups at the heart of AI innovation - CroAI’s opinion on the European
Commission’s Artificial Intelligence Act
In April 2021, the European Commission (EC) published its much-awaited Artificial Intelligence
Act (AIA), the first global attempt to establish a legal framework for a technology that, as the AIA
states, carries both benefits and risks to humans and society. The Croatian AI Association
(CroAI) welcomes the EC’s efforts to set its own approach to AI, as it previously did with privacy.
However, our main concern is that the AIA does not adequately address the needs of start-ups,
who are the main drivers of innovation.
CroAI believes that the AIA must be an enabler of AI innovation and strongly stand behind
startu-ps, especially during prototyping and testing while pursuing a product-market fit. We
therefore advocate for the AIA to include unequivocal support for innovators by mandating the
following measures:
1.
Startups are allowed to create their own sandboxes on a case-by-case basis rather than
a one-size-fits all approach, due to the unique nature of each test.
2.
Start-ups will follow a Code of conduct. The Code of conduct will help them mitigate risks
while testing through tools such as limiting the number of test users, human oversight,
purchasing insurance, transparency, and accountability.
3.
While in their sandboxes, startups do not need to involve supervisory authorities. Still,
they are accountable for complying with the Code of conduct.
4.
When leaving their sandboxes, which means that they have found a product-market fit,
startups need to invest in fully complying with AIA rules and regulations, which will make
much more sense at that time in a product’s development cycle.
The principal concern founders and investors have when thinking about doing AI in Europe is
the cost and unpredictability of complying with the AIA. They see it as an unnecessary risk and
a burden that they can easily avoid by moving a start-up to some other innovation hub in the
world. CroAI believes that by integrating these measures into the AIA, the EC will address most
of those concerns and make the EU an excellent choice for AI innovation.
CroAI is at the European Commission’s disposal to elaborate more on the reasoning behind this
proposal and how to get it to life.
</pre>",POSITIVE
tika_2662182_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662182.pdf,5,3,2662182,attachments/2662182.pdf#page=3,Is the line order correct?,"<pre>3 
 


parezca real ha sido elaborado por medios automáticos, en nuestra opinión, también se 


debe exigir la divulgación de qué contenido concreto ha sido empleado para la 


elaboración de ese nuevo contenido. Ello es indispensable para garantizar que los 


creadores y otros titulares de derechos sean remunerados por la utilización de sus obras 


o prestaciones.  





Por otro lado, se exige la regulación de un sistema de responsabilidad para aquellos 


casos en los que sistemas autónomos generen contenido que vulnere DPI. Si el 


resultado generado por un sistema autónomo transforma o reproduce contenido ajeno 


¿ante quién deben reclamar los titulares de derechos afectados?. La falta de respuesta 


legal a esta cuestión no puede dejar desamparados a los creadores. 





En este sentido, en la Resolución del Parlamento Europeo, de 20 de octubre de 2020, 


con recomendaciones destinadas a la Comisión sobre un régimen de responsabilidad 


civil en materia de inteligencia artificial (2020/2014(INL)) se considera que es necesario 


realizar adaptaciones específicas y coordinadas de los regímenes de responsabilidad 


civil para evitar situaciones en las que personas que sufran un daño o un menoscabo a 


su patrimonio por el empleo de sistemas de IA acaben sin indemnización. 





SEGUNDO: CONTENIDO CREADO POR SISTEMAS DE INTELIGENCIA ARTIFICIAL  





Buena parte de la doctrina se ha volcado en el planteamiento del reto que supone la 


protección de los resultados o contenido obtenido mediante sistemas de IA. Mientras 


que el empleo de sistemas de IA por parte de un creador como una mera herramienta 


en la creación no debe suponer ningún reto ni variación a la legislación existente, sí que 


plantea dudas la protección de los resultados de aquellos sistemas que puedan llegar a 


crear de forma autónoma.  





En primer lugar, queremos destacar que la posible protección de los contenidos creados 


por sistemas autónomos no debe suponer un menoscabo a los intereses o derechos de 


los creadores humanos. Las creaciones obtenidas por máquinas no deben llegar a 


competir ni a sustituir a las creaciones humanas. Es altamente probable que las 


creaciones obtenidas de forma autónoma por sistemas artificiales lleguen a ser 


monopolio de unas pocas empresas tecnológicas, por lo que se ha de evitar que estas</pre>",POSITIVE
PyPDF2_2661333_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2661333.pdf,2,2,2661333,attachments/2661333.pdf#page=2,Is the line order correct?,"<pre> 
 Region Västra  Götaland ’s position paper on EU AI Act  |  2021 -07-055 
2 
and innovation on the other. Thus,  it is central  that the proposal highlights  the use of so -called 
regulatory sandboxes . It is an important part in p romoting  innovation  and streamline regulatory 
compliance for future AI systems . Test and experimental environments, as well as regulatory 
sandboxes, are central to the development of reliable AI.   
For AI to contribute with desired effects, data needs to be made available. It is with the help of large 
amounts of data that AI can be used , for example, in healthcare diagnose s and contribute to 
preventive health. A prerequisite  is the possibility to share and use data in a secured manner . 
Furthermore, data needs to be of high quality.  
In the proposal, p arts of the activities  under regional responsibility  falls under  areas of possible 
high-risk AI application s. It is therefore  important that the right conditions are created  to ensure that 
the public administration s have the right tools  to make an adequate risk assessment of often 
complex AI value chains . It is of great importance that the consequences of the proposal for both 
public and private activities are analysed  in detail . The possible increased administrative burden for 
regions and municipalities also needs to be analysed . 
Furthermore, digital skills and compe tence s are key factor s. A prerequisite for sustainable 
introduction and application of trusted AI syste ms is the understanding and commitment to AI 
development . The need for digital skills  is growing  but there is simultaneously  a shortage of digital 
skills and excellence  in many parts of the EU, for example in Sweden. This is a challenge, not least 
for the development of AI. The proposal for a new AI regulation can be expected to further inc rease 
the need for digital competence in various businesses and industries. Resources need to be invested 
in skills development in both  public and private sectors. Investments in lifelong learning, efforts to 
attract i nternational talent and  improved  measure s to match the supply and demand in the labour  
market  are needed  in the EU . Region Västra Götaland therefore welcomes the European 
Commission's new coordinated plan on AI. It is important to work strategically on measures for 
skills supply, digital skills and increased investment, and to make use of the already existing 
structures at local, regional, national and European  level.  Regions have long experience of 
supporting and coll aborating with actors in different ecosystems and infrastructures at regional 
level. These already existing forms of cooperation and ecosystems should be considered in the 
future work in the field of digitalisation and AI.
 
Region Västra Götaland  
Region Västra Götaland, governed by democratically elected politicians, has around 50 000 employees and is in 
addition to regional development also responsible for providing health care and public transport for all 
inhabitants in Västra Götaland.  Västra Götaland is home to 1.7 million inhabi tants.  As a large procuring 
organisation and employer, Region Västra Götaland has the ambition to act as a forerunner within sustainable 
development and as test bed for new ideas and innovations. Together with the 49 municipalities, trade and 
industry, org anisations and academia, we drive development with Västra Götaland’s best interests as objective.  
 
 </pre>",POSITIVE
PyPDF2_2663366_9,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663366.pdf,13,9,2663366,attachments/2663366.pdf#page=9,Is the line order correct?,"<pre> 
  29 July 2021  
 
 
 
9 set nutrition labels,” that can potentially provide a snapshot of how a system was developed and how 
it performs. These tools, however, are still being developed. There are no clear standards for such 
documentation efforts, nor have they yet been demonstra ted to be practical at a meaningful scale.  
 
7. Conformity assessments  
 
 Self-assessments  – CIPL believes that relying on organisations’ self-assessments  for most high -risk AI 
systems strikes a good balance between innovation and the preservation of fundamental rights . 
Requiring prior approval  for all high -risk AI systems would be harmful to innovation  (especially as the 
Commission  notes that expertise for AI auditing is only now being accumulated ). Many organisations 
have already established  self-assessment proce sses in the context of GDPR  and will be able to leverage 
them for high -risk AI assessment purposes.  
 No double reporting  – CIPL underlines that i t would be  overly burdensome  for all products that require 
conformity assessments under legislation listed in Annex II  to also be subject  to additional AI -specific 
requirements set out in the AI Act . Some video conferencing products ’ endpoints, for example, require 
conformity assessments under the Radio Equipment Directive listed in Annex II of the AI A ct. The use 
of AI in such systems is intended to enhance collaboration between employees and is therefore 
unlikely to be high -risk. Although the infrastructure with conformity assessment bodies is well -
established and efficient, it will be essential to make sure th at there is no double reporting  requirement  
and that these bodies are properly equipped to deal with this new role .  
8. Regulatory Sandboxes  
 
 More clarity  – CIPL welcomes the inclusion of a legal basis for regulatory sandboxes in the AI Act. The 
relevant provisions should be underpinned by clear  and unambiguous rules for those making use of 
sandboxes, including sufficient guidance to regulators about their operation , and the need for 
consistency in approaches.  In particular, the provisions should address how insights and learnings 
obtained from the regulatory sandbox exercises can inform the policy making process of the AI Act 
(including modifications to the  Annexes) , as well as its enforcement.  
 
 Incentives - The AI Act needs to more clearly lay out the incentives for organisations  to join sandboxes 
and the outcomes  they  can expect.8 CIPL  encourage s the Commission and the relevant regulators to 
work with industry to : (1) think about the specific functioning of sandboxes ; (2) set  them up in a way 
that truly helps companies to drive innovation in a protected environment to unearth learnings for all 
stakeholders involved ; and (3) enable sandboxes to reach bey ond SMEs and  be made more inclusive. 9 
                                                 
8 See CIPL Paper Regulatory Sandboxes in Data Protection – Constructive Engagement and Innovative Regulation in 
Practice . 
9 The AI Act currently prioritises regulatory sandboxes to small -scale providers and start -ups. The possible impact on 
a level playing field needs to be assessed as sandboxes can only take in a number of applications at any given time, 
and this is likely to be far smaller a number than the amount of AI innovations being developed in the market place.  </pre>",POSITIVE
tika_2662473_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662473.pdf,2,1,2662473,attachments/2662473.pdf#page=1,Is the line order correct?,"<pre>Fon +32 2 282 05-50 
info@dsv-europa.de 


 
 
 
 
www.dsv-europa.de 
Transparenzregister 
Nr. 917393784-31 


 
 
 
 
Deutsche Sozialversicherung 
Europavertretung 
Rue d’Arlon 50 
B-1000 Bruxelles 





Die öffentliche Konsultation der EU-
Kommission zum Vorschlag eines „Ge-
setzes über Künstliche Intelligenz“, COM 
(2021)206 final  
Stellungnahme der Deutschen Sozialversicherung vom 
14.07.2021 


Die Deutsche Rentenversicherung Bund (DRV Bund), die Deutsche Gesetzliche 
Unfallversicherung (DGUV), der GKV-Spitzenverband und die Verbände der ge-
setzlichen Kranken- und Pflegekassen auf Bundesebene und die Sozialversiche-
rung für Landwirtschaft, Forsten und Gartenbau (SVLFG) haben sich mit Blick auf 
ihre gemeinsamen europapolitischen Interessen zur „Deutschen Sozialversiche-
rung Arbeitsgemeinschaft Europa e. V.“ zusammengeschlossen.  


Der Verein vertritt die Interessen seiner Mitglieder gegenüber den Organen der 
Europäischen Union (EU) sowie anderen europäischen Institutionen und berät die 
relevanten Akteure im Rahmen aktueller Gesetzgebungsvorhaben und Initiativen.  


Die Kranken- und Pflegeversicherung, die Rentenversicherung und die Unfallver-
sicherung bieten als Teil eines gesetzlichen Versicherungssystems wirksamen 
Schutz vor den Folgen großer Lebensrisiken. 


Stellungnahme 


Die Deutsche Sozialversicherung begrüßt den Entwurf eines Gesetzes über 
Künstliche Intelligenz (KI). Er hat weitreichende Auswirkungen auf die Entwicklung 
und den Einsatz von KI, nicht zuletzt auch im Bereich der öffentlichen Verwaltung. 
Die Deutsche Sozialversicherung sieht mögliche mitgliedschafts-, beitrags- und 
leistungsrechtliche Bezüge und ist sich ihrer Verantwortung im Umgang mit KI be-
wusst. Sie begrüßt daher eine Klärung unter anderem der ethischen und haftungs-
rechtlichen Fragen. Dabei wird im Rahmen der weiteren Verhandlungen des Ver-
ordnungsentwurfs und darüber hinaus zu klären sein, bis zu welcher Detail- und 
Entscheidungstiefe ein europäisches Handeln erforderlich ist.</pre>",POSITIVE
pdfminer_2665624_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2665624.pdf,11,4,2665624,attachments/2665624.pdf#page=4,Is the line order correct?,"<pre>monitoring” or “serious 
incident 
A device is considered a high-
risk AI system, if the following 
two conditions are met (article 
6):  
(a) the AI system is intended to 
be used as a safety component 
of a product, or is itself a 
product, covered by the Union 
harmonization legislation listed 
in Annex II; 
(b) the product whose safety 
component is the AI system, or 
the AI system itself as a 
product, is required to 
undergo a third-party 
conformity assessment with a 
view to the placing on the 
market or putting into service 
of that product pursuant to the 
Union harmonization 
legislation listed in Annex II. 
Medical devices are covered 
by the regulations listed in 
Annex II, because the MDR 
and IVDR are mentioned. To 
fulfill the MDR, medical 
devices class IIa and higher 
must undergo a conformity 
assessment procedure.  
Does this mean all software 
as medical device using AI is 
considered to be a high-risk 
product? 
MDR rule 11 classifies 
software, independent of risk, 
in most cases in class IIa or 
higher. Thus, the extensive 
requirements for high-risk 
products would apply for 
medical devices. The negative 
effects of rule 11 would be 
amplified by the AI act. 
In article 10 the AI act requires  
„training, validation, and 
testing data sets shall be 
relevant, representative, free of 
errors and complete. “ 
Real-world data is rarely “free 
of error” and “complete”. It is 
also unclear what “complete” 
means. Do all datasets need 
to be available (whatever this 
means) or is the complete 
data of one dataset required? 
Recital (31) states: The 
classification of an AI system as 
high-risk pursuant to this 
Regulation should not 
necessarily mean that the 
product whose safety 
component is the AI system, or 
the AI system itself as a 
product, is considered ‘high-
risk’ under the criteria 
established in the relevant 
Union harmonization 
legislation that applies to the 
product. This is notably the 
case for Regulation (EU) 
2017/745 of the European 
Parliament and of the 
Council47 and Regulation (EU) 
2017/746 of the European 
Parliament and of the 
Council48, where a third-party 
conformity assessment is 
provided for medium-risk and 
high-risk products. 
→ This should be considered in 
the AI act 
This requirement should be 
annulled. More suitable seems 
the requirement, that 
manufactures must define 
quality standards and verify 
their compliance. Another 
possible requirement could be 
the claim that the definition of 
the quality standards has to be 
risk-based.  
Further, definitions i.a. “correct” 
are needed.  
 Johner Institut GmbH 
Reichenaustraße 1 
78467 Konstanz 
T +49 7531 94500-20 
info@johner-institut.de 
johner-institut.de  
 
 
 
 
 
 
 
 
</pre>",POSITIVE
PyPDF2_2665406_3,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665406.pdf,10,3,2665406,attachments/2665406.pdf#page=3,Is the line order correct?,"<pre> 
3 
 systems that are used to predict, prescribe, or make decisions  with effects on human beings . It 
underlines the  socio- technical nature of such systems, which are inevitably  embedded in societal 
contexts that need to be taken into account when assessing the implications of their use.3 We 
strongly agree with the European Center for Not -for-Profit Law’s position  that the risks and 
opportunities of AI systems should not be judged on a binary basis. Instead, it is necessary to 
account for the targeted population, context , and situation when considering the risks and 
opportunities of these systems.4 In addition, the  definition setting the  scope of the Act  focuses on  
the type of technology involved , which opens the d oor for operators to circumvent  the scope of its 
provision  by denying that their system falls under the respective definition.  
II In general, the proposed Act covers uses by both public and private actors , which presents a  critical 
opportunity to streamline requirements on uses where fundamental rights are concerned , 
regardless of the actors involved . However, specific provisions are limited to public author ities. 
Some of them explicitly mention that they also apply to private actors acting on behalf of public 
authorities or in the framework of Public -Private -Partnerships. At the same time, the absence of 
this key addition in other provisions indicates that the extension to such private actors does not 
automatically apply, creating potential loopholes public authorities could try to exploit by 
outsourcing the use of certain systems.  
III Another aspect that is important for the effective  protection of  fundamental rights and that we 
welcome is the applicatory geographical scope of the proposed Act , which would apply whenever 
an AI -based system is used within the EU, regardless of where the operator is based —or whenever 
an output of such a system is used within the EU, regardless of where the system itself is based  
(Art. 2(1)) . The wide extraterritorial effects this implies ensure that geographical loopholes 
cannot be exploited to evade the Act’s reach , guaranteeing protect ion across the Union . At the 
same time, focusing  on the location where a system is used  implies that neither the development 
nor the sale and export of any systems are covered  by the Act  if they are put to use elsewhere—
including systems whose use would be  prohibited or classified as high -risk according to the Act . 
From a fundamental rights perspective, this creates a protection vacuum for people in third states , 
whose rights could be infringed by the uses of AI systems developed by EU -based providers .  
IV A related loophole stems from excluding from the scope of the proposed regulation any systems 
used by public authorities in third states or international organizations in the framework of international law enforcement and judicial cooperation with the EU or i ts Member States (Art. 2(4)).  
/ We call on the Council and the Parliament to clarify the applicatory scope of the AI Act with 
regard to the above aspects, making sure it is a coherent , consistent , and reliable instrument 
for protecting human beings from violations of their fundamental rights caused by the use 
of ADM systems—regardless of the specific technology or the type of actors involved.  
2 Mitigate the  Self -Defeating Potential of the Risk -based A pproach  
It is a key achie vement that the Commission recogni zes that the use of AI -based systems can come 
with serious risks for fundamental rights and that these risks  need to be addressed by a 
governance framework , and this is an important message by  itself that should be recognized as such. 
We are relieved to see that the approach has improved compared to the White Paper, recognizing 
sensitive areas, such as when AI systems are used for recruiting, to evaluate creditworthiness, to 
determine access to social be nefits, for predictive policing, to control migration , and to assist judicial 
interpretation. Furthermore, the misleading criterion ‘sector’ to determine high -risk AI practices has 
                                                 
3 Following our definition, ADM systems encompass the design procedures to gather data, the c ollection of data, the 
development of algorithms to analyze the data, the interpretation of the results of this analysis based on a human -defined 
interpretation model, and the automatic action based on the interpretation as determined in a human -defined de cision -
making model.  
4 ECNL Position Statement on the EU AI Act, 23 July 2021, https://ecnl.org/sites/default/files/2021 -
07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf  </pre>",POSITIVE
fitz_2665616_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665616.pdf,3,3,2665616,attachments/2665616.pdf#page=3,Is the line order correct?,"<pre> 
14. In order to make these provisions effective, FNA calls for greater market vigilance by a dedicated service being empowered to taking 
emergency measures to restore competition. In 2010, the European Economic and Social Committee (EESC) recommended this continuous 
market surveillance to ensure consumer’s choice: “fair competition is the best way to promote economic efficiency, consumer choice and 
safety in the car repair market. The actual choice should be analysed regularly. If the analysis concludes that there is market distortion, any 
corrective measures to be introduced should be assessed.7” 
15. National competition authorities do need more complete powers, in order to give consumers the best possible protection from 
anticompetitive behaviours, as stressed by Commissioner Margrethe VESTAGER’s statement.8 They need the financial and human resources 
to collect and go through the evidence. 
Federation of Craft Businesses in the automotive sector and in mobility services (FNA)                                                    August 2021 
 
7 Information Report of the European Economic and Social Committee (EESC) INT / 501 of 6 September 2010, paragraph 1.1 
 Collision damage claims management: how to ensure the consumer's freedom of choice and security? INT/501 – CESE 395/2010 fin EN/o-FR/NT/nm 
8 Statement 17/726 by Commissioner VESTAGER on Commission proposal to make national competition authorities even more effective enforcers for the 
benefit of jobs and growth, 22 March 2017 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
pdfminer_2663276_69,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2663276.pdf,82,69,2663276,attachments/2663276.pdf#page=69,Is the line order correct?,"<pre>- 
- 
For images, visual scene synthesis. 
For any data type, the extraction of representative (or typical) exemplars from a dataset, and 
of particularly atypical exemplars as well (respectively called prototypes and criticisms: Kim, 
2016). 
Explainable feature engineering 
This last type of pre-modelling explanatory method stems from the observation that an explanation 
for a predictive model is only as good as the predictive features it relies on. Therefore, particular care 
must be taken to the feature engineering stage when designing an ML system, i.e. to the construction 
of predictor variables from original variables in order to adequately re-structure the training data for 
the algorithm. 
Two such methods should be mentioned (Murdoch, 2019): 
- 
The intervention of domain experts, who are sufficiently knowledgeable about the source data 
to extract variables (combination of other variables, intermediate computation results, etc.) 
which  increase  a  model’s  predictive  accuracy  while  maintaining  the  interpretability  of  its 
results.  In  other  words,  human  expertise  enables  in  certain  cases  to  sidestep  the  usually 
inevitable trade-off between efficacy and explainability of an ML model (cf. section 10.1.1). 
-  A modelling-based, automated approach: usual data analysis techniques are then used, such 
as dimensionality reduction and clustering, so as to extract predictor variables as compact and 
representative as possible. 
11.2.  Explainable modelling 
Some  methods  enable  simultaneously  training  the  predictive  model  and  building  an  associated 
explanatory model. This category of explanatory method is referred to as explainable modelling. 
Such methods are however far less frequently implemented than pre- and even more post-modelling   
explanatory approaches, for several reasons:  
- 
- 
- 
Explainable  modelling  requires  access  to  the  source  code  which  produces  the  predictive 
model, and the possibility to modify the algorithm. On the contrary, access to the model itself 
is sufficient for post-modelling explanatory methods, which makes them much more widely 
applicable. 
Explainable modelling is useful when explanations are necessary as early as the design phase 
of the ML algorithm, which demands a more mature engineering methodology and adequate 
planning during the introduction of AI into a business process. 
Lastly, explainable modelling is not very suitable for audit, all the more so when the predictive 
model is only available as a black box, without a documentation of the algorithm itself. 
The primary, highly ambitious goal of explainable modelling is to avoid as much as possible the already 
mentioned  trade-off  between  efficacy  and  explainability,  as  they  strive  to  provide  additional 
explainability without necessarily sacrificing predictive accuracy. 
A few methods for explainable modelling are described in what follows. 
Intrinsically explainable models 
An  intrinsically  explainable  model  can  be  chosen  from  the  outset,  for  example  linear  models  or 
decision-tree-based models. This is the most trivial kind of explainable modelling approach, assuming 
that  the  simplicity/efficacy  trade-off  is  kept  in  mind,  and  that  the  specific  model  produced  by  the 
69 
 
 
</pre>",POSITIVE
PyPDF2_2665600_2,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665600.pdf,6,2,2665600,attachments/2665600.pdf#page=2,Is the line order correct?,"<pre>Page 1 of 5 
 1. Introduction  
The technical complexity , pervasive penetration and multifaceted  social  implications of the use of artificial 
intelligence (“AI”) in every -day life of EU citizens unequivocally  necessitate the introduction of clear and 
sensible rules for development, deployment and use of AI. The endeavors in this respect of the European 
Union institutions, and of the European Commission  in particular , are laudable.  The strive to shap e those 
rules in a way that promote s innovation  and the uptake of AI  in the European Union  (the “ EU”) is also 
praiseworthy .  
However, any future regulation of AI should also be introspective, smart and  fair. Introspective  in the sense 
that it should learn from and avoid deficiencies and past mistakes in other relevant areas of business 
regulation  (e.g. data protection, antitrust) . Smart in the sense that it should have built in already now a 
conceptual design and  a toolkit that would allow addressing issues arising from the ad vance in AI (e.g. the 
emergence of strong and  general AI). Fair  in the sense that a future regulation of AI  should – while creating 
a favourable business environment – also account and cater for the interest s of society at large , including  
end users and citizens m ore generally . 
This position paper focuses on analysis of the proposal for a regulation of the  European Parliament and the 
Council laying down harmonized  rules on artificial intelligence ( Artificial Intelligence Act)  (the “ Draft AI 
Regulation ” or the “Draft ”) and recommendations on the conceptual design of a nd certain fundamental 
rules  under the Draft with view to the need of an introspe ctive, smart and fair legal framework for design, 
development, deployment  and use  of AI in the EU.   
 
2. Risk -based approach and ethical/human rights backbone  
The Draft A I Regulation  retains the risk -based approach and the ethical/human rights backbone  initially 
contemplated in the European Commission’s White Paper on AI1 and the recommendations of the 
European Parliament of October 2020,2 while  build ing upon regulation in other areas ( e.g. data protection, 
consumer protection, standardizatio n) to address the more specific technical aspects of AI design, 
development, deployment and use. This approach generally makes sense give n how contextual ly 
dependent  the deployment and use of AI  are and how quickly the technology evolves.  Too legalistic 
requirements might result in under - or overregulation.  
However, clearer e mphasis needs to be put  in the final text  of the Artificial Intelligence Act on outcomes , 
rather than on formal course of action . Art. 13 to Art. 15 of the Draft already  require attainment of certain 
overall outcome s (transparency , accuracy, etc.). Yet, other obligations  under Chapters  2 and 3  of Title II I 
presuppose mere process -like actions (e.g. documen ting, record -keeping, risk management based on step -
by-step actions) in order to demonstrate compliance. If the emphasis in this framework does not firmly and 
eloquently lie with ultimate outcomes  – and that various  processes are one of the means to that end – 
compliance would  morph into a  “box-ticking” - and “window -dressing” -type of adherence to the formal 
requirements under the Artificial Intelligence Act , as is the case with similar rather “technical ” requirements  
under the European Union Genera Data Protection Regulation.3 
 
3. Prohibited use of AI  
The explicit bans under Art.  5 of the Draft AI Regulation are a welcome development in an attempt to limit 
the use of AI to manipulative and/or privacy -invasive ends .  
However, the proposed language and approach have some deficiencies, as set out below.   
3.1 AI employing subliminal techniques or exploiting vulnerabilities  </pre>",POSITIVE
PyPDF2_2662770_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662770.pdf,6,2,2662770,attachments/2662770.pdf#page=2,Is the line order correct?,"<pre> 
Mouvement des Entreprises de France  
Legal Department  - July 2021  2 On 21 April  2021, the European Commission proposed  a new  regulation on Artificial Intelligence (A I) aimi ng 
at providing a  harmoni zed legal  framework at European level.  
It is essential today to build a European ecosystem for in novation and AI that respects European values. The 
health crisis has indeed revealed the importance of technological and digital infrastructures and the fact that 
Europe must equip itself with such infrastructures, in particular AI. To do this, the develop ment of AI must 
be made available to all companies, whatever their size, whatever their resources.  
Indeed, to build a trustworthy, efficient and sustainable AI framework, regulations must benefit citizens, 
but also European businesses by allowing them to d evelop and benefit from cutting -edge technologies in 
order to remain competitive.  
This European initiative is all the more important as it aims to prepare the ground  for AI regulation worldwide.  
While MEDEF welcomes the work carried out  by the European Co mmission and considers that the risk -based 
approach adopted in the proposed regulation is the best possible approach insofar as it promotes 
confidence in AI, this proposal  raises man y concerns and question s, in particular  with regard to : 
− the legal uncertainty linked to very broad and insufficiently precise definitions;  
− the lack of consistency, or even the incompatibility, of this proposal with other European texts 
(GDPR, Machinery Directive, etc.);  
− the sometimes very heavy or even disproportionate obligations.  
On genera l provisions  
The risk -based approach of the European Commission seems to be the best possible approach as it fosters 
confidence in AI without hampering its responsible development. It is entirely relevant to define the 
obligati ons and requirements according to the risk (high or low risk) of the technology and its use.  
Nevertheless, it is essential to keep a margin of innovation . Particular attention must therefore be paid to 
definitions, in particular those of AI systems an d high -risk systems, because the related obligations and 
requirements are very onerous and difficult to implement. Clear and sufficiently precise definitions are all 
the more important as they will be called upon to serve as references in other texts. They  must therefore 
not lead to the creation of legal uncertainty . 
Some  concepts, such as ""known and foreseeable risks"", ""reasonably foreseeable misuse"", ""generally 
ackno wledged  state of the art"", are for example very unclear  and can therefore create legal unce rtainty 
leading to different interpretations according to countries or authorities . 
On the definition of artificial intelligence    
 Article 3 (1) defines an AI system as “ software that is developed with  one or more of the techniques and 
approaches listed in Annex I and can, for a given set of human -defined objectives , generate outputs  such 
as content, predictions, recommendations , or decisions i nfluencing the environments they interact  with ”.  
If we understand the Commission's objective of making this regulation neutral and adaptable to 
technological developments,  the current proposed definition of AI (and the list of techniques in Annex 
I) is very broad in that it can include all types of systems or sof tware applications that do not involve 
the same risks. The inclusion of such systems or applications within the scope of the regulation would risk 
hampering innovation in technology companies, especially smaller ones. However, in a context of 
international  competitiveness, it is essential to encourage technological development and not to prevent 
SMEs from accessing these markets.  
In general, if AI can involve risks in its implementation, it is mainly with regard to its direct or indirect 
impact on in dividuals. However, it should be remembered that many uses of AI systems have little impact 
on individuals. This is the case, for example, with AI methods for internal modeling needs (for example 
ALM models for the banking sector) or for corporate scoring.  
It should also be noted that some AI  systems  even have the potential to increase the productivity of 
companies or the well -being of the workforce, in particular by efficiently distributing tasks between 
humans and machines, by providing tools  for skills develop ment and providing access to better working 
conditions, in particular health and safety.  
In this sense, it would be useful, on the one hand, not to put  all systems or applications to the same 
scale. A benefit / risk balance of technological developments should be put in place in order to promote </pre>",POSITIVE
PyPDF2_2665582_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665582.pdf,3,2,2665582,attachments/2665582.pdf#page=2,Is the line order correct?,"<pre> 
2   This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. 786641   
SHERPA (Shaping the ethical dimensions of smart information systems (SIS) – a European perspective) is a project that focuses on ethical and human rights aspects of smart information systems (artificial intelligence and big data analytics).   Regulatory governance systems are a key part of the SHERPA Final Recommendations,1 which include a call for creating an EU regulatory framework2 and establishing an EU Agency for AI.3 SHERPA believes a robust, mandatory legal framework at the EU level is needed to ensure that ethical issues and human rights concerns related to AI are adequately addressed. There are many specific elements of the SHERPA recommendations that appear in the proposed text of the Regulation, like red lines for some AI applications, mandatory requirements for high-risk AI systems, and the creation of a centralised body. However, the draft text does not do enough to protect fundamental rights, often lacks conceptual clarity, and leaves many questions unanswered  For more information, please visit: https://www.project-sherpa.eu  Risk-based approach The proposed Regulation adopts a four-tiered risk-based approach, where AI systems are subject to different rules depending on the level of risk. While one purpose of the regulatory framework is to guarantee safety and fundamental rights of EU citizens, the risk-based approach adopted by the Commission may not be sufficient.  There is no reference to the EU Agency for Fundamental Rights, nor are there provisions on complaint and redress mechanisms available to those whose rights are violated by AI systems. Furthermore, the proposed regulation has a somewhat binary approach, failing to adequately take into account impacts across the spectrum of risk. Most mandatory requirements apply only to high-risk systems; by comparison, low-risk AI systems are only subject to transparency requirements and minimal-risk AI systems have no requirements. Definition of AI SHERPA recommended that AI be clearly defined in each use context with regard to relevant issues. While it is a challenge to precisely define AI, definitions used in the proposed Regulation are often overly broad and too open to interpretation. Additionally, despite attempts to be “technology neutral and as future proof as possible”, the proposed definition of AI is linked to ‘software’, leaving out potential future developments of AI. The proposal takes into account the difficulty of defining AI by moving the definition into an appendix which is subject to review and revision. While this is reasonable in light of the problematic nature of the term AI, it does add to uncertainty about the future scope of the Regulation.  Red lines Under the proposal’s risk-based approach, AI systems that pose the highest level of risk to fundamental rights and safety are prohibited. The short list of banned AI systems – only four categories – includes social scoring and AI that subliminally manipulates human behaviour in a harmful way. While SHERPA welcomes the explicit inclusion of red lines in the regulatory framework, the short list is incomplete and has many loopholes.  For example, use of remote,  1 https://www.project-sherpa.eu/recommendations/ 2 https://www.project-sherpa.eu/regulatory-framework/ 3 https://www.project-sherpa.eu/european-agency-for-ai/ </pre>",NEUTRAL
PyPDF2_2665462_26,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665462.pdf,31,26,2665462,attachments/2665462.pdf#page=26,Is the line order correct?,"<pre>Access Now’s submission to the European Commission’s
adoption
consultation on the AI Act
(ii) public safety and public health, including disease prevention, control and treatment;
(iii) a high level of protection and improvement of
the quality of the environment;
While we are generally concerned about establishing
any grounds for the further processing of
personal data, due to the risk that such provisions
pose to undermining the protections oﬀered by
existing data protection law, we are particularly
concerned about paragraph (i). Perhaps the riskiest,
and most problematic uses of AI are in the “prevention,
investigation, detection or prosecution of
criminal oﬀences or the execution of criminal penalties,
including the safeguarding against and the
prevention of threats to public security.” Such uses
of AI systems should, rather than being
incentivised or freed from scrutiny, be subject to
the utmost caution, if they are to be pursued at
all.
We therefore recommend that the use cases under paragraph
(i) be removed from Article 54.
We
also recommend that the term “innovative” be deleted
from Article 53, paragraph 2:
-
Member States shall ensure that to the extent the
innovative
AI systems involve the processing
of personal data or otherwise fall under the supervisory
remit of other national authorities or
competent authorities providing or supporting access
to data the national data protection
authorities and those other national authorities are
associated to the operation of the AI
regulatory sandbox
AI systems should fall under the remit of the authorities
regardless of whether they are innovative, so
this word should be deleted to prevent any loophole.
Regarding Article 54 in general, we support the points
raised in the EDPS-EDPB Joint Opinion that
“relationship of Article 54(1) of the Proposal to
Article 54(2) and recital 41 of the Proposal and thus
also
to existing EU data protection law remains unclear.”
As noted in the Joint Opinion, “the GDPR and the
EUDPR already have an established basis for ‘further
processing,’” and we fully support the additional
point that “balancing between the controller’s interests
and the data subject’s interests do not have to
hinder innovation.” The legal basis of the grounds
for further processing must be clarified, and the
necessity of any such grounds for further processing
be considered against the risk they pose to
undermining existing protections.
Finally, Access Now believes that the use of AI regulatory
sandboxes must be subject to the highest
level of public scrutiny and transparency. The intention
of such sandboxes is to facilitate the
development of AI systems “for safeguarding substantial
public interest”: as such, the public must be
able to know exactly what types of systems are being
developed in them. In line with initiative such as
Findata,
Finland’s innovative Social and Health Data
Permit Authority that promotes secondary use
46
of health and social data, the AI regulatory sandboxes
should provide publicly available information
regarding all requests to make use of the sandbox,
all accepted and rejected applications, information
about project currently in development in the context
of the sandbox, and follow up information on
what happens with projects a",NEGATIVE
PyPDF2_2663341_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663341.pdf,4,1,2663341,attachments/2663341.pdf#page=1,Is the line order correct?,"<pre> 
 
 About the GFII : creat ed in 1979, the GFII, the French organization of information professionals, is 
a unique association in the data landscape that brings together private and public data producers 
and re -users, such as the French Ministry of Interior, INPI, IGN, Total, BNP Pa ribas , Roquette frères, 
Saint -Gobain, Wolters -Kluwer, Elsevier, Françis Lefebvre -Dalloz group, Altarès . It gathers lawyers, 
engineers, data experts , compliance officers ... 
 
The GFII aims to promote the economy of data, that means the recognition of the cost s necessary 
for their manufacture, maintenance, development and dissemination in an assumed commercial 
environment, which does not exclude free of charge data sharing, but which puts more emphasis 
on the interoperability of data, their valorisation and the ir reusability. The 6 working groups 
produce positions papers and white papers in order to help shaping the future of data policy in 
France and in the EU by offering a balanced and economically sustainable  point of view  about data 
and IA . The GFII promotes a sustainable and ethical use of data and work s closely with its members 
for offering the most expert and efficient feedback about the implementation of data policies.  GFII 
members may be AI systems providers, users or both.  
 
REGULATION OF THE EUR OPEAN PARLIAMENT AND OF THE COUNCIL LAYING 
DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL 
INTELLIGENCE ACT)  
First of all, thanks to the European Commission to enable the GFII to answer this consultation on the 
draft regulation on AI.  
 1) Rea dability of the text  
The draft is rather complex to understand; understanding difficulties may then generate difficulties for 
being compliant, especially for SMEs and start up.  
 a) We invite then the legislator:  
- to amend the structure of the document by  separating:  
- requirements dedicated to AI systems defined in the article 6 (1) / annex II and possibly 
Annex III (1)  
- requirements dedicated to AI systems defined in the article 6 (2)  
- requirements dedicated to AI systems defined in the annex III (6 t o 8) 
- to define the requirements from the AI provider point of view, ie the operator that will have to comply 
with the future regulation, with a logical ie process oriented redactional architecture, step by step, 
</pre>",POSITIVE
pdfminer_2665504_10,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665504.pdf,11,10,2665504,attachments/2665504.pdf#page=10,Is the line order correct?,"<pre>(d) to determine the safety and compatibility with potential recipients;
(e) to predict treatment response or reactions;
(f) to define or monitoring therapeutic measures.
Later on, in Chapter  II of that regulation,  EU regulates more specifically the requirements 
regarding performance, design and manufacture.  Here, all articles apply also to what would be
called “AI” systems, given the previous definition.  But some extra specificities are added, in 
point 16: 
16.   Electronic programmable systems — devices that incorporate electronic programmable 
systems and software that are devices in themselves
16.1.   Devices that incorporate electronic programmable systems, including software, or 
software that are devices in themselves, shall be designed to ensure repeatability, reliability 
and performance in line with their intended use. In the event of a single fault condition, 
appropriate means shall be adopted to eliminate or reduce as far as possible consequent risks
or impairment of performance.
16.2.   For devices that incorporate software or for software that are devices in themselves, the 
software shall be developed and manufactured in accordance with the state of the art taking 
into account the principles of development life cycle, risk management, including information 
security, verification and validation.
16.3.   Software referred to in this Section that is intended to be used in combination with 
mobile computing platforms shall be designed and manufactured taking into account the 
specific features of the mobile platform (e.g. size and contrast ratio of the screen) and the 
external factors related to their use (varying environment as regards level of light or noise).
16.4.   Manufacturers shall set out minimum requirements concerning hardware, IT networks 
characteristics and IT security measures, including protection against unauthorised access, 
necessary to run the software as intended.
We see here that the article regulated important aspects also pursued by the proposal of 
regulation on AI: security, availability, performance qualification, …  It is not the aim of this note 
to dive more into  IVD regulation, but the interested reader can verify that many other aspects 
of interest in the project of regulation on AI are properly dealt within that regulation, without 
once entering into the details of the technologies (which makes it stronger, more future-proof).
Proceeding this way is also a guarantee that any manufacturer of a specific kind of product, 
service, application will manage properly with the necessary requirements.  It will also 
guarantee there is no conflict between an application-specific regulation and a technology-
specific regulation. 
Copyright @T.Helleputte – 2021
Page 10/11
</pre>",POSITIVE
PyPDF2_2662780_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2662780.pdf,4,1,2662780,attachments/2662780.pdf#page=1,Is the line order correct?,"<pre>Education International  
Internationale de l'Education  
Internacional de la Educación  
 
http://www.ei -ie.org  
EUROPEAN REGION - 
ETUCE  
 
President  
Larry FLANAGAN  
 
Vice-Presidents  
Odile CORDELIER  
Andreas KELLER  
Trudy KERPERIEN  
Dorte LANGE  
Galina MERKULOVA  
Branimir STRUKELJ   
 
 
 
Boulevard Bischoffsheim, 15 
1000 Brussels , Belgium  
Tel +32 2 224 06 91/92  
Fax +32 2 224 06 94  
secretariat@csee -etuce.org  
http://www.csee -etuce.org  
 
European Director  
Susan FLOCKEN  
 
Treasurer  
Joan DONEGAN  
 
 
 
ETUCE  
European Trade Union Committee for Education  
EI European Region  
 
  
ETUCE position on the EU Regulation on  Artificial  Intelligence  
(Adopted b y the ETUCE Bureau on 7 June 2021 ) 
Background:  
On 21 April 2021, the European Commission published a proposal for a “Regulation on a 
European Approach for Artificial intelligence ” (the AI Regulation). With this proposal, the 
European Commission follows up on its White Paper on Artificia l Intelligence  (February 
2020), based on the results of a broad consultation process to which ETUCE  contributed . 
The aim of the initiative is t o establish the first EU legal framework  regulat ing the entire 
lifecycle of the use of Artificial Intelligence  (AI) in all sectors , including education.   
The AI Regulation classifies  the use of Artificial Intelligence in various sectors based on the  
risk that the AI tools have  on the health and safety and the fundamental rights  of 
individuals.  Concerning education, t he proposal consider s the use of A rtificial Intelligence  
tools in education as high -risk as potentially harmful to the right to education and training 
as well as the right not to be discriminated  in education. For high -risk sector s, the  AI 
Regulation  establishes  stricter  horizontal legal  requirements  to which AI tools must comply 
before being auth orised on the market. These include  risk management system during the 
entire lifecycle of the AI system.     
Following the publication of the proposal, on 26 April  2021 , the European Commission 
issue d a public consultation  that will run until 20 July 2021 , accompanied by a n impact 
assessment report .  
The following text is the ETUCE response to the public consultation bringing the perspective 
of teachers, a cademics and other education personnel on the sections  of the AI Regulation  
that touch upon the education sector.  
 
ETUCE reply : 
ETUCE  welcomes th e publication of the AI Regulation  as it sets the ground for the first 
comprehensive EU regulation  on Artificial Intelligence  to ensure a controlled development 
of AI tools in education and  address the risks connected to the ir use by teachers , academic, 
other education personnel and students . While ETUCE recognises the potential of digital 
technologies and  Artificial Intelligence tools to bring about improvements  in education , it 
also underlines the numerous ethical concern s related to their trustworthiness,  data 
privacy, accountability, transparency and their impact on equality and inclusion in 
education . ETUCE underlines  that further research  at na tional and European level  is needed 
to assess and address the risks connected to  the use of Artificial Intelligence in education  
with constant and meaningful  consultation with education social partners .  </pre>",POSITIVE
PyPDF2_2665590_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665590.pdf,1,1,2665590,attachments/2665590.pdf#page=1,Is the line order correct?,"<pre>Putting startups at the heart of AI innovation - CroAI’s
opinion on the European
Commission’s Artificial Intelligence Act
In
April
2021,
the
European
Commission
(EC)
published
its
much-awaited
Artificial
Intelligence
Act
(AIA),
the
first
global
attempt
to
establish
a
legal
framework
for
a
technology
that,
as
the
AIA
states,
carries
both
benefits
and
risks
to
humans
and
society.
The
Croatian
AI
Association
(CroAI)
welcomes
the
EC’s
efforts
to
set
its
own
approach
to
AI,
as
it
previously
did
with
privacy.
However,
our
main
concern
is
that
the
AIA
does
not
adequately
address
the
needs
of
start-ups,
who are the main drivers of innovation.
CroAI
believes
that
the
AIA
must
be
an
enabler
of
AI
innovation
and
strongly
stand
behind
startu-ps,
especially
during
prototyping
and
testing
while
pursuing
a
product-market
fit.
We
therefore
advocate
for
the
AIA
to
include
unequivocal
support
for
innovators
by
mandating
the
following measures:
1.
Startups
are
allowed
to
create
their
own
sandboxes
on
a
case-by-case
basis
rather
than
a one-size-fits all approach, due to the unique nature
of each test.
2.
Start-ups
will
follow
a
Code
of
conduct.
The
Code
of
conduct
will
help
them
mitigate
risks
while
testing
through
tools
such
as
limiting
the
number
of
test
users,
human
oversight,
purchasing insurance, transparency, and accountability.
3.
While
in
their
sandboxes,
startups
do
not
need
to
involve
supervisory
authorities.
Still,
they are accountable for complying with the Code of
conduct.
4.
When
leaving
their
sandboxes,
which
means
that
they
have
found
a
product-market
fit,
startups
need
to
invest
in
fully
complying
with
AIA
rules
and
regulations,
which
will
make
much more sense at that time in a product’s development
cycle.
The
principal
concern
founders
and
investors
have
when
thinking
about
doing
AI
in
Europe
is
the
cost
and
unpredictability
of
complying
with
the
AIA.
They
see
it
as
an
unnecessary
risk
and
a
burden
that
they
can
easily
avoid
by
moving
a
start-up
to
some
other
innovation
hub
in
the
world.
CroAI
believes
that
by
integrating
these
measures
into
the
AIA,
the
EC
will
address
most
of those concerns and make the EU an excellent choice
for AI innovation.
CroAI
is
at
the
European
Commission’s
disposal
to
elaborate
more
on
the
reasoning
behind
this
proposal and how to get it to life.
</pre>",NEUTRAL
tika_2665314_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2665314.pdf,6,2,2665314,attachments/2665314.pdf#page=2,Is the line order correct?,"<pre>Side 2 
 





We agree that under such a risk-based approach, most applications should fall outside of 


the scope of the regulation. 


4 PROHIBITED AI PRACTICES  


The Norwegian government supports the Commission's proposal for AI practices that 


shall be prohibited. However, we have a concern regarding the Article 5 – 1 (a) stating 


that an AI system that deploys subliminal techniques is prohibited if it is likely to cause 


physical or psychological harm. In our opinion, such manipulative practices are often 


used in cases that cause financial harm to people, which may in turn lead to 


psychological harm. We would propose that the prohibition of AI systems that deploy 


subliminal techniques is expanded to also include financial harm.  





Regarding Article 5 – 1 (c) the Norwegian Government supports the suggestion of 


prohibiting AI systems for evaluation or classification of the worthiness of natural persons 


where such use of AI-systems may lead to unjustified or disproportionate treatment of 


individuals. However, it should be carefully considered whether the terms 'unjustified or 


disproportionate' will provide sufficient safeguards against unwanted use of this type of 


AI-systems. The Norwegian government is concerned that this, in some cases, could 


allow for exclusion of individuals from the use of certain fundamental services that should 


be available to all, for instance health and care services.  


5 HIGH RISK AI SYSTEMS 


The Norwegian Government supports the overall approach to high risk AI systems in the 


Commission's proposal. We support the proposal for an EU database for high-risk AI 


systems. 


Determining borderline cases 


The term 'high risk system' is defined in article 6 with reference to lists in Annexes II and 


III. The Commission will be empowered to adopt delegated acts to update these lists. 


Deciding whether a system qualifies as a high-risk AI-system may require difficult 


considerations. It is our view that it may be necessary to adopt a procedure to determine 


borderline cases. We propose that such a procedure could be modelled after Article 4 in 


Regulation 2017/745 on medical devices. In addition, in order to ensure the best possible 


end user protection, we propose to include in the regulation that: 'In cases of doubt, 


where, taking into account all its characteristics, an ""AI system"" may be considered a 


""high-risk AI system"" the provisions of Title III, Chapter 2 of the Regulation apply'. Such 


provisions have been used in other EU regulations.  


Autonomous shipping 


Norway is at the forefront in the development of autonomous vessels and hosts a number 


of test sites. We recognise that safety systems for the navigation of such vessels are 


included in 'High-risk AI systems' through the inclusion of Directive 2014/90/EU in the list 


in Annex II. However, we see a growing activity around AI based systems intended for 


decision support to increase safety in harbours and fairways. This includes AI systems for</pre>",POSITIVE
PyPDF2_2665616_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665616.pdf,3,3,2665616,attachments/2665616.pdf#page=3,Is the line order correct?,"<pre> 
14. In order to make these provisions effective, FNA  calls for greater market vigilance by a dedicated service being empowered to taking 
emergency measures to restore competition.  In 2010, the European Economic and Social Committee (EESC) recommended this continuous 
market surveillance to ensure consumer’s choice:  “fair competition is the best way to promote economic efficiency, consumer choice and 
safety in the car repair market. The actual choice should be analysed regularly . If the analysis concludes that there is market distortion, any 
corrective measures to be introduced should be assessed.7” 
15. Nationa l competition authorities do need more complete powers , in order to give consumers the best possible protection from 
anticompetitive behaviours,  as stressed by  Commissioner Margrethe VESTAGER ’s statement.8 They need the financial and human resources 
to collect and go through the evidence.  
Federation of Craft Businesses in the automotive sector and in mobility services (FNA)                                                     August 2021  
 
7 Information Report of the European Economic and Social Committee (EESC) INT / 501 of 6 September 2010, paragraph 1.1  
 Collision damage claims management: how to ensure the consumer's freedom of choice and security? INT/501 – CESE 395/2010 fin EN/o -FR/NT/nm  
8 Statement  17/726 by Commissioner VESTAGER  on Commission proposal to make national competition authorities even more effective enforcers for the 
benefit of  jobs and growth, 22 March 2017  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </pre>",POSITIVE
fitz_2663276_69,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2663276.pdf,82,69,2663276,attachments/2663276.pdf#page=69,Is the line order correct?,"<pre> 
69 
 
- 
For images, visual scene synthesis. 
- 
For any data type, the extraction of representative (or typical) exemplars from a dataset, and 
of particularly atypical exemplars as well (respectively called prototypes and criticisms: Kim, 
2016). 
Explainable feature engineering 
This last type of pre-modelling explanatory method stems from the observation that an explanation 
for a predictive model is only as good as the predictive features it relies on. Therefore, particular care 
must be taken to the feature engineering stage when designing an ML system, i.e. to the construction 
of predictor variables from original variables in order to adequately re-structure the training data for 
the algorithm. 
Two such methods should be mentioned (Murdoch, 2019): 
- 
The intervention of domain experts, who are sufficiently knowledgeable about the source data 
to extract variables (combination of other variables, intermediate computation results, etc.) 
which increase a model’s predictive accuracy while maintaining the interpretability of its 
results. In other words, human expertise enables in certain cases to sidestep the usually 
inevitable trade-off between efficacy and explainability of an ML model (cf. section 10.1.1). 
- 
A modelling-based, automated approach: usual data analysis techniques are then used, such 
as dimensionality reduction and clustering, so as to extract predictor variables as compact and 
representative as possible. 
11.2. Explainable modelling 
Some methods enable simultaneously training the predictive model and building an associated 
explanatory model. This category of explanatory method is referred to as explainable modelling. 
Such methods are however far less frequently implemented than pre- and even more post-modelling   
explanatory approaches, for several reasons:  
- 
Explainable modelling requires access to the source code which produces the predictive 
model, and the possibility to modify the algorithm. On the contrary, access to the model itself 
is sufficient for post-modelling explanatory methods, which makes them much more widely 
applicable. 
- 
Explainable modelling is useful when explanations are necessary as early as the design phase 
of the ML algorithm, which demands a more mature engineering methodology and adequate 
planning during the introduction of AI into a business process. 
- 
Lastly, explainable modelling is not very suitable for audit, all the more so when the predictive 
model is only available as a black box, without a documentation of the algorithm itself. 
The primary, highly ambitious goal of explainable modelling is to avoid as much as possible the already 
mentioned trade-off between efficacy and explainability, as they strive to provide additional 
explainability without necessarily sacrificing predictive accuracy. 
A few methods for explainable modelling are described in what follows. 
Intrinsically explainable models 
An intrinsically explainable model can be chosen from the outset, for example linear models or 
decision-tree-based models. This is the most trivial kind of explainable modelling approach, assuming 
that the simplicity/efficacy trade-off is kept in mind, and that the specific model produced by the 
</pre>",POSITIVE
fitz_2665502_8,other,../24212003_requirements_for_artificial_intelligence/attachments/2665502.pdf,10,8,2665502,attachments/2665502.pdf#page=8,Is the line order correct?,"<pre> 
 
 
 
 
 
 
 
8 
authorities and – due to unequal investments in the various member states’ authorities – citizen 
protection is not at the same level in each EU country.  
Also in the context of this regulation, it should be considered that too much emphasis on the 
national level can lead to a risk of unequal implementation in different member states, at different 
speeds and potentially different interpretations. Belgium has, for instance, been lagging behind 
with regard to the implementation of the GDPR; this delay may affect innovation and a European 
level playing field, and the same risks to happen in the field of AI. Strong coordination at the 
European level will hence be crucial; also given the fact that many AI systems may be used 
transnationally and may be imported from third party countries. Moreover, given the importance 
of the risks attached to the use of AI as set out in this regulation, it will be essential that these 
authorities receive proper funding (and a sufficiently skilled workforce – which may be difficult in 
this field) so that they can provide adequate guidance for organizations and ensure a high level 
of citizen protection. 
The issue of different implementation speeds will also affect the creation of codes of conduct that 
can be voluntarily applied to AI systems other than high-risk systems. If it is assumed that the 
creation of a code of conduct is roughly the same effort and cost for any sector or member state, 
this absolute cost will mean that there may be more codes of conducts for sectors and member 
states with a higher turnover. Smaller member states with smaller markets will thus likely have 
less means to create these codes of conduct. This is another reason why a common European 
approach would be preferential. 
In this regard, the obligation to appoint an authorized representative established in the European 
Union in case an importer cannot be identified (recital 56 and article 25) is welcomed. Building on 
the experience with the GDPR, it is crucial to allow all organizations in charge of the 
implementation and enforcement of the regulation – be it the national competent authorities 
(NCAs), market surveillance authorities or other bodies – to be able to conduct all necessary steps 
towards the authorized representative, independently of where in the European Union it is 
established. Related to this point, it is essential that several NCAs can oversee the notified bodies 
and technical services performing the conformity assessment. In other words, not only the NCA 
of the country in which the notified bodies and technical services are established, but also NCA 
from other European member states should be able to do so, especially to ensure protection in 
case a specific NCA would be too under-resourced. 
Furthermore, while currently not foreseen in the proposed regulation, citizens should be provided 
with measures for redress and a right to file a complaint with national authorities, since this will 
not only help closing the protection gap of the proposal, but it can also help national authorities to 
assess and establish potential breaches of the regulation. In this way, public and private 
enforcement can be more complementary, and citizens will have a more active role in ensuring 
the protection of their rights. In the same line of thought, the link between the GDPR and this 
regulation should be highlighted. 
More than the fact that the Proposal does not, currently, foresee any mechanisms through which 
citizens can file a complaint, it seems to ignore the rights of citizens altogether. Despite the fact 
that the Recitals make numerous references to protecting “health, safety and fundamental rights,” 
the conceptual structure of the proposal is built on existing market surveillance schemes known 
from product safety legislation. The proposal seems to combine two concepts that are 
</pre>",POSITIVE
tika_2660610_7,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2660610.pdf,8,7,2660610,attachments/2660610.pdf#page=7,Is the line order correct?,"<pre>3.2.2 Art. 9.


Purpose limitation can prevent effective testing of the AI. Therefore, the fol-
lowing change to Art. 9(6) is suggested: Testing procedures shall be suitable to
achieve the intended purpose of the AI system. It is not surprising that ”in-
tended purpose” is the model of intention, but since the Regulation goes into
”actual use” in for example the prohibitions, the Commission should consider
whether to change the wording for the Regulation, regardless of its connection
to the product legislation in the EU in general10.


3.2.3 Art. 10.


Two choices are worth taking noticing in this article. Art. 10(2)(a) uses the
term ”relevant”, which poses the question as to whether there are ”irrelevant”
design choices. Irrelevant design choices can still affect the AI, so why not just
write design choices? There is little reason not to exclude ”relevant” here.


Art. 10(5) does not acknowledge the different ways which bias can be pre-
vented. It could do so to prevent the creation of the expectation that personal
data must be used in any way. This can be considered, but otherwise the section
is very agreeable.


3.2.4 Art. 14.


Human oversight is not the only solution to the dangers of AI, and while most
of the article is worded appropriately, there is one detail that could be changed.
Art. 14(2) mentions that ”human oversight shall aim”, but choosing ”shall” is
not appropriate, the human oversight must ”aim”. The use of ”shall” conflicts
with the rest of the article, and it is suggested that it is excluded, purely in Art.
14(2).


3.2.5 Art. 15.


Why is ”state-of-the-art” not used to describe the state of the (cyber)security
of the AI? Because of the consequences of failure of the defences when it comes
to AI, it is suggested that Art. 15(4), second sentence is changed to the follow-
ing: The technical solutions aimed at ensuring the cybersecurity of high-risk AI
systems shall be state-of-the-art.


This, combined with existing guidance on the subject matter, should guar-
antee a higher level of defences, which is more than adequate for such potentially
dangerous tools.


3.2.6 Art. 41.


Art. 41(3) seems either redundant (in that the high-risk AI may fulfill it re-
gardless of this paragraph) or allows for circumvention of fulfillment of Chapter


10This could be achieved with a dedicated article on how intention is perceived differently
when it comes to AI than other products, but this suggestion is not included here.</pre>",POSITIVE
fitz_2661333_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2661333.pdf,2,2,2661333,attachments/2661333.pdf#page=2,Is the line order correct?,"<pre> 
 
Region Västra Götaland’s position paper on EU AI Act |  2021-07-055 
2 
and innovation on the other. Thus, it is central that the proposal highlights the use of so-called 
regulatory sandboxes. It is an important part in promoting innovation and streamline regulatory 
compliance for future AI systems. Test and experimental environments, as well as regulatory 
sandboxes, are central to the development of reliable AI.  
For AI to contribute with desired effects, data needs to be made available. It is with the help of large 
amounts of data that AI can be used, for example, in healthcare diagnoses and contribute to 
preventive health. A prerequisite is the possibility to share and use data in a secured manner. 
Furthermore, data needs to be of high quality. 
In the proposal, parts of the activities under regional responsibility falls under areas of possible 
high-risk AI applications. It is therefore important that the right conditions are created to ensure that 
the public administrations have the right tools to make an adequate risk assessment of often 
complex AI value chains. It is of great importance that the consequences of the proposal for both 
public and private activities are analysed in detail. The possible increased administrative burden for 
regions and municipalities also needs to be analysed. 
Furthermore, digital skills and competences are key factors. A prerequisite for sustainable 
introduction and application of trusted AI systems is the understanding and commitment to AI 
development. The need for digital skills is growing but there is simultaneously a shortage of digital 
skills and excellence in many parts of the EU, for example in Sweden. This is a challenge, not least 
for the development of AI. The proposal for a new AI regulation can be expected to further increase 
the need for digital competence in various businesses and industries. Resources need to be invested 
in skills development in both public and private sectors. Investments in lifelong learning, efforts to 
attract international talent and improved measures to match the supply and demand in the labour 
market are needed in the EU. Region Västra Götaland therefore welcomes the European 
Commission's new coordinated plan on AI. It is important to work strategically on measures for 
skills supply, digital skills and increased investment, and to make use of the already existing 
structures at local, regional, national and European level. Regions have long experience of 
supporting and collaborating with actors in different ecosystems and infrastructures at regional 
level. These already existing forms of cooperation and ecosystems should be considered in the 
future work in the field of digitalisation and AI.
 
Region Västra Götaland  
Region Västra Götaland, governed by democratically elected politicians, has around 50 000 employees and is in 
addition to regional development also responsible for providing health care and public transport for all 
inhabitants in Västra Götaland. Västra Götaland is home to 1.7 million inhabitants. As a large procuring 
organisation and employer, Region Västra Götaland has the ambition to act as a forerunner within sustainable 
development and as test bed for new ideas and innovations. Together with the 49 municipalities, trade and 
industry, organisations and academia, we drive development with Västra Götaland’s best interests as objective. 
 
 
</pre>",POSITIVE
fitz_2665582_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665582.pdf,3,2,2665582,attachments/2665582.pdf#page=2,Is the line order correct?,"<pre> 
2 
 
 
This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. 786641 
 
 
SHERPA (Shaping the ethical dimensions of smart information systems (SIS) – a European 
perspective) is a project that focuses on ethical and human rights aspects of smart information 
systems (artificial intelligence and big data analytics).  
 
Regulatory governance systems are a key part of the SHERPA Final Recommendations,1 which 
include a call for creating an EU regulatory framework2 and establishing an EU Agency for AI.3 
SHERPA believes a robust, mandatory legal framework at the EU level is needed to ensure that 
ethical issues and human rights concerns related to AI are adequately addressed. There are many 
specific elements of the SHERPA recommendations that appear in the proposed text of the 
Regulation, like red lines for some AI applications, mandatory requirements for high-risk AI 
systems, and the creation of a centralised body. However, the draft text does not do enough to 
protect fundamental rights, often lacks conceptual clarity, and leaves many questions 
unanswered 
 
For more information, please visit: https://www.project-sherpa.eu 
 
Risk-based approach 
The proposed Regulation adopts a four-tiered risk-based approach, where AI systems are subject 
to different rules depending on the level of risk. While one purpose of the regulatory framework 
is to guarantee safety and fundamental rights of EU citizens, the risk-based approach adopted by 
the Commission may not be sufficient.  There is no reference to the EU Agency for Fundamental 
Rights, nor are there provisions on complaint and redress mechanisms available to those whose 
rights are violated by AI systems. Furthermore, the proposed regulation has a somewhat binary 
approach, failing to adequately take into account impacts across the spectrum of risk. Most 
mandatory requirements apply only to high-risk systems; by comparison, low-risk AI systems are 
only subject to transparency requirements and minimal-risk AI systems have no requirements. 
Definition of AI 
SHERPA recommended that AI be clearly defined in each use context with regard to relevant 
issues. While it is a challenge to precisely define AI, definitions used in the proposed Regulation 
are often overly broad and too open to interpretation. Additionally, despite attempts to be 
“technology neutral and as future proof as possible”, the proposed definition of AI is linked to 
‘software’, leaving out potential future developments of AI. The proposal takes into account the 
difficulty of defining AI by moving the definition into an appendix which is subject to review and 
revision. While this is reasonable in light of the problematic nature of the term AI, it does add to 
uncertainty about the future scope of the Regulation.  
Red lines 
Under the proposal’s risk-based approach, AI systems that pose the highest level of risk to 
fundamental rights and safety are prohibited. The short list of banned AI systems – only four 
categories – includes social scoring and AI that subliminally manipulates human behaviour in a 
harmful way. While SHERPA welcomes the explicit inclusion of red lines in the regulatory 
framework, the short list is incomplete and has many loopholes.  For example, use of remote, 
 
1 https://www.project-sherpa.eu/recommendations/ 
2 https://www.project-sherpa.eu/regulatory-framework/ 
3 https://www.project-sherpa.eu/european-agency-for-ai/ 
</pre>",POSITIVE
fitz_2665624_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2665624.pdf,11,4,2665624,attachments/2665624.pdf#page=4,Is the line order correct?,"<pre> 
 
 
 
johner-institut.de  
 
 Johner Institut GmbH 
Reichenaustraße 1 
78467 Konstanz 
T +49 7531 94500-20 
info@johner-institut.de 
monitoring” or “serious 
incident 
A device is considered a high-
risk AI system, if the following 
two conditions are met (article 
6):  
(a) the AI system is intended to 
be used as a safety component 
of a product, or is itself a 
product, covered by the Union 
harmonization legislation listed 
in Annex II; 
(b) the product whose safety 
component is the AI system, or 
the AI system itself as a 
product, is required to 
undergo a third-party 
conformity assessment with a 
view to the placing on the 
market or putting into service 
of that product pursuant to the 
Union harmonization 
legislation listed in Annex II. 
 
Medical devices are covered 
by the regulations listed in 
Annex II, because the MDR 
and IVDR are mentioned. To 
fulfill the MDR, medical 
devices class IIa and higher 
must undergo a conformity 
assessment procedure.  
 
Does this mean all software 
as medical device using AI is 
considered to be a high-risk 
product? 
 
MDR rule 11 classifies 
software, independent of risk, 
in most cases in class IIa or 
higher. Thus, the extensive 
requirements for high-risk 
products would apply for 
medical devices. The negative 
effects of rule 11 would be 
amplified by the AI act. 
Recital (31) states: The 
classification of an AI system as 
high-risk pursuant to this 
Regulation should not 
necessarily mean that the 
product whose safety 
component is the AI system, or 
the AI system itself as a 
product, is considered ‘high-
risk’ under the criteria 
established in the relevant 
Union harmonization 
legislation that applies to the 
product. This is notably the 
case for Regulation (EU) 
2017/745 of the European 
Parliament and of the 
Council47 and Regulation (EU) 
2017/746 of the European 
Parliament and of the 
Council48, where a third-party 
conformity assessment is 
provided for medium-risk and 
high-risk products. 
→ This should be considered in 
the AI act 
In article 10 the AI act requires  
„training, validation, and 
testing data sets shall be 
relevant, representative, free of 
errors and complete. “ 
Real-world data is rarely “free 
of error” and “complete”. It is 
also unclear what “complete” 
means. Do all datasets need 
to be available (whatever this 
means) or is the complete 
data of one dataset required? 
This requirement should be 
annulled. More suitable seems 
the requirement, that 
manufactures must define 
quality standards and verify 
their compliance. Another 
possible requirement could be 
the claim that the definition of 
the quality standards has to be 
risk-based.  
Further, definitions i.a. “correct” 
are needed.  
</pre>",NEUTRAL
fitz_2662780_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2662780.pdf,4,1,2662780,attachments/2662780.pdf#page=1,Is the line order correct?,"<pre>Education International 
Internationale de l'Education 
Internacional de la Educación 
 
http://www.ei-ie.org 
EUROPEAN REGION- 
ETUCE 
 
President 
Larry FLANAGAN 
 
Vice-Presidents 
Odile CORDELIER  
Andreas KELLER 
Trudy KERPERIEN 
Dorte LANGE 
Galina MERKULOVA  
Branimir STRUKELJ  
 
 
 
Boulevard Bischoffsheim, 15 
1000 Brussels, Belgium 
Tel +32 2 224 06 91/92 
Fax +32 2 224 06 94 
secretariat@csee-etuce.org 
http://www.csee-etuce.org 
 
European Director 
Susan FLOCKEN 
 
Treasurer 
Joan DONEGAN 
 
 
 
ETUCE  
European Trade Union Committee for Education 
EI European Region  
 
 
 
ETUCE position on the EU Regulation on Artificial Intelligence 
(Adopted by the ETUCE Bureau on 7 June 2021) 
Background: 
On 21 April 2021, the European Commission published a proposal for a “Regulation on a 
European Approach for Artificial intelligence” (the AI Regulation). With this proposal, the 
European Commission follows up on its White Paper on Artificial Intelligence (February 
2020), based on the results of a broad consultation process to which ETUCE contributed. 
The aim of the initiative is to establish the first EU legal framework regulating the entire 
lifecycle of the use of Artificial Intelligence (AI) in all sectors, including education.  
The AI Regulation classifies the use of Artificial Intelligence in various sectors based on the 
risk that the AI tools have on the health and safety and the fundamental rights of 
individuals. Concerning education, the proposal considers the use of Artificial Intelligence 
tools in education as high-risk as potentially harmful to the right to education and training 
as well as the right not to be discriminated in education. For high-risk sectors, the AI 
Regulation establishes stricter horizontal legal requirements to which AI tools must comply 
before being authorised on the market. These include risk management system during the 
entire lifecycle of the AI system.     
Following the publication of the proposal, on 26 April 2021, the European Commission 
issued a public consultation that will run until 20 July 2021, accompanied by an impact 
assessment report.  
The following text is the ETUCE response to the public consultation bringing the perspective 
of teachers, academics and other education personnel on the sections of the AI Regulation 
that touch upon the education sector.  
 
ETUCE reply: 
ETUCE welcomes the publication of the AI Regulation as it sets the ground for the first 
comprehensive EU regulation on Artificial Intelligence to ensure a controlled development 
of AI tools in education and address the risks connected to their use by teachers, academic, 
other education personnel and students. While ETUCE recognises the potential of digital 
technologies and Artificial Intelligence tools to bring about improvements in education, it 
also underlines the numerous ethical concerns related to their trustworthiness, data 
privacy, accountability, transparency and their impact on equality and inclusion in 
education. ETUCE underlines that further research at national and European level is needed 
to assess and address the risks connected to the use of Artificial Intelligence in education 
with constant and meaningful consultation with education social partners.  
</pre>",POSITIVE
fitz_2665452_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665452.pdf,4,2,2665452,attachments/2665452.pdf#page=2,Is the line order correct?,"<pre> 
 
 
 
 Press release 
Date  12 October 2020 
Subject 
 BMW Group code of ethics for artificial intelligence. 
Page 
 2 
 
 
 
 
Corporate Communications 
Seven principles covering the development and application of artificial 
intelligence at the BMW Group: 
• 
Human agency and oversight.  
The BMW Group implements appropriate human monitoring of decisions made by 
AI applications and considers possible ways that humans can overrule algorithmic 
decisions. 
• 
Technical robustness and safety.  
The BMW Group aims to develop robust AI applications and observes the applicable 
safety standards designed to decrease the risk of unintended consequences and 
errors. 
• 
Privacy and data governance.  
The BMW Group extends its state-of-the-art data privacy and data security 
measures to cover storage and processing in AI applications. 
• 
Transparency.  
The BMW Group aims for explainability of AI applications and open communication 
where respective technologies are used. 
• 
Diversity, non-discrimination and fairness.  
The BMW Group respects human dignity and therefore sets out to build fair AI 
applications. This includes preventing non-compliance by AI applications. 
• 
Environmental and societal well-being.  
The BMW Group is committed to developing and using AI applications that promote 
the well-being of customers, employees and partners. This aligns with the BMW 
Group’s goals in the areas of human rights and sustainability, which includes climate 
change and environmental protection. 
</pre>",POSITIVE
tika_2665430_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665430.pdf,8,2,2665430,attachments/2665430.pdf#page=2,Is the line order correct?,"<pre>its functioning.7 There is nothing similar to this in the AI Act in relation to patients. But they need 
to be informed too - to make their own decisions about health and to enjoy their dignity and 
self-determination. Also, their relevant obligations might be an important element of AI 
safety and quality.  


 
Although the requirements of informed medical consent and transparency obligations under the 
GDPR8 partly cover this issue, a technology-related and harmonized approach is needed. The rules 
on informed medical consent are not tailored to AI features (opaqueness, complexity, autonomy, 
self-learning). It might lead to negative effects both for patients (in receiving accessible and 
meaningful information) and for healthcare professionals (in deciding on their own what kind of 
information and how to provide to patients). In addition, the rules on informed medical consent are 
defined by the national EU legislations and the lack of the harmonized approach (that sets at least 
minimum standards) can hinder the development of the EU-wide relevant initiatives such as the 
proposed AI Act and creation of the common EU data spaces.9  
 
Another aspect is the obligations of subjects affected by the decisions made with the use of AI. 
In the age of the information society, correcting and update of the information about a specific 
person10 might be seen not only as a right, but also as an obligation. Taking the healthcare example, 
it is a patient who observes the final outcomes of the AI system’s use. In many cases, it is made in 
collaboration with doctors, but sometimes the outcome is visible only outside the environment of 
medical organizations and patients do not always report about the experienced side effects or 
benefits. This affects not only the non-reporting patient but also the other population (because in 
this case, AI does not receive the proper information to learn from it).11  
 
The AI Act requires AI users ‘to ensure that input data is relevant in view of the intended purpose 
of the high-risk AI system (to the extent the user exercises control over the input data).’12 
Considering this, the relevant obligation for subjects whose data is used in AI system to provide 
updates of the information to users seems to be justifiable. It would not only increase the safety and 
quality of AI applications but also support responsible and sustainable data use.13 In addition, it 
would enable AI users to properly comply with their obligations – on the basis of the information 
provided by beneficiaries of AI’s use. 
 
In a similar way all other requirements for high-risk AI systems and built in the already existing 
current conformity assessment procedures, the minimum requirements in relation to the role of 
subjects affected by the decisions made with the use of AI, can complement already existing rules 
(such the GDPR and domain-specific rights and obligations).  


 
7 EC Proposal for the AI Act, art. 29.  
8 The transparency requirements under the GDPR include, for example, informed consent when applicable and obligations to 
provide information to data subjects in articles 12-15. But the scopes of the relevant legislations are different and thus influence 
the types, form, and amount of information provided to beneficiaries of decisions made with the use of AI systems. While the aim 
of transparency obligations under the GDPR is to protect the interests of data subjects in relation to the use of their personal data, 
their rights as the subjects affected by decisions made with the use of AI shall be concentrated around the decision and its 
consequences (like safety and quality of the decisions, respect for fundamental rights, risks and benefits of the use AI). For example, 
respect for their right to self-determination (referring generally to AI-assisted decisions) is broader than informational self-
determination (which is relevant to data protection). Although the specific requirements might be tailored in accordance with the 
narrow sector of AI use, some broader guidance on the rights of subjects affected by decisions made with the use of AI is needed.  
9 European Commission, Communication to the European Parliament, the Council, the European Economic and Social Committee 
and the Committee of the Regions ‘A European Strategy for Data’ COM (2020) 66 final (EU Strategy for Data). 
10 Which is guaranteed by the GDPR.   
11 Of course, this obligation might be seen as the limitation of informational self-determination, but neither privacy nor data 
protection are the absolute rights, and they shall be balanced with other important interest (defined by law).  
12 EC Proposal for the AI Act, art. 29 (3).  
13 See more about sustainable data usage in Linnet Taylor and Nadezhda Purtova, ‘What Is Responsible and Sustainable Data 
Science?’ (2019) 1 Big Data & Society July–December 6, 1. How the AI Act is related to data initiatives such as the EU Data 
Strategy see in Kiseleva, A. & de Hert, P. ‘Creating a European Health Data Space: Obstacles in Four Key Legal Areas’, European 
Pharmaceutical Law Review, Volume 5, Issue 1 (2021), pp. 21 – 36.</pre>",POSITIVE
PyPDF2_2665430_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665430.pdf,8,2,2665430,attachments/2665430.pdf#page=2,Is the line order correct?,"<pre>its functioning.7 There is nothing similar to this in the AI Act in relation to patients. But they need to be informed too - to make their own decisions about health and to enjoy their dignity and self-determination. Also, their relevant obligations might be an important element of AI safety and quality.   Although the requirements of informed medical consent and transparency obligations under the GDPR8 partly cover this issue, a technology-related and harmonized approach is needed. The rules on informed medical consent are not tailored to AI features (opaqueness, complexity, autonomy, self-learning). It might lead to negative effects both for patients (in receiving accessible and meaningful information) and for healthcare professionals (in deciding on their own what kind of information and how to provide to patients). In addition, the rules on informed medical consent are defined by the national EU legislations and the lack of the harmonized approach (that sets at least minimum standards) can hinder the development of the EU-wide relevant initiatives such as the proposed AI Act and creation of the common EU data spaces.9   Another aspect is the obligations of subjects affected by the decisions made with the use of AI. In the age of the information society, correcting and update of the information about a specific person10 might be seen not only as a right, but also as an obligation. Taking the healthcare example, it is a patient who observes the final outcomes of the AI system’s use. In many cases, it is made in collaboration with doctors, but sometimes the outcome is visible only outside the environment of medical organizations and patients do not always report about the experienced side effects or benefits. This affects not only the non-reporting patient but also the other population (because in this case, AI does not receive the proper information to learn from it).11   The AI Act requires AI users ‘to ensure that input data is relevant in view of the intended purpose of the high-risk AI system (to the extent the user exercises control over the input data).’12 Considering this, the relevant obligation for subjects whose data is used in AI system to provide updates of the information to users seems to be justifiable. It would not only increase the safety and quality of AI applications but also support responsible and sustainable data use.13 In addition, it would enable AI users to properly comply with their obligations – on the basis of the information provided by beneficiaries of AI’s use.  In a similar way all other requirements for high-risk AI systems and built in the already existing current conformity assessment procedures, the minimum requirements in relation to the role of subjects affected by the decisions made with the use of AI, can complement already existing rules (such the GDPR and domain-specific rights and obligations).   7 EC Proposal for the AI Act, art. 29.  8 The transparency requirements under the GDPR include, for example, informed consent when applicable and obligations to provide information to data subjects in articles 12-15. But the scopes of the relevant legislations are different and thus influence the types, form, and amount of information provided to beneficiaries of decisions made with the use of AI systems. While the aim of transparency obligations under the GDPR is to protect the interests of data subjects in relation to the use of their personal data, their rights as the subjects affected by decisions made with the use of AI shall be concentrated around the decision and its consequences (like safety and quality of the decisions, respect for fundamental rights, risks and benefits of the use AI). For example, respect for their right to self-determination (referring generally to AI-assisted decisions) is broader than informational self-determination (which is relevant to data protection). Although the specific requirements might be tailored in accordance with the narrow sector of AI use, some broader guidance on the rights of subjects affected by decisions made with the use of AI is needed.  9 European Commission, Communication to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions ‘A European Strategy for Data’ COM (2020) 66 final (EU Strategy for Data). 10 Which is guaranteed by the GDPR.   11 Of course, this obligation might be seen as the limitation of informational self-determination, but neither privacy nor data protection are the absolute rights, and they shall be balanced with other important interest (defined by law).  12 EC Proposal for the AI Act, art. 29 (3).  13 See more about sustainable data usage in Linnet Taylor and Nadezhda Purtova, ‘What Is Responsible and Sustainable Data Science?’ (2019) 1 Big Data & Society July–December 6, 1. How the AI Act is related to data initiatives such as the EU Data Strategy see in Kiseleva, A. & de Hert, P. ‘Creating a European Health Data Space: Obstacles in Four Key Legal Areas’, European Pharmaceutical Law Review, Volume 5, Issue 1 (2021), pp. 21 – 36.   </pre>",NEUTRAL
pdfminer_2662226_7,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2662226.pdf,8,7,2662226,attachments/2662226.pdf#page=7,Is the line order correct?,"<pre>be complementary to strict safety measures and should not exonerate manufacturers 
from ensuring that products do not present a risk to consumers.  
inherent 
However,  the 
information  asymmetry  associated  with  AI  or  an 
evolving/machine  learning  system,  makes  the  function  of  information  different  from 
information  linked  to  traditional,  non-AI  products  (e.g.  Ecolabel)  where  the 
technological content of the product is “static”. The information is not very helpful if the 
behaviour  of  products  changes  over  time  but  the  information  stays  the  same.  One 
reason  more  for  us  to  seriously  wonder  about  the  inclusion  of  emotion  recognition 
system or a biometric categorisation system and ‘deep fake’ in the level of low-risk AI 
systems,  especially  as  consumers  will  not  benefit  from  the  right  to  opt-out  of  the 
system.  
6 | Information sharing and market surveillance 
Market  surveillance  authorities  should  have  sufficient  resources  to  enforce  the  AI 
requirements. We stress the need to ensure national supervisory authorities have the 
financial,  technical  and  technological  means  to  carry  out  their  mission.  The 
possibility of imposing mandatory inspection fees – as done in Food Safety legislation- 
should be explored. The proceedings of the fines should be used to finance the market 
surveillance activities.  
6.1 Reporting of serious incidents and of malfunctioning (art.62) 
We  think  that  serious  incident  or  any  malfunctioning  of  AI  having  an  impact  on 
consumer  safety should  also  be  reported.  We  refer  to our  long-lasting  call  for  a  pan 
European accidents and injuries database in order to assess whether a product is 
posing  a  high  risk  for  consumers,  with  the  aim  of  achieving  a  high  quality, 
representative and up-to-date data sample for the entire Single Market5. 
6.2 Procedure for dealing with AI systems presenting a risk at national level (art.65) 
The precautionary principle allows market surveillance authorities to take temporary 
and preventive measures in the absence of a definitive proof of harm to consumers or 
the environment. As such, we think that this fundamental principle should be present 
in the AI Act which is dealing with new technologies and unforeseen effects. 
In  current  market  surveillance  practice,  legal  obstacles  prevent  an  exchange  of 
information  in  both  the  harmonised  and  non-harmonised  areas  about  dangerous 
products  with  other  countries/jurisdictions.  It  is  therefore  important  that  the  AI  Act 
provides for a strengthening of international cooperation by allowing the exchange 
of information beyond confidentiality rules. 
ENDS 
5 European consumer safety needs solid injury data, ANEC-EuroSafe position paper, November 2020 
Raising standards for consumers 
ANEC-DIGITAL-2021-G-071 – July 2021 
  7 
 
 
 
 
 
</pre>",NEGATIVE
pdfminer_2665590_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665590.pdf,1,1,2665590,attachments/2665590.pdf#page=1,Is the line order correct?,"<pre>Putting startups at the heart of AI innovation - CroAI’s opinion on the European
Commission’s Artificial Intelligence Act
In April 2021, the European Commission (EC) published its much-awaited Artificial Intelligence
Act (AIA), the first global attempt to establish a legal framework for a technology that, as the AIA
states, carries both benefits and risks to humans and society. The Croatian AI Association
(CroAI) welcomes the EC’s efforts to set its own approach to AI, as it previously did with privacy.
However, our main concern is that the AIA does not adequately address the needs of start-ups,
who are the main drivers of innovation.
CroAI believes that the AIA must be an enabler of AI innovation and strongly stand behind
startu-ps, especially during prototyping and testing while pursuing a product-market fit. We
therefore advocate for the AIA to include unequivocal support for innovators by mandating the
following measures:
1. Startups are allowed to create their own sandboxes on a case-by-case basis rather than
a one-size-fits all approach, due to the unique nature of each test.
2. Start-ups will follow a Code of conduct. The Code of conduct will help them mitigate risks
while testing through tools such as limiting the number of test users, human oversight,
purchasing insurance, transparency, and accountability.
3. While in their sandboxes, startups do not need to involve supervisory authorities. Still,
they are accountable for complying with the Code of conduct.
4. When leaving their sandboxes, which means that they have found a product-market fit,
startups need to invest in fully complying with AIA rules and regulations, which will make
much more sense at that time in a product’s development cycle.
The principal concern founders and investors have when thinking about doing AI in Europe is
the cost and unpredictability of complying with the AIA. They see it as an unnecessary risk and
a burden that they can easily avoid by moving a start-up to some other innovation hub in the
world. CroAI believes that by integrating these measures into the AIA, the EC will address most
of those concerns and make the EU an excellent choice for AI innovation.
CroAI is at the European Commission’s disposal to elaborate more on the reasoning behind this
proposal and how to get it to life.
</pre>",POSITIVE
PyPDF2_2660610_7,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2660610.pdf,8,7,2660610,attachments/2660610.pdf#page=7,Is the line order correct?,"<pre>3.2.2 Art. 9.
Purpose limitation can prevent eective testing of the AI. Therefore, the fol-
lowing change to Art. 9(6) is suggested: Testing procedures shall be suitable to
achieve the intended purpose of the AI system. It is not surprising that ""in-
tended purpose"" is the model of intention, but since the Regulation goes into
""actual use"" in for example the prohibitions, the Commission should consider
whether to change the wording for the Regulation, regardless of its connection
to the product legislation in the EU in general10.
3.2.3 Art. 10.
Two choices are worth taking noticing in this article. Art. 10(2)(a) uses the
term ""relevant"", which poses the question as to whether there are ""irrelevant""
design choices. Irrelevant design choices can still aect the AI, so why not just
write design choices ? There is little reason not to exclude ""relevant"" here.
Art. 10(5) does not acknowledge the dierent ways which bias can be pre-
vented. It could do so to prevent the creation of the expectation that personal
data must be used in any way. This can be considered, but otherwise the section
is very agreeable.
3.2.4 Art. 14.
Human oversight is not the only solution to the dangers of AI, and while most
of the article is worded appropriately, there is one detail that could be changed.
Art. 14(2) mentions that ""human oversight shall aim"", but choosing ""shall"" is
not appropriate, the human oversight must ""aim"". The use of ""shall"" conicts
with the rest of the article, and it is suggested that it is excluded, purely in Art.
14(2).
3.2.5 Art. 15.
Why is ""state-of-the-art"" not used to describe the state of the (cyber)security
of the AI? Because of the consequences of failure of the defences when it comes
to AI, it is suggested that Art. 15(4), second sentence is changed to the follow-
ing: The technical solutions aimed at ensuring the cybersecurity of high-risk AI
systems shall be state-of-the-art.
This, combined with existing guidance on the subject matter, should guar-
antee a higher level of defences, which is more than adequate for such potentially
dangerous tools.
3.2.6 Art. 41.
Art. 41(3) seems either redundant (in that the high-risk AI may fulll it re-
gardless of this paragraph) or allows for circumvention of fulllment of Chapter
10This could be achieved with a dedicated article on how intention is perceived dierently
when it comes to AI than other products, but this suggestion is not included here.</pre>",POSITIVE
tika_2663324_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2663324.pdf,7,4,2663324,attachments/2663324.pdf#page=4,Is the line order correct?,"<pre>Page 4 of 7 





Establish a unified standard for data exchange 


Incentives for organizations/vendors that align 


with common data standards 


Promote data interoperability and exchange 


protocol, based on the experience of ERNs 


(Collaboration between several member states on 


the same topic) 





Allow transfer of data between local centres 


Centralise data collection from regional data sets 








REGULATORY CHALLENGES 


Validation process and Reimbursement 


Member States are slowly but surely adopting national legislatures allowing for digital health solutions to 


apply for reimbursement. While it is far from being general practice, the trend is growing, and regulatory 


agencies or Health Technology Assessment (HTA) bodies are adapting their pathways to include the 


assessment of digital health solutions, including AI-based solutions. 


While AI providing health data based on continuous monitoring (both passively and actively) of daily 


activities has a greater potential for reimbursement, the solutions utilizing AI without immediate health 


benefits might find a more challenging process. 


The data set used by the AI applications or used as secondary data (for instance, when an AI-based tool 


provides information on an internal decision point) in the application dossier for HTA, reimbursement 


process or post-marketing studies must be of the highest standards of quality. It is crucial to demonstrate 


the relevance of the data in a lifecycle approach: in the context of approval and, at a later stage, 


reimbursement. Should the regulator dispute the quality and/or the validity of the data used by the algorithm, 


the entire process could be severely delayed, henceforth, delaying access to patients. 


While one can understand the challenge in adapting to an extremely changing world, as well as to the 


massive disruption AI is about to bring to healthcare, it is indisputable that regulators need to issue clear 


guidance on quality requirements for validation processes, reimbursement criteria, and post-marketing 


studies12. We also call for clearer visibility on the possible evolution of the regulatory framework, to take 


into account the coming development of technology (e.g.: adaptive AI). 





12 Be it for surveillance or studies in the context of real-world evidence generation.</pre>",NEUTRAL
PyPDF2_2665205_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665205.pdf,4,3,2665205,attachments/2665205.pdf#page=3,Is the line order correct?,"<pre> 
 3 preserving  the professional  and pedagogical autonomy  and academic freedom  of teachers  
and academics.  
 
Transparency and AI literacy and CPD  of teachers on AI : 
ETUCE welcomes that the  proposal of AI Regulation requires  that users of AI tools  (who 
include  students, teachers, academic and education staff  for the education sector) must  be 
adequately informed  about the intended purpose, level of accuracy, residual risks  of AI 
tools . Nevert heless, ETUCE highlight s that providing information is not sufficient  to ensure 
the transparency of  the AI tools  when users miss  the adequate digital skills and data  and AI  
literacy  to interpret it. Therefore, it is of utmost importan ce to improve the importance of 
digital skills, AI literacy and d ata literacy  in educational curricula  and raise awareness on  
the risks  related to the use of AI tools  in education . It is also essential to ensure that  
infrastructures  of education institutions are adequately equipped for digital education as 
well as to provide equal access to digital  technologies and ICT  tools  to all teachers and 
students,  with particular attention to the most disadvantaged group s. To these purposes , 
sustainable public investment should be provided by national governments and the 
European Commission should provide financial support through European funding such as 
Horizon E urope, Digital Europe and in the framework of National Recovery and Resilience 
Facility.  
While the AI Regulation blandly mention to the possibility o f providing users with training 
on Artificial Intelligence , ETUCE emphasises that it is crucial that sustai nable public funding 
are provided at national and European  level  to ensure that teachers, trainers, academics 
and other education personnel receive up-to-date and  free of charge continuous training 
and professional development on the use of AI tools  in acc ordance with their professional 
needs .  
 
EdTech expansion and issues of intellectual property rights, data privacy of teachers :   
ETUCE  points out  that the development  of the use of Artificial Intelligence in education  has 
been  accompanied  by the expansion of Ed-tech companies  that are  progressively 
increasing  their influence in the education sector , especially under the pressure of 
emergency online teaching and learnin g during the COVID -19 pandemic.  ETUCE reminds 
that education is a human right and public goo d whose value need s to be protected. ETUCE 
calls for further public responsibility from national governments that should not limit their 
scope to regulat ing the EdTech sector  and should develop and implement public platforms 
for online teaching and learning to protect the public value of education.  In addition, public 
platforms should be implemented in full respect of professional autonomy  of teachers and 
education personnel as well as academic freedom  and autonomy of education institutions,  
without  creating  pressure on teachers and education personnel regarding the education 
material and pedagogical methods they use. It is also essential t o protect the accountability 
and transparency  in the governance of public education systems  from the influence of 
private  and commercial interests and actors .  </pre>",POSITIVE
tika_2665406_3,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665406.pdf,10,3,2665406,attachments/2665406.pdf#page=3,Is the line order correct?,"<pre>3 





systems that are used to predict, prescribe, or make decisions with effects on human beings. It 
underlines the socio-technical nature of such systems, which are inevitably embedded in societal 
contexts that need to be taken into account when assessing the implications of their use.3 We 
strongly agree with the European Center for Not-for-Profit Law’s position that the risks and 
opportunities of AI systems should not be judged on a binary basis. Instead, it is necessary to 
account for the targeted population, context, and situation when considering the risks and 
opportunities of these systems.4 In addition, the definition setting the scope of the Act focuses on 
the type of technology involved, which opens the door for operators to circumvent the scope of its 
provision by denying that their system falls under the respective definition.  


II In general, the proposed Act covers uses by both public and private actors, which presents a critical 
opportunity to streamline requirements on uses where fundamental rights are concerned, 
regardless of the actors involved. However, specific provisions are limited to public authorities. 
Some of them explicitly mention that they also apply to private actors acting on behalf of public 
authorities or in the framework of Public-Private-Partnerships. At the same time, the absence of 
this key addition in other provisions indicates that the extension to such private actors does not 
automatically apply, creating potential loopholes public authorities could try to exploit by 
outsourcing the use of certain systems.  


III Another aspect that is important for the effective protection of fundamental rights and that we 
welcome is the applicatory geographical scope of the proposed Act, which would apply whenever 
an AI-based system is used within the EU, regardless of where the operator is based—or whenever 
an output of such a system is used within the EU, regardless of where the system itself is based 
(Art. 2(1)). The wide extraterritorial effects this implies ensure that geographical loopholes 
cannot be exploited to evade the Act’s reach, guaranteeing protection across the Union. At the 
same time, focusing on the location where a system is used implies that neither the development 
nor the sale and export of any systems are covered by the Act if they are put to use elsewhere—
including systems whose use would be prohibited or classified as high-risk according to the Act. 
From a fundamental rights perspective, this creates a protection vacuum for people in third states, 
whose rights could be infringed by the uses of AI systems developed by EU-based providers.  


IV A related loophole stems from excluding from the scope of the proposed regulation any systems 
used by public authorities in third states or international organizations in the framework of 
international law enforcement and judicial cooperation with the EU or its Member States (Art. 2(4)).  


/ We call on the Council and the Parliament to clarify the applicatory scope of the AI Act with 
regard to the above aspects, making sure it is a coherent, consistent, and reliable instrument 
for protecting human beings from violations of their fundamental rights caused by the use 
of ADM systems—regardless of the specific technology or the type of actors involved.  


2 Mitigate the Self-Defeating Potential of the Risk-based Approach 


It is a key achievement that the Commission recognizes that the use of AI-based systems can come 
with serious risks for fundamental rights and that these risks need to be addressed by a 
governance framework, and this is an important message by itself that should be recognized as such. 
We are relieved to see that the approach has improved compared to the White Paper, recognizing 
sensitive areas, such as when AI systems are used for recruiting, to evaluate creditworthiness, to 
determine access to social benefits, for predictive policing, to control migration, and to assist judicial 
interpretation. Furthermore, the misleading criterion ‘sector’ to determine high-risk AI practices has 


                                                 
3 Following our definition, ADM systems encompass the design procedures to gather data, the collection of data, the 
development of algorithms to analyze the data, the interpretation of the results of this analysis based on a human-defined 
interpretation model, and the automatic action based on the interpretation as determined in a human-defined decision-
making model. 
4 ECNL Position Statement on the EU AI Act, 23 July 2021, https://ecnl.org/sites/default/files/2021-
07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf 



https://ecnl.org/sites/default/files/2021-07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf

https://ecnl.org/sites/default/files/2021-07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf

https://ecnl.org/sites/default/files/2021-07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf</pre>",NEUTRAL
PyPDF2_2665397_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665397.pdf,15,1,2665397,attachments/2665397.pdf#page=1,Is the line order correct?,"<pre>   
 
   
 Commentary to the  Commission’s  proposal for the “AI Act”  – 
Response to s elected issues  
 
 Centre for Commercial Law, School of Law, University of Aberdeen  
 
 
This response  is provided  by a working group of the Centre for Commercial  Law (CCL)  at the 
University of Aberdeen. The working group  consists  of Dr Péter Cserne, Dr Rossana Ducato, 
and Dr Patricia  Živković , and the response  incorporates  comments  by Prof Abbe Brown, Dr 
Irène Couzigou, Dr Georgios Leontidis, Prof Nir Or en, Dr Clare Sutherland,  Dr Paula Sweeney, 
Dr Burcu Yüksel  Ripley . 
The analysis provided in this response contains a preliminary analysis of selected issues.  
The views and opinions reported in this response  are submitted on behalf of the Centre for 
Commercial Law  and do not  necessarily  express the position of the School of Psychology, 
School of Divinity, History, Philosophy & Art History, and the School of Natu ral and Computing 
Science.   
 
Table of Contents  
1. Market integration, market  regulation and fundamental rights  ................................ .... 1 
2. The scope of application  ................................ ................................ ................................  2 
2.1. F ocus on the pre -market stage  ................................ ................................ ...................  2 
2.2. The extra -territorial effect  ................................ ................................ ...........................  3 
3. The risk -based approach  ................................ ................................ ................................  4 
3.1. Prohibited AI practices  ................................ ................................ ...........................  4 
3.2. High risk AI systems  ................................ ................................ ................................  8 
4. Measures in support of  innovation  ................................ ................................ ..............  11 
5. A place for the “AI subject”  ................................ ................................ ..........................  13 
6. Plain language and beyond  ................................ ................................ ..........................  14 
 
 
 
 
 
 
  </pre>",POSITIVE
fitz_2662226_7,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2662226.pdf,8,7,2662226,attachments/2662226.pdf#page=7,Is the line order correct?,"<pre> 
Raising standards for consumers 
ANEC-DIGITAL-2021-G-071 – July 2021 
 
 
7 
be complementary to strict safety measures and should not exonerate manufacturers 
from ensuring that products do not present a risk to consumers.  
However, the inherent information asymmetry associated with AI or an 
evolving/machine learning system, makes the function of information different from 
information linked to traditional, non-AI products (e.g. Ecolabel) where the 
technological content of the product is “static”. The information is not very helpful if the 
behaviour of products changes over time but the information stays the same. One 
reason more for us to seriously wonder about the inclusion of emotion recognition 
system or a biometric categorisation system and ‘deep fake’ in the level of low-risk AI 
systems, especially as consumers will not benefit from the right to opt-out of the 
system.  
6 | Information sharing and market surveillance 
 
Market surveillance authorities should have sufficient resources to enforce the AI 
requirements. We stress the need to ensure national supervisory authorities have the 
financial, technical and technological means to carry out their mission. The 
possibility of imposing mandatory inspection fees – as done in Food Safety legislation- 
should be explored. The proceedings of the fines should be used to finance the market 
surveillance activities.  
6.1 Reporting of serious incidents and of malfunctioning (art.62) 
We think that serious incident or any malfunctioning of AI having an impact on 
consumer safety should also be reported. We refer to our long-lasting call for a pan 
European accidents and injuries database in order to assess whether a product is 
posing a high risk for consumers, with the aim of achieving a high quality, 
representative and up-to-date data sample for the entire Single Market5. 
6.2 Procedure for dealing with AI systems presenting a risk at national level (art.65) 
The precautionary principle allows market surveillance authorities to take temporary 
and preventive measures in the absence of a definitive proof of harm to consumers or 
the environment. As such, we think that this fundamental principle should be present 
in the AI Act which is dealing with new technologies and unforeseen effects. 
In current market surveillance practice, legal obstacles prevent an exchange of 
information in both the harmonised and non-harmonised areas about dangerous 
products with other countries/jurisdictions. It is therefore important that the AI Act 
provides for a strengthening of international cooperation by allowing the exchange 
of information beyond confidentiality rules. 
ENDS 
 
 
5 European consumer safety needs solid injury data, ANEC-EuroSafe position paper, November 2020 
</pre>",POSITIVE
pdfminer_2665527_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665527.pdf,5,2,2665527,attachments/2665527.pdf#page=2,Is the line order correct?,"<pre>I. Introduction
Moje Państwo Foundation (“Foundation”)
is an organization working for
the
development of democracy, open and transparent public authority and civic engagement.
By exercising the right of access to public information and the right to re-use public
sector information, the Foundation collects publicly available data sets and makes them
available to citizens through the Foundation's services.
Due to the fact that the context of the consultations run by the European Commission
from 26th April to 6th August 2021 are related to many relevant aspects for the
Foundation’s activity, we present our position in regards to the Proposal for a Regulation
Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) (“the
AI Act” or “the European Commission’s Proposal”) below – with particular emphasis
on the use of artificial intelligence systems (“AI systems”) by public institutions.
II. General remarks
We appreciate the work of the European Commission undertaken to create a regulation
that aims to ensure that artificial
intelligence is safe, lawful and in line with EU
fundamental
rights. The European Commission’s Proposal contains many valuable
solutions creating a legal framework for artificial intelligence.
From the perspective of our Foundation, the AI Act should – to a greater extent
– ensure transparency of the public sector in relation to the use of artificial
intelligence systems.
AI systems can be used by public institutions in many key areas, such as, for example,
health protection, education, social services, the judiciary and the economy. As a
consequence, these systems can have a very broad and varied impact and can affect
many different social groups, including those particularly vulnerable to discrimination.
The use of AI systems by the state in many situations may lead to shaping the scope of
rights and obligations of citizens by these systems.
The European Commission’s Proposal presupposes measures to regulate the use of AI
systems in relation to public authorities - for example, the proposal prohibits AI-based
social scoring for general purposes done by public authorities; the use of ‘real time’
remote biometric identification systems in publicly accessible spaces for the purpose of
law enforcement is also prohibited unless certain limited exceptions apply; Annex III
1
</pre>",POSITIVE
tika_2665582_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665582.pdf,3,2,2665582,attachments/2665582.pdf#page=2,Is the line order correct?,"<pre>2 





This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement no. 786641 





SHERPA (Shaping the ethical dimensions of smart information systems (SIS) – a European 
perspective) is a project that focuses on ethical and human rights aspects of smart information 
systems (artificial intelligence and big data analytics).  
 
Regulatory governance systems are a key part of the SHERPA Final Recommendations,1 which 
include a call for creating an EU regulatory framework2 and establishing an EU Agency for AI.3 
SHERPA believes a robust, mandatory legal framework at the EU level is needed to ensure that 
ethical issues and human rights concerns related to AI are adequately addressed. There are many 
specific elements of the SHERPA recommendations that appear in the proposed text of the 
Regulation, like red lines for some AI applications, mandatory requirements for high-risk AI 
systems, and the creation of a centralised body. However, the draft text does not do enough to 
protect fundamental rights, often lacks conceptual clarity, and leaves many questions 
unanswered 
 
For more information, please visit: https://www.project-sherpa.eu 
 


Risk-based approach 


The proposed Regulation adopts a four-tiered risk-based approach, where AI systems are subject 
to different rules depending on the level of risk. While one purpose of the regulatory framework 
is to guarantee safety and fundamental rights of EU citizens, the risk-based approach adopted by 
the Commission may not be sufficient.  There is no reference to the EU Agency for Fundamental 
Rights, nor are there provisions on complaint and redress mechanisms available to those whose 
rights are violated by AI systems. Furthermore, the proposed regulation has a somewhat binary 
approach, failing to adequately take into account impacts across the spectrum of risk. Most 
mandatory requirements apply only to high-risk systems; by comparison, low-risk AI systems are 
only subject to transparency requirements and minimal-risk AI systems have no requirements. 


Definition of AI 


SHERPA recommended that AI be clearly defined in each use context with regard to relevant 
issues. While it is a challenge to precisely define AI, definitions used in the proposed Regulation 
are often overly broad and too open to interpretation. Additionally, despite attempts to be 
“technology neutral and as future proof as possible”, the proposed definition of AI is linked to 
‘software’, leaving out potential future developments of AI. The proposal takes into account the 
difficulty of defining AI by moving the definition into an appendix which is subject to review and 
revision. While this is reasonable in light of the problematic nature of the term AI, it does add to 
uncertainty about the future scope of the Regulation.  


Red lines 


Under the proposal’s risk-based approach, AI systems that pose the highest level of risk to 
fundamental rights and safety are prohibited. The short list of banned AI systems – only four 
categories – includes social scoring and AI that subliminally manipulates human behaviour in a 
harmful way. While SHERPA welcomes the explicit inclusion of red lines in the regulatory 
framework, the short list is incomplete and has many loopholes.  For example, use of remote, 


 
1 https://www.project-sherpa.eu/recommendations/ 
2 https://www.project-sherpa.eu/regulatory-framework/ 
3 https://www.project-sherpa.eu/european-agency-for-ai/</pre>",POSITIVE
pdfminer_2665462_26,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665462.pdf,31,26,2665462,attachments/2665462.pdf#page=26,Is the line order correct?,"<pre>Access Now’s submission to the European Commission’s adoption
consultation on the AI Act
(ii) public safety and public health, including disease prevention, control and treatment;
(iii) a high level of protection and improvement of the quality of the environment;
While we are generally concerned about establishing any grounds for the further processing of
personal data, due to the risk that such provisions pose to undermining the protections oﬀered by
existing data protection law, we are particularly concerned about paragraph (i). Perhaps the riskiest,
and most problematic uses of AI are in the “prevention, investigation, detection or prosecution of
criminal oﬀences or the execution of criminal penalties, including the safeguarding against and the
prevention of threats to public security.” Such uses of AI systems should, rather than being
incentivised or freed from scrutiny, be subject to the utmost caution, if they are to be pursued at all.
We therefore recommend that the use cases under paragraph (i) be removed from Article 54. We
also recommend that the term “innovative” be deleted from Article 53, paragraph 2:
- Member States shall ensure that to the extent the innovative AI systems involve the processing
of personal data or otherwise fall under the supervisory remit of other national authorities or
competent authorities providing or supporting access to data the national data protection
authorities and those other national authorities are associated to the operation of the AI
regulatory sandbox
AI systems should fall under the remit of the authorities regardless of whether they are innovative, so
this word should be deleted to prevent any loophole.
Regarding Article 54 in general, we support the points raised in the EDPS-EDPB Joint Opinion that
“relationship of Article 54(1) of the Proposal to Article 54(2) and recital 41 of the Proposal and thus also
to existing EU data protection law remains unclear.” As noted in the Joint Opinion, “the GDPR and the
EUDPR already have an established basis for ‘further processing,’” and we fully support the additional
point that “balancing between the controller’s interests and the data subject’s interests do not have to
hinder innovation.” The legal basis of the grounds for further processing must be clarified, and the
necessity of any such grounds for further processing be considered against the risk they pose to
undermining existing protections.
Finally, Access Now believes that the use of AI regulatory sandboxes must be subject to the highest
level of public scrutiny and transparency. The intention of such sandboxes is to facilitate the
development of AI systems “for safeguarding substantial public interest”: as such, the public must be
able to know exactly what types of systems are being developed in them. In line with initiative such as
Findata, Finland’s innovative Social and Health Data Permit Authority that promotes secondary use
of health and social data, the AI regulatory sandboxes should provide publicly available information
regarding all requests to make use of the sandbox, all accepted and rejected applications, information
about project currently in development in the context of the sandbox, and follow up information on
what happens with projects a",NEGATIVE
fitz_2665345_14,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665345.pdf,17,14,2665345,attachments/2665345.pdf#page=14,Is the line order correct?,"<pre>Es ist deshalb zu definieren, wann man noch von Daten ohne Personenbezug reden kann. (siehe dazu 
auch https://www.akeuropa.eu/de/evaluation-der-datenschutz-grundverordnung-dsgvo)  
Die DSGVO enthält allgemeine Grundsätze, die die vielen Rechtskonflikte zwischen Geheimhaltungs- 
und Verwertungsinteressen nicht unmittelbar lösen können. Unzulässige Verarbeitungspraktiken aus-
zuforschen und rechtlich richtig zu würdigen, überfordert nicht nur KonsumentInnen, sondern aufwands-
bedingt zunehmend auch die Aufsichtsbehörden. Dies schadet der Rechtssicherheit und mindert das 
Vertrauen in die Vorteile von KI. Die Ausstattung der Behörden entspricht nicht dem Bedarf, um rasch, 
sorgfältig, technikkundig und investigativ den vielfältigen Aufsichtsaufgaben nachzukommen. Die Ver-
lagerung von einer ex-ante Prüfpflicht in sensiblen Fällen zu einer nachträglichen Aufarbeitung von 
Rechtsverletzungen samt Schadenersatzansprüchen eröffnet schwerwiegende Schutzlücken, wenn 
Rechtsdurchsetzung nicht rasch und reibungslos funktioniert. 
 
Verbot von Anwendungen, bei denen die „Accountability“ an Grenzen stößt: 
Mit der Selbstlernfähigkeit der Systeme können Softwareentwickler oft selbst nicht mehr nachvollziehen, 
welchen logischen Weg Algorithmen einschlagen. Entscheidet KI aber selbst darüber, welche Daten sie 
für welchen Zweck nutzt, widerspricht dies fundamental dem Rechtsgrundsatz der „Accountability“ (Zu-
rechnung, Verantwortung, Haftung), und kollidiert auch mit der Pflicht, im Erhebungszeitpunkt bereits 
den genauen Verwendungszweck der Daten anzugeben 
Alle Entscheidungen, Produkte und Dienste die auf Algorithmen basieren, müssen erklär- und überprüf-
bar bleiben. KonsumentInnen dürfen angesichts einer Vielzahl an Beteiligten (Entwickler, Hersteller, 
Anwender, Dienstleister) nicht zum Spielball unklarer Verantwortlichkeiten werden. Sie sollen im Sinne 
einer Solidarhaftung Unterlassungs- und Schadenersatzansprüche gegen jeden Beteiligten in der Wert-
schöpfungskette richten können (mit anbieterseitigen Regressmöglichkeiten). 
 
Einbindung der Betroffenen:  
Daten- und Privatsphärenschutz sollten wirtschaftlichen Interessen grundsätzlich vorgehen. Wie verhält 
es sich aber, wenn Eingriffe in diese Rechte mit lebenswichtigen Interessen einzelner Personen, von 
Gruppen oder der Gesamtgesellschaft begründet werden? Interessenskollisionen sind vorprogram-
miert, sobald KI-Anwendungen im Gesundheitssektor Verbesserung bei der Erkennung, Behandlung 
und Heilung von Krankheiten oder im sicherheitspolizeilichen Einsatz eine bessere Kriminalitätspräven-
tion bzw -aufklärung versprechen. Der Preis für diesen (potentiellen) Fortschritt ist hoch: Interessen von 
großen Bevölkerungsteilen können damit gefährdet werden. Vor diesem Hintergrund braucht es für die 
Mehrzahl an KI-Anwendungen, die Grundrechte berühren, eine ex ante-Genehmigung durch ein unab-
hängiges Gremium. In dieses sind neben Datenschutzbehörden und Technikexperten auch VertreterIn-
nen der jeweils betroffenen Gruppen (ArbeitnehmerInnen, KonsumentInnen, PatientInnen, etc) mitein-
zubeziehen. Denn auch bei der Klärung von Rechtsfragen wird sorgfältig zwischen verschiedenen Inte-
ressen, Verhältnismäßigkeiten, Werten etc. abzuwägen sein. Diese Entscheidungen können abhängig 
von der jeweiligen Betroffenheit und dem jeweiligen weltanschaulichen Hintergrund sehr verschieden 
ausfallen. Die gesellschaftliche Akzeptanz von Entscheidungen für oder gegen einzelne KI-Anwendun-
gen und flankierende Auflagen fällt höher aus, wenn bei der Zusammensetzung des Entscheidungsgre-
miums auf eine breite Beteiligung aller betroffenen Gruppen geachtet wird.   
 
Produkthaftungsregeln aktualisieren: 
Die Produkthaftungs-RL aus dem Jahr 1985 kennt für digitale Trends wie KI keine Antworten. Eine 
überarbeitete RL muss auf alle materiellen und nicht materiellen Sachen, digitale Dienstleistungen und 
digitalen Inhalte anwendbar sein und sollte deshalb auch Cybersicherheitsrisiken, mangelnde Software-
updates und unzureichende DSGVO-Konformität zu den „Defekten“ eines Produktes zählen. Ebenso 
Schäden, die durch die Fähigkeit selbst zu lernen und autonome Entscheidungen zu treffen oder durch 
einen Missbrauch der verwendeten Daten entstehen. Ausgezeichnete Detailvorschläge für die Überar-
beitung der Produkthaftungs-RL enthält das BEUC-Positionspapier „Product Liability 2.0“  
 
</pre>",POSITIVE
PyPDF2_2663276_69,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2663276.pdf,82,69,2663276,attachments/2663276.pdf#page=69,Is the line order correct?,"<pre> 
69 
 - For images, visual scene synthesis.  
- For any data type, the extraction of representative (or typical) exemplars from a dataset, and 
of particularly atypical exemplars as well ( respectively  called prototypes and criticisms:  Kim, 
2016) . 
Explainable feature engineering  
This last type of pre -modelling  explana tory method stems from the observation that an explanation 
for a predictive model is only as good as the predictive features it relies on. Therefore, particular care 
must be taken to the feature engineering stage when designing an ML system, i.e. to the co nstruction 
of predictor variable s from original variables in order to adequately re -structure the training data for 
the algorithm.  
Two such methods should be mentioned  (Murdoch, 2019 ): 
- The intervention of domain experts, who are sufficiently knowledgeable about the source data 
to extract variables (combination of other variables, intermediate computation results, etc.) 
which increase a model’s predictive accuracy while maintaining the interpretability of its 
results. In other words, human expertise enables in certain cases to sidestep the usually 
inevitable trade -off between efficacy and explainability of an ML model ( cf. section 10.1.1 ). 
- A modelling -based, automated approach: usual data analysis techniques are then used, such 
as dimensionality reduction and clustering, so as to extract predictor variable s as compact and 
repr esentative as possible.  
11.2.  Explainable modelling  
Some methods enable simultaneously training the predictive model and building an associated 
explanatory model. This category of explanatory method is referred to as explainable modelling . 
Such methods are howev er far less frequently implemented than pre - and even more post -modelling    
explanatory approaches, for several reasons:   
- Explainable modelling  requires access to the source code which produces the predictive 
model, and the possibility to modify the algorithm. On the contrary, access to the model itself 
is sufficient for post -modelling  explanatory methods, which makes them much more widely 
applic able.  
- Explainable modelling  is useful when explanations are necessary as early as the design phase 
of the ML algorithm, which demands a more mature engineering methodology and adequate 
planning during the introduction of AI into a business process.  
- Lastly , explainable modelling  is not very suitable for audit, all the more so when the predictive 
model is only available as a black box, without a documentation of the algorithm itself.  
The primary, highly ambitious goal of explainable modelling  is to avoid as m uch as possible the already 
mentioned trade -off between efficacy and explainability, as they strive to provide additional 
explainability without necessarily sacrificing predictive accuracy.  
A few methods for explainable modelling  are described in what foll ows. 
Intrinsically explainable models  
An intrinsically explainable model can be chosen from the outset, for example linear models or 
decision -tree-based models. This is the most trivial kind of explainable modelling  approach, assuming 
that the simplicity/e fficacy trade -off is kept in mind, and that the specific model produced by the </pre>",POSITIVE
pdfminer_2665627_6,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665627.pdf,11,6,2665627,attachments/2665627.pdf#page=6,Is the line order correct?,"<pre>From our perspective we wonder if there is evidence that regulatory sandboxes are an 
effective strategy for facilitating innovation. We would like to see a more elaborate report on 
the consequences of the regulation in terms of its effect on innovation, specifically in relation 
to regulatory sandboxes and SMEs.  
Another way forward would be for an SME to partner with a larger actor with the ability to 
take the full administrative work onboard. The deal could have an impact on IP rights and 
revenue streams which in turn could mean that the regulation serves to protect the established 
actors on the market. The benefit for SMEs would be that they can focus on their innovative 
ideas and make the administration of the AI system a question of commercial contracts 
instead of a negotiation with national authorities. The latter demands another set of 
competences that not all SMEs have made a priority to recruit. 
The proposed regulation mentions another possibility in Article 17.2 that states that the 
quality management system ensuring compliance with the regulation should be in relation to 
the size of the organization. This could be used by SMEs instead of regulatory sandboxes or 
difficult negotiations with large and established actors. But it could also be used by large 
organisations that want to reduce their administration by relating the quality management 
system to the size of the unit developing the AI system, not the whole organization. 
Regulatory sandboxes is a topic we will return to from the perspective of public governance 
further down in our response. 
Responsibilities in a system-of-systems 
In this section we will focus on AI systems and their development as systems-of-systems, but 
also on AI systems as components in other systems and what effects the proposed regulation 
can have in terms of responsibilities among and between actors. 
System boundaries 
A challenge for actors with the proposed regulation is that artefacts and systems developed 
for other purposes can be used for future development of high-risk AI systems and therefore 
covered by the regulation (Annex IV.2). It could be services and platforms providing data 
regarding road conditions [5] or the placement of wastewater wells [6] (depending on the 
interpretation of the definition in Annex I they might be high-risk AI systems or mere data 
sources). If the supplied data is used for developing AI systems which can be used for 
managing vital infrastructure like roads and water supply (Annex III) they will be part of the 
technology chain behind the AI system and need to be administered as such.   
The same goes for developers of models, such as digital twins, if they are used for developing 
AI systems that are classified as high-risk systems. Providers of data regarding populations 
face the same uncertainty if the data covers for instance taxable income, number of residents 
at a specific address or fluctuation of property prices since they can be used to determine 
strategies or decisions for social benefits or targeted interventions against social exclusion 
(Annex III.5 and III.8). In the long run all providers of digital artefacts face the probability 
that their service, data or technology will be used for developing high-risk AI and therefore 
need to have the right documentation to conform with the regulation and avoid fines. 
At the same time there are political initiatives that promote public authorities and actors to 
facilitate data sharing as well as a need for digital simulations and models for societal and 
business planning. These artefacts could be high-risk AI systems in themselves or become 
 
 
 
 
 
 
 
</pre>",POSITIVE
tika_2662770_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662770.pdf,6,2,2662770,attachments/2662770.pdf#page=2,Is the line order correct?,"<pre>Mouvement des Entreprises de France 
Legal Department - July 2021 2 


On 21 April 2021, the European Commission proposed a new regulation on Artificial Intelligence (AI) aiming 
at providing a harmonized legal framework at European level. 


It is essential today to build a European ecosystem for innovation and AI that respects European values. The 
health crisis has indeed revealed the importance of technological and digital infrastructures and the fact that 
Europe must equip itself with such infrastructures, in particular AI. To do this, the development of AI must 
be made available to all companies, whatever their size, whatever their resources. 


Indeed, to build a trustworthy, efficient and sustainable AI framework, regulations must benefit citizens, 
but also European businesses by allowing them to develop and benefit from cutting-edge technologies in 
order to remain competitive. 


This European initiative is all the more important as it aims to prepare the ground for AI regulation worldwide. 


While MEDEF welcomes the work carried out by the European Commission and considers that the risk-based 
approach adopted in the proposed regulation is the best possible approach insofar as it promotes 
confidence in AI, this proposal raises many concerns and questions, in particular with regard to: 


− the legal uncertainty linked to very broad and insufficiently precise definitions; 


− the lack of consistency, or even the incompatibility, of this proposal with other European texts 
(GDPR, Machinery Directive, etc.); 


− the sometimes very heavy or even disproportionate obligations. 


On general provisions 


The risk-based approach of the European Commission seems to be the best possible approach as it fosters 
confidence in AI without hampering its responsible development. It is entirely relevant to define the 
obligations and requirements according to the risk (high or low risk) of the technology and its use. 


Nevertheless, it is essential to keep a margin of innovation. Particular attention must therefore be paid to 
definitions, in particular those of AI systems and high-risk systems, because the related obligations and 
requirements are very onerous and difficult to implement. Clear and sufficiently precise definitions are all 
the more important as they will be called upon to serve as references in other texts. They must therefore 
not lead to the creation of legal uncertainty. 


Some concepts, such as ""known and foreseeable risks"", ""reasonably foreseeable misuse"", ""generally 
acknowledged state of the art"", are for example very unclear and can therefore create legal uncertainty 
leading to different interpretations according to countries or authorities. 


On the definition of artificial intelligence   


 Article 3 (1) defines an AI system as “software that is developed with one or more of the techniques and 
approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such 
as content, predictions, recommendations, or decisions influencing the environments they interact with”.  


If we understand the Commission's objective of making this regulation neutral and adaptable to 
technological developments, the current proposed definition of AI (and the list of techniques in Annex 
I) is very broad in that it can include all types of systems or software applications that do not involve 
the same risks. The inclusion of such systems or applications within the scope of the regulation would risk 
hampering innovation in technology companies, especially smaller ones. However, in a context of 
international competitiveness, it is essential to encourage technological development and not to prevent 
SMEs from accessing these markets. 


In general, if AI can involve risks in its implementation, it is mainly with regard to its direct or indirect 
impact on individuals. However, it should be remembered that many uses of AI systems have little impact 
on individuals. This is the case, for example, with AI methods for internal modeling needs (for example 
ALM models for the banking sector) or for corporate scoring. 


It should also be noted that some AI systems even have the potential to increase the productivity of 
companies or the well-being of the workforce, in particular by efficiently distributing tasks between 
humans and machines, by providing tools for skills development and providing access to better working 
conditions, in particular health and safety. 


In this sense, it would be useful, on the one hand, not to put all systems or applications to the same 
scale. A benefit / risk balance of technological developments should be put in place in order to promote</pre>",POSITIVE
tika_2665345_14,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665345.pdf,17,14,2665345,attachments/2665345.pdf#page=14,Is the line order correct?,"<pre>Es ist deshalb zu definieren, wann man noch von Daten ohne Personenbezug reden kann. (siehe dazu 


auch https://www.akeuropa.eu/de/evaluation-der-datenschutz-grundverordnung-dsgvo)  


Die DSGVO enthält allgemeine Grundsätze, die die vielen Rechtskonflikte zwischen Geheimhaltungs- 


und Verwertungsinteressen nicht unmittelbar lösen können. Unzulässige Verarbeitungspraktiken aus-


zuforschen und rechtlich richtig zu würdigen, überfordert nicht nur KonsumentInnen, sondern aufwands-


bedingt zunehmend auch die Aufsichtsbehörden. Dies schadet der Rechtssicherheit und mindert das 


Vertrauen in die Vorteile von KI. Die Ausstattung der Behörden entspricht nicht dem Bedarf, um rasch, 


sorgfältig, technikkundig und investigativ den vielfältigen Aufsichtsaufgaben nachzukommen. Die Ver-


lagerung von einer ex-ante Prüfpflicht in sensiblen Fällen zu einer nachträglichen Aufarbeitung von 


Rechtsverletzungen samt Schadenersatzansprüchen eröffnet schwerwiegende Schutzlücken, wenn 


Rechtsdurchsetzung nicht rasch und reibungslos funktioniert. 





Verbot von Anwendungen, bei denen die „Accountability“ an Grenzen stößt: 


Mit der Selbstlernfähigkeit der Systeme können Softwareentwickler oft selbst nicht mehr nachvollziehen, 


welchen logischen Weg Algorithmen einschlagen. Entscheidet KI aber selbst darüber, welche Daten sie 


für welchen Zweck nutzt, widerspricht dies fundamental dem Rechtsgrundsatz der „Accountability“ (Zu-


rechnung, Verantwortung, Haftung), und kollidiert auch mit der Pflicht, im Erhebungszeitpunkt bereits 


den genauen Verwendungszweck der Daten anzugeben 


Alle Entscheidungen, Produkte und Dienste die auf Algorithmen basieren, müssen erklär- und überprüf-


bar bleiben. KonsumentInnen dürfen angesichts einer Vielzahl an Beteiligten (Entwickler, Hersteller, 


Anwender, Dienstleister) nicht zum Spielball unklarer Verantwortlichkeiten werden. Sie sollen im Sinne 


einer Solidarhaftung Unterlassungs- und Schadenersatzansprüche gegen jeden Beteiligten in der Wert-


schöpfungskette richten können (mit anbieterseitigen Regressmöglichkeiten). 





Einbindung der Betroffenen:  


Daten- und Privatsphärenschutz sollten wirtschaftlichen Interessen grundsätzlich vorgehen. Wie verhält 


es sich aber, wenn Eingriffe in diese Rechte mit lebenswichtigen Interessen einzelner Personen, von 


Gruppen oder der Gesamtgesellschaft begründet werden? Interessenskollisionen sind vorprogram-


miert, sobald KI-Anwendungen im Gesundheitssektor Verbesserung bei der Erkennung, Behandlung 


und Heilung von Krankheiten oder im sicherheitspolizeilichen Einsatz eine bessere Kriminalitätspräven-


tion bzw -aufklärung versprechen. Der Preis für diesen (potentiellen) Fortschritt ist hoch: Interessen von 


großen Bevölkerungsteilen können damit gefährdet werden. Vor diesem Hintergrund braucht es für die 


Mehrzahl an KI-Anwendungen, die Grundrechte berühren, eine ex ante-Genehmigung durch ein unab-


hängiges Gremium. In dieses sind neben Datenschutzbehörden und Technikexperten auch VertreterIn-


nen der jeweils betroffenen Gruppen (ArbeitnehmerInnen, KonsumentInnen, PatientInnen, etc) mitein-


zubeziehen. Denn auch bei der Klärung von Rechtsfragen wird sorgfältig zwischen verschiedenen Inte-


ressen, Verhältnismäßigkeiten, Werten etc. abzuwägen sein. Diese Entscheidungen können abhängig 


von der jeweiligen Betroffenheit und dem jeweiligen weltanschaulichen Hintergrund sehr verschieden 


ausfallen. Die gesellschaftliche Akzeptanz von Entscheidungen für oder gegen einzelne KI-Anwendun-


gen und flankierende Auflagen fällt höher aus, wenn bei der Zusammensetzung des Entscheidungsgre-


miums auf eine breite Beteiligung aller betroffenen Gruppen geachtet wird.   





Produkthaftungsregeln aktualisieren: 


Die Produkthaftungs-RL aus dem Jahr 1985 kennt für digitale Trends wie KI keine Antworten. Eine 


überarbeitete RL muss auf alle materiellen und nicht materiellen Sachen, digitale Dienstleistungen und 


digitalen Inhalte anwendbar sein und sollte deshalb auch Cybersicherheitsrisiken, mangelnde Software-


updates und unzureichende DSGVO-Konformität zu den „Defekten“ eines Produktes zählen. Ebenso 


Schäden, die durch die Fähigkeit selbst zu lernen und autonome Entscheidungen zu treffen oder durch 


einen Missbrauch der verwendeten Daten entstehen. Ausgezeichnete Detailvorschläge für die Überar-


beitung der Produkthaftungs-RL enthält das BEUC-Positionspapier „Product Liability 2.0“  


 



https://www.akeuropa.eu/de/evaluation-der-datenschutz-grundverordnung-dsgvo

http://www.beuc.eu/publications/product-liability-20-how-make-eu-rules-fit-consumers-digital-age/html</pre>",POSITIVE
fitz_2663391_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663391.pdf,2,1,2663391,attachments/2663391.pdf#page=1,Is the line order correct?,"<pre> 
Velázquez, 64-66, 2ª planta - 28001 MADRID  
www.asnef.com  -  asnef@asnef.com 
CIF: G28516003 
Página 1 
 
Madrid, 30 de julio de 2021 
 
 
La Asociación Nacional de Establecimientos Financieros de Crédito (ASNEF), 
representada por su Secretario General, D. Ignacio Pla Vidal, y debidamente inscrita en el 
Registro de Transparencia de la Unión Europea con el nº 11218815591-29, presenta las 
siguientes observaciones dentro del plazo de consulta pública abierta en relación con: 
 
Propuesta De Reglamento Del Parlamento Europeo Y Del Consejo Para El Establecimiento De 
Reglas 
Armonizadas 
Sobre 
Inteligencia 
Artificial 
(Ley 
De 
Inteligencia 
Artificial) 
[COM(2021)206] 
 
 
1. 
En primer lugar, con relación al considerando 37, la propuesta de Reglamento menciona 
lo siguiente: 
“Habida cuenta del alcance sumamente limitado de su impacto y de las escasas 
alternativas disponibles en el mercado, conviene dejar exentos a los sistemas de IA 
destinados a evaluar la solvencia y la calificación crediticia cuando los pongan en servicio 
proveedores a pequeña escala para su propio uso.”  
Consideramos de suma importancia que se especifique el significado de ""proveedores a 
pequeña escala” y “para su propio uso” en el contexto de esta excepción a la consideración 
como riesgo alto y a la aplicación de los consiguientes requisitos y obligaciones. 
2. 
En segundo lugar, rogamos que se especifique cómo deberán cumplir aquellos sistemas 
ya en producción que, según esta regulación, son de alto riesgo. 
3. 
Por último, solicitamos que se incluya un amplio periodo transitorio para su 
aplicabilidad, al objeto de que las entidades que ya están utilizando Inteligencia artificial para la 
evaluación de la solvencia y credit score puedan adaptarse a los nuevos requisitos. 
 
 
 
</pre>",POSITIVE
tika_2663366_9,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663366.pdf,13,9,2663366,attachments/2663366.pdf#page=9,Is the line order correct?,"<pre>29 July 2021 
 





 
9 


set nutrition labels,” that can potentially provide a snapshot of how a system was developed and how 


it performs. These tools, however, are still being developed. There are no clear standards for such 


documentation efforts, nor have they yet been demonstrated to be practical at a meaningful scale.  





7. Conformity assessments  





 Self-assessments – CIPL believes that relying on organisations’ self-assessments for most high-risk AI 


systems strikes a good balance between innovation and the preservation of fundamental rights. 


Requiring prior approval for all high-risk AI systems would be harmful to innovation (especially as the 


Commission notes that expertise for AI auditing is only now being accumulated). Many organisations 


have already established self-assessment processes in the context of GDPR and will be able to leverage 


them for high-risk AI assessment purposes.  


 No double reporting – CIPL underlines that it would be overly burdensome for all products that require 


conformity assessments under legislation listed in Annex II to also be subject to additional AI-specific 


requirements set out in the AI Act. Some video conferencing products’ endpoints, for example, require 


conformity assessments under the Radio Equipment Directive listed in Annex II of the AI Act. The use 


of AI in such systems is intended to enhance collaboration between employees and is therefore 


unlikely to be high-risk. Although the infrastructure with conformity assessment bodies is well-


established and efficient, it will be essential to make sure that there is no double reporting requirement 


and that these bodies are properly equipped to deal with this new role.  


8. Regulatory Sandboxes 





 More clarity – CIPL welcomes the inclusion of a legal basis for regulatory sandboxes in the AI Act. The 


relevant provisions should be underpinned by clear and unambiguous rules for those making use of 


sandboxes, including sufficient guidance to regulators about their operation, and the need for 


consistency in approaches. In particular, the provisions should address how insights and learnings 


obtained from the regulatory sandbox exercises can inform the policy making process of the AI Act 


(including modifications to the Annexes), as well as its enforcement.  





 Incentives - The AI Act needs to more clearly lay out the incentives for organisations to join sandboxes 


and the outcomes they can expect.8 CIPL encourages the Commission and the relevant regulators to 


work with industry to: (1) think about the specific functioning of sandboxes; (2) set them up in a way 


that truly helps companies to drive innovation in a protected environment to unearth learnings for all 


stakeholders involved; and (3) enable sandboxes to reach beyond SMEs and be made more inclusive. 9 


                                                 
8 See CIPL Paper Regulatory Sandboxes in Data Protection – Constructive Engagement and Innovative Regulation in 
Practice. 
9 The AI Act currently prioritises regulatory sandboxes to small-scale providers and start-ups. The possible impact on 


a level playing field needs to be assessed as sandboxes can only take in a number of applications at any given time, 


and this is likely to be far smaller a number than the amount of AI innovations being developed in the market place.  



https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl_white_paper_on_regulatory_sandboxes_in_data_protection_-_constructive_engagement_and_innovative_regulation_in_practice__8_march_2019_.pdf

https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl_white_paper_on_regulatory_sandboxes_in_data_protection_-_constructive_engagement_and_innovative_regulation_in_practice__8_march_2019_.pdf</pre>",POSITIVE
PyPDF2_2665314_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2665314.pdf,6,2,2665314,attachments/2665314.pdf#page=2,Is the line order correct?,"<pre> 
 
 
Side 2 
  
We agree that under such a risk -based approach, most applications should fall outside of 
the scope of the regulation.  
4 PROHIBITED AI PRACTICES  
The Norwegian government supports the Commission's proposal for AI practices that 
shall be pr ohibited.  However, we have a concern regarding the Article 5 – 1 (a) stating 
that an AI system that deploys subliminal techniques is prohibite d if it is likely to cause 
physical or psychological  harm. In our opinion , such manipulative practices are often 
used in cases that cause financial harm  to people , which may in turn lead to 
psychological  harm. We would propose that the prohibition of AI systems that deploy 
subliminal techniques  is expanded to also include financial harm .  
 
Regardi ng Article 5 – 1 (c) the Norwegian Government supports the suggestion of 
prohibiting AI systems for evaluation or classification of the worthiness of natural persons 
where such use of AI -system s may lead to unjustified or disproportionate  treatment of 
individuals . However, it should be carefully considered whether the terms 'unjustified or 
disproportionate ' will provide sufficient safeguards against unwanted use of this type of 
AI-system s. The Norwegian government is concerned that this, in some cases, coul d 
allow for exclusion  of individuals  from the use of certain fundamental services that should 
be available to all, for instance health and care services.  
5 HIGH RISK AI SYSTEMS  
The Norwegian Governmen t supports the overall approach to high risk AI systems i n the 
Commission's proposal. We support the proposal for an EU database for high -risk AI 
systems.  
Determining borderline cases  
The term 'high risk system ' is defined in article 6 with reference to lists in Annexe s II and 
III. The Commission will be empowered to adopt delegated acts to update these  lists. 
Deciding whether a system qualifies as a high-risk AI-system may require  difficult 
considerations. It is our view that it may be necessary to adopt a procedure to determine 
borderline cases . We propose that such a procedure could be  modelled after Article 4 in 
Regulation 2017/745 on medical devices. In addition, in order to ensure the best possible 
end user protection, we propose to include  in the regulation that : 'In cases of doubt, 
where, taking into account all its characteristics, an ""AI system "" may be considered a 
""high-risk AI system "" the provisions of Title III, Chapter 2 of the Regulation appl y'. Such  
provisions have been used in other EU regulations.  
Autonomous shipping  
Norway is at the forefront in the development of autonomous vessels and host s a number 
of test sites. We recognise that safety systems for the navigation of such vessels are 
included in 'High-risk AI systems ' through the inclusion of Directive 2014/90/EU in the list 
in Annex II. However, we see a growing activity around AI based systems intend ed for 
decision support to increase safety in harbours and fairways. This includes AI systems for </pre>",POSITIVE
pdfminer_2665469_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665469.pdf,3,3,2665469,attachments/2665469.pdf#page=3,Is the line order correct?,"<pre>Sandboxes and data spaces 
2.3 
Another requirement is the so-called AI regulatory sandboxes (Title V Article 53) for testing 
and validating AI systems before placement on the market.  
It is unclear whether these sandbox environments will be provided by member states’ 
authorities themselves or the European Data Protection Supervisor, or if the task will be 
outsourced to private companies. The same goes for the European common data spaces 
(Recital 45). KMD suggests a further clarification in the proposal. 
3 
Clarification of various terms and concepts 
3.1  High-risk classification 
As for the protection of public interest in high-risk AI systems, the proposal states that 
common normative standards should be established (Recital 13). KMD suggests a clarification 
of whether these common normative standards are part of the report on AI standards2. 
The requirements regarding accuracy, consistency, robustness, appropriateness etc. in high-
risk AI systems (Title III Article 15) furthermore lacks specification. 
Title XII Article 83(2) regards the application of the regulation to high-risk AI systems already 
on the market. It states that these solutions will not be subject to the new regulation unless 
there are significant changes to design or purpose. KMD suggests a further elaboration on 
what constitutes a “significant change”. 
Placing on the market and putting into service 
3.2 
The difference between “placing on the market” and “putting into service” (Title 1 Article 
3(9,11)) is subtle but regards first time a solution is made available to the market respectively 
first use. This raises a question about retrained AI systems for a new problem dataset. A 
clarification of whether using retrained AI systems are considered “first use” would be 
beneficial. 
2 Cf. JRC Publications Repository - AI Standardisation Landscape: state of play and link to the EC proposal for an AI regulatory 
framework (europa.eu) 
V1.0 5. august 2021 KMD  
Side 3 af 3 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",NEGATIVE
PyPDF2_2662182_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662182.pdf,5,3,2662182,attachments/2662182.pdf#page=3,Is the line order correct?,"<pre> 
 
3 
 
parezca real ha sido elaborado por medios automáticos , en nuestra opinión, también se 
debe exigir la divulgación de qué contenido co ncreto ha sido empleado para la 
elaboración de ese nuevo contenido. Ello es indispensable para garantizar que los 
creadores y otros titulares de derechos sean remunerados por la utilización de sus obras 
o prestaciones.  
 
Por otro lado, se exige la regulaci ón de un sistema de responsabilidad para aquellos 
casos en los que sistemas autónomos generen contenido que vulnere DPI. Si el 
resultado generado por un sistema autónomo transforma o reproduce contenido ajeno 
¿ante quién deben reclamar los titulares de der echos afectados?. La falta de respuesta 
legal a esta cuestión no puede dejar desamparados a los creadores.  
 
En este sentido , en la Resolución del Parlamento Europeo, de 20 de octubre de 2020, 
con recomendaciones destinadas a la Comisión sobre un régimen de responsabilidad 
civil en materia de inteligencia artificial (2020/2014(INL)) se considera que es necesario 
realizar adaptaciones específicas y coordinadas de los regímenes de responsabilidad 
civil para evitar situaciones en las que personas que sufran un daño o un menoscabo a 
su patrimonio por el empleo de sistemas de IA acaben sin indemnización.  
 
SEGUNDO: CONTENIDO CREADO POR SISTEMAS DE INTELIGENCIA ARTIFICIAL  
 
Buena parte de la doctrina se ha volcado en el planteamiento del reto que supone la  
protección de los resultados o contenido obtenido mediante sistemas de IA. Mientras 
que el empleo de sistemas de IA por parte de un creador como una mera herramienta  
en la creación  no debe suponer ningún reto ni variación a la legislación existente , sí que  
plantea dudas la protección de los resultados de  aquellos sistemas que puedan llegar a 
crear de forma autónoma.  
 
En primer lugar, queremos destacar que la posible protección de los contenidos creados 
por sistemas autónomos no debe suponer un menos cabo a los intereses o derechos de 
los creadores humanos.  Las creaciones obtenidas por máquinas no deben llegar a 
competir ni a sustituir a las creaciones humanas. Es altamente probable que las 
creaciones obtenidas de forma autónoma por sistemas artificial es lleguen a ser 
monopolio de unas pocas empresas tecnológicas, por lo que se ha de evitar que estas </pre>",POSITIVE
fitz_2665480_38,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665480.pdf,64,38,2665480,attachments/2665480.pdf#page=38,Is the line order correct?,"<pre> 
   
33 
Despite not relying on personal data of natural persons, the fundamental rights implications of 
these systems are important because they are used to determine who can be subject to increased 
police intervention (based on geographical location), how these interventions occur, and with 
what frequency. Used for law enforcement purposes, without due care, resource optimisation 
systems may contribute to over-policing and surveillance of specific geographical locations 
caused by data ‘feedback loops,’58 and in doing so, may further exacerbate existing problems 
with systemic discrimination arising from historical racial and socio-economic biases in 
existing policing datasets (as geographical location is often a proxy for race or economic class), 
with little opportunity for redress, and no transparency for affected individuals. Drawing from 
Philip Alston speaking about the SyRI welfare fraud detection system in the Netherlands, 
targeting entire neighbourhoods as suspect and “subject to special scrutiny” with a combination 
of digital and physical methods threatens the very essence of privacy, by contributing to general 
unease, potential prejudice, and chilling effects on behaviour.59 It matters little whether the 
personal data of natural persons is implicated here. 
Were the final text of the Regulation therefore to exclude resource optimisation systems, this 
could amount to a significant failure to recognise the systemic social assumptions that are built 
into AI systems – as socio-technical systems which mediate social institutions and structures – 
which by virtue of their implementation, further mediate the enjoyment of individuals’ 
fundamental rights, including respect for their human dignity, equality, liberty and other 
freedoms. We therefore recommend that geospatial AI systems are included under Annex 
III(6), and that reference is made to AI systems which affect the distribution of law enforcement 
resources.    
e) The requirements that high-risk AI systems must comply with need to be strengthened and 
clarified 
Our final remarks on the proposed regulatory framework for high-risk AI systems concerns the 
strength and clarity of the requirements for such systems. While vague language – which might 
cause legal uncertainty and a weak protection against AI’s adverse effects – can be found under 
several requirements for high-risk systems, we highlight a few questions and concerns 
regarding three requirements in particular: those pertaining to data governance, transparency 
and human oversight.  
Data governance obligations  
The first requirement which raises questions is ‘data governance.’ Firstly, the large discretion 
for providers of high-risk AI systems mentioned earlier is also reflected in the requirements for 
data governance. Article 10 of the Proposal, dealing with requirements of data quality and 
governance, also lets the term ‘appropriate’ do some heavy lifting. Indeed, it requires that 
training, validation and testing data sets shall be subject to ‘appropriate’ data governance and 
management practices, that the data sets shall have ‘appropriate’ statistical properties, and that 
the processing of special categories of data to avoid the risk of bias is carried out subject to 
‘appropriate’ safeguards for fundamental rights. While the Article can be commended for 
specifying the minimal considerations that should be taken into account for the data 
management process to be considered ‘appropriate,’ it leaves open what constitutes an 
 
58  The Law Society of England and Wales, “Algorithms in the Criminal Justice System,” June 4, 2019, 
https://www.lawsociety.org.uk/en/topics/research/algorithm-use-in-the-criminal-justice-system-report, 35. 
59  Philip Alston, “Brief by the United Nations Special Rapporteur on extreme poverty and human rights as 
Amicus Curiae in the case of NJCM c.s./De Staat der Nederlanden (SyRI) before the District Court of the 
Hague 
(case 
number: 
C/9/550982/HA 
ZA 
18/388),” 
OHCHR, 
September 
26, 
2019, 
https://www.ohchr.org/Documents/Issues/Poverty/Amicusfinalversionsigned.pdf, 29. 
</pre>",POSITIVE
pdfminer_2662770_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662770.pdf,6,2,2662770,attachments/2662770.pdf#page=2,Is the line order correct?,"<pre>On 21 April 2021, the European Commission proposed a new regulation on Artificial Intelligence (AI) aiming 
at providing a harmonized legal framework at European level. 
It is essential today to build a European ecosystem for innovation and AI that respects European values. The 
health crisis has indeed revealed the importance of technological and digital infrastructures and the fact that 
Europe must equip itself with such infrastructures, in particular AI. To do this, the development of AI must 
be made available to all companies, whatever their size, whatever their resources. 
Indeed, to build a trustworthy, efficient and sustainable AI framework, regulations must benefit citizens, 
but also European businesses by allowing them to develop and benefit from cutting-edge technologies in 
order to remain competitive. 
This European initiative is all the more important as it aims to prepare the ground for AI regulation worldwide. 
While MEDEF welcomes the work carried out by the European Commission and considers that the risk-based 
approach  adopted  in  the  proposed  regulation  is  the  best  possible  approach  insofar  as  it  promotes 
confidence in AI, this proposal raises many concerns and questions, in particular with regard to: 
− 
− 
− 
the legal uncertainty linked to very broad and insufficiently precise definitions; 
the  lack  of  consistency,  or  even  the  incompatibility,  of  this  proposal  with  other  European  texts 
(GDPR, Machinery Directive, etc.); 
the sometimes very heavy or even disproportionate obligations. 
On general provisions 
The risk-based approach of the European Commission seems to be the best possible approach as it fosters 
confidence  in  AI  without  hampering  its  responsible  development.  It  is  entirely  relevant  to  define  the 
obligations and requirements according to the risk (high or low risk) of the technology and its use. 
Nevertheless, it is essential to keep a margin of innovation. Particular attention must therefore be paid to 
definitions,  in  particular  those  of  AI  systems  and  high-risk  systems,  because  the  related  obligations  and 
requirements are very onerous and difficult to implement. Clear and sufficiently precise definitions are all 
the more important as they will be called upon to serve as references in other texts. They must therefore 
not lead to the creation of legal uncertainty. 
Some  concepts,  such  as  ""known  and  foreseeable  risks"",  ""reasonably  foreseeable  misuse"",  ""generally 
acknowledged  state  of  the  art"",  are  for  example  very  unclear  and  can  therefore  create  legal  uncertainty 
leading to different interpretations according to countries or authorities. 
On the definition of artificial intelligence   
  Article 3 (1) defines an AI system as “software that is developed with one or more of the techniques and 
approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such 
as content, predictions, recommendations, or decisions influencing the environments they interact with”.  
If  we  understand  the  Commission's  objective  of  making  this  regulation  neutral  and  adaptable  to 
technological developments, the current proposed definition of AI (and the list of techniques in Annex 
I) is very broad in that it can include all types of systems or software applications that do not involve 
the same risks. The inclusion of such systems or applications within the scope of the regulation would risk 
hampering  innovation  in  technology  companies,  especially  smaller  ones.  However,  in  a  context  of 
international competitiveness, it is essential to encourage technological development and not to prevent 
SMEs from accessing these markets. 
In general, if AI can involve risks in its implementation, it is mainly with regard to its direct or indirect 
impact on individuals. However, it should be remembered that many uses of AI systems have little impact 
on individuals. This is the case, for example, with AI methods for internal modeling needs (for example 
ALM models for the banking sector) or for corporate scoring. 
It  should  also  be  noted  that  some  AI  systems  even  have  the  potential  to  increase  the  productivity  of 
companies  or  the  well-being  of  the  workforce,  in  particular  by  efficiently  distributing  tasks  between 
humans and machines, by providing tools for skills development and providing access to better working 
conditions, in particular health and safety. 
In this sense, it would be useful, on the one hand, not to put all systems or applications to the same 
scale. A benefit / risk balance of technological developments should be put in place in order to promote 
Mouvement des Entreprises de France 
Legal Department - July 2021  2 
 
</pre>",POSITIVE
PyPDF2_2663391_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663391.pdf,2,1,2663391,attachments/2663391.pdf#page=1,Is the line order correct?,"<pre> 
Velázquez, 64 -66, 2ª planta - 28001 MADRID  
www.asnef.com   -  asnef@asnef.com  
CIF: G28516003  
Página 1 
 
Madrid, 30 de julio  de 2021  
 
 
La Asociación Nacional de Establecimientos Financieros de Crédito (ASNEF) , 
representada por su Secretario General , D. Ignacio Pla Vidal, y debidamente inscrita en el 
Registro de Transparencia de la Unión Europea con el nº 11218815591 -29, presenta las 
siguientes observaciones dentro del plazo de consulta pública abierta en relación con:  
 
Propuesta De Reglamento Del Parlamento Europeo Y Del Consejo Para El Establecimiento De 
Reglas Armonizadas Sobre Inteligencia Artificial (Ley De Inteligencia Artificial) 
[COM(2021)206 ] 
 
 
1. En primer lugar, con relación al  considerando 37, la propuesta de Reg lamento menciona  
lo siguiente:  
“Habida cuenta del alcance sumamente limitado de su impacto y de las escasas 
alternativas disponibles en el mercado, conviene dejar exentos a los sistemas de IA 
destinados a evaluar la solvencia y la calificación crediticia c uando los pongan en servicio 
proveedores a pequeña escala para su propio uso.”   
Consideramos de suma importancia  que se especifique el significado de  ""proveedores a 
pequeña escala” y “para su propio uso” en el contexto de esta excepción a la consideración 
como riesgo alto y a la aplicación de los consiguientes requisitos y obligaciones.  
2. En segundo lugar, rogamos que se especifique cómo deberán cumplir aquellos  sistemas 
ya en producción  que, según esta regulación, son de alto riesgo.  
3. Por último, solicitamos que se incluya un amplio periodo transitorio para su 
aplicabilidad , al objeto de  que las entidades que ya están utilizando I nteligencia artificial para la 
evaluación de la solvencia y credit score puedan adaptarse a los nuevos requisitos.  
 
 
 </pre>",POSITIVE
fitz_2665527_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665527.pdf,5,2,2665527,attachments/2665527.pdf#page=2,Is the line order correct?,"<pre>I. Introduction
Moje
Państwo
Foundation
(“Foundation”)
is
an
organization
working
for
the
development of democracy, open and transparent public authority and civic engagement.
By exercising the right of access to public information and the right to re-use public
sector information, the Foundation collects publicly available data sets and makes them
available to citizens through the Foundation's services.
Due to the fact that the context of the consultations run by the European Commission
from 26th April to 6th August 2021 are related to many relevant aspects for the
Foundation’s activity, we present our position in regards to the Proposal for a Regulation
Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) (“the
AI Act” or “the European Commission’s Proposal”) below – with particular emphasis
on the use of artificial intelligence systems (“AI systems”) by public institutions.
II. General remarks
We appreciate the work of the European Commission undertaken to create a regulation
that
aims
to
ensure that artificial intelligence is safe, lawful and in line with EU
fundamental
rights.
The
European
Commission’s
Proposal
contains
many
valuable
solutions creating a legal framework for artificial intelligence.
From the perspective of our Foundation, the AI Act should – to a greater extent
– ensure transparency of the public sector in relation to the use of artificial
intelligence systems.
AI systems can be used by public institutions in many key areas, such as, for example,
health protection, education, social services, the judiciary and the economy. As a
consequence, these systems can have a very broad and varied impact and can affect
many different social groups, including those particularly vulnerable to discrimination.
The use of AI systems by the state in many situations may lead to shaping the scope of
rights and obligations of citizens by these systems.
The European Commission’s Proposal presupposes measures to regulate the use of AI
systems in relation to public authorities - for example, the proposal prohibits AI-based
social scoring for general purposes done by public authorities; the use of ‘real time’
remote biometric identification systems in publicly accessible spaces for the purpose of
law enforcement is also prohibited unless certain limited exceptions apply; Annex III
1
</pre>",POSITIVE
fitz_2665462_26,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665462.pdf,31,26,2665462,attachments/2665462.pdf#page=26,Is the line order correct?,"<pre>Access Now’s submission to the European Commission’s adoption
consultation on the AI Act
(ii) public safety and public health, including disease prevention, control and treatment;
(iii) a high level of protection and improvement of the quality of the environment;
While we are generally concerned about establishing any grounds for the further processing of
personal data, due to the risk that such provisions pose to undermining the protections oﬀered by
existing data protection law, we are particularly concerned about paragraph (i). Perhaps the riskiest,
and most problematic uses of AI are in the “prevention, investigation, detection or prosecution of
criminal oﬀences or the execution of criminal penalties, including the safeguarding against and the
prevention of threats to public security.” Such uses of AI systems should, rather than being
incentivised or freed from scrutiny, be subject to the utmost caution, if they are to be pursued at all.
We therefore recommend that the use cases under paragraph (i) be removed from Article 54. We
also recommend that the term “innovative” be deleted from Article 53, paragraph 2:
-
Member States shall ensure that to the extent the innovative AI systems involve the processing
of personal data or otherwise fall under the supervisory remit of other national authorities or
competent authorities providing or supporting access to data the national data protection
authorities and those other national authorities are associated to the operation of the AI
regulatory sandbox
AI systems should fall under the remit of the authorities regardless of whether they are innovative, so
this word should be deleted to prevent any loophole.
Regarding Article 54 in general, we support the points raised in the EDPS-EDPB Joint Opinion that
“relationship of Article 54(1) of the Proposal to Article 54(2) and recital 41 of the Proposal and thus also
to existing EU data protection law remains unclear.” As noted in the Joint Opinion, “the GDPR and the
EUDPR already have an established basis for ‘further processing,’” and we fully support the additional
point that “balancing between the controller’s interests and the data subject’s interests do not have to
hinder innovation.” The legal basis of the grounds for further processing must be clarified, and the
necessity of any such grounds for further processing be considered against the risk they pose to
undermining existing protections.
Finally, Access Now believes that the use of AI regulatory sandboxes must be subject to the highest
level of public scrutiny and transparency. The intention of such sandboxes is to facilitate the
development of AI systems “for safeguarding substantial public interest”: as such, the public must be
able to know exactly what types of systems are being developed in them. In line with initiative such as
Findata, Finland’s innovative Social and Health Data Permit Authority that promotes secondary use
46
of health and social data, the AI regulatory sandboxes should provide publicly available information
regarding all requests to make use of the sandbox, all accepted and rejected applications, information
about project currently in development in the context of the sandbox, and follow up information on
what happens with projects a�erwards. Finally, sandboxing involves close scrutiny from regulators,
46 https://findata.fi/en/
26
</pre>",NEUTRAL
tika_2665462_26,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665462.pdf,31,26,2665462,attachments/2665462.pdf#page=26,Is the line order correct?,"<pre>Access Now’s submission to the European Commission’s adoption
consultation on the AI Act


(ii) public safety and public health, including disease prevention, control and treatment;


(iii) a high level of protection and improvement of the quality of the environment;


While we are generally concerned about establishing any grounds for the further processing of
personal data, due to the risk that such provisions pose to undermining the protections offered by
existing data protection law, we are particularly concerned about paragraph (i). Perhaps the riskiest,
and most problematic uses of AI are in the “prevention, investigation, detection or prosecution of
criminal offences or the execution of criminal penalties, including the safeguarding against and the
prevention of threats to public security.” Such uses of AI systems should, rather than being
incentivised or freed from scrutiny, be subject to the utmost caution, if they are to be pursued at all.
We therefore recommend that the use cases under paragraph (i) be removed from Article 54. We
also recommend that the term “innovative” be deleted from Article 53, paragraph 2:


- Member States shall ensure that to the extent the innovative AI systems involve the processing
of personal data or otherwise fall under the supervisory remit of other national authorities or
competent authorities providing or supporting access to data the national data protection
authorities and those other national authorities are associated to the operation of the AI
regulatory sandbox


AI systems should fall under the remit of the authorities regardless of whether they are innovative, so
this word should be deleted to prevent any loophole.


Regarding Article 54 in general, we support the points raised in the EDPS-EDPB Joint Opinion that
“relationship of Article 54(1) of the Proposal to Article 54(2) and recital 41 of the Proposal and thus also
to existing EU data protection law remains unclear.” As noted in the Joint Opinion, “the GDPR and the
EUDPR already have an established basis for ‘further processing,’” and we fully support the additional
point that “balancing between the controller’s interests and the data subject’s interests do not have to
hinder innovation.” The legal basis of the grounds for further processing must be clarified, and the
necessity of any such grounds for further processing be considered against the risk they pose to
undermining existing protections.


Finally, Access Now believes that the use of AI regulatory sandboxes must be subject to the highest
level of public scrutiny and transparency. The intention of such sandboxes is to facilitate the
development of AI systems “for safeguarding substantial public interest”: as such, the public must be
able to know exactly what types of systems are being developed in them. In line with initiative such as
Findata, Finland’s innovative Social and Health Data Permit Authority that promotes secondary use46


of health and social data, the AI regulatory sandboxes should provide publicly available information
regarding all requests to make use of the sandbox, all accepted and rejected applications, information
about project currently in development in the context of the sandbox, and follow up information on
what happens with projects a�erwards. Finally, sandboxing involves close scrutiny from regulators,


46 https://findata.fi/en/


26



https://findata.fi/en/</pre>",POSITIVE
pdfminer_2665249_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665249.pdf,7,2,2665249,attachments/2665249.pdf#page=2,Is the line order correct?,"<pre>Feedback to the Public Consultation on the 
Proposal for an Artificial Intelligence Act 
commitment to high-skilled labour and high value-added industries as the core of the European economic 
model. 
IndustriAll  Europe  further welcomes  that the  underlying  message  of  the  draft Regulation  is  clear: that 
European values should be at the core of this Regulation and that not everything that is technically feasible 
should  be  allowed.  We  therefore  specifically  welcome  the  prohibition  of  certain  AI  practices  and 
applications, as well as the introduction of mandatory transparency measures. We further welcome the 
proposed EU database on stand-alone, high-risk AI systems, which should be publicly accessible. This will 
contribute to foster trust in AI technologies and make the technology more transparent and reliable. 
IndustriAll Europe’s assessment:  
What we criticise about the proposal 
IndustriAll Europe believes that, even though the draft Regulation is quite comprehensive, there are still 
a number of points that should be made more precise, corrected, or added altogether. 
•  First of all, we are of the opinion that the proposal is full of loopholes and exceptions that should 
be closed. Too many vague formulations and definitions leave too much room for interpretation. 
Relevant  categories  when  discussing  data  are  not  addressed  at  all,  namely  that  of  ‘inclusivity’, 
‘non-discrimination’  and  ‘fairness’.  An  ambitious  landmark  legislation,  such  as  the  current 
proposal, should deal with all relevant categories that the subject it regulates touches upon. 
•  Secondly, we  think that the  definition of ‘high-risk’ AI is too narrow  and ignores too many  use 
cases  which  affect  workers  and  citizens  in  their  everyday  life.  Annex  III,  which  lists  high-risk 
applications, reads “AI systems intended to be used for...” which, again, leaves too much room for 
interpretation. In addition to the used wording, the different risk categories are too broad. A fully 
differentiated risk pyramid with more risk layers would be helpful to regulate the different types 
of  applications  in  question  in  a  more  application-oriented  manner.  One  example  for  a  more 
granular approach has been cited already in the EU Commission’s White Paper on AI (COM(2020) 
65 final) when referring to the five-level risk-based system as proposed by the German Data Ethics 
Commission.  We  would  like  to  underline,  however,  that  the  risk-based  approach  is  not  fit  for 
purpose and that a rights-based approach to the Regulation would have been preferable as this 
would allow for tailored regulations for AI/ML applications.  
•  Thirdly, it is highly problematic that the definition of ‘unacceptable’ AI seems to be final, with no 
mechanism in place to introduce new kinds of ‘unacceptable’ AI to the list. This would, in the worst 
case, lead to a situation in which new types of harmful AI are being developed without an adequate 
legal mechanism in place to prohibit its use or it being put on the market. There is also no provision 
in  place  to  prevent  AI/ML  applications  from  developing  features  that  would  be  considered 
‘unacceptable’, and it is not clear how those applications should be dealt with. This adds to our 
observation  that  the  language  used  is  often  insufficient.  Terms  such  as  “disproportionate”  or 
“unjustified” to describe “detrimental treatment” in the context  of social scoring are not  fit to 
contribute to a robust regulatory framework. Instead, legal terms that clearly indicate scope and 
intention of the prohibition should be introduced. 
•  We criticise the fact that the ban on remote biometric identification systems is only halfhearted, 
and only for law enforcement purposes. And even this already narrow ban contains a number of 
problematic loopholes, as it leaves wide discretionary power to the authorities, i.e. by including 
industriAll Europe Trade Union 
Page 2 of 7 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
PyPDF2_2665432_6,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665432.pdf,7,6,2665432,attachments/2665432.pdf#page=6,Is the line order correct?,"<pre> 
6 
Furthermore, t he proposal does not include  the possibility of harmed consumers to be 
represented by an NGO, including consumer organisations,  in the  exercise of their rights.  
An article similar to Art 80 GDPR  (Representation of data subjects)  or Article 68 of th e 
proposal  for a Digital Services Act  (Representation)  should  be introduced.  In the context 
of the AI Act , this representation shall not be subject to a mandate.  
 
Also, t o enable collective redress actions, t his proposal should be included  in the Annex of 
the Representative Actions Directive26. A similar provision was included in the Commission’s 
proposal for a Digital Services Act.27 
6. The governance and enforcement structure must be clear and ensure 
an effective  and consistent application of the rules  at European and 
national level  
The proposed  governance and enforcement structure mainly rests at national level  and 
raises issues  in relation to the obligations, competences and powers of the different actors 
involved28 and the different processes  envisaged .  
 
For example :  
- The need to ensure a coherent  and coordinated  EU-wide enforcement. While the 
Commission plays a central role if a national supervisory authority notifies its intention 
to adopt measures against an AI system in its territory29, the Commission  has no powers 
to proactively take the lead in case of inaction by national authori ties. The autonomy 
and powers of the European AI Board30, comprised of high-level representatives of the 
national supervisory authorit ies and chaired by the Commission , also seem quite 
limited.  
 
- There is a need to clarify potential overlaps with existing bodies such as the European 
Data Protection Board , as well as the role of Data Protection Authorities  – a view that 
other  stakeholders  also share31. 
 
- It is also important to ensure a co mmon approach when it comes to the cooperation  
between the different competent authorities and supervisory authorities at national 
level, to ensure effective and swift enforce ment as well as to  avoid different approache s 
by Member States . 
 
- Given the lack of redress mechanisms for consumers  in the proposal , there is no  
authority that is entrusted with dealing with consumer complaints in case of breaches 
of the regulation.  
 
 
26 Directive (EU) 2020/1828  on representative actions for the pr otection of the collective interests of consumers 
and repealing Directive 2009/22/EC ; 
27 See Article 72 of the Proposal for a Regulation on a Single Market For Digital Services ( Digital Services Act ) 
and amending Directive 2000/31/EC ;  
28 Namely: the European Artificial Intelligence Board (EAIB), the national competent authorities, the national 
supervisory authority, the Commission,  the European Data P rotection Supervisor (EDPS) and the AI system 
providers ); 
29 See Article s 65 (5) and 66 of the proposa l; 
30 See Articles 56 – 58 of the proposal;  
31 See, for example, Access Now: https://www.accessnow.org/eu -minimal -steps -to-regulate -harmful -ai-
systems/ . </pre>",POSITIVE
pdfminer_2665314_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2665314.pdf,6,2,2665314,attachments/2665314.pdf#page=2,Is the line order correct?,"<pre>We agree that under such a risk-based approach, most applications should fall outside of 
the scope of the regulation. 
4  PROHIBITED AI PRACTICES  
The Norwegian government supports the Commission's proposal for AI practices that 
shall be prohibited. However, we have a concern regarding the Article 5 – 1 (a) stating 
that an AI system that deploys subliminal techniques is prohibited if it is likely to cause 
physical or psychological harm. In our opinion, such manipulative practices are often 
used in cases that cause financial harm to people, which may in turn lead to 
psychological harm. We would propose that the prohibition of AI systems that deploy 
subliminal techniques is expanded to also include financial harm.  
Regarding Article 5 – 1 (c) the Norwegian Government supports the suggestion of 
prohibiting AI systems for evaluation or classification of the worthiness of natural persons 
where such use of AI-systems may lead to unjustified or disproportionate treatment of 
individuals. However, it should be carefully considered whether the terms 'unjustified or 
disproportionate' will provide sufficient safeguards against unwanted use of this type of 
AI-systems. The Norwegian government is concerned that this, in some cases, could 
allow for exclusion of individuals from the use of certain fundamental services that should 
be available to all, for instance health and care services.  
5  HIGH RISK AI SYSTEMS 
The Norwegian Government supports the overall approach to high risk AI systems in the 
Commission's proposal. We support the proposal for an EU database for high-risk AI 
systems. 
Determining borderline cases 
The term 'high risk system' is defined in article 6 with reference to lists in Annexes II and 
III. The Commission will be empowered to adopt delegated acts to update these lists. 
Deciding whether a system qualifies as a high-risk AI-system may require difficult 
considerations. It is our view that it may be necessary to adopt a procedure to determine 
borderline cases. We propose that such a procedure could be modelled after Article 4 in 
Regulation 2017/745 on medical devices. In addition, in order to ensure the best possible 
end user protection, we propose to include in the regulation that: 'In cases of doubt, 
where, taking into account all its characteristics, an ""AI system"" may be considered a 
""high-risk AI system"" the provisions of Title III, Chapter 2 of the Regulation apply'. Such 
provisions have been used in other EU regulations.  
Autonomous shipping 
Norway is at the forefront in the development of autonomous vessels and hosts a number 
of test sites. We recognise that safety systems for the navigation of such vessels are 
included in 'High-risk AI systems' through the inclusion of Directive 2014/90/EU in the list 
in Annex II. However, we see a growing activity around AI based systems intended for 
decision support to increase safety in harbours and fairways. This includes AI systems for 
Side 2 
 
 
 
 
 
 
</pre>",POSITIVE
PyPDF2_2665527_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665527.pdf,5,2,2665527,attachments/2665527.pdf#page=2,Is the line order correct?,"<pre>I. Introduction
Moje
Państwo
Foundation
(“
Foundation
”)
is
an
organization
working
for
the
development
of
democracy,
open
and
transparent
public
authority
and
civic
engagement.
By
exercising
the
right
of
access
to
public
information
and
the
right
to
re-use
public
sector
information,
the
Foundation
collects
publicly
available
data
sets
and
makes
them
available to citizens through the Foundation's services.
Due
to
the
fact
that
t
he
context
of
the
consultations
run
by
the
European
Commission
from
26th
April
to
6th
August
2021
are
related
to
many
relevant
aspects
for
the
Foundation’s
activity,
we
present
our
position
in
regards
to
the
Proposal
for
a
Regulation
Laying
Down
Harmonised
Rules
on
Artificial
Intelligence
(Artificial
Intelligence
Act)
(
“the
AI
Act
”
or
“
the
European
Commission’s
Proposal
”)
below
–
with
particular
emphasis
on the use of artificial intelligence systems (“
AI
systems
”) by public institutions.
II. General remarks
We
appreciate
the
work
of
the
European
Commission
undertaken
to
create
a
regulation
that
aims
to
ensure
that
artificial
intelligence
is
safe,
lawful
and
in
line
with
EU
fundamental
rights.
The
European
Commission’s
Proposal
contains
many
valuable
solutions creating a legal framework for artificial
intelligence.
From
the
perspective
of
our
Foundation,
the
AI
Act
should
–
to
a
greater
extent
–
ensure
transparency
of
the
public
sector
in
relation
to
the
use
of
artificial
intelligence systems.
AI
systems
can
be
used
by
public
institutions
in
many
key
areas,
such
as,
for
example,
health
protection,
education,
social
services,
the
judiciary
and
the
economy.
As
a
consequence,
these
systems
can
have
a
very
broad
and
varied
impact
and
can
affect
many
different
social
groups,
including
those
particularly
vulnerable
to
discrimination.
The
use
of
AI
systems
by
the
state
in
many
situations
may
lead
to
shaping
the
scope
of
rights and obligations of citizens by these systems.
The
European
Commission’s
Proposal
presupposes
measures
to
regulate
the
use
of
AI
systems
in
relation
to
public
authorities
-
for
example,
the
proposal
prohibits
AI-based
social
scoring
for
general
purposes
done
by
public
authorities;
the
use
of
‘real
time’
remote
biometric
identification
systems
in
publicly
accessible
spaces
for
the
purpose
of
law
enforcement
is
also
prohibited
unless
certain
limited
exceptions
apply;
Annex
III
1</pre>",NEUTRAL
PyPDF2_2665502_8,other,../24212003_requirements_for_artificial_intelligence/attachments/2665502.pdf,10,8,2665502,attachments/2665502.pdf#page=8,Is the line order correct?,"<pre>    
    
8 authorities and – due to unequal investments in the various member states’ authorities – citizen protection is not at the same level in each EU country.  Also in the context of this regulation, it should be considered that too much emphasis on the national level can lead to a risk of unequal implementation in different member states, at different speeds and potentially different interpretations. Belgium has, for instance, been lagging behind with regard to the implementation of the GDPR; this delay may affect innovation and a European level playing field, and the same risks to happen in the field of AI. Strong coordination at the European level will hence be crucial; also given the fact that many AI systems may be used transnationally and may be imported from third party countries. Moreover, given the importance of the risks attached to the use of AI as set out in this regulation, it will be essential that these authorities receive proper funding (and a sufficiently skilled workforce – which may be difficult in this field) so that they can provide adequate guidance for organizations and ensure a high level of citizen protection. The issue of different implementation speeds will also affect the creation of codes of conduct that can be voluntarily applied to AI systems other than high-risk systems. If it is assumed that the creation of a code of conduct is roughly the same effort and cost for any sector or member state, this absolute cost will mean that there may be more codes of conducts for sectors and member states with a higher turnover. Smaller member states with smaller markets will thus likely have less means to create these codes of conduct. This is another reason why a common European approach would be preferential. In this regard, the obligation to appoint an authorized representative established in the European Union in case an importer cannot be identified (recital 56 and article 25) is welcomed. Building on the experience with the GDPR, it is crucial to allow all organizations in charge of the implementation and enforcement of the regulation – be it the national competent authorities (NCAs), market surveillance authorities or other bodies – to be able to conduct all necessary steps towards the authorized representative, independently of where in the European Union it is established. Related to this point, it is essential that several NCAs can oversee the notified bodies and technical services performing the conformity assessment. In other words, not only the NCA of the country in which the notified bodies and technical services are established, but also NCA from other European member states should be able to do so, especially to ensure protection in case a specific NCA would be too under-resourced. Furthermore, while currently not foreseen in the proposed regulation, citizens should be provided with measures for redress and a right to file a complaint with national authorities, since this will not only help closing the protection gap of the proposal, but it can also help national authorities to assess and establish potential breaches of the regulation. In this way, public and private enforcement can be more complementary, and citizens will have a more active role in ensuring the protection of their rights. In the same line of thought, the link between the GDPR and this regulation should be highlighted. More than the fact that the Proposal does not, currently, foresee any mechanisms through which citizens can file a complaint, it seems to ignore the rights of citizens altogether. Despite the fact that the Recitals make numerous references to protecting “health, safety and fundamental rights,” the conceptual structure of the proposal is built on existing market surveillance schemes known from product safety legislation. The proposal seems to combine two concepts that are </pre>",NEUTRAL
pdfminer_2665345_14,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665345.pdf,17,14,2665345,attachments/2665345.pdf#page=14,Is the line order correct?,"<pre>Es ist deshalb zu definieren, wann man noch von Daten ohne Personenbezug reden kann. (siehe dazu 
auch https://www.akeuropa.eu/de/evaluation-der-datenschutz-grundverordnung-dsgvo)  
Die DSGVO enthält allgemeine Grundsätze, die die vielen Rechtskonflikte zwischen Geheimhaltungs- 
und Verwertungsinteressen nicht unmittelbar lösen können. Unzulässige Verarbeitungspraktiken aus-
zuforschen und rechtlich richtig zu würdigen, überfordert nicht nur KonsumentInnen, sondern aufwands-
bedingt zunehmend auch die Aufsichtsbehörden. Dies schadet der Rechtssicherheit und mindert das 
Vertrauen in die Vorteile von KI. Die Ausstattung der Behörden entspricht nicht dem Bedarf, um rasch, 
sorgfältig, technikkundig und investigativ den vielfältigen Aufsichtsaufgaben nachzukommen. Die Ver-
lagerung  von  einer  ex-ante  Prüfpflicht  in  sensiblen  Fällen  zu  einer  nachträglichen  Aufarbeitung  von 
Rechtsverletzungen  samt  Schadenersatzansprüchen  eröffnet  schwerwiegende  Schutzlücken,  wenn 
Rechtsdurchsetzung nicht rasch und reibungslos funktioniert. 
Verbot von Anwendungen, bei denen die „Accountability“ an Grenzen stößt: 
Mit der Selbstlernfähigkeit der Systeme können Softwareentwickler oft selbst nicht mehr nachvollziehen, 
welchen logischen Weg Algorithmen einschlagen. Entscheidet KI aber selbst darüber, welche Daten sie 
für welchen Zweck nutzt, widerspricht dies fundamental dem Rechtsgrundsatz der „Accountability“ (Zu-
rechnung, Verantwortung, Haftung), und kollidiert auch mit der Pflicht, im Erhebungszeitpunkt bereits 
den genauen Verwendungszweck der Daten anzugeben 
Alle Entscheidungen, Produkte und Dienste die auf Algorithmen basieren, müssen erklär- und überprüf-
bar  bleiben.  KonsumentInnen  dürfen  angesichts  einer  Vielzahl  an  Beteiligten  (Entwickler,  Hersteller, 
Anwender, Dienstleister) nicht zum Spielball unklarer Verantwortlichkeiten werden. Sie sollen im Sinne 
einer Solidarhaftung Unterlassungs- und Schadenersatzansprüche gegen jeden Beteiligten in der Wert-
schöpfungskette richten können (mit anbieterseitigen Regressmöglichkeiten). 
Einbindung der Betroffenen:  
Daten- und Privatsphärenschutz sollten wirtschaftlichen Interessen grundsätzlich vorgehen. Wie verhält 
es sich aber, wenn Eingriffe in diese Rechte mit lebenswichtigen Interessen einzelner Personen, von 
Gruppen  oder  der  Gesamtgesellschaft  begründet  werden?  Interessenskollisionen  sind  vorprogram-
miert, sobald KI-Anwendungen  im Gesundheitssektor Verbesserung  bei der  Erkennung,  Behandlung 
und Heilung von Krankheiten oder im sicherheitspolizeilichen Einsatz eine bessere Kriminalitätspräven-
tion bzw -aufklärung versprechen. Der Preis für diesen (potentiellen) Fortschritt ist hoch: Interessen von 
großen Bevölkerungsteilen können damit gefährdet werden. Vor diesem Hintergrund braucht es für die 
Mehrzahl an KI-Anwendungen, die Grundrechte berühren, eine ex ante-Genehmigung durch ein unab-
hängiges Gremium. In dieses sind neben Datenschutzbehörden und Technikexperten auch VertreterIn-
nen der jeweils betroffenen Gruppen (ArbeitnehmerInnen, KonsumentInnen, PatientInnen, etc) mitein-
zubeziehen. Denn auch bei der Klärung von Rechtsfragen wird sorgfältig zwischen verschiedenen Inte-
ressen, Verhältnismäßigkeiten, Werten etc. abzuwägen sein. Diese Entscheidungen können abhängig 
von der jeweiligen Betroffenheit und dem jeweiligen weltanschaulichen Hintergrund sehr verschieden 
ausfallen. Die gesellschaftliche Akzeptanz von Entscheidungen für oder gegen einzelne KI-Anwendun-
gen und flankierende Auflagen fällt höher aus, wenn bei der Zusammensetzung des Entscheidungsgre-
miums auf eine breite Beteiligung aller betroffenen Gruppen geachtet wird.   
Produkthaftungsregeln aktualisieren: 
Die  Produkthaftungs-RL  aus  dem  Jahr  1985  kennt  für  digitale  Trends  wie  KI  keine  Antworten.  Eine 
überarbeitete RL muss auf alle materiellen und nicht materiellen Sachen, digitale Dienstleistungen und 
digitalen Inhalte anwendbar sein und sollte deshalb auch Cybersicherheitsrisiken, mangelnde Software-
updates und unzureichende DSGVO-Konformität zu den „Defekten“ eines Produktes zählen. Ebenso 
Schäden, die durch die Fähigkeit selbst zu lernen und autonome Entscheidungen zu treffen oder durch 
einen Missbrauch der verwendeten Daten entstehen. Ausgezeichnete Detailvorschläge für die Überar-
beitung der Produkthaftungs-RL enthält das BEUC-Positionspapier „Product Liability 2.0“  
 
 
 
 
</pre>",POSITIVE
pdfminer_2665397_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665397.pdf,15,1,2665397,attachments/2665397.pdf#page=1,Is the line order correct?,"<pre>Commentary to the Commission’s proposal for the “AI Act” – 
Response to selected issues 
 Centre for Commercial Law, School of Law, University of Aberdeen 
This response is provided by a working group of the Centre for Commercial Law (CCL) at the 
University of Aberdeen. The working group consists of Dr Péter Cserne, Dr Rossana Ducato, 
and Dr Patricia Živković, and the response incorporates comments by Prof Abbe Brown, Dr 
Irène Couzigou, Dr Georgios Leontidis, Prof Nir Oren, Dr Clare Sutherland, Dr Paula Sweeney, 
Dr Burcu Yüksel Ripley. 
The analysis provided in this response contains a preliminary analysis of selected issues.  
The views and opinions reported in this response are submitted on behalf of the Centre for 
Commercial  Law  and do  not necessarily express  the  position  of  the  School  of  Psychology, 
School of Divinity, History, Philosophy & Art History, and the School of Natural and Computing 
Science.  
Table of Contents 
1.  Market integration, market regulation and fundamental rights .................................... 1 
2. 
The scope of application ................................................................................................ 2 
2.1. Focus on the pre-market stage ................................................................................... 2 
2.2. The extra-territorial effect ........................................................................................... 3 
3. 
The risk-based approach ................................................................................................ 4 
3.1. 
3.2. 
Prohibited AI practices ........................................................................................... 4 
High risk AI systems ................................................................................................ 8 
4.  Measures in support of innovation .............................................................................. 11 
5.  A place for the “AI subject” .......................................................................................... 13 
6. 
Plain language and beyond .......................................................................................... 14 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",NEUTRAL
PyPDF2_2665558_56,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665558.pdf,61,56,2665558,attachments/2665558.pdf#page=56,Is the line order correct?,"<pre>Forthcoming in Computer Law & Security Review  
- 56 - 
 Outsourcing in AI supply chains is similar to outsourcing in supply chains for physical 
products to evade workers’ rights, environmental protections, and minimum wage laws. In 
effect, the cross -border nature of technical supply chains potentially allows AI aaS providers 
to outsource key work underpinning AI services to other jurisdictions – escaping privacy, 
data protection, employment, and possibly other laws – and then offer those services to 
European customers. Any future regulatory initiatives relating t o AIaaS should consider 
whether AI systems offered as a service in the EU should be trained on data obtained, 
labelled, cleaned, and processed in accordance with European data protection, 
employment, and other relevant laws.  
 
4.3. Surveillance  
 
An overarching p olicy issue with AIaaS is how these services can enable AI -augmented 
surveillance. Internet -enabled surveillance (both public and private) has for some years 
crept into greater areas of contemporary life. AIaaS potentially facilitates a kind of AI -
augmente d ‘Surveillance as a Service’, particularly for those who would otherwise lack the 
technical capabilities or resources to develop systems of their own. Exacerbating these 
concerns is that physical spaces are increasingly monitored with cameras, microphones , and 
a range of sensors , collecting data from and about people in  those spaces  (potentially 
without those people’s knowledge  or their awareness of how it will be used) . For example, 
AI services could help retailers track customers through their stores and  analyse their 
behaviour; facial recognition or other biometric services could allow public spaces to be 
surveilled by public or private actors and otherwise ‘anonymous’ individuals identified and 
monitored; speech and voice recognition services could simi larly be used to monitor 
otherwise private conversations and identify individuals by voice.  
 
Rolling out AI -augmented surveillance systems cheaply and at scale, enabled by AIaaS, could 
potentially transform spaces (both physical and virtual) and relations hips and power 
dynamics between watchers and watched. AI capabilities enable those undertaking 
surveillance to move from passively watching people (which can itself affect their choices, 
 
spreads globally' Financial Times (24 July 2019) <https://www.ft.com/content/56dde36c -aa40-11e9 -984c -
fac8325aaa04 > acce ssed 13 November 2020.  </pre>",POSITIVE
fitz_2665518_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665518.pdf,2,2,2665518,attachments/2665518.pdf#page=2,Is the line order correct?,"<pre> 
Page 2 of 2 
 
The proposed regulation does not address the role of shop stewards in ensuring that worker's 
rights are safeguarded when artificial intelligence is introduced in the workplace. The best 
solutions are found through local dialogue between management and worker's representatives. 
To ensure co-determination from employees, Negotia believes that the social dialogue should 
have a clearly defined role in the regulation of artificial intelligence in the workplace. We also 
believe that the conformity assessment of artificial intelligence solutions that are intended to 
be used in the workplace should be performed by an independent third party. 
In conclusion, Negotia believes the European Commission's proposal to regulate artificial 
intelligence, is a great step in the right direction, but that it has clear weaknesses when it 
comes to regulating technology in the workplace. Further measures should be put in place to 
ensure workers' rights, in the face of technological developments. 
We are grateful for the opportunity to provide feedback on the proposal,and look forward to 
following the legislative process going forward. 
 
 
Monica A. Paulsen 
President of Negotia 
 
</pre>",POSITIVE
PyPDF2_2663324_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2663324.pdf,7,4,2663324,attachments/2663324.pdf#page=4,Is the line order correct?,"<pre>  
 
  
Page 4 of 7 
 
Establish a unified standard  for data exchan ge 
Incentives for organizations/vendors that align 
with common data standards  
Promote data interoperability  and exchange 
protocol , based on the experience of ERNs  
(Collaboration between several member states on 
the same topic)  
 Allow transfer of data between local centres  
Centralise data collection from regional data sets  
 
 
REGULATORY CHALLENGE S 
Validation process and Reimbursement  
Member States are slowly but surely adopting national legislatures allowing for digital health solutions to 
apply for reimbursement . While it is far from being general practice, the trend is growing, and regulatory 
agencies or Health Technology Assessment (HTA) bodies are adapting their pathways to include the 
assessment of digital health solutions, including AI -based solutions.  
While AI providing health data based on continuous monitoring (both passively and actively) of daily 
activities has a greater pote ntial for reimbursement, the solutions utilizing AI without immediate health 
benefits might find a more challengin g process.  
The data set used by the AI applications or used as secondary data (for instance, when an AI -based tool 
provides information on a n internal decision point ) in the application dossier for HTA, reimbursement 
process or post -marketing studies must be of the highest standards of quality . It is crucial to demonstrate 
the relevance  of the data in a lifecycle approach: in the context of appr oval and, at a later stage, 
reimbursement. Should the regulator dispute the quality and /or the validity of the data used by the algorithm, 
the entire process could be severely delayed, henceforth, delaying access to patients . 
While one can understand the c hallenge in adapting to an extremely changing world, as well as  to the 
massive disruption AI is about to bring to healthcare, it is indisputable that regulators need to issue clear 
guidance  on quality requirements for validation processes, reimbursement cr iteria , and post-marketing 
studies12. We also call for clearer visibility on the possible evolution of the regulatory framework, to take 
into account the coming development of technology (e.g.: adaptive AI).  
                                                      
12 Be it for surveillance or studies in the context of real -world evidence generation.  </pre>",POSITIVE
fitz_2663324_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2663324.pdf,7,4,2663324,attachments/2663324.pdf#page=4,Is the line order correct?,"<pre>  
 
  
Page 4 of 7 
 
Establish a unified standard for data exchange 
Incentives for organizations/vendors that align 
with common data standards 
Promote data interoperability and exchange 
protocol, based on the experience of ERNs 
(Collaboration between several member states on 
the same topic) 
 
Allow transfer of data between local centres 
Centralise data collection from regional data sets 
 
 
REGULATORY CHALLENGES 
Validation process and Reimbursement 
Member States are slowly but surely adopting national legislatures allowing for digital health solutions to 
apply for reimbursement. While it is far from being general practice, the trend is growing, and regulatory 
agencies or Health Technology Assessment (HTA) bodies are adapting their pathways to include the 
assessment of digital health solutions, including AI-based solutions. 
While AI providing health data based on continuous monitoring (both passively and actively) of daily 
activities has a greater potential for reimbursement, the solutions utilizing AI without immediate health 
benefits might find a more challenging process. 
The data set used by the AI applications or used as secondary data (for instance, when an AI-based tool 
provides information on an internal decision point) in the application dossier for HTA, reimbursement 
process or post-marketing studies must be of the highest standards of quality. It is crucial to demonstrate 
the relevance of the data in a lifecycle approach: in the context of approval and, at a later stage, 
reimbursement. Should the regulator dispute the quality and/or the validity of the data used by the algorithm, 
the entire process could be severely delayed, henceforth, delaying access to patients. 
While one can understand the challenge in adapting to an extremely changing world, as well as to the 
massive disruption AI is about to bring to healthcare, it is indisputable that regulators need to issue clear 
guidance on quality requirements for validation processes, reimbursement criteria, and post-marketing 
studies12. We also call for clearer visibility on the possible evolution of the regulatory framework, to take 
into account the coming development of technology (e.g.: adaptive AI). 
                                                      
12 Be it for surveillance or studies in the context of real-world evidence generation. 
</pre>",POSITIVE
PyPDF2_2665469_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665469.pdf,3,3,2665469,attachments/2665469.pdf#page=3,Is the line order correct?,"<pre> 
 
 
 
V1.0 5. august 2021  KMD  Side 3 af 3 
 
 
 
2.3 Sandboxes and data spaces  
Another requirement is the so -called AI regulatory sandboxes  (Title V Article 53 ) for testing 
and validating AI systems before placement on the market.  
 
It is unclear whether these sandbox environments will be provided by member states’ 
authorities themselves or the European Data Protection Supervisor, or if the task will be  
outsourced to private companies . The same goes for the European common data spaces  
(Recital 45 ). KMD suggests a further clarification in the proposa l. 
 
 
3 Clarification of various terms and concepts  
 
3.1 High -risk classification  
As for the protection of public interest in high -risk AI systems, the proposal states that 
common normative standards should be established  (Recital 13 ). KMD suggests a clarification 
of whether these common normative standards are part of the report on AI standards2. 
 
The requirements regarding accuracy, consistency , robustness , appro priateness  etc. in high -
risk AI systems ( Title III Article 15 ) furthermore lacks specification.  
 
Title XII A rticle 83 (2) regards the application of the regulation to high -risk AI systems already 
on the market. It states that these solutions will not be subject to the new regulation unless 
there are significant changes to design or purpose. KMD suggests a further elaboration on 
what constitutes a “significant change”.  
 
3.2 Placing o n the market  and putting into service  
The difference between “placing on the market” and “putting into service” ( Title 1 Article 
3(9,11 )) is subtle  but regards first time  a solution is made available to the market respectively 
first use . This raises a question about retrained AI systems f or a new problem dataset . A 
clarification of whether using retrained AI systems are  considered “first use”  would be 
beneficial.  
 
 
 
 
2 Cf. JRC Publications Repository - AI Standardisation Landscape: state of play and link to the EC proposal for an AI regulatory 
framework (europa.eu)  </pre>",POSITIVE
pdfminer_2662473_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662473.pdf,2,1,2662473,attachments/2662473.pdf#page=1,Is the line order correct?,"<pre>Fon +32 2 282 05-50 
info@dsv-europa.de 
www.dsv-europa.de 
Transparenzregister 
Nr. 917393784-31 
Deutsche Sozialversicherung 
Europavertretung 
Rue d’Arlon 50 
B-1000 Bruxelles 
Die öffentliche Konsultation der EU-
Kommission zum Vorschlag eines „Ge-
setzes über Künstliche Intelligenz“, COM 
(2021)206 final  
Stellungnahme der Deutschen Sozialversicherung vom 
14.07.2021 
Die Deutsche Rentenversicherung Bund (DRV Bund), die Deutsche Gesetzliche 
Unfallversicherung (DGUV), der GKV-Spitzenverband und die Verbände der ge-
setzlichen Kranken- und Pflegekassen auf Bundesebene und die Sozialversiche-
rung für Landwirtschaft, Forsten und Gartenbau (SVLFG) haben sich mit Blick auf 
ihre gemeinsamen europapolitischen Interessen zur „Deutschen Sozialversiche-
rung Arbeitsgemeinschaft Europa e. V.“ zusammengeschlossen.  
Der Verein vertritt die Interessen seiner Mitglieder gegenüber den Organen der 
Europäischen Union (EU) sowie anderen europäischen Institutionen und berät die 
relevanten Akteure im Rahmen aktueller Gesetzgebungsvorhaben und Initiativen.  
Die Kranken- und Pflegeversicherung, die Rentenversicherung und die Unfallver-
sicherung bieten als Teil eines gesetzlichen Versicherungssystems wirksamen 
Schutz vor den Folgen großer Lebensrisiken. 
Stellungnahme 
Die Deutsche Sozialversicherung begrüßt den Entwurf eines Gesetzes über 
Künstliche Intelligenz (KI). Er hat weitreichende Auswirkungen auf die Entwicklung 
und den Einsatz von KI, nicht zuletzt auch im Bereich der öffentlichen Verwaltung. 
Die Deutsche Sozialversicherung sieht mögliche mitgliedschafts-, beitrags- und 
leistungsrechtliche Bezüge und ist sich ihrer Verantwortung im Umgang mit KI be-
wusst. Sie begrüßt daher eine Klärung unter anderem der ethischen und haftungs-
rechtlichen Fragen. Dabei wird im Rahmen der weiteren Verhandlungen des Ver-
ordnungsentwurfs und darüber hinaus zu klären sein, bis zu welcher Detail- und 
Entscheidungstiefe ein europäisches Handeln erforderlich ist.    
 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
fitz_2662381_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662381.pdf,4,4,2662381,attachments/2662381.pdf#page=4,Is the line order correct?,"<pre> 
  
 
 
  
 
Pagina 4 van 4 
  
mogelijk ook meerdere lidstaten bevoegd zijn ten aanzien van één 
provider? 
• 
Is jurisdictie verbonden aan de vestigingsplaats van de provider, 
producent of gebruiker? Welke lidstaat is bevoegd om tot handhaving over 
te gaan, is nog onvoldoende scherp. Meer duiding of aanscherping is 
gewenst.  
 
6. Coördinatie tussen toezichthouders 
• 
Gedeelde verantwoordelijkheden betekent dat in de verschillende fasen 
van het toezicht de bevoegde autoriteit zaakkennis moet overdragen of 
dat een organisatie te maken krijgt met meerdere toezichthouders. Hoe 
wordt informatiedeling en coördinatie van toezichtsactiviteiten voorzien? 
En wat is de juridische basis? 
• 
Daarnaast vraagt effectief markttoezicht in Europa om naast nationaal 
toezicht een Europees netwerk op te richten waar aan gezamenlijke 
toezichtsactiviteiten, onderzoeken en handhaving wordt gewerkt.   
 
7. Rechtsbescherming burgers  
Er is onvoldoende voorzien in de mogelijkheid voor burgers om individuele 
problemen met AI te rapporteren, waardoor de bescherming tekortschiet. Op 
basis van de AI Act kijkt een toezichthouder naar het AI-systeem en de werking in 
het geheel, niet naar de gevolgen voor een individueel geval. Om de burgers 
voldoende rechtsbescherming te bieden moeten zij kunnen aangeven dat zij zijn 
benadeeld door de toepassing van AI.   
 
Het maatschappelijk vertrouwen in AI-toepassingen is afhankelijk van 
rechtsbescherming, effectief toezicht, transparante markttoegang en 
standaardisatie.  
 
i NISD. Directive (EU) 2016/1148 of the European Parliament and of the Council of 
6 July 2016 concerning measures for a high common level of security of network 
and information systems across the Union. 
ii eIDAS. VERORDENING (EU) Nr. 910/2014 VAN HET EUROPEES PARLEMENT EN DE 
RAAD van 23 juli 2014 betreffende elektronische identificatie en vertrouwensdiensten 
voor elektronische transacties in de interne markt. 
iii RED. Directive 2014/53/EU of the European Parliament and of the Council of 16 
April 2014 on the harmonisation of the laws of the Member States relating to the 
making available on the market of radio equipment. 
iv CSA. VERORDENING (EU) 2019/881 VAN HET EUROPEES PARLEMENT EN DE RAAD 
van 17 april 2019 zake ENISA (het Agentschap van de Europese Unie voor 
cyberbeveiliging), en inzake de certificering van de cyberbeveiliging van informatie- 
en communicatietechnologie. 
 
</pre>",POSITIVE
pdfminer_2663391_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663391.pdf,2,1,2663391,attachments/2663391.pdf#page=1,Is the line order correct?,"<pre>Madrid, 30 de julio de 2021 
La  Asociación  Nacional  de  Establecimientos  Financieros  de  Crédito  (ASNEF), 
representada  por  su  Secretario  General,  D.  Ignacio  Pla  Vidal,  y  debidamente  inscrita  en  el 
Registro  de  Transparencia  de  la  Unión  Europea  con  el  nº  11218815591-29,  presenta  las 
siguientes observaciones dentro del plazo de consulta pública abierta en relación con: 
Propuesta De Reglamento Del Parlamento Europeo Y Del Consejo Para El Establecimiento De 
Reglas  Armonizadas  Sobre 
Inteligencia  Artificial 
(Ley  De 
Inteligencia  Artificial) 
[COM(2021)206] 
En primer lugar, con relación al considerando 37, la propuesta de Reglamento menciona 
1. 
lo siguiente: 
“Habida  cuenta  del  alcance  sumamente  limitado  de  su  impacto  y  de  las  escasas 
alternativas  disponibles  en  el  mercado,  conviene  dejar  exentos  a  los  sistemas  de  IA 
destinados a evaluar la solvencia y la calificación crediticia cuando los pongan en servicio 
proveedores a pequeña escala para su propio uso.”  
Consideramos  de  suma  importancia  que  se  especifique  el  significado  de  ""proveedores  a 
pequeña  escala”  y  “para  su  propio  uso” en el  contexto  de  esta  excepción  a  la  consideración 
como riesgo alto y a la aplicación de los consiguientes requisitos y obligaciones. 
En segundo lugar, rogamos que se especifique cómo deberán cumplir aquellos sistemas 
2. 
ya en producción que, según esta regulación, son de alto riesgo. 
3. 
Por  último,  solicitamos  que  se  incluya  un  amplio  periodo  transitorio  para  su 
aplicabilidad, al objeto de que las entidades que ya están utilizando Inteligencia artificial para la 
evaluación de la solvencia y credit score puedan adaptarse a los nuevos requisitos. 
Velázquez, 64-66, 2ª planta - 28001 MADRID  
www.asnef.com  -  asnef@asnef.com 
CIF: G28516003 
Página 1 
 
 
 
 
 
 
 
 
 
 
</pre>",NEGATIVE
pdfminer_2662603_1,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662603.pdf,9,1,2662603,attachments/2662603.pdf#page=1,Is the line order correct?,"<pre>An Assessment of the AI Regulation Proposed by
the European Commission
Patrick Glauner
Abstract In April 2021, the European Commission published a proposed regulation
on AI. It intends to create a uniform legal framework for AI within the European
Union (EU). In this chapter, we analyze and assess the proposal. We show that the
proposed regulation is actually not needed due to existing regulations. We also argue
that the proposal clearly poses the risk of overregulation. As a consequence, this
would make the use or development of AI applications in safety-critical application
areas, such as in healthcare, almost impossible in the EU. This would also likely
further strengthen Chinese and US corporations in their technology leadership. Our
assessment is based on the oral evidence we gave in May 2021 to the joint session
of the European Union aﬀairs committees of the German federal parliament and the
French National Assembly.
1 Introduction
Artiﬁcial intelligence (AI) aims to automate human decision-making behavior and
is therefore also considered the next phase of the industrial revolution. We have
previously reviewed the state of the art and challenges of AI applications in healthcare
(Glauner, 2021a). This book provides a forecast of how AI and other technologies are
likelty to skyrocket healthcare in the foreseeable future. The European Commission
published proposed regulation in April 2021 (European Commission, 2021) that
intends to create a uniform legal framework for AI within the European Union (EU).
The proposal particularly addresses safety-critical applications, a category that also
Patrick Glauner
Deggendorf Institute of Technology, Deggendorf, Germany e-mail: patrick@glauner.info
Preprint. To appear in the 2022 Springer book “The Future Circle of Healthcare: AI, 3D
Printing, Longevity, Ethics, and Uncertainty Mitigation"" edited by Sepehr Ehsani, Patrick Glauner,
Philipp Plugmann and Florian M. Thieringer.
1
</pre>",POSITIVE
PyPDF2_2665425_8,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665425.pdf,27,8,2665425,attachments/2665425.pdf#page=8,Is the line order correct?,"<pre> 
 Artificial intelligence needs real world regulation  8 l 27 Verbraucherzentrale Bundesverband e.V.  
Legislators must complement  Article 3 with a definition  for non-professional user s 
of AI systems . This must entail p eople using AI systems in their capacity as con-
sumers  and citize ns. It must also consider  consumers  who are affected  by AI sys-
tems employed  by professional users . 
1.3 Article 3 (3 4) – Emotion recognition system  
The definition of ‘emotion recognition systems’ in  Art. 3 (34) is too narrow. The defini-
tion relies on the definition of biometric data  as defined in  Art. 3 (33 ), which itself is 
taken over from the General Data Prot ection Regulation (GDPR) .6 It holds that bio-
metric data must “allow or confirm the unique identification of that natural person.” As a 
consequence ‘emotion recognition systems’ that do not rely on data allowing  the unique 
identification of a natural person , will fall out of the scope of the AIA. However, vzbv 
holds that these types of systems should also fall under the AIA’s scope.   
This could include systems that rely only on the analysis of clicking, typing and cursor 
movement data for example. Also,  for an AI system supporting a retail salesperson  in a 
shop , it is not important to know th e identity of a potential customer entering the shop. 
The AI system can provide the shop personnel with valuable  real time personality/emo-
tion analysis data, based on the  customer behaviour.  For example inferences from 
measures on the relative tone/height, rhythm, and the speed of a voice, but not the 
voice itself.  
 The definition of ‘emotion recognition systems’ in Art. 3 (34) should not refer to  bio-
metric data  but to pers onal data . Otherwise , there is a significant risk for circumven-
tion of the legislation . 
2. THE SCOPE IS TOO NARROW  PART  I: NEGLECT OF ECONOMIC H ARMS AND 
VIOLATION S OF CONSUMER RIGHTS  
In general,  the s cope of the proposed AIA is too narrow and the legislation does not fo-
cus on consumers . The E uropean  Commission’s proposal f ocus es on problems of 
(product -)safety, health and fundamental rights linked to the use of AI systems . It 
mostly  deals with  high risks to people in their capacity  as citizens and employees , ne-
glect ing that  AI systems  can lead to significant  economic/financial welfare losses for 
consumers or  to violations of consumers’  rights.  
2.1 AI applications  with large economic/financial impact or effects on consumer 
rights must be regarded as high -risk  
The European Commission ’s proposal sees high-risks of AI systems nearly exclusively 
in the areas of (product -)safety, health and fundamental rights. The draft AIA focuses  
on mitigating risks to people in their capacity  as citize ns, patients, employees and stu-
dents (“education”). However, m ost AI systems  in these areas are already subject to 
European  legislation . Therefore,  in practices,  it can be doubted that  consumers will 
benefit much  from the draft AIA in these areas . 
____________________________________________________________ _______________________________  
6 Compare Art. 4 (14):  GDPR:  European Parliament: EU General Data Pro tection Regulation (GDPR) Regulation (EU) 
2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with 
regard to the processing of personal data and on the free movement of such data, and repealing Direc tive 95/46/EC 
(General Data Protection Regulation, OJ L 119, 4.5.2016, 2016.  </pre>",POSITIVE
pdfminer_2663366_9,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663366.pdf,13,9,2663366,attachments/2663366.pdf#page=9,Is the line order correct?,"<pre>set nutrition labels,” that can potentially provide a snapshot of how a system was developed and how 
it  performs.  These  tools,  however, are  still  being  developed.  There  are  no  clear  standards  for  such 
documentation efforts, nor have they yet been demonstrated to be practical at a meaningful scale.  
29 July 2021 
7.  Conformity assessments  
  Self-assessments – CIPL believes that relying on organisations’ self-assessments for most high-risk AI 
systems  strikes  a  good  balance  between  innovation  and  the  preservation  of  fundamental  rights. 
Requiring prior approval for all high-risk AI systems would be harmful to innovation (especially as the 
Commission notes that expertise for AI auditing is only now being accumulated). Many organisations 
have already established self-assessment processes in the context of GDPR and will be able to leverage 
them for high-risk AI assessment purposes.  
  No double reporting – CIPL underlines that it would be overly burdensome for all products that require 
conformity assessments under legislation listed in Annex II to also be subject to additional AI-specific 
requirements set out in the AI Act. Some video conferencing products’ endpoints, for example, require 
conformity assessments under the Radio Equipment Directive listed in Annex II of the AI Act. The use 
of  AI  in  such  systems  is  intended  to  enhance  collaboration  between  employees  and  is  therefore 
unlikely  to  be  high-risk.  Although  the  infrastructure  with  conformity  assessment  bodies  is  well-
established and efficient, it will be essential to make sure that there is no double reporting requirement 
and that these bodies are properly equipped to deal with this new role.  
8.  Regulatory Sandboxes 
  More clarity – CIPL welcomes the inclusion of a legal basis for regulatory sandboxes in the AI Act. The 
relevant provisions should be underpinned by clear and unambiguous rules for those making use of 
sandboxes,  including  sufficient  guidance  to  regulators  about  their  operation,  and  the  need  for 
consistency  in  approaches.  In  particular,  the  provisions  should  address  how  insights  and  learnings 
obtained from the regulatory sandbox exercises can inform the policy making process of the AI Act 
(including modifications to the Annexes), as well as its enforcement.  
 
Incentives - The AI Act needs to more clearly lay out the incentives for organisations to join sandboxes 
and the outcomes they can expect.8 CIPL encourages the Commission and the relevant regulators to 
work with industry to: (1) think about the specific functioning of sandboxes; (2) set them up in a way 
that truly helps companies to drive innovation in a protected environment to unearth learnings for all 
stakeholders involved; and (3) enable sandboxes to reach beyond SMEs and be made more inclusive. 9 
8 See CIPL Paper Regulatory Sandboxes in Data Protection – Constructive Engagement and Innovative Regulation in 
Practice. 
9 The AI Act currently prioritises regulatory sandboxes to small-scale providers and start-ups. The possible impact on 
a level playing field needs to be assessed as sandboxes can only take in a number of applications at any given time, 
and this is likely to be far smaller a number than the amount of AI innovations being developed in the market place.  
9 
 
 
 
 
 
 
 
 
 
 
                                                 
</pre>",POSITIVE
pdfminer_2665430_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665430.pdf,8,2,2665430,attachments/2665430.pdf#page=2,Is the line order correct?,"<pre>its functioning.7 There is nothing similar to this in the AI Act in relation to patients. But they need 
to be informed too - to make their own decisions about health and to enjoy their dignity and 
self-determination.  Also,  their  relevant  obligations  might  be  an  important  element  of  AI 
safety and quality.  
Although the requirements of informed medical consent and transparency obligations under the 
GDPR8 partly cover this issue, a technology-related and harmonized approach is needed. The rules 
on informed medical consent are not tailored to AI features (opaqueness, complexity, autonomy, 
self-learning).  It  might  lead  to  negative  effects  both  for  patients  (in  receiving  accessible  and 
meaningful information) and for healthcare professionals (in deciding on their own what kind of 
information and how to provide to patients). In addition, the rules on informed medical consent are 
defined by the national EU legislations and the lack of the harmonized approach (that sets at least 
minimum standards) can hinder the development of the EU-wide relevant initiatives such as the 
proposed AI Act and creation of the common EU data spaces.9  
Another aspect is the obligations of subjects affected by the decisions made with the use of AI. 
In the age of the information society, correcting and update of the information about  a specific 
person10 might be seen not only as a right, but also as an obligation. Taking the healthcare example, 
it is a patient who observes the final outcomes of the AI system’s use. In many cases, it is made in 
collaboration with doctors, but sometimes the outcome is visible only outside the environment of 
medical  organizations  and  patients  do  not  always  report  about  the  experienced  side  effects  or 
benefits. This affects not only the non-reporting patient but also the other population (because in 
this case, AI does not receive the proper information to learn from it).11  
The AI Act requires AI users ‘to ensure that input data is relevant in view of the intended purpose 
of  the  high-risk  AI  system  (to  the  extent  the  user  exercises  control  over  the  input  data).’12 
Considering this, the relevant obligation for subjects whose data is used in AI system to provide 
updates of the information to users seems to be justifiable. It would not only increase the safety and 
quality of AI applications but also support responsible and sustainable data use.13 In addition, it 
would enable AI users to properly comply with their obligations – on the basis of the information 
provided by beneficiaries of AI’s use. 
In a similar way all other requirements for high-risk AI systems and built in the already existing 
current  conformity  assessment  procedures,  the  minimum  requirements  in  relation  to  the  role  of 
subjects affected by the decisions made with the use of AI, can complement already existing rules 
(such the GDPR and domain-specific rights and obligations).  
7 EC Proposal for the AI Act, art. 29.  
8  The  transparency  requirements  under  the  GDPR  include,  for  example,  informed  consent  when  applicable  and  obligations  to 
provide information to data subjects in articles 12-15. But the scopes of the relevant legislations are different and thus influence 
the types, form, and amount of information provided to beneficiaries of decisions made with the use of AI systems. While the aim 
of transparency obligations under the GDPR is to protect the interests of data subjects in relation to the use of their personal data, 
their  rights  as  the  subjects  affected  by  decisions  made  with  the  use  of  AI  shall  be  concentrated  around  the  decision  and  its 
consequences (like safety and quality of the decisions, respect for fundamental rights, risks and benefits of the use AI). For example, 
respect  for  their  right  to  self-determination  (referring  generally  to  AI-assisted  decisions)  is  broader  than  informational  self-
determination (which is relevant to data protection). Although the specific requirements might be tailored in accordance with the 
narrow sector of AI use, some broader guidance on the rights of subjects affected by decisions made with the use of AI is needed.  
9 European Commission, Communication to the European Parliament, the Council, the European Economic and Social Committee 
and the Committee of the Regions ‘A European Strategy for Data’ COM (2020) 66 final (EU Strategy for Data). 
10 Which is guaranteed by the GDPR.   
11  Of  course,  this  obligation  might  be  seen  as  the  limitation  of  informational  self-determination,  but  neither  privacy  nor  data 
protection are the absolute rights, and they shall be balanced with other important interest (defined by law).  
12 EC Proposal for the AI Act, art. 29 (3).  
13 See more about sustainable data usage in Linnet Taylor and Nadezhda Purtova, ‘What Is Responsible and Sustainable Data 
Science?’ (2019) 1 Big Data & Society July–December 6, 1. How the AI Act is related to data initiatives such as the EU Data 
Strategy see in Kiseleva, A. & de Hert, P. ‘Creating a European Health Data Space: Obstacles in Four Key Legal Areas’, European 
Pharmaceutical Law Review, Volume 5, Issue 1 (2021), pp. 21 – 36.   
 
 
 
 
 
</pre>",POSITIVE
tika_2662780_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2662780.pdf,4,1,2662780,attachments/2662780.pdf#page=1,Is the line order correct?,"<pre>Education International 


Internationale de l'Education 


Internacional de la Educación 





http://www.ei-ie.org 


EUROPEAN REGION- 


ETUCE 





President 
Larry FLANAGAN 
 
Vice-Presidents 


Odile CORDELIER  
Andreas KELLER 
Trudy KERPERIEN 
Dorte LANGE 
Galina MERKULOVA  
Branimir STRUKELJ  
 


 
 
Boulevard Bischoffsheim, 15 
1000 Brussels, Belgium 
Tel +32 2 224 06 91/92 
Fax +32 2 224 06 94 
secretariat@csee-etuce.org 
http://www.csee-etuce.org 
 


European Director 
Susan FLOCKEN 
 
Treasurer 
Joan DONEGAN 











ETUCE  
European Trade Union Committee for Education 


EI European Region  
 








ETUCE position on the EU Regulation on Artificial Intelligence 


(Adopted by the ETUCE Bureau on 7 June 2021) 


Background: 


On 21 April 2021, the European Commission published a proposal for a “Regulation on a 


European Approach for Artificial intelligence” (the AI Regulation). With this proposal, the 


European Commission follows up on its White Paper on Artificial Intelligence (February 


2020), based on the results of a broad consultation process to which ETUCE contributed. 


The aim of the initiative is to establish the first EU legal framework regulating the entire 


lifecycle of the use of Artificial Intelligence (AI) in all sectors, including education.  


The AI Regulation classifies the use of Artificial Intelligence in various sectors based on the 


risk that the AI tools have on the health and safety and the fundamental rights of 


individuals. Concerning education, the proposal considers the use of Artificial Intelligence 


tools in education as high-risk as potentially harmful to the right to education and training 


as well as the right not to be discriminated in education. For high-risk sectors, the AI 


Regulation establishes stricter horizontal legal requirements to which AI tools must comply 


before being authorised on the market. These include risk management system during the 


entire lifecycle of the AI system.     


Following the publication of the proposal, on 26 April 2021, the European Commission 


issued a public consultation that will run until 20 July 2021, accompanied by an impact 


assessment report.  


The following text is the ETUCE response to the public consultation bringing the perspective 


of teachers, academics and other education personnel on the sections of the AI Regulation 


that touch upon the education sector.  





ETUCE reply: 


ETUCE welcomes the publication of the AI Regulation as it sets the ground for the first 


comprehensive EU regulation on Artificial Intelligence to ensure a controlled development 


of AI tools in education and address the risks connected to their use by teachers, academic, 


other education personnel and students. While ETUCE recognises the potential of digital 


technologies and Artificial Intelligence tools to bring about improvements in education, it 


also underlines the numerous ethical concerns related to their trustworthiness, data 


privacy, accountability, transparency and their impact on equality and inclusion in 


education. ETUCE underlines that further research at national and European level is needed 


to assess and address the risks connected to the use of Artificial Intelligence in education 


with constant and meaningful consultation with education social partners.  



http://www.csee-etuce.org/

https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence

https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence

https://ec.europa.eu/info/sites/default/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf

https://www.csee-etuce.org/en/resources/statements/3714-etuce-statement-on-the-european-commission-white-paper-on-artificial-intelligence

https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements_en

https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements_en

https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements_en</pre>",NEGATIVE
tika_2665600_2,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665600.pdf,6,2,2665600,attachments/2665600.pdf#page=2,Is the line order correct?,"<pre>Page 1 of 5 
 


1. Introduction 


The technical complexity, pervasive penetration and multifaceted social implications of the use of artificial 


intelligence (“AI”) in every-day life of EU citizens unequivocally necessitate the introduction of clear and 


sensible rules for development, deployment and use of AI. The endeavors in this respect of the European 


Union institutions, and of the European Commission in particular, are laudable. The strive to shape those 


rules in a way that promotes innovation and the uptake of AI in the European Union (the “EU”) is also 


praiseworthy.  


However, any future regulation of AI should also be introspective, smart and fair. Introspective in the sense 


that it should learn from and avoid deficiencies and past mistakes in other relevant areas of business 


regulation (e.g. data protection, antitrust). Smart in the sense that it should have built in already now a 


conceptual design and a toolkit that would allow addressing issues arising from the advance in AI (e.g. the 


emergence of strong and general AI). Fair in the sense that a future regulation of AI should – while creating 


a favourable business environment – also account and cater for the interests of society at large, including 


end users and citizens more generally. 


This position paper focuses on analysis of the proposal for a regulation of the European Parliament and the 


Council laying down harmonized rules on artificial intelligence (Artificial Intelligence Act) (the “Draft AI 


Regulation” or the “Draft”) and recommendations on the conceptual design of and certain fundamental 


rules under the Draft with view to the need of an introspective, smart and fair legal framework for design, 


development, deployment and use of AI in the EU.  





2. Risk-based approach and ethical/human rights backbone 


The Draft AI Regulation retains the risk-based approach and the ethical/human rights backbone initially 


contemplated in the European Commission’s White Paper on AI1 and the recommendations of the 


European Parliament of October 2020,2 while building upon regulation in other areas (e.g. data protection, 


consumer protection, standardization) to address the more specific technical aspects of AI design, 


development, deployment and use. This approach generally makes sense given how contextually 


dependent the deployment and use of AI are and how quickly the technology evolves. Too legalistic 


requirements might result in under- or overregulation.  


However, clearer emphasis needs to be put in the final text of the Artificial Intelligence Act on outcomes, 


rather than on formal course of action. Art. 13 to Art. 15 of the Draft already require attainment of certain 


overall outcomes (transparency, accuracy, etc.). Yet, other obligations under Chapters 2 and 3 of Title III 


presuppose mere process-like actions (e.g. documenting, record-keeping, risk management based on step-


by-step actions) in order to demonstrate compliance. If the emphasis in this framework does not firmly and 


eloquently lie with ultimate outcomes – and that various processes are one of the means to that end – 


compliance would morph into a “box-ticking”- and “window-dressing”-type of adherence to the formal 


requirements under the Artificial Intelligence Act, as is the case with similar rather “technical” requirements 


under the European Union Genera Data Protection Regulation.3 





3. Prohibited use of AI 


The explicit bans under Art. 5 of the Draft AI Regulation are a welcome development in an attempt to limit 


the use of AI to manipulative and/or privacy-invasive ends.  


However, the proposed language and approach have some deficiencies, as set out below.  


3.1 AI employing subliminal techniques or exploiting vulnerabilities</pre>",POSITIVE
tika_2662381_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662381.pdf,4,4,2662381,attachments/2662381.pdf#page=4,Is the line order correct?,"<pre>Pagina 4 van 4 
  


mogelijk ook meerdere lidstaten bevoegd zijn ten aanzien van één 


provider? 


• Is jurisdictie verbonden aan de vestigingsplaats van de provider, 


producent of gebruiker? Welke lidstaat is bevoegd om tot handhaving over 


te gaan, is nog onvoldoende scherp. Meer duiding of aanscherping is 


gewenst.  





6. Coördinatie tussen toezichthouders 


• Gedeelde verantwoordelijkheden betekent dat in de verschillende fasen 


van het toezicht de bevoegde autoriteit zaakkennis moet overdragen of 


dat een organisatie te maken krijgt met meerdere toezichthouders. Hoe 


wordt informatiedeling en coördinatie van toezichtsactiviteiten voorzien? 


En wat is de juridische basis? 


• Daarnaast vraagt effectief markttoezicht in Europa om naast nationaal 


toezicht een Europees netwerk op te richten waar aan gezamenlijke 


toezichtsactiviteiten, onderzoeken en handhaving wordt gewerkt.   





7. Rechtsbescherming burgers  


Er is onvoldoende voorzien in de mogelijkheid voor burgers om individuele 


problemen met AI te rapporteren, waardoor de bescherming tekortschiet. Op 


basis van de AI Act kijkt een toezichthouder naar het AI-systeem en de werking in 


het geheel, niet naar de gevolgen voor een individueel geval. Om de burgers 


voldoende rechtsbescherming te bieden moeten zij kunnen aangeven dat zij zijn 


benadeeld door de toepassing van AI.   





Het maatschappelijk vertrouwen in AI-toepassingen is afhankelijk van 


rechtsbescherming, effectief toezicht, transparante markttoegang en 


standaardisatie.  





i NISD. Directive (EU) 2016/1148 of the European Parliament and of the Council of 


6 July 2016 concerning measures for a high common level of security of network 


and information systems across the Union. 
ii eIDAS. VERORDENING (EU) Nr. 910/2014 VAN HET EUROPEES PARLEMENT EN DE 


RAAD van 23 juli 2014 betreffende elektronische identificatie en vertrouwensdiensten 


voor elektronische transacties in de interne markt. 
iii RED. Directive 2014/53/EU of the European Parliament and of the Council of 16 


April 2014 on the harmonisation of the laws of the Member States relating to the 


making available on the market of radio equipment. 
iv CSA. VERORDENING (EU) 2019/881 VAN HET EUROPEES PARLEMENT EN DE RAAD 


van 17 april 2019 zake ENISA (het Agentschap van de Europese Unie voor 


cyberbeveiliging), en inzake de certificering van de cyberbeveiliging van informatie- 


en communicatietechnologie.</pre>",POSITIVE
fitz_2663341_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663341.pdf,4,1,2663341,attachments/2663341.pdf#page=1,Is the line order correct?,"<pre> 
 
 About the GFII: created in 1979, the GFII, the French organization of information professionals, is 
a unique association in the data landscape that brings together private and public data producers 
and re-users, such as the French Ministry of Interior, INPI, IGN, Total, BNP Paribas, Roquette frères, 
Saint-Gobain, Wolters-Kluwer, Elsevier, Françis Lefebvre-Dalloz group, Altarès. It gathers lawyers, 
engineers, data experts, compliance officers... 
 
The GFII aims to promote the economy of data, that means the recognition of the costs necessary 
for their manufacture, maintenance, development and dissemination in an assumed commercial 
environment, which does not exclude free of charge data sharing, but which puts more emphasis 
on the interoperability of data, their valorisation and their reusability. The 6 working groups 
produce positions papers and white papers in order to help shaping the future of data policy in 
France and in the EU by offering a balanced and economically sustainable point of view about data 
and IA. The GFII promotes a sustainable and ethical use of data and works closely with its members 
for offering the most expert and efficient feedback about the implementation of data policies. GFII 
members may be AI systems providers, users or both. 
 
REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING 
DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL 
INTELLIGENCE ACT) 
First of all, thanks to the European Commission to enable the GFII to answer this consultation on the 
draft regulation on AI. 
 1) Readability of the text 
The draft is rather complex to understand; understanding difficulties may then generate difficulties for 
being compliant, especially for SMEs and start up.  
 a) We invite then the legislator: 
- to amend the structure of the document by separating: 
- requirements dedicated to AI systems defined in the article 6 (1) / annex II and possibly 
Annex III (1) 
- requirements dedicated to AI systems defined in the article 6 (2) 
- requirements dedicated to AI systems defined in the annex III (6 to 8) 
- to define the requirements from the AI provider point of view, ie the operator that will have to comply 
with the future regulation, with a logical ie process oriented redactional architecture, step by step, 
</pre>",POSITIVE
PyPDF2_2665480_38,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665480.pdf,64,38,2665480,attachments/2665480.pdf#page=38,Is the line order correct?,"<pre> 
   33 Despite not relying on personal data of natural persons, the fundamental rights implications of 
these systems are important because they are used to determine who can be subject to increased 
police intervention (based on geographical location), how  these interventions occur,  and with 
what frequency. Used for law enforcement purposes, without due care, resource optimisation 
systems may contribute to over-policing and surveillance of specific geographical locations 
caused by data ‘feedback loops,’58 and in doing so, may further exacerbate existing problems 
with systemic discrimination arising from historical racial and socio -economic biases in 
existing  policing datasets (as geographical location is often a proxy for race or economic class), 
with little opportunity for redress, and no transparency for affected individuals. Drawing from 
Philip Alston speaking about the SyRI welfare fraud detection system in the Netherlands, 
targeting entire neighbourhoods as suspect and “subject to special scrutiny” with a combination 
of digital and physical methods threatens the very essence of privacy, by contributing to general 
unease, potential prejudice,  and chilling effects on behaviour.59 It matters little whether the 
personal data of natural persons is implicated here.  
Were  the final text of the Regulation therefore to exclude resource optimisation systems, this 
could amount  to a significant failure to recognise the systemic social assumptions that are built 
into AI systems – as socio -technical systems which mediate social i nstitutions and structures – 
which by virtue of their implementation, further mediate the enjoyment of individuals’ 
fundamental rights, including respect for their human dignity, equality, liberty and other 
freedoms. We therefore recommend that geospatial AI systems are included under Annex 
III(6), and that reference is made to AI systems which affect the distribution of law enforcement 
resources.    
e) The requirements that high -risk AI systems must  comply with need to be strengthened and 
clarified  
Our final remarks on the proposed regulatory framework for high -risk AI systems concerns the 
strength and clarity of the requirements for such systems. While vague language – which might 
cause  legal uncertainty and a weak protection against AI’s adverse effects – can be found under 
several requirements for high -risk systems, we highlight a few questions and concerns 
regarding three requirements in particular: those pertaining to data governance , transparency  
and human oversight.  
Data governance obligations  
The first  requirement which raises questions is ‘data governance.’  Firstly, the large discretion 
for providers of high -risk AI systems mentioned earlier is also reflected in the requirements for 
data governance . Article 10 of the Pro posal, dealing with requirements of data quality and 
governance, also lets the term ‘appropriate’ do some heavy lifting. Indeed, it requires that 
training, validation and testing data sets shall be subject to ‘appropriate’ data governance and 
management pr actices, that the data sets shall have ‘appropriate’ statistical properties, and that 
the processing of special categories of data to avoid the risk of bias is carried out subject to 
‘appropriate’ safeguards for fundamental rights. While the Article can be  commended for 
specifying the minimal considerations that should be taken into account for the data 
management process to be considered ‘appropriate,’ it leaves open what constitutes an 
 
58  The Law Society of England  and Wales, “Algorithms in the Criminal Justice System,” June 4, 2019, 
https://www.lawsociety.org.uk/en/topics/research/algorithm -use-in-the-criminal -justice -system -report, 35.  
59  Philip Alston, “Brief by the United Nations Special Rapporteur on extreme po verty and human rights as 
Amicus Curiae  in the case of NJCM c.s./De Staat der Nederlanden (SyRI) before the District Court of the 
Hague (case number: C/9/550982/HA ZA 18/388),” OHCHR , September 26, 2019, 
https://www.ohchr.org/Documents/Issues/Poverty/Amicusfinalversionsigned.pdf, 29.  </pre>",POSITIVE
fitz_2665430_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665430.pdf,8,2,2665430,attachments/2665430.pdf#page=2,Is the line order correct?,"<pre>its functioning.7 There is nothing similar to this in the AI Act in relation to patients. But they need 
to be informed too - to make their own decisions about health and to enjoy their dignity and 
self-determination. Also, their relevant obligations might be an important element of AI 
safety and quality.  
 
Although the requirements of informed medical consent and transparency obligations under the 
GDPR8 partly cover this issue, a technology-related and harmonized approach is needed. The rules 
on informed medical consent are not tailored to AI features (opaqueness, complexity, autonomy, 
self-learning). It might lead to negative effects both for patients (in receiving accessible and 
meaningful information) and for healthcare professionals (in deciding on their own what kind of 
information and how to provide to patients). In addition, the rules on informed medical consent are 
defined by the national EU legislations and the lack of the harmonized approach (that sets at least 
minimum standards) can hinder the development of the EU-wide relevant initiatives such as the 
proposed AI Act and creation of the common EU data spaces.9  
 
Another aspect is the obligations of subjects affected by the decisions made with the use of AI. 
In the age of the information society, correcting and update of the information about a specific 
person10 might be seen not only as a right, but also as an obligation. Taking the healthcare example, 
it is a patient who observes the final outcomes of the AI system’s use. In many cases, it is made in 
collaboration with doctors, but sometimes the outcome is visible only outside the environment of 
medical organizations and patients do not always report about the experienced side effects or 
benefits. This affects not only the non-reporting patient but also the other population (because in 
this case, AI does not receive the proper information to learn from it).11  
 
The AI Act requires AI users ‘to ensure that input data is relevant in view of the intended purpose 
of the high-risk AI system (to the extent the user exercises control over the input data).’12 
Considering this, the relevant obligation for subjects whose data is used in AI system to provide 
updates of the information to users seems to be justifiable. It would not only increase the safety and 
quality of AI applications but also support responsible and sustainable data use.13 In addition, it 
would enable AI users to properly comply with their obligations – on the basis of the information 
provided by beneficiaries of AI’s use. 
 
In a similar way all other requirements for high-risk AI systems and built in the already existing 
current conformity assessment procedures, the minimum requirements in relation to the role of 
subjects affected by the decisions made with the use of AI, can complement already existing rules 
(such the GDPR and domain-specific rights and obligations).  
 
7 EC Proposal for the AI Act, art. 29.  
8 The transparency requirements under the GDPR include, for example, informed consent when applicable and obligations to 
provide information to data subjects in articles 12-15. But the scopes of the relevant legislations are different and thus influence 
the types, form, and amount of information provided to beneficiaries of decisions made with the use of AI systems. While the aim 
of transparency obligations under the GDPR is to protect the interests of data subjects in relation to the use of their personal data, 
their rights as the subjects affected by decisions made with the use of AI shall be concentrated around the decision and its 
consequences (like safety and quality of the decisions, respect for fundamental rights, risks and benefits of the use AI). For example, 
respect for their right to self-determination (referring generally to AI-assisted decisions) is broader than informational self-
determination (which is relevant to data protection). Although the specific requirements might be tailored in accordance with the 
narrow sector of AI use, some broader guidance on the rights of subjects affected by decisions made with the use of AI is needed.  
9 European Commission, Communication to the European Parliament, the Council, the European Economic and Social Committee 
and the Committee of the Regions ‘A European Strategy for Data’ COM (2020) 66 final (EU Strategy for Data). 
10 Which is guaranteed by the GDPR.   
11 Of course, this obligation might be seen as the limitation of informational self-determination, but neither privacy nor data 
protection are the absolute rights, and they shall be balanced with other important interest (defined by law).  
12 EC Proposal for the AI Act, art. 29 (3).  
13 See more about sustainable data usage in Linnet Taylor and Nadezhda Purtova, ‘What Is Responsible and Sustainable Data 
Science?’ (2019) 1 Big Data & Society July–December 6, 1. How the AI Act is related to data initiatives such as the EU Data 
Strategy see in Kiseleva, A. & de Hert, P. ‘Creating a European Health Data Space: Obstacles in Four Key Legal Areas’, European 
Pharmaceutical Law Review, Volume 5, Issue 1 (2021), pp. 21 – 36.   
</pre>",POSITIVE
fitz_2665397_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665397.pdf,15,1,2665397,attachments/2665397.pdf#page=1,Is the line order correct?,"<pre> 
 
 
 
 
 
 
 
Commentary to the Commission’s proposal for the “AI Act” – 
Response to selected issues 
 
 Centre for Commercial Law, School of Law, University of Aberdeen 
 
 
This response is provided by a working group of the Centre for Commercial Law (CCL) at the 
University of Aberdeen. The working group consists of Dr Péter Cserne, Dr Rossana Ducato, 
and Dr Patricia Živković, and the response incorporates comments by Prof Abbe Brown, Dr 
Irène Couzigou, Dr Georgios Leontidis, Prof Nir Oren, Dr Clare Sutherland, Dr Paula Sweeney, 
Dr Burcu Yüksel Ripley. 
The analysis provided in this response contains a preliminary analysis of selected issues.  
The views and opinions reported in this response are submitted on behalf of the Centre for 
Commercial Law and do not necessarily express the position of the School of Psychology, 
School of Divinity, History, Philosophy & Art History, and the School of Natural and Computing 
Science.  
 
Table of Contents 
1. 
Market integration, market regulation and fundamental rights .................................... 1 
2. 
The scope of application ................................................................................................ 2 
2.1. Focus on the pre-market stage ................................................................................... 2 
2.2. The extra-territorial effect ........................................................................................... 3 
3. 
The risk-based approach ................................................................................................ 4 
3.1. 
Prohibited AI practices ........................................................................................... 4 
3.2. 
High risk AI systems ................................................................................................ 8 
4. 
Measures in support of innovation .............................................................................. 11 
5. 
A place for the “AI subject” .......................................................................................... 13 
6. 
Plain language and beyond .......................................................................................... 14 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
fitz_2665504_10,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665504.pdf,11,10,2665504,attachments/2665504.pdf#page=10,Is the line order correct?,"<pre>(d) to determine the safety and compatibility with potential recipients;
(e) to predict treatment response or reactions;
(f) to define or monitoring therapeutic measures.
Later on, in Chapter II of that regulation, EU regulates more specifically the requirements 
regarding performance, design and manufacture. Here, all articles apply also to what would be
called “AI” systems, given the previous definition. But some extra specificities are added, in 
point 16: 
16.   Electronic programmable systems — devices that incorporate electronic programmable 
systems and software that are devices in themselves
16.1.   Devices that incorporate electronic programmable systems, including software, or 
software that are devices in themselves, shall be designed to ensure repeatability, reliability 
and performance in line with their intended use. In the event of a single fault condition, 
appropriate means shall be adopted to eliminate or reduce as far as possible consequent risks
or impairment of performance.
16.2.   For devices that incorporate software or for software that are devices in themselves, the 
software shall be developed and manufactured in accordance with the state of the art taking 
into account the principles of development life cycle, risk management, including information 
security, verification and validation.
16.3.   Software referred to in this Section that is intended to be used in combination with 
mobile computing platforms shall be designed and manufactured taking into account the 
specific features of the mobile platform (e.g. size and contrast ratio of the screen) and the 
external factors related to their use (varying environment as regards level of light or noise).
16.4.   Manufacturers shall set out minimum requirements concerning hardware, IT networks 
characteristics and IT security measures, including protection against unauthorised access, 
necessary to run the software as intended.
We see here that the article regulated important aspects also pursued by the proposal of 
regulation on AI: security, availability, performance qualification, … It is not the aim of this note 
to dive more into IVD regulation, but the interested reader can verify that many other aspects 
of interest in the project of regulation on AI are properly dealt within that regulation, without 
once entering into the details of the technologies (which makes it stronger, more future-proof).
Proceeding this way is also a guarantee that any manufacturer of a specific kind of product, 
service, application will manage properly with the necessary requirements. It will also 
guarantee there is no conflict between an application-specific regulation and a technology-
specific regulation. 
Copyright @T.Helleputte – 2021
Page 10/11
</pre>",POSITIVE
fitz_2665558_56,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665558.pdf,61,56,2665558,attachments/2665558.pdf#page=56,Is the line order correct?,"<pre>Forthcoming in Computer Law & Security Review 
- 56 - 
 
Outsourcing in AI supply chains is similar to outsourcing in supply chains for physical 
products to evade workers’ rights, environmental protections, and minimum wage laws. In 
effect, the cross-border nature of technical supply chains potentially allows AIaaS providers 
to outsource key work underpinning AI services to other jurisdictions – escaping privacy, 
data protection, employment, and possibly other laws – and then offer those services to 
European customers. Any future regulatory initiatives relating to AIaaS should consider 
whether AI systems offered as a service in the EU should be trained on data obtained, 
labelled, cleaned, and processed in accordance with European data protection, 
employment, and other relevant laws. 
 
4.3. Surveillance 
 
An overarching policy issue with AIaaS is how these services can enable AI-augmented 
surveillance. Internet-enabled surveillance (both public and private) has for some years 
crept into greater areas of contemporary life. AIaaS potentially facilitates a kind of AI-
augmented ‘Surveillance as a Service’, particularly for those who would otherwise lack the 
technical capabilities or resources to develop systems of their own. Exacerbating these 
concerns is that physical spaces are increasingly monitored with cameras, microphones, and 
a range of sensors, collecting data from and about people in those spaces (potentially 
without those people’s knowledge or their awareness of how it will be used). For example, 
AI services could help retailers track customers through their stores and analyse their 
behaviour; facial recognition or other biometric services could allow public spaces to be 
surveilled by public or private actors and otherwise ‘anonymous’ individuals identified and 
monitored; speech and voice recognition services could similarly be used to monitor 
otherwise private conversations and identify individuals by voice.  
 
Rolling out AI-augmented surveillance systems cheaply and at scale, enabled by AIaaS, could 
potentially transform spaces (both physical and virtual) and relationships and power 
dynamics between watchers and watched. AI capabilities enable those undertaking 
surveillance to move from passively watching people (which can itself affect their choices, 
 
spreads globally' Financial Times (24 July 2019) <https://www.ft.com/content/56dde36c-aa40-11e9-984c-
fac8325aaa04> accessed 13 November 2020. 
</pre>",NEUTRAL
fitz_2660134_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660134.pdf,4,2,2660134,attachments/2660134.pdf#page=2,Is the line order correct?,"<pre>Real Decreto 1828/1999, de 3 de diciembre, es un registro jurídico de titularidades y gravámenes 
sobre bienes muebles, donde rigen y se aplican los principios hipotecarios reguladores del 
Registro de la Propiedad y Mercantil, que tiene por objeto la inscripción de actos y contratos 
sobre bienes muebles, previo control de su legalidad, proporcionando publicidad y 
transparencia a la propiedad y a las cargas o gravámenes que sobre bienes muebles se pudieran 
establecer. 
El Registro de Bienes Muebles está integrado, por razón de su objeto, por las siguientes 
secciones: (i) Sección de buques y aeronaves; (ii) Sección de automóviles y otros vehículos a 
motor; (iii) Sección de maquinaria industrial, establecimientos mercantiles y bienes de equipo; 
(iv) Sección de garantías reales; (v) Sección de otros bines muebles registrables y (vi) Sección de 
condiciones generales de la contratación. 
La inscripción en dichos registros jurídicos nacionales elevaría el nivel de protección jurídica de 
los ciudadanos que tienen relación con los sistemas de inteligencia artificial, algoritmos y demás 
elementos relacionados con este tipo de tecnología ya que dicha inscripción se produciría tras 
el cumplimiento de los requerimientos de publicidad, transparencia y seguridad jurídica 
prescritos en la legislación hipotecaria, especialmente el control de legalidad de la operación, 
exigido por el artículo 72 de la Ley de Hipoteca Mobiliaria y prenda sin desplazamiento. Esta 
normativa hipotecaria garantiza la oponibilidad erga omnes y presunción de exactitud, 
legitimación y fe pública de los derechos inscritos sobre activos mobiliarios en los registros 
jurídicos de bienes muebles. La transparencia y seguridad jurídica de las titularidades y cargas 
de dichos activos está también garantizada mediante su publicidad, profesionalmente 
responsable y respetuosa con las exigencias de la normativa de protección de datos, que se 
puede obtener por canales telemáticos existentes en los mencionados registros públicos 
conforme a lo establecido por el artículo 78 de la Ley de Hipoteca Mobiliaria y Prenda sin 
desplazamiento.  
Dicho incremento de la seguridad jurídica llevaría consigo asociado el fortalecimiento de las 
posibilidades de financiación de sus creadores, inversores o titulares ofreciendo dichos activos 
como garantía inscrita en los registros jurídicos correspondientes.  
Los usuarios de estos sistemas o algoritmos podrían identificar perfectamente quiénes serían 
los responsables en caso de mal funcionamiento de los mismos no teniendo que seguir un largo 
camino hasta su identificación. Siendo, por tanto, posible la fácil asignación a un titular de la 
responsabilidad por mal funcionamiento.  
En relación a dicha concreción de titularidad para la reclamación de responsabilidades en la 
Resolución de 2017 antes citada se va un paso más y se indica como recomendación que se 
podría dotar a los robots inteligentes de “una personalidad jurídica específica” de modo que 
los “robots autónomos más complejos puedan ser considerados personas electrónicas 
responsables de reparar los daños que puedan causar y posiblemente aplicar la personalidad 
electrónica a aquellos supuestos en los que los robots tomen decisiones autónomas inteligentes 
o interactúen con terceros de forma independiente.”  
La inscripción en el registro de bienes muebles correspondiente al lugar donde el titular de los 
sistemas de inteligencia artificial, algoritmos y demás elementos tenga su centro de actividades, 
su residencia habitual o su centro de administración, o alternativamente, donde tuviera una 
relevante conexión con el sistema empleado por causa del archivo en una base de datos pública 
nacional,  podría además vincularse a través de la referencia, vía artículo 75 LHMPSD, con la 
inscripción correspondiente del titular (generalmente una sociedad) en el Registro Mercantil 
</pre>",POSITIVE
pdfminer_2665452_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665452.pdf,4,2,2665452,attachments/2665452.pdf#page=2,Is the line order correct?,"<pre>Corporate Communications 
  Press release 
 12 October 2020 
Date 
Subject 
  BMW Group code of ethics for artificial intelligence. 
Page 
  2 
Seven principles covering the development and application of artificial 
intelligence at the BMW Group: 
•  Human agency and oversight.  
The BMW Group implements appropriate human monitoring of decisions made by 
AI applications and considers possible ways that humans can overrule algorithmic 
decisions. 
•  Technical robustness and safety.  
The BMW Group aims to develop robust AI applications and observes the applicable 
safety standards designed to decrease the risk of unintended consequences and 
errors. 
•  Privacy and data governance.  
The BMW Group extends its state-of-the-art data privacy and data security 
measures to cover storage and processing in AI applications. 
•  Transparency.  
The BMW Group aims for explainability of AI applications and open communication 
where respective technologies are used. 
•  Diversity, non-discrimination and fairness.  
The BMW Group respects human dignity and therefore sets out to build fair AI 
applications. This includes preventing non-compliance by AI applications. 
•  Environmental and societal well-being.  
The BMW Group is committed to developing and using AI applications that promote 
the well-being of customers, employees and partners. This aligns with the BMW 
Group’s goals in the areas of human rights and sustainability, which includes climate 
change and environmental protection. 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
tika_2665480_38,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665480.pdf,64,38,2665480,attachments/2665480.pdf#page=38,Is the line order correct?,"<pre>33 


Despite not relying on personal data of natural persons, the fundamental rights implications of 


these systems are important because they are used to determine who can be subject to increased 


police intervention (based on geographical location), how these interventions occur, and with 


what frequency. Used for law enforcement purposes, without due care, resource optimisation 


systems may contribute to over-policing and surveillance of specific geographical locations 


caused by data ‘feedback loops,’58 and in doing so, may further exacerbate existing problems 


with systemic discrimination arising from historical racial and socio-economic biases in 


existing policing datasets (as geographical location is often a proxy for race or economic class), 


with little opportunity for redress, and no transparency for affected individuals. Drawing from 


Philip Alston speaking about the SyRI welfare fraud detection system in the Netherlands, 


targeting entire neighbourhoods as suspect and “subject to special scrutiny” with a combination 


of digital and physical methods threatens the very essence of privacy, by contributing to general 


unease, potential prejudice, and chilling effects on behaviour.59 It matters little whether the 


personal data of natural persons is implicated here. 


Were the final text of the Regulation therefore to exclude resource optimisation systems, this 


could amount to a significant failure to recognise the systemic social assumptions that are built 


into AI systems – as socio-technical systems which mediate social institutions and structures – 


which by virtue of their implementation, further mediate the enjoyment of individuals’ 


fundamental rights, including respect for their human dignity, equality, liberty and other 


freedoms. We therefore recommend that geospatial AI systems are included under Annex 


III(6), and that reference is made to AI systems which affect the distribution of law enforcement 


resources.    


e) The requirements that high-risk AI systems must comply with need to be strengthened and 
clarified 


Our final remarks on the proposed regulatory framework for high-risk AI systems concerns the 


strength and clarity of the requirements for such systems. While vague language – which might 


cause legal uncertainty and a weak protection against AI’s adverse effects – can be found under 


several requirements for high-risk systems, we highlight a few questions and concerns 


regarding three requirements in particular: those pertaining to data governance, transparency 


and human oversight.  


Data governance obligations  


The first requirement which raises questions is ‘data governance.’ Firstly, the large discretion 


for providers of high-risk AI systems mentioned earlier is also reflected in the requirements for 


data governance. Article 10 of the Proposal, dealing with requirements of data quality and 


governance, also lets the term ‘appropriate’ do some heavy lifting. Indeed, it requires that 


training, validation and testing data sets shall be subject to ‘appropriate’ data governance and 


management practices, that the data sets shall have ‘appropriate’ statistical properties, and that 


the processing of special categories of data to avoid the risk of bias is carried out subject to 


‘appropriate’ safeguards for fundamental rights. While the Article can be commended for 


specifying the minimal considerations that should be taken into account for the data 


management process to be considered ‘appropriate,’ it leaves open what constitutes an 


 
58  The Law Society of England and Wales, “Algorithms in the Criminal Justice System,” June 4, 2019, 


https://www.lawsociety.org.uk/en/topics/research/algorithm-use-in-the-criminal-justice-system-report, 35. 
59  Philip Alston, “Brief by the United Nations Special Rapporteur on extreme poverty and human rights as 


Amicus Curiae in the case of NJCM c.s./De Staat der Nederlanden (SyRI) before the District Court of the 


Hague (case number: C/9/550982/HA ZA 18/388),” OHCHR, September 26, 2019, 


https://www.ohchr.org/Documents/Issues/Poverty/Amicusfinalversionsigned.pdf, 29.</pre>",POSITIVE
tika_2665558_56,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665558.pdf,61,56,2665558,attachments/2665558.pdf#page=56,Is the line order correct?,"<pre>Forthcoming in Computer Law & Security Review 


- 56 - 
 


Outsourcing in AI supply chains is similar to outsourcing in supply chains for physical 


products to evade workers’ rights, environmental protections, and minimum wage laws. In 


effect, the cross-border nature of technical supply chains potentially allows AIaaS providers 


to outsource key work underpinning AI services to other jurisdictions – escaping privacy, 


data protection, employment, and possibly other laws – and then offer those services to 


European customers. Any future regulatory initiatives relating to AIaaS should consider 


whether AI systems offered as a service in the EU should be trained on data obtained, 


labelled, cleaned, and processed in accordance with European data protection, 


employment, and other relevant laws. 





4.3. Surveillance 





An overarching policy issue with AIaaS is how these services can enable AI-augmented 


surveillance. Internet-enabled surveillance (both public and private) has for some years 


crept into greater areas of contemporary life. AIaaS potentially facilitates a kind of AI-


augmented ‘Surveillance as a Service’, particularly for those who would otherwise lack the 


technical capabilities or resources to develop systems of their own. Exacerbating these 


concerns is that physical spaces are increasingly monitored with cameras, microphones, and 


a range of sensors, collecting data from and about people in those spaces (potentially 


without those people’s knowledge or their awareness of how it will be used). For example, 


AI services could help retailers track customers through their stores and analyse their 


behaviour; facial recognition or other biometric services could allow public spaces to be 


surveilled by public or private actors and otherwise ‘anonymous’ individuals identified and 


monitored; speech and voice recognition services could similarly be used to monitor 


otherwise private conversations and identify individuals by voice.  





Rolling out AI-augmented surveillance systems cheaply and at scale, enabled by AIaaS, could 


potentially transform spaces (both physical and virtual) and relationships and power 


dynamics between watchers and watched. AI capabilities enable those undertaking 


surveillance to move from passively watching people (which can itself affect their choices, 


 
spreads globally' Financial Times (24 July 2019) <https://www.ft.com/content/56dde36c-aa40-11e9-984c-
fac8325aaa04> accessed 13 November 2020. 



https://www.ft.com/content/56dde36c-aa40-11e9-984c-fac8325aaa04

https://www.ft.com/content/56dde36c-aa40-11e9-984c-fac8325aaa04</pre>",POSITIVE
PyPDF2_2662381_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662381.pdf,4,4,2662381,attachments/2662381.pdf#page=4,Is the line order correct?,"<pre>    
 
   Pagina 4 van 4 
  mogelijk ook meerdere lidstaten bevoegd  zijn ten aanzien van één 
provider ? 
• Is jurisdictie verbonden aan de vestigingsplaats  van de provider, 
producent of gebruiker?  Welke lidstaat is bevoegd om tot handhaving over 
te gaan, is nog onvoldoend e scherp. Meer duiding of aanscherping is 
gewenst.  
 
6. Coördinatie  tussen toezichthouders  
• Gedeelde verantwoordelijkheden betekent dat in de verschillende fasen 
van het toezicht de bevoegde autoriteit zaakkennis moet overdragen of 
dat een organisatie te make n krijgt met meerdere toezichthouders. Hoe 
wordt informatiedeling en coördinatie van toezichtsactiviteiten voorzien? 
En wat is de juridische  basis?  
• Daarnaast vraagt effectief markttoezicht in Europa om naast nationaal 
toezicht een Europees netwerk op te richten waar  aan gezamenlijke 
toezichtsactiviteiten, onderzoeken en handhaving  wordt gewerkt.   
 
7. Rechtsbescherming burgers   
Er is onvoldoende voorzien  in de mogelijkheid voor  burgers om individuele 
problemen met AI te rapporteren , waardoor de bescherming  tekortschiet . Op 
basis van de AI Act kijkt een toezichthouder naar het AI-systeem  en de werking in 
het geheel, niet naar de gevolgen voor een individueel geval.  Om de burger s 
voldoende rechts bescherming te bieden moeten zij kunnen aangeven dat zij zijn 
benadeeld door de toepass ing van AI .   
 
Het maatschappelijk vertrouwen in AI-toepassingen  is afhankelijk van 
rechtsbescherming, effectief toezicht, transparante markttoegang en 
standaardisatie.  
 
i NISD. Directive (EU) 2016/1148 of the European Parliament and of the Council of 
6 July 2016 concerning measures for a high common level of security of network 
and information systems across the Union . 
ii eIDAS. VERORDENING (EU) Nr. 910/2014 VAN HET EUROPEES PARLEMENT EN DE 
RAAD van 23 juli 2014  betreffende elektronische identificatie en vertrouwensdiensten 
voor elektronische transacties in de interne markt . 
iii RED. Directive 2014/53/EU of the European Parliament and of the Council of 16 
April 2014 on the harmonisation of the laws of the Member States relating  to the 
making available on the market of radio equipment . 
iv CSA. VERORDENING (EU) 2019/881 VAN HET EUROPEES PARLEMENT EN DE RAAD 
van 17 april 2019 zake E NISA (het Agentschap van de Europese Unie voor 
cyberbeveiliging), en inzake de certificering van de cyberbeveiliging van informatie - 
en communicatietechnologie .  </pre>",POSITIVE
fitz_2665249_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665249.pdf,7,2,2665249,attachments/2665249.pdf#page=2,Is the line order correct?,"<pre>industriAll Europe Trade Union 
 
 
Page 2 of 7 
 
 
Feedback to the Public Consultation on the 
Proposal for an Artificial Intelligence Act 
 
 
 
commitment to high-skilled labour and high value-added industries as the core of the European economic 
model. 
 
IndustriAll Europe further welcomes that the underlying message of the draft Regulation is clear: that 
European values should be at the core of this Regulation and that not everything that is technically feasible 
should be allowed. We therefore specifically welcome the prohibition of certain AI practices and 
applications, as well as the introduction of mandatory transparency measures. We further welcome the 
proposed EU database on stand-alone, high-risk AI systems, which should be publicly accessible. This will 
contribute to foster trust in AI technologies and make the technology more transparent and reliable. 
 
 
IndustriAll Europe’s assessment:  
 
 
IndustriAll Europe believes that, even though the draft Regulation is quite comprehensive, there are still 
a number of points that should be made more precise, corrected, or added altogether. 
 
• 
First of all, we are of the opinion that the proposal is full of loopholes and exceptions that should 
be closed. Too many vague formulations and definitions leave too much room for interpretation. 
Relevant categories when discussing data are not addressed at all, namely that of ‘inclusivity’, 
‘non-discrimination’ and ‘fairness’. An ambitious landmark legislation, such as the current 
proposal, should deal with all relevant categories that the subject it regulates touches upon. 
• 
Secondly, we think that the definition of ‘high-risk’ AI is too narrow and ignores too many use 
cases which affect workers and citizens in their everyday life. Annex III, which lists high-risk 
applications, reads “AI systems intended to be used for...” which, again, leaves too much room for 
interpretation. In addition to the used wording, the different risk categories are too broad. A fully 
differentiated risk pyramid with more risk layers would be helpful to regulate the different types 
of applications in question in a more application-oriented manner. One example for a more 
granular approach has been cited already in the EU Commission’s White Paper on AI (COM(2020) 
65 final) when referring to the five-level risk-based system as proposed by the German Data Ethics 
Commission. We would like to underline, however, that the risk-based approach is not fit for 
purpose and that a rights-based approach to the Regulation would have been preferable as this 
would allow for tailored regulations for AI/ML applications.  
• 
Thirdly, it is highly problematic that the definition of ‘unacceptable’ AI seems to be final, with no 
mechanism in place to introduce new kinds of ‘unacceptable’ AI to the list. This would, in the worst 
case, lead to a situation in which new types of harmful AI are being developed without an adequate 
legal mechanism in place to prohibit its use or it being put on the market. There is also no provision 
in place to prevent AI/ML applications from developing features that would be considered 
‘unacceptable’, and it is not clear how those applications should be dealt with. This adds to our 
observation that the language used is often insufficient. Terms such as “disproportionate” or 
“unjustified” to describe “detrimental treatment” in the context of social scoring are not fit to 
contribute to a robust regulatory framework. Instead, legal terms that clearly indicate scope and 
intention of the prohibition should be introduced. 
• 
We criticise the fact that the ban on remote biometric identification systems is only halfhearted, 
and only for law enforcement purposes. And even this already narrow ban contains a number of 
problematic loopholes, as it leaves wide discretionary power to the authorities, i.e. by including 
What we criticise about the proposal 
</pre>",NEUTRAL
fitz_2665627_6,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665627.pdf,11,6,2665627,attachments/2665627.pdf#page=6,Is the line order correct?,"<pre>From our perspective we wonder if there is evidence that regulatory sandboxes are an 
effective strategy for facilitating innovation. We would like to see a more elaborate report on 
the consequences of the regulation in terms of its effect on innovation, specifically in relation 
to regulatory sandboxes and SMEs.  
 
Another way forward would be for an SME to partner with a larger actor with the ability to 
take the full administrative work onboard. The deal could have an impact on IP rights and 
revenue streams which in turn could mean that the regulation serves to protect the established 
actors on the market. The benefit for SMEs would be that they can focus on their innovative 
ideas and make the administration of the AI system a question of commercial contracts 
instead of a negotiation with national authorities. The latter demands another set of 
competences that not all SMEs have made a priority to recruit. 
 
The proposed regulation mentions another possibility in Article 17.2 that states that the 
quality management system ensuring compliance with the regulation should be in relation to 
the size of the organization. This could be used by SMEs instead of regulatory sandboxes or 
difficult negotiations with large and established actors. But it could also be used by large 
organisations that want to reduce their administration by relating the quality management 
system to the size of the unit developing the AI system, not the whole organization. 
 
Regulatory sandboxes is a topic we will return to from the perspective of public governance 
further down in our response. 
 
Responsibilities in a system-of-systems 
In this section we will focus on AI systems and their development as systems-of-systems, but 
also on AI systems as components in other systems and what effects the proposed regulation 
can have in terms of responsibilities among and between actors. 
 
System boundaries 
A challenge for actors with the proposed regulation is that artefacts and systems developed 
for other purposes can be used for future development of high-risk AI systems and therefore 
covered by the regulation (Annex IV.2). It could be services and platforms providing data 
regarding road conditions [5] or the placement of wastewater wells [6] (depending on the 
interpretation of the definition in Annex I they might be high-risk AI systems or mere data 
sources). If the supplied data is used for developing AI systems which can be used for 
managing vital infrastructure like roads and water supply (Annex III) they will be part of the 
technology chain behind the AI system and need to be administered as such.   
 
The same goes for developers of models, such as digital twins, if they are used for developing 
AI systems that are classified as high-risk systems. Providers of data regarding populations 
face the same uncertainty if the data covers for instance taxable income, number of residents 
at a specific address or fluctuation of property prices since they can be used to determine 
strategies or decisions for social benefits or targeted interventions against social exclusion 
(Annex III.5 and III.8). In the long run all providers of digital artefacts face the probability 
that their service, data or technology will be used for developing high-risk AI and therefore 
need to have the right documentation to conform with the regulation and avoid fines. 
 
At the same time there are political initiatives that promote public authorities and actors to 
facilitate data sharing as well as a need for digital simulations and models for societal and 
business planning. These artefacts could be high-risk AI systems in themselves or become 
</pre>",POSITIVE
PyPDF2_2662226_7,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2662226.pdf,8,7,2662226,attachments/2662226.pdf#page=7,Is the line order correct?,"<pre> 
Raising standards for consumers  
ANEC -DIGITAL -2021-G-071 – July 2021   7 be complementary to strict safety measures and should not exonerate manufacturers 
from ensuring that products do not  present a risk  to consumers .  
However, the inherent information asymmetry  associated with AI or an 
evolving/machine learning system, makes the function of information different from 
information linked to traditional, non -AI products (e.g. Ecolabel) where the 
technologic al content of the product is “static”.  The information is not very helpful if the 
behaviour  of products  changes over time but the information stays the same. One 
reason more for us to seriously wonder about the inclusion of emotion recognition 
system or a biometric categorisation system and ‘deep fake’ in the level  of low -risk AI 
systems , especially as consumers will not benefit from the right to opt -out of the 
system .  
6 | Information sharing and market surveillance  
 
Market surveillance authorities should hav e sufficient resources to enforce the AI 
requirements . We stress the need to ensure national supervisory authorit ies have the 
financial, technical and technological means  to carry out their mission. The 
possibility of imposing mandatory inspection fees – as done in Food Safety legislation - 
should be explored. The proceedings of the fines should be used to finance the market 
surveillance activities.  
6.1 Reporting of serious incidents and of malfunctioning  (art.62)  
We think that serious incident or any malfunctioning  of AI having an impact on 
consumer safety should also be reported. We refer to our long -lasting call for a pan 
European accidents and injuries database in order to assess whether a  product  is 
posing a high risk for consumers , with the aim of  achieving a high quality, 
representative and up -to-date data sample for the entire Single Market5. 
6.2 Procedure for dealing with AI systems presenting a risk at national level  (art.65)  
The precautionary principle  allows market surveillance authorities t o take temporary 
and preventive measures in the absence of a definitive  proof of harm to consumers or 
the environment. As such, we think that this fundamental principle should be present 
in the AI Act which is dealing with new technologies and unforeseen e ffects.  
In current market surveillance practice , legal obstacles  prevent an exchange of 
information in both the  harmonised and non -harmonised areas about dangerous 
products  with other countries/jurisdictions . It is therefore important that the AI Act  
provides for a strengthening of international cooperation  by allowing the exchange 
of information beyond confidentiality rules . 
ENDS  
 
 
5 European consumer safety needs solid injury data, ANEC -EuroSafe position paper , November 2020  </pre>",POSITIVE
pdfminer_2662901_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662901.pdf,10,2,2662901,attachments/2662901.pdf#page=2,Is the line order correct?,"<pre>EUROPEAN ASSOCIATION OF CO-OPERATIVE BANKS 
The Co-operative Difference : Sustainability, Proximity, Governance 
Introduction 
The European Association of Co-operative Banks (EACB) is happy to contribute to the discussion 
on the Artificial Intelligence (AI) legislative proposal. 
The EACB recognises that the AI proposal is the Commission’s first ever legal framework on the 
matter, which addresses the risks of AI and aims to position Europe to play a leading role globally. 
It should be recognised that this is a risky bet. If European values were not ultimately adopted 
on  an  international  scale,  European  companies  would  be  at  a  disadvantage  compared  to  non-
European players active in less restrictive regulatory environments. 
We  believe  that  the  European  Commission,  the  European  Parliament  and  the  Council  should 
remain vigilant to ensure that European players are not unduly constrained in their prospect of 
developing innovative AI solutions compared to international competitors. 
We would like to highlight the following points: 
•  The EACB welcomes the Commission’s  risk-based approach as basis for a proportionate 
legal  text.  The  Commission  suggests  a  risk-pyramid  approach:  the  higher  the  risk  (for 
users) using AI system, the more additional measures. 
•  We appreciate the technology-neutral and future-proof definition of AI, recognising that 
AI is a “fast evolving family of technologies” that is constantly developing. Nevertheless, 
combining the definition of artificial intelligence system together with the techniques and 
approaches  of  Annex  I  of  the  proposal,  we  observe  that  the  scope  of  the  Regulation  is 
becoming quite wide as it also includes rule-based approaches. 
•  We believe it is of paramount importance to make sure that the AI proposal will not add 
new  and  burdensome  requirements  for  the  banking  sector  and  create  conflicts  and 
overlaps with existing rules: e.g., sector-specific regulation (CRD, CRR). 
•  We particularly value the Commission’s human-centric perspective in designing AI rules: 
o  The responsibility for an action or a decision still lies with a human being; 
o  Actions  and  decisions  of  an  AI  system  have  to  be  traceable  and  understandable  by 
humans using it; and 
o  Actions and decisions of an AI system can always be changed/corrected by a human 
being (human oversight). 
•  We  understand  that  the  Regulation’s  intention  is  to  protect  the  safety  and  fundamental 
rights of EU citizens, thus that the requirements for high-risk AI systems are only targeted 
at AI applications that could possibly pose risks to natural persons. 
•  Generally, some provisions of the Regulation contain somewhat vague wording, e.g., the 
definitions provided for “remote biometric identification system” and “user”. Moreover, we 
believe that the definition  of ‘developer’ and ‘end user’ are missing from the legal text. 
These points should be further clarified in order to guarantee legal certainty for providers, 
developers and users of AI systems. 
2 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</pre>",POSITIVE
fitz_2665406_3,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665406.pdf,10,3,2665406,attachments/2665406.pdf#page=3,Is the line order correct?,"<pre> 
3 
 
systems that are used to predict, prescribe, or make decisions with effects on human beings. It 
underlines the socio-technical nature of such systems, which are inevitably embedded in societal 
contexts that need to be taken into account when assessing the implications of their use.3 We 
strongly agree with the European Center for Not-for-Profit Law’s position that the risks and 
opportunities of AI systems should not be judged on a binary basis. Instead, it is necessary to 
account for the targeted population, context, and situation when considering the risks and 
opportunities of these systems.4 In addition, the definition setting the scope of the Act focuses on 
the type of technology involved, which opens the door for operators to circumvent the scope of its 
provision by denying that their system falls under the respective definition.  
II In general, the proposed Act covers uses by both public and private actors, which presents a critical 
opportunity to streamline requirements on uses where fundamental rights are concerned, 
regardless of the actors involved. However, specific provisions are limited to public authorities. 
Some of them explicitly mention that they also apply to private actors acting on behalf of public 
authorities or in the framework of Public-Private-Partnerships. At the same time, the absence of 
this key addition in other provisions indicates that the extension to such private actors does not 
automatically apply, creating potential loopholes public authorities could try to exploit by 
outsourcing the use of certain systems.  
III Another aspect that is important for the effective protection of fundamental rights and that we 
welcome is the applicatory geographical scope of the proposed Act, which would apply whenever 
an AI-based system is used within the EU, regardless of where the operator is based—or whenever 
an output of such a system is used within the EU, regardless of where the system itself is based 
(Art. 2(1)). The wide extraterritorial effects this implies ensure that geographical loopholes 
cannot be exploited to evade the Act’s reach, guaranteeing protection across the Union. At the 
same time, focusing on the location where a system is used implies that neither the development 
nor the sale and export of any systems are covered by the Act if they are put to use elsewhere—
including systems whose use would be prohibited or classified as high-risk according to the Act. 
From a fundamental rights perspective, this creates a protection vacuum for people in third states, 
whose rights could be infringed by the uses of AI systems developed by EU-based providers.  
IV A related loophole stems from excluding from the scope of the proposed regulation any systems 
used by public authorities in third states or international organizations in the framework of 
international law enforcement and judicial cooperation with the EU or its Member States (Art. 2(4)).  
/ 
We call on the Council and the Parliament to clarify the applicatory scope of the AI Act with 
regard to the above aspects, making sure it is a coherent, consistent, and reliable instrument 
for protecting human beings from violations of their fundamental rights caused by the use 
of ADM systems—regardless of the specific technology or the type of actors involved.  
2 Mitigate the Self-Defeating Potential of the Risk-based Approach 
It is a key achievement that the Commission recognizes that the use of AI-based systems can come 
with serious risks for fundamental rights and that these risks need to be addressed by a 
governance framework, and this is an important message by itself that should be recognized as such. 
We are relieved to see that the approach has improved compared to the White Paper, recognizing 
sensitive areas, such as when AI systems are used for recruiting, to evaluate creditworthiness, to 
determine access to social benefits, for predictive policing, to control migration, and to assist judicial 
interpretation. Furthermore, the misleading criterion ‘sector’ to determine high-risk AI practices has 
                                                 
3 Following our definition, ADM systems encompass the design procedures to gather data, the collection of data, the 
development of algorithms to analyze the data, the interpretation of the results of this analysis based on a human-defined 
interpretation model, and the automatic action based on the interpretation as determined in a human-defined decision-
making model. 
4 ECNL Position Statement on the EU AI Act, 23 July 2021, https://ecnl.org/sites/default/files/2021-
07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf 
</pre>",POSITIVE
fitz_2662603_1,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662603.pdf,9,1,2662603,attachments/2662603.pdf#page=1,Is the line order correct?,"<pre>An Assessment of the AI Regulation Proposed by
the European Commission
Patrick Glauner
Abstract In April 2021, the European Commission published a proposed regulation
on AI. It intends to create a uniform legal framework for AI within the European
Union (EU). In this chapter, we analyze and assess the proposal. We show that the
proposed regulation is actually not needed due to existing regulations. We also argue
that the proposal clearly poses the risk of overregulation. As a consequence, this
would make the use or development of AI applications in safety-critical application
areas, such as in healthcare, almost impossible in the EU. This would also likely
further strengthen Chinese and US corporations in their technology leadership. Our
assessment is based on the oral evidence we gave in May 2021 to the joint session
of the European Union aﬀairs committees of the German federal parliament and the
French National Assembly.
1 Introduction
Artiﬁcial intelligence (AI) aims to automate human decision-making behavior and
is therefore also considered the next phase of the industrial revolution. We have
previously reviewed the state of the art and challenges of AI applications in healthcare
(Glauner, 2021a). This book provides a forecast of how AI and other technologies are
likelty to skyrocket healthcare in the foreseeable future. The European Commission
published proposed regulation in April 2021 (European Commission, 2021) that
intends to create a uniform legal framework for AI within the European Union (EU).
The proposal particularly addresses safety-critical applications, a category that also
Patrick Glauner
Deggendorf Institute of Technology, Deggendorf, Germany e-mail: patrick@glauner.info
Preprint. To appear in the 2022 Springer book “The Future Circle of Healthcare: AI, 3D
Printing, Longevity, Ethics, and Uncertainty Mitigation"" edited by Sepehr Ehsani, Patrick Glauner,
Philipp Plugmann and Florian M. Thieringer.
1
</pre>",POSITIVE
fitz_2665648_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665648.pdf,8,1,2665648,attachments/2665648.pdf#page=1,Is the line order correct?,"<pre>CHAI Position Paper on the EU Arti�cial IntelligenceAct
University of California, Berkeley
The Center for Human-Compatible AI (CHAI) is a research group based at UC Berkeley, with
academic a�liates at a variety of other universities. CHAI’s goal is to develop the conceptual and
technical wherewithal to reorient the general thrust of AI research towards provably bene�cial systems.
CHAI is led by Prof. Stuart Russell.
Recommendations
Make the regulation future-proof and prepare for higher-risksystems
Update the regulation to address increasingly generalized AI systems that have multiple
purposes, such as OpenAI’s Generative Pre-trained Transformer 3 and DeepMind’s AlphaFold
system
Article 3(13) “reasonably foreseeable misuse” and “interaction with other systems”: Explicitly
consider the issue of high-risk and societal-scale consequences stemming from the interaction
of many low-risk systems
Article 6 classi�cation rules for high-risk systems: Include recommender systems in the
classi�cation rules for high-risk systems
Article 6 classi�cation rules for high-risk systems: Include the requirements to document
perceptual inputs, action outputs, objectives, and the operational environment for high-risk
systems
Article 6 classi�cation rules for high-risk systems: Include the requirements to document
time-of-sale properties of systems
List of high-risk categories in Annex III: Add categories
Protect people from psychological harm
Article 5 (1) (a) on psychological manipulation: Consider expanding the current de�nition of
“subliminal techniques beyond a person’s consciousness”
Section 3.5 of the Explanatory Memorandum: Include the protection of mental integrity in
the Explanatory Memorandum
Article 53 (AI regulatory sandboxes): Recognize the limit of sandboxes
1
</pre>",POSITIVE
fitz_2662473_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662473.pdf,2,1,2662473,attachments/2662473.pdf#page=1,Is the line order correct?,"<pre>Fon +32 2 282 05-50 
info@dsv-europa.de 
 
 
 
 
www.dsv-europa.de 
Transparenzregister 
Nr. 917393784-31 
 
 
 
 
Deutsche Sozialversicherung 
Europavertretung 
Rue d’Arlon 50 
B-1000 Bruxelles 
 
Die öffentliche Konsultation der EU-
Kommission zum Vorschlag eines „Ge-
setzes über Künstliche Intelligenz“, COM 
(2021)206 final  
Stellungnahme der Deutschen Sozialversicherung vom 
14.07.2021 
Die Deutsche Rentenversicherung Bund (DRV Bund), die Deutsche Gesetzliche 
Unfallversicherung (DGUV), der GKV-Spitzenverband und die Verbände der ge-
setzlichen Kranken- und Pflegekassen auf Bundesebene und die Sozialversiche-
rung für Landwirtschaft, Forsten und Gartenbau (SVLFG) haben sich mit Blick auf 
ihre gemeinsamen europapolitischen Interessen zur „Deutschen Sozialversiche-
rung Arbeitsgemeinschaft Europa e. V.“ zusammengeschlossen.  
Der Verein vertritt die Interessen seiner Mitglieder gegenüber den Organen der 
Europäischen Union (EU) sowie anderen europäischen Institutionen und berät die 
relevanten Akteure im Rahmen aktueller Gesetzgebungsvorhaben und Initiativen.  
Die Kranken- und Pflegeversicherung, die Rentenversicherung und die Unfallver-
sicherung bieten als Teil eines gesetzlichen Versicherungssystems wirksamen 
Schutz vor den Folgen großer Lebensrisiken. 
Stellungnahme 
Die Deutsche Sozialversicherung begrüßt den Entwurf eines Gesetzes über 
Künstliche Intelligenz (KI). Er hat weitreichende Auswirkungen auf die Entwicklung 
und den Einsatz von KI, nicht zuletzt auch im Bereich der öffentlichen Verwaltung. 
Die Deutsche Sozialversicherung sieht mögliche mitgliedschafts-, beitrags- und 
leistungsrechtliche Bezüge und ist sich ihrer Verantwortung im Umgang mit KI be-
wusst. Sie begrüßt daher eine Klärung unter anderem der ethischen und haftungs-
rechtlichen Fragen. Dabei wird im Rahmen der weiteren Verhandlungen des Ver-
ordnungsentwurfs und darüber hinaus zu klären sein, bis zu welcher Detail- und 
Entscheidungstiefe ein europäisches Handeln erforderlich ist.    
 
</pre>",POSITIVE
PyPDF2_2662473_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662473.pdf,2,1,2662473,attachments/2662473.pdf#page=1,Is the line order correct?,"<pre>Fon +32 2 282 05 -50 
info@dsv -europa.de   
 
 
 
www.dsv -europa.de 
Transparenzregister  
Nr. 917393784 -31  
  
 
Deutsche Sozialversicherung Europ avertretung  
Rue d’Arlon 50 
B-1000 Bruxelles  
 
Die öffentliche Konsultation der EU -
Kommission zum Vorschlag eines „Ge-
setzes über Künstliche Intelligenz“ , COM 
(2021)206 final   
Stellungnahme  der Deutschen Sozialversicherung vom 
14.07.2021  
Die Deutsche Rentenversicherung Bund (DRV Bund), die Deutsche Gesetzliche 
Unfallversicherung (DGUV), der GKV -Spitzenverband und die Verbände der ge-
setzlichen Kranken - und Pflegekassen auf Bundesebene und die Sozialversiche-
rung für Landwirtschaft, Forsten u nd Gartenbau (SVLFG) haben sich mit Blick auf 
ihre gemeinsamen europapolitischen Interessen zur „Deutschen Sozialversiche-
rung Arbeitsgemeinschaft Europa e. V.“ zusammengeschlossen.  
Der Verein vertritt die Interessen seiner Mitglieder gegenüber den Organen  der 
Europäischen Union (EU) sowie anderen europäischen Institutionen und berät die 
relevanten Akteure im Rahmen aktueller Gesetzgebungsvorhaben und Initiativen.  
Die Kranken - und Pflegeversicherung, die Rentenversicherung und die Unfallver-
sicherung bieten als Teil eines gesetzlichen Versicherungssystems wirksamen 
Schutz vor den Folgen großer Lebensrisiken.  
Stellungnahme  
Die Deutsche Sozialversicherung begrüßt den Entwurf eines Gesetzes über 
Künstliche Intelligenz  (KI). Er hat weitreichende Auswirkungen auf  die Entwicklung 
und den Einsatz von KI, nicht zuletzt auch im Bereich der öffentlichen Verwaltung. 
Die Deutsche Sozialversicherung sieht mögliche mitgliedschafts -, beitrags - und 
leistungsrechtliche Bezüge und ist sich ihrer Verantwortung im Umgang mit KI be-
wusst. Sie begrüßt daher eine Klärung unter anderem der ethischen und haftungs-
rechtlichen Fragen. Dabei wird im Rahmen der weiteren Verhandlungen des Ver-ordnungsentwurfs und darüber hinaus zu klären sein, bis zu welcher Detail - und 
Entscheidungstiefe ein europäisches Handeln erforderlich ist.    
 </pre>",POSITIVE
fitz_2665432_6,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665432.pdf,7,6,2665432,attachments/2665432.pdf#page=6,Is the line order correct?,"<pre> 
6 
Furthermore, the proposal does not include the possibility of harmed consumers to be 
represented by an NGO, including consumer organisations, in the exercise of their rights. 
An article similar to Art 80 GDPR (Representation of data subjects) or Article 68 of the 
proposal for a Digital Services Act (Representation) should be introduced. In the context 
of the AI Act, this representation shall not be subject to a mandate. 
 
Also, to enable collective redress actions, this proposal should be included in the Annex of 
the Representative Actions Directive26. A similar provision was included in the Commission’s 
proposal for a Digital Services Act.27 
6. The governance and enforcement structure must be clear and ensure 
an effective and consistent application of the rules at European and 
national level 
The proposed governance and enforcement structure mainly rests at national level and 
raises issues in relation to the obligations, competences and powers of the different actors 
involved28 and the different processes envisaged.  
 
For example:  
- 
The need to ensure a coherent and coordinated EU-wide enforcement. While the 
Commission plays a central role if a national supervisory authority notifies its intention 
to adopt measures against an AI system in its territory29, the Commission has no powers 
to proactively take the lead in case of inaction by national authorities. The autonomy 
and powers of the European AI Board30, comprised of high-level representatives of the 
national supervisory authorities and chaired by the Commission, also seem quite 
limited.  
 
- 
There is a need to clarify potential overlaps with existing bodies such as the European 
Data Protection Board, as well as the role of Data Protection Authorities – a view that 
other stakeholders also share31. 
 
- 
It is also important to ensure a common approach when it comes to the cooperation 
between the different competent authorities and supervisory authorities at national 
level, to ensure effective and swift enforcement as well as to avoid different approaches 
by Member States. 
 
- 
Given the lack of redress mechanisms for consumers in the proposal, there is no 
authority that is entrusted with dealing with consumer complaints in case of breaches 
of the regulation. 
 
 
26 Directive (EU) 2020/1828 on representative actions for the protection of the collective interests of consumers 
and repealing Directive 2009/22/EC; 
27 See Article 72 of the Proposal for a Regulation on a Single Market For Digital Services (Digital Services Act) 
and amending Directive 2000/31/EC;  
28 Namely: the European Artificial Intelligence Board (EAIB), the national competent authorities, the national 
supervisory authority, the Commission, the European Data Protection Supervisor (EDPS) and the AI system 
providers); 
29 See Articles 65 (5) and 66 of the proposal; 
30 See Articles 56 – 58 of the proposal; 
31 See, for example, Access Now: https://www.accessnow.org/eu-minimal-steps-to-regulate-harmful-ai-
systems/. 
</pre>",POSITIVE
fitz_2662182_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662182.pdf,5,3,2662182,attachments/2662182.pdf#page=3,Is the line order correct?,"<pre> 
 
3 
 
parezca real ha sido elaborado por medios automáticos, en nuestra opinión, también se 
debe exigir la divulgación de qué contenido concreto ha sido empleado para la 
elaboración de ese nuevo contenido. Ello es indispensable para garantizar que los 
creadores y otros titulares de derechos sean remunerados por la utilización de sus obras 
o prestaciones.  
 
Por otro lado, se exige la regulación de un sistema de responsabilidad para aquellos 
casos en los que sistemas autónomos generen contenido que vulnere DPI. Si el 
resultado generado por un sistema autónomo transforma o reproduce contenido ajeno 
¿ante quién deben reclamar los titulares de derechos afectados?. La falta de respuesta 
legal a esta cuestión no puede dejar desamparados a los creadores. 
 
En este sentido, en la Resolución del Parlamento Europeo, de 20 de octubre de 2020, 
con recomendaciones destinadas a la Comisión sobre un régimen de responsabilidad 
civil en materia de inteligencia artificial (2020/2014(INL)) se considera que es necesario 
realizar adaptaciones específicas y coordinadas de los regímenes de responsabilidad 
civil para evitar situaciones en las que personas que sufran un daño o un menoscabo a 
su patrimonio por el empleo de sistemas de IA acaben sin indemnización. 
 
SEGUNDO: CONTENIDO CREADO POR SISTEMAS DE INTELIGENCIA ARTIFICIAL  
 
Buena parte de la doctrina se ha volcado en el planteamiento del reto que supone la 
protección de los resultados o contenido obtenido mediante sistemas de IA. Mientras 
que el empleo de sistemas de IA por parte de un creador como una mera herramienta 
en la creación no debe suponer ningún reto ni variación a la legislación existente, sí que 
plantea dudas la protección de los resultados de aquellos sistemas que puedan llegar a 
crear de forma autónoma.  
 
En primer lugar, queremos destacar que la posible protección de los contenidos creados 
por sistemas autónomos no debe suponer un menoscabo a los intereses o derechos de 
los creadores humanos. Las creaciones obtenidas por máquinas no deben llegar a 
competir ni a sustituir a las creaciones humanas. Es altamente probable que las 
creaciones obtenidas de forma autónoma por sistemas artificiales lleguen a ser 
monopolio de unas pocas empresas tecnológicas, por lo que se ha de evitar que estas 
</pre>",POSITIVE
tika_2665452_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665452.pdf,4,2,2665452,attachments/2665452.pdf#page=2,Is the line order correct?,"<pre>Press release 


Date 
 12 October 2020 


Subject  BMW Group code of ethics for artificial intelligence. 


Page  2 














Corporate Communications 


Seven principles covering the development and application of artificial 


intelligence at the BMW Group: 


• Human agency and oversight.  


The BMW Group implements appropriate human monitoring of decisions made by 


AI applications and considers possible ways that humans can overrule algorithmic 


decisions. 


• Technical robustness and safety.  


The BMW Group aims to develop robust AI applications and observes the applicable 


safety standards designed to decrease the risk of unintended consequences and 


errors. 


• Privacy and data governance.  


The BMW Group extends its state-of-the-art data privacy and data security 


measures to cover storage and processing in AI applications. 


• Transparency.  


The BMW Group aims for explainability of AI applications and open communication 


where respective technologies are used. 


• Diversity, non-discrimination and fairness.  


The BMW Group respects human dignity and therefore sets out to build fair AI 


applications. This includes preventing non-compliance by AI applications. 


• Environmental and societal well-being.  


The BMW Group is committed to developing and using AI applications that promote 


the well-being of customers, employees and partners. This aligns with the BMW 


Group’s goals in the areas of human rights and sustainability, which includes climate 


change and environmental protection.</pre>",POSITIVE
tika_2663276_69,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2663276.pdf,82,69,2663276,attachments/2663276.pdf#page=69,Is the line order correct?,"<pre>69 
 


- For images, visual scene synthesis. 
- For any data type, the extraction of representative (or typical) exemplars from a dataset, and 


of particularly atypical exemplars as well (respectively called prototypes and criticisms: Kim, 
2016). 


Explainable feature engineering 


This last type of pre-modelling explanatory method stems from the observation that an explanation 


for a predictive model is only as good as the predictive features it relies on. Therefore, particular care 


must be taken to the feature engineering stage when designing an ML system, i.e. to the construction 


of predictor variables from original variables in order to adequately re-structure the training data for 


the algorithm. 


Two such methods should be mentioned (Murdoch, 2019): 


- The intervention of domain experts, who are sufficiently knowledgeable about the source data 
to extract variables (combination of other variables, intermediate computation results, etc.) 
which increase a model’s predictive accuracy while maintaining the interpretability of its 
results. In other words, human expertise enables in certain cases to sidestep the usually 
inevitable trade-off between efficacy and explainability of an ML model (cf. section 10.1.1). 


- A modelling-based, automated approach: usual data analysis techniques are then used, such 
as dimensionality reduction and clustering, so as to extract predictor variables as compact and 
representative as possible. 


11.2. Explainable modelling 


Some methods enable simultaneously training the predictive model and building an associated 


explanatory model. This category of explanatory method is referred to as explainable modelling. 


Such methods are however far less frequently implemented than pre- and even more post-modelling   


explanatory approaches, for several reasons:  


- Explainable modelling requires access to the source code which produces the predictive 
model, and the possibility to modify the algorithm. On the contrary, access to the model itself 
is sufficient for post-modelling explanatory methods, which makes them much more widely 
applicable. 


- Explainable modelling is useful when explanations are necessary as early as the design phase 
of the ML algorithm, which demands a more mature engineering methodology and adequate 
planning during the introduction of AI into a business process. 


- Lastly, explainable modelling is not very suitable for audit, all the more so when the predictive 
model is only available as a black box, without a documentation of the algorithm itself. 


The primary, highly ambitious goal of explainable modelling is to avoid as much as possible the already 


mentioned trade-off between efficacy and explainability, as they strive to provide additional 


explainability without necessarily sacrificing predictive accuracy. 


A few methods for explainable modelling are described in what follows. 


Intrinsically explainable models 


An intrinsically explainable model can be chosen from the outset, for example linear models or 


decision-tree-based models. This is the most trivial kind of explainable modelling approach, assuming 


that the simplicity/efficacy trade-off is kept in mind, and that the specific model produced by the</pre>",POSITIVE
tika_2665504_10,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665504.pdf,11,10,2665504,attachments/2665504.pdf#page=10,Is the line order correct?,"<pre>(d) to determine the safety and compatibility with potential recipients;


(e) to predict treatment response or reactions;


(f) to define or monitoring therapeutic measures.


Later on, in Chapter II of that regulation, EU regulates more specifically the requirements 
regarding performance, design and manufacture. Here, all articles apply also to what would be
called “AI” systems, given the previous definition. But some extra specificities are added, in 
point 16: 


16.   Electronic programmable systems — devices that incorporate electronic programmable 
systems and software that are devices in themselves


16.1.   Devices that incorporate electronic programmable systems, including software, or 
software that are devices in themselves, shall be designed to ensure repeatability, reliability 
and performance in line with their intended use. In the event of a single fault condition, 
appropriate means shall be adopted to eliminate or reduce as far as possible consequent risks
or impairment of performance.


16.2.   For devices that incorporate software or for software that are devices in themselves, the 
software shall be developed and manufactured in accordance with the state of the art taking 
into account the principles of development life cycle, risk management, including information 
security, verification and validation.


16.3.   Software referred to in this Section that is intended to be used in combination with 
mobile computing platforms shall be designed and manufactured taking into account the 
specific features of the mobile platform (e.g. size and contrast ratio of the screen) and the 
external factors related to their use (varying environment as regards level of light or noise).


16.4.   Manufacturers shall set out minimum requirements concerning hardware, IT networks 
characteristics and IT security measures, including protection against unauthorised access, 
necessary to run the software as intended.


We see here that the article regulated important aspects also pursued by the proposal of 
regulation on AI: security, availability, performance qualification, … It is not the aim of this note 
to dive more into IVD regulation, but the interested reader can verify that many other aspects 
of interest in the project of regulation on AI are properly dealt within that regulation, without 
once entering into the details of the technologies (which makes it stronger, more future-proof).


Proceeding this way is also a guarantee that any manufacturer of a specific kind of product, 
service, application will manage properly with the necessary requirements. It will also 
guarantee there is no conflict between an application-specific regulation and a technology-
specific regulation. 


Copyright @T.Helleputte – 2021
Page 10/11</pre>",POSITIVE
PyPDF2_2665249_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665249.pdf,7,2,2665249,attachments/2665249.pdf#page=2,Is the line order correct?,"<pre>industriAll Europe Trade Union   
 Page 2 of 7 
  
Feedback to the Public Consultation on the  
Proposal for an Artificial Intelligence Act   
 
 
commitment to high -skilled labour and high value -added industries as the core of the European economic 
model.  
 
IndustriAll Europe further welcomes that the underlying message of the draft Regulation is clear: that 
European values shou ld be at the core of this Regulation and that not everything that is technically feasible 
should be allowed. We therefore specifically welcome the prohibition of certain AI practices and 
applications, as well as the introduction of mandatory transparency m easures. We further welcome the 
proposed EU database on stand -alone , high -risk AI systems , which should be publicly accessible . This will 
contribute to foster trust in AI technologies and make the technology more transparent and reliable.  
 
 
IndustriAll Europe’s assessment:  
 
 
IndustriAll Europe believes  that, even though the draft Regulation is quite comprehensive, there are still 
a number of points that should be made more precis e, corrected , or added altogether . 
 
• First of all, we are of the opinion that the proposal is full of loopholes and exceptions t hat should 
be closed. Too many vague formulations and definitions leave too much room for interpretation. 
Relevant categories when discussing data are not addressed at all, namely that of ‘inclusivity’, 
‘non -discrimination’ and ‘fairness’. An ambitious lan dmark legislation , such as the current 
proposal , should deal with all relevant categories that the subject it regulates touches upon.  
• Secondly, we think that the definition of ‘high -risk’ AI is too narrow and ignores too many use 
cases  which affect workers and citizens in their everyday life.  Annex III , which lists high -risk 
applications , reads “AI systems intended  to be used for...” whi ch, again, leaves too much room for 
interpretation.  In addition to the used wording, the different risk categories are too broad. A fully 
differentiated risk pyramid with more risk layers would be helpful to regulate the different types 
of applications in question in a more application -oriented manner. One example for a more 
granular approach has been cited already in the EU Commission’s White Paper on AI (COM(2020) 
65 final) when referring to the five -level risk -based system as proposed by the German Data Ethics 
Commission. We would like to underline, however, that the risk -based approach is not fit for 
purpose and that a rights -based approach to the Regulation would have been preferable  as this 
would allow for tailored regulations for AI/ML applications.   
• Thirdly, it is highly problematic that the definition of ‘unacceptable’ AI seems to be final, with no 
mechanism in place to introduce new kinds of ‘unacceptable’ AI to the list. This wou ld, in the worst 
case, lead to a situation in which new types of harmful AI are being developed without an adequate 
legal mechanism in place to prohibit its use or it being  put on the market.  There  is also no provision 
in place to prevent AI/ML application s from developing features that would be considered 
‘unacceptable ’, and it is not clear how those applications should be dealt with.  This adds to our 
observation that the language used is often insufficient. Terms such as “disproportionate” or 
“unjustified” to describe “detrimental treatment” in the context of social scoring are not fit to 
contribute to a robust regulatory framework. I nstea d, legal terms that clearly indicate scope and 
intention of the prohibition should be introduced.  
• We criticise th e fact that  the ban on remote biometric identification systems is only halfhearted , 
and only for law enforcement purposes. And even this already narrow ban contains a number of 
problematic loopholes, as it leaves wide discretionary power to the authorities, i.e. by including What we criticise about the proposal  </pre>",NEGATIVE
fitz_2665425_8,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665425.pdf,27,8,2665425,attachments/2665425.pdf#page=8,Is the line order correct?,"<pre> 
 
Artificial intelligence needs real world regulation 
8 l 27 
Verbraucherzentrale Bundesverband e.V. 
Legislators must complement Article 3 with a definition for non-professional users 
of AI systems. This must entail people using AI systems in their capacity as con-
sumers and citizens. It must also consider consumers who are affected by AI sys-
tems employed by professional users. 
1.3 Article 3 (34) – Emotion recognition system 
The definition of ‘emotion recognition systems’ in Art. 3 (34) is too narrow. The defini-
tion relies on the definition of biometric data as defined in Art. 3 (33), which itself is 
taken over from the General Data Protection Regulation (GDPR).6 It holds that bio-
metric data must “allow or confirm the unique identification of that natural person.” As a 
consequence ‘emotion recognition systems’ that do not rely on data allowing the unique 
identification of a natural person, will fall out of the scope of the AIA. However, vzbv 
holds that these types of systems should also fall under the AIA’s scope.  
This could include systems that rely only on the analysis of clicking, typing and cursor 
movement data for example. Also, for an AI system supporting a retail salesperson in a 
shop, it is not important to know the identity of a potential customer entering the shop. 
The AI system can provide the shop personnel with valuable real time personality/emo-
tion analysis data, based on the customer behaviour. For example inferences from 
measures on the relative tone/height, rhythm, and the speed of a voice, but not the 
voice itself. 
 The definition of ‘emotion recognition systems’ in Art. 3 (34) should not refer to bio-
metric data but to personal data. Otherwise, there is a significant risk for circumven-
tion of the legislation. 
2. THE SCOPE IS TOO NARROW PART I: NEGLECT OF ECONOMIC HARMS AND 
VIOLATIONS OF CONSUMER RIGHTS 
In general, the scope of the proposed AIA is too narrow and the legislation does not fo-
cus on consumers. The European Commission’s proposal focuses on problems of 
(product-)safety, health and fundamental rights linked to the use of AI systems. It 
mostly deals with high risks to people in their capacity as citizens and employees, ne-
glecting that AI systems can lead to significant economic/financial welfare losses for 
consumers or to violations of consumers’ rights. 
2.1 AI applications with large economic/financial impact or effects on consumer 
rights must be regarded as high-risk  
The European Commission’s proposal sees high-risks of AI systems nearly exclusively 
in the areas of (product-)safety, health and fundamental rights. The draft AIA focuses 
on mitigating risks to people in their capacity as citizens, patients, employees and stu-
dents (“education”). However, most AI systems in these areas are already subject to 
European legislation. Therefore, in practices, it can be doubted that consumers will 
benefit much from the draft AIA in these areas. 
___________________________________________________________________________________________ 
6 Compare Art. 4 (14): GDPR: European Parliament: EU General Data Protection Regulation (GDPR) Regulation (EU) 
2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with 
regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC 
(General Data Protection Regulation, OJ L 119, 4.5.2016, 2016. 
</pre>",POSITIVE
fitz_2662770_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662770.pdf,6,2,2662770,attachments/2662770.pdf#page=2,Is the line order correct?,"<pre> 
Mouvement des Entreprises de France 
Legal Department - July 2021 2 
On 21 April 2021, the European Commission proposed a new regulation on Artificial Intelligence (AI) aiming 
at providing a harmonized legal framework at European level. 
It is essential today to build a European ecosystem for innovation and AI that respects European values. The 
health crisis has indeed revealed the importance of technological and digital infrastructures and the fact that 
Europe must equip itself with such infrastructures, in particular AI. To do this, the development of AI must 
be made available to all companies, whatever their size, whatever their resources. 
Indeed, to build a trustworthy, efficient and sustainable AI framework, regulations must benefit citizens, 
but also European businesses by allowing them to develop and benefit from cutting-edge technologies in 
order to remain competitive. 
This European initiative is all the more important as it aims to prepare the ground for AI regulation worldwide. 
While MEDEF welcomes the work carried out by the European Commission and considers that the risk-based 
approach adopted in the proposed regulation is the best possible approach insofar as it promotes 
confidence in AI, this proposal raises many concerns and questions, in particular with regard to: 
− the legal uncertainty linked to very broad and insufficiently precise definitions; 
− the lack of consistency, or even the incompatibility, of this proposal with other European texts 
(GDPR, Machinery Directive, etc.); 
− the sometimes very heavy or even disproportionate obligations. 
On general provisions 
The risk-based approach of the European Commission seems to be the best possible approach as it fosters 
confidence in AI without hampering its responsible development. It is entirely relevant to define the 
obligations and requirements according to the risk (high or low risk) of the technology and its use. 
Nevertheless, it is essential to keep a margin of innovation. Particular attention must therefore be paid to 
definitions, in particular those of AI systems and high-risk systems, because the related obligations and 
requirements are very onerous and difficult to implement. Clear and sufficiently precise definitions are all 
the more important as they will be called upon to serve as references in other texts. They must therefore 
not lead to the creation of legal uncertainty. 
Some concepts, such as ""known and foreseeable risks"", ""reasonably foreseeable misuse"", ""generally 
acknowledged state of the art"", are for example very unclear and can therefore create legal uncertainty 
leading to different interpretations according to countries or authorities. 
On the definition of artificial intelligence   
 Article 3 (1) defines an AI system as “software that is developed with one or more of the techniques and 
approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such 
as content, predictions, recommendations, or decisions influencing the environments they interact with”.  
If we understand the Commission's objective of making this regulation neutral and adaptable to 
technological developments, the current proposed definition of AI (and the list of techniques in Annex 
I) is very broad in that it can include all types of systems or software applications that do not involve 
the same risks. The inclusion of such systems or applications within the scope of the regulation would risk 
hampering innovation in technology companies, especially smaller ones. However, in a context of 
international competitiveness, it is essential to encourage technological development and not to prevent 
SMEs from accessing these markets. 
In general, if AI can involve risks in its implementation, it is mainly with regard to its direct or indirect 
impact on individuals. However, it should be remembered that many uses of AI systems have little impact 
on individuals. This is the case, for example, with AI methods for internal modeling needs (for example 
ALM models for the banking sector) or for corporate scoring. 
It should also be noted that some AI systems even have the potential to increase the productivity of 
companies or the well-being of the workforce, in particular by efficiently distributing tasks between 
humans and machines, by providing tools for skills development and providing access to better working 
conditions, in particular health and safety. 
In this sense, it would be useful, on the one hand, not to put all systems or applications to the same 
scale. A benefit / risk balance of technological developments should be put in place in order to promote 
</pre>",POSITIVE
pdfminer_2665488_5,company,../24212003_requirements_for_artificial_intelligence/attachments/2665488.pdf,5,5,2665488,attachments/2665488.pdf#page=5,Is the line order correct?,"<pre>Bei Rückfragen oder Anmerkungen stehen Ihnen zur Verfügung: 
Simon Kessel  
Referent für Digitales und Mobilität 
VKU-Europabüro 
Telefon: +32 2 740 16-55 
E-Mail: Kessel@vku.de  
5 / 5 
 
 
 
 
</pre>",POSITIVE
tika_2665249_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665249.pdf,7,2,2665249,attachments/2665249.pdf#page=2,Is the line order correct?,"<pre>industriAll Europe Trade Union  
 


Page 2 of 7 





 
Feedback to the Public Consultation on the 
Proposal for an Artificial Intelligence Act  
 





commitment to high-skilled labour and high value-added industries as the core of the European economic 
model. 
 
IndustriAll Europe further welcomes that the underlying message of the draft Regulation is clear: that 
European values should be at the core of this Regulation and that not everything that is technically feasible 
should be allowed. We therefore specifically welcome the prohibition of certain AI practices and 
applications, as well as the introduction of mandatory transparency measures. We further welcome the 
proposed EU database on stand-alone, high-risk AI systems, which should be publicly accessible. This will 
contribute to foster trust in AI technologies and make the technology more transparent and reliable. 
 
 
IndustriAll Europe’s assessment:  
 
 
IndustriAll Europe believes that, even though the draft Regulation is quite comprehensive, there are still 
a number of points that should be made more precise, corrected, or added altogether. 
 


• First of all, we are of the opinion that the proposal is full of loopholes and exceptions that should 


be closed. Too many vague formulations and definitions leave too much room for interpretation. 


Relevant categories when discussing data are not addressed at all, namely that of ‘inclusivity’, 


‘non-discrimination’ and ‘fairness’. An ambitious landmark legislation, such as the current 


proposal, should deal with all relevant categories that the subject it regulates touches upon. 


• Secondly, we think that the definition of ‘high-risk’ AI is too narrow and ignores too many use 


cases which affect workers and citizens in their everyday life. Annex III, which lists high-risk 


applications, reads “AI systems intended to be used for...” which, again, leaves too much room for 


interpretation. In addition to the used wording, the different risk categories are too broad. A fully 


differentiated risk pyramid with more risk layers would be helpful to regulate the different types 


of applications in question in a more application-oriented manner. One example for a more 


granular approach has been cited already in the EU Commission’s White Paper on AI (COM(2020) 


65 final) when referring to the five-level risk-based system as proposed by the German Data Ethics 


Commission. We would like to underline, however, that the risk-based approach is not fit for 


purpose and that a rights-based approach to the Regulation would have been preferable as this 


would allow for tailored regulations for AI/ML applications.  


• Thirdly, it is highly problematic that the definition of ‘unacceptable’ AI seems to be final, with no 


mechanism in place to introduce new kinds of ‘unacceptable’ AI to the list. This would, in the worst 


case, lead to a situation in which new types of harmful AI are being developed without an adequate 


legal mechanism in place to prohibit its use or it being put on the market. There is also no provision 


in place to prevent AI/ML applications from developing features that would be considered 


‘unacceptable’, and it is not clear how those applications should be dealt with. This adds to our 


observation that the language used is often insufficient. Terms such as “disproportionate” or 


“unjustified” to describe “detrimental treatment” in the context of social scoring are not fit to 


contribute to a robust regulatory framework. Instead, legal terms that clearly indicate scope and 


intention of the prohibition should be introduced. 


• We criticise the fact that the ban on remote biometric identification systems is only halfhearted, 


and only for law enforcement purposes. And even this already narrow ban contains a number of 


problematic loopholes, as it leaves wide discretionary power to the authorities, i.e. by including 


What we criticise about the proposal</pre>",NEGATIVE
pdfminer_2665518_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665518.pdf,2,2,2665518,attachments/2665518.pdf#page=2,Is the line order correct?,"<pre>The proposed regulation does not address the role of shop stewards in ensuring that worker's 
rights are safeguarded when artificial intelligence is introduced in the workplace. The best 
solutions are found through local dialogue between management and worker's representatives. 
To ensure co-determination from employees, Negotia believes that the social dialogue should 
have a clearly defined role in the regulation of artificial intelligence in the workplace. We also 
believe that the conformity assessment of artificial intelligence solutions that are intended to 
be used in the workplace should be performed by an independent third party. 
In conclusion, Negotia believes the European Commission's proposal to regulate artificial 
intelligence, is a great step in the right direction, but that it has clear weaknesses when it 
comes to regulating technology in the workplace. Further measures should be put in place to 
ensure workers' rights, in the face of technological developments. 
We are grateful for the opportunity to provide feedback on the proposal,and look forward to 
following the legislative process going forward. 
Monica A. Paulsen 
President of Negotia 
Page 2 of 2 
 
 
 
 
 
</pre>",POSITIVE
fitz_2665488_5,company,../24212003_requirements_for_artificial_intelligence/attachments/2665488.pdf,5,5,2665488,attachments/2665488.pdf#page=5,Is the line order correct?,"<pre> 
 
5 / 5 
Bei Rückfragen oder Anmerkungen stehen Ihnen zur Verfügung: 
 
Simon Kessel  
Referent für Digitales und Mobilität 
VKU-Europabüro 
 
Telefon: +32 2 740 16-55 
E-Mail: Kessel@vku.de  
</pre>",POSITIVE
pdfminer_2665558_56,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665558.pdf,61,56,2665558,attachments/2665558.pdf#page=56,Is the line order correct?,"<pre>Forthcoming in Computer Law & Security Review 
Outsourcing in AI supply chains is similar to outsourcing in supply chains for physical 
products to evade workers’ rights, environmental protections, and minimum wage laws. In 
effect, the cross-border nature of technical supply chains potentially allows AIaaS providers 
to outsource key work underpinning AI services to other jurisdictions – escaping privacy, 
data protection, employment, and possibly other laws – and then offer those services to 
European customers. Any future regulatory initiatives relating to AIaaS should consider 
whether AI systems offered as a service in the EU should be trained on data obtained, 
labelled, cleaned, and processed in accordance with European data protection, 
employment, and other relevant laws. 
4.3.  Surveillance 
An overarching policy issue with AIaaS is how these services can enable AI-augmented 
surveillance. Internet-enabled surveillance (both public and private) has for some years 
crept into greater areas of contemporary life. AIaaS potentially facilitates a kind of AI-
augmented ‘Surveillance as a Service’, particularly for those who would otherwise lack the 
technical capabilities or resources to develop systems of their own. Exacerbating these 
concerns is that physical spaces are increasingly monitored with cameras, microphones, and 
a range of sensors, collecting data from and about people in those spaces (potentially 
without those people’s knowledge or their awareness of how it will be used). For example, 
AI services could help retailers track customers through their stores and analyse their 
behaviour; facial recognition or other biometric services could allow public spaces to be 
surveilled by public or private actors and otherwise ‘anonymous’ individuals identified and 
monitored; speech and voice recognition services could similarly be used to monitor 
otherwise private conversations and identify individuals by voice.  
Rolling out AI-augmented surveillance systems cheaply and at scale, enabled by AIaaS, could 
potentially transform spaces (both physical and virtual) and relationships and power 
dynamics between watchers and watched. AI capabilities enable those undertaking 
surveillance to move from passively watching people (which can itself affect their choices, 
spreads globally' Financial Times (24 July 2019) <https://www.ft.com/content/56dde36c-aa40-11e9-984c-
fac8325aaa04> accessed 13 November 2020. 
- 56 - 
 
 
 
 
 
</pre>",POSITIVE
pdfminer_2665406_3,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665406.pdf,10,3,2665406,attachments/2665406.pdf#page=3,Is the line order correct?,"<pre>systems  that  are  used  to  predict,  prescribe,  or  make  decisions  with  effects  on  human  beings.  It 
underlines the socio-technical nature of such systems, which are inevitably embedded in societal 
contexts  that  need  to  be  taken  into  account  when  assessing  the  implications  of  their  use.3  We 
strongly  agree  with  the  European  Center  for  Not-for-Profit  Law’s  position  that  the  risks  and 
opportunities  of  AI  systems  should  not  be  judged  on  a  binary  basis.  Instead,  it  is  necessary  to 
account  for  the  targeted  population,  context,  and  situation  when  considering  the  risks  and 
opportunities of these systems.4 In addition, the definition setting the scope of the Act focuses on 
the type of technology involved, which opens the door for operators to circumvent the scope of its 
provision by denying that their system falls under the respective definition.  
II 
In general, the proposed Act covers uses by both public and private actors, which presents a critical 
opportunity  to  streamline  requirements  on  uses  where  fundamental  rights  are  concerned, 
regardless  of  the  actors  involved.  However,  specific  provisions  are  limited  to  public  authorities. 
Some of them explicitly mention that they also apply to private actors acting on behalf of public 
authorities  or  in  the  framework  of  Public-Private-Partnerships.  At  the  same  time,  the  absence  of 
this key addition in other provisions indicates that the extension to such private actors does not 
automatically  apply,  creating  potential  loopholes  public  authorities  could  try  to  exploit  by 
outsourcing the use of certain systems.  
III  Another  aspect  that  is  important  for  the  effective  protection  of  fundamental  rights  and  that  we 
welcome is the applicatory geographical scope of the proposed Act, which would apply whenever 
an AI-based system is used within the EU, regardless of where the operator is based—or whenever 
an output of such a system is used within the EU, regardless of where the system itself is based 
(Art.  2(1)).  The  wide  extraterritorial  effects  this  implies  ensure  that  geographical  loopholes 
cannot be exploited to evade the Act’s reach, guaranteeing protection across the Union. At the 
same time, focusing on the location where a system is used implies that neither the development 
nor the sale and export of any systems are covered by the Act if they are put to use elsewhere—
including  systems  whose  use  would  be  prohibited  or  classified  as  high-risk  according  to  the  Act. 
From a fundamental rights perspective, this creates a protection vacuum for people in third states, 
whose rights could be infringed by the uses of AI systems developed by EU-based providers.  
IV  A related loophole stems from excluding from the scope of the proposed regulation any systems 
used  by  public  authorities  in  third  states  or  international  organizations  in  the  framework  of 
international law enforcement and judicial cooperation with the EU or its Member States (Art. 2(4)).  
/  We call on the Council and the Parliament to clarify the applicatory scope of the AI Act with 
regard to the above aspects, making sure it is a coherent, consistent, and reliable instrument 
for protecting human beings from violations of their fundamental rights caused by the use 
of ADM systems—regardless of the specific technology or the type of actors involved.  
2 Mitigate the Self-Defeating Potential of the Risk-based Approach 
It is a key achievement that the Commission recognizes that the use of AI-based systems can come 
with  serious  risks  for  fundamental  rights  and  that  these  risks  need  to  be  addressed  by  a 
governance framework, and this is an important message by itself that should be recognized as such. 
We  are  relieved  to  see  that  the  approach  has  improved  compared  to  the  White  Paper,  recognizing 
sensitive  areas,  such  as  when  AI  systems  are  used  for  recruiting,  to  evaluate  creditworthiness,  to 
determine access to social benefits, for predictive policing, to control migration, and to assist judicial 
interpretation.  Furthermore,  the  misleading  criterion  ‘sector’  to  determine  high-risk  AI  practices  has 
3 Following our definition, ADM systems encompass the design procedures to gather data, the collection of data, the 
development of algorithms to analyze the data, the interpretation of the results of this analysis based on a human-defined 
interpretation model, and the automatic action based on the interpretation as determined in a human-defined decision-
making model. 
4 ECNL Position Statement on the EU AI Act, 23 July 2021, https://ecnl.org/sites/default/files/2021-
07/ECNL%20EU%20AI%20Act%20Position%20Paper.pdf 
3 
 
 
                                                 
</pre>",POSITIVE
PyPDF2_2662603_1,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662603.pdf,9,1,2662603,attachments/2662603.pdf#page=1,Is the line order correct?,"<pre>AnAssessmentoftheAIRegulationProposedby
the European Commission
Patrick Glauner
Abstract InApril2021,theEuropeanCommissionpublishedaproposedregulation
on AI. It intends to create a uniform legal framework for AI within the European
Union (EU). In this chapter, we analyze and assess the proposal. We show that the
proposedregulationisactuallynotneededduetoexistingregulations.Wealsoargue
that the proposal clearly poses the risk of overregulation. As a consequence, this
would make the use or development of AI applications in safety-critical application
areas, such as in healthcare, almost impossible in the EU. This would also likely
furtherstrengthenChineseandUScorporationsintheirtechnologyleadership.Our
assessment is based on the oral evidence we gave in May 2021 to the joint session
oftheEuropeanUnionaﬀairscommitteesoftheGermanfederalparliamentandthe
French National Assembly.
1 Introduction
Artiﬁcial intelligence (AI) aims to automate human decision-making behavior and
is therefore also considered the next phase of the industrial revolution. We have
previouslyreviewedthestateoftheartandchallengesofAIapplicationsinhealthcare
(Glauner,2021a).ThisbookprovidesaforecastofhowAIandothertechnologiesare
likeltytoskyrockethealthcareintheforeseeablefuture.TheEuropeanCommission
published proposed regulation in April 2021 (European Commission, 2021) that
intendstocreateauniformlegalframeworkforAIwithintheEuropeanUnion(EU).
The proposal particularly addresses safety-critical applications, a category that also
Patrick Glauner
Deggendorf Institute of Technology, Deggendorf, Germany e-mail: patrick@glauner.info
Preprint. To appear in the 2022 Springer book “ The Future Circle of Healthcare: AI, 3D
Printing,Longevity,Ethics,andUncertaintyMitigation ""editedbySepehrEhsani,PatrickGlauner,
Philipp Plugmann and Florian M. Thieringer.
1</pre>",NEUTRAL
pdfminer_2663341_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663341.pdf,4,1,2663341,attachments/2663341.pdf#page=1,Is the line order correct?,"<pre> About the GFII: created in 1979, the GFII, the French organization of information professionals, is 
a unique association in the data landscape that brings together private and public data producers 
and re-users, such as the French Ministry of Interior, INPI, IGN, Total, BNP Paribas, Roquette frères, 
Saint-Gobain, Wolters-Kluwer, Elsevier, Françis Lefebvre-Dalloz group, Altarès. It gathers lawyers, 
engineers, data experts, compliance officers... 
The GFII aims to promote the economy of data, that means the recognition of the costs necessary 
for their manufacture, maintenance, development and dissemination in an assumed commercial 
environment, which does not exclude free of charge data sharing, but which puts more emphasis 
on  the  interoperability  of  data,  their  valorisation  and  their  reusability.  The  6  working  groups 
produce positions papers and white papers in order to help shaping the future of data policy in 
France and in the EU by offering a balanced and economically sustainable point of view about data 
and IA. The GFII promotes a sustainable and ethical use of data and works closely with its members 
for offering the most expert and efficient feedback about the implementation of data policies. GFII 
members may be AI systems providers, users or both. 
REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING 
DOWN  HARMONISED  RULES  ON  ARTIFICIAL 
INTELLIGENCE  (ARTIFICIAL 
INTELLIGENCE ACT) 
First of all, thanks to the European Commission to enable the GFII to answer this consultation on the 
draft regulation on AI. 
 1) Readability of the text 
The draft is rather complex to understand; understanding difficulties may then generate difficulties for 
being compliant, especially for SMEs and start up.  
 a) We invite then the legislator: 
- to amend the structure of the document by separating: 
- requirements dedicated to AI systems defined in the article 6 (1) / annex II and possibly 
Annex III (1) 
- requirements dedicated to AI systems defined in the article 6 (2) 
- requirements dedicated to AI systems defined in the annex III (6 to 8) 
- to define the requirements from the AI provider point of view, ie the operator that will have to comply 
with the future regulation, with a logical ie process oriented redactional architecture, step by step, 
 
 
 
 
</pre>",POSITIVE
fitz_2665205_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665205.pdf,4,3,2665205,attachments/2665205.pdf#page=3,Is the line order correct?,"<pre> 
 
3 
preserving the professional and pedagogical autonomy and academic freedom of teachers 
and academics. 
 
Transparency and AI literacy and CPD of teachers on AI: 
ETUCE welcomes that the proposal of AI Regulation requires that users of AI tools (who 
include students, teachers, academic and education staff for the education sector) must be 
adequately informed about the intended purpose, level of accuracy, residual risks of AI 
tools. Nevertheless, ETUCE highlights that providing information is not sufficient to ensure 
the transparency of the AI tools when users miss the adequate digital skills and data and AI 
literacy to interpret it. Therefore, it is of utmost importance to improve the importance of 
digital skills, AI literacy and data literacy in educational curricula and raise awareness on 
the risks related to the use of AI tools in education. It is also essential to ensure that 
infrastructures of education institutions are adequately equipped for digital education as 
well as to provide equal access to digital technologies and ICT tools to all teachers and 
students, with particular attention to the most disadvantaged groups. To these purposes, 
sustainable public investment should be provided by national governments and the 
European Commission should provide financial support through European funding such as 
Horizon Europe, Digital Europe and in the framework of National Recovery and Resilience 
Facility. 
While the AI Regulation blandly mention to the possibility of providing users with training 
on Artificial Intelligence, ETUCE emphasises that it is crucial that sustainable public funding 
are provided at national and European level to ensure that teachers, trainers, academics 
and other education personnel receive up-to-date and free of charge continuous training 
and professional development on the use of AI tools in accordance with their professional 
needs.  
 
EdTech expansion and issues of intellectual property rights, data privacy of teachers:   
ETUCE points out that the development of the use of Artificial Intelligence in education has 
been accompanied by the expansion of Ed-tech companies that are progressively 
increasing their influence in the education sector, especially under the pressure of 
emergency online teaching and learning during the COVID-19 pandemic. ETUCE reminds 
that education is a human right and public good whose value needs to be protected. ETUCE 
calls for further public responsibility from national governments that should not limit their 
scope to regulating the EdTech sector and should develop and implement public platforms 
for online teaching and learning to protect the public value of education. In addition, public 
platforms should be implemented in full respect of professional autonomy of teachers and 
education personnel as well as academic freedom and autonomy of education institutions, 
without creating pressure on teachers and education personnel regarding the education 
material and pedagogical methods they use. It is also essential to protect the accountability 
and transparency in the governance of public education systems from the influence of 
private and commercial interests and actors.  
</pre>",POSITIVE
tika_2665170_5,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665170.pdf,12,5,2665170,attachments/2665170.pdf#page=5,Is the line order correct?,"<pre>Hub France IA – Groupe de Travail Banques et Auditabilité  p 5/ 12  


Il serait nécessaire d’élaborer une analyse d’impact sur les coûts (et la perte de bénéfice des secteurs « à haut 
risque ») plus complète que celle indiquée au §3.3. 


Recommandation 3 : dans les domaines à haut risque, préciser deux sous-catégories de système IA à haut risque 
permettant de différencier deux niveaux de mise en œuvre de la conformité. Préciser les impacts économiques 
complets des secteurs à haut risque concernés. 


Par ailleurs, la définition en extension du périmètre de tels systèmes pourrait faire apparaître un risque juridique 
si l’usage visé n’apparait pas dans la liste au moment de la mise sur le marché, mais que celle-ci est ensuite 
modifiée. L’Article 7 (Modifications de l’annexe III) semble indiquer que la liste des domaines de l’annexe III 
(points 1 à 8) ne peut pas être modifiée : 


« La Commission est habilitée à adopter des actes délégués conformément à l’article 73 afin de mettre 
à jour la liste figurant à l’annexe III en y ajoutant des systèmes d’IA à haut risque lorsque les deux 
conditions suivantes sont remplies: 
(a) les systèmes d’IA sont destinés à être utilisés dans l’un des domaines énumérés à l’annexe III, 
points 1 à 8; 
(b) les systèmes d’IA présentent un risque de préjudice pour la santé et la sécurité, ou un risque 
d’incidence négative sur les droits fondamentaux, qui, eu égard à sa gravité et à sa probabilité 
d’occurrence, est équivalent ou supérieur au risque de préjudice ou d’incidence négative que présentent 
les systèmes d’IA à haut risque déjà visés à l’annexe III » 


Un usage ne figurant pas dans la liste des « domaines énumérés à l’annexe III » ne peut donc pas y être intégré 
ensuite. Cependant s’il y figure et que la liste de l’Annexe III est donc modifiée, comment les acteurs seront-ils 
notifiés ? la mise en conformité sera-t-elle être rétroactive ? dans quels délais ? 


De même, si nous restons dans la perspective d’un établissement des risques ex ante fondé sur des cas d’usage, 
quels sont les processus qui seront mis en place afin de réévaluer au fil du temps le niveau de risque des cas 
d’usage ? Le recours aux « actes délégués » est très imprécis et leur périmètre devrait être très clairement 
encadré. 


Recommandation 4 : préciser les règles de modification de l’Annexe III pour les systèmes IA à haut risque par le 
biais d’actes délégués. 


PROBLEMES CONCRETS POUR L’IMPLEMENTATION DE LA MISE EN CONFORMITE 


Nous avons identifié de nombreux cas où l’implémentation pratique de la mise en conformité semble 
extrêmement coûteuse, voire impossible. Notre crainte repose principalement sur la mise en place d’une 
réglementation qui sera vue comme un frein à l’innovation permise par l’IA, compte tenu de ces contraintes de 
mise en conformité. Par ailleurs, nous ne voyons pas comment il serait possible d’appliquer la mise en conformité 
sur certains systèmes déjà en production mais pour lesquels les exigences requises ne peuvent être 
rétroactivement mises en œuvre. Pour chaque contrainte de mise en conformité devrait être conduite une 
analyse d’impact sur l’innovation. 


Nous listons ici quelques cas qui nous apparaissent critiques.  


MISES A JOUR FREQUENTES  


Une fois qu’une solution comprenant un composant d’apprentissage a été mise sur le marché, elle doit être 
monitorée pour contrôler ses performances. Habituellement, les données dérivent peu à peu et leur distribution</pre>",POSITIVE
pdfminer_2663324_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2663324.pdf,7,4,2663324,attachments/2663324.pdf#page=4,Is the line order correct?,"<pre>Establish a unified standard for data exchange 
Allow transfer of data between local centres 
Incentives for organizations/vendors that align 
with common data standards 
Promote data interoperability and exchange 
protocol, based on the experience of ERNs 
(Collaboration between several member states on 
the same topic) 
Centralise data collection from regional data sets 
REGULATORY CHALLENGES 
Validation process and Reimbursement 
Member States are slowly but surely adopting national legislatures allowing for digital health solutions to 
apply for reimbursement. While it is far from being general practice, the trend is growing, and regulatory 
agencies  or  Health  Technology  Assessment  (HTA)  bodies  are  adapting  their  pathways  to  include  the 
assessment of digital health solutions, including AI-based solutions. 
While  AI  providing  health  data  based  on  continuous  monitoring  (both  passively  and  actively)  of  daily 
activities  has  a  greater  potential  for  reimbursement,  the  solutions  utilizing  AI  without  immediate  health 
benefits might find a more challenging process. 
The data set used by the AI applications or used as secondary data (for instance, when an AI-based tool 
provides  information  on  an  internal  decision  point)  in  the  application  dossier  for  HTA,  reimbursement 
process or post-marketing studies must be of the highest standards of quality. It is crucial to demonstrate 
the  relevance  of  the  data  in  a  lifecycle  approach:  in  the  context  of  approval  and,  at  a  later  stage, 
reimbursement. Should the regulator dispute the quality and/or the validity of the data used by the algorithm, 
the entire process could be severely delayed, henceforth, delaying access to patients. 
While  one  can  understand  the  challenge  in  adapting  to  an  extremely  changing  world,  as  well  as  to  the 
massive disruption AI is about to bring to healthcare, it is indisputable that regulators need to issue  clear 
guidance  on  quality  requirements  for  validation  processes,  reimbursement  criteria,  and  post-marketing 
studies12. We also call for clearer visibility on the possible evolution of the regulatory framework, to take 
into account the coming development of technology (e.g.: adaptive AI). 
12 Be it for surveillance or studies in the context of real-world evidence generation. 
Page 4 of 7 
  
 
  
 
 
 
 
                                                      
</pre>",NEUTRAL
tika_2665518_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665518.pdf,2,2,2665518,attachments/2665518.pdf#page=2,Is the line order correct?,"<pre>Page 2 of 2 





The proposed regulation does not address the role of shop stewards in ensuring that worker's 


rights are safeguarded when artificial intelligence is introduced in the workplace. The best 


solutions are found through local dialogue between management and worker's representatives. 


To ensure co-determination from employees, Negotia believes that the social dialogue should 


have a clearly defined role in the regulation of artificial intelligence in the workplace. We also 


believe that the conformity assessment of artificial intelligence solutions that are intended to 


be used in the workplace should be performed by an independent third party. 


In conclusion, Negotia believes the European Commission's proposal to regulate artificial 


intelligence, is a great step in the right direction, but that it has clear weaknesses when it 


comes to regulating technology in the workplace. Further measures should be put in place to 


ensure workers' rights, in the face of technological developments. 


We are grateful for the opportunity to provide feedback on the proposal,and look forward to 


following the legislative process going forward. 








Monica A. Paulsen 


President of Negotia</pre>",POSITIVE
fitz_2665469_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665469.pdf,3,3,2665469,attachments/2665469.pdf#page=3,Is the line order correct?,"<pre> 
 
 
 
V1.0 5. august 2021 KMD  
Side 3 af 3 
 
 
 
2.3 
Sandboxes and data spaces 
Another requirement is the so-called AI regulatory sandboxes (Title V Article 53) for testing 
and validating AI systems before placement on the market.  
 
It is unclear whether these sandbox environments will be provided by member states’ 
authorities themselves or the European Data Protection Supervisor, or if the task will be 
outsourced to private companies. The same goes for the European common data spaces 
(Recital 45). KMD suggests a further clarification in the proposal. 
 
 
3 
Clarification of various terms and concepts 
 
3.1 
High-risk classification 
As for the protection of public interest in high-risk AI systems, the proposal states that 
common normative standards should be established (Recital 13). KMD suggests a clarification 
of whether these common normative standards are part of the report on AI standards2. 
 
The requirements regarding accuracy, consistency, robustness, appropriateness etc. in high-
risk AI systems (Title III Article 15) furthermore lacks specification. 
 
Title XII Article 83(2) regards the application of the regulation to high-risk AI systems already 
on the market. It states that these solutions will not be subject to the new regulation unless 
there are significant changes to design or purpose. KMD suggests a further elaboration on 
what constitutes a “significant change”. 
 
3.2 
Placing on the market and putting into service 
The difference between “placing on the market” and “putting into service” (Title 1 Article 
3(9,11)) is subtle but regards first time a solution is made available to the market respectively 
first use. This raises a question about retrained AI systems for a new problem dataset. A 
clarification of whether using retrained AI systems are considered “first use” would be 
beneficial. 
 
 
 
 
2 Cf. JRC Publications Repository - AI Standardisation Landscape: state of play and link to the EC proposal for an AI regulatory 
framework (europa.eu) 
</pre>",POSITIVE
tika_2665432_6,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665432.pdf,7,6,2665432,attachments/2665432.pdf#page=6,Is the line order correct?,"<pre>6 


Furthermore, the proposal does not include the possibility of harmed consumers to be 


represented by an NGO, including consumer organisations, in the exercise of their rights. 


An article similar to Art 80 GDPR (Representation of data subjects) or Article 68 of the 


proposal for a Digital Services Act (Representation) should be introduced. In the context 


of the AI Act, this representation shall not be subject to a mandate. 





Also, to enable collective redress actions, this proposal should be included in the Annex of 


the Representative Actions Directive26. A similar provision was included in the Commission’s 


proposal for a Digital Services Act.27 


6. The governance and enforcement structure must be clear and ensure 
an effective and consistent application of the rules at European and 


national level 


The proposed governance and enforcement structure mainly rests at national level and 


raises issues in relation to the obligations, competences and powers of the different actors 


involved28 and the different processes envisaged.  





For example:  


- The need to ensure a coherent and coordinated EU-wide enforcement. While the 


Commission plays a central role if a national supervisory authority notifies its intention 


to adopt measures against an AI system in its territory29, the Commission has no powers 


to proactively take the lead in case of inaction by national authorities. The autonomy 


and powers of the European AI Board30, comprised of high-level representatives of the 


national supervisory authorities and chaired by the Commission, also seem quite 


limited.  





- There is a need to clarify potential overlaps with existing bodies such as the European 


Data Protection Board, as well as the role of Data Protection Authorities – a view that 


other stakeholders also share31. 





- It is also important to ensure a common approach when it comes to the cooperation 


between the different competent authorities and supervisory authorities at national 


level, to ensure effective and swift enforcement as well as to avoid different approaches 


by Member States. 





- Given the lack of redress mechanisms for consumers in the proposal, there is no 


authority that is entrusted with dealing with consumer complaints in case of breaches 


of the regulation. 





 
26 Directive (EU) 2020/1828 on representative actions for the protection of the collective interests of consumers 
and repealing Directive 2009/22/EC; 
27 See Article 72 of the Proposal for a Regulation on a Single Market For Digital Services (Digital Services Act) 
and amending Directive 2000/31/EC;  
28 Namely: the European Artificial Intelligence Board (EAIB), the national competent authorities, the national 
supervisory authority, the Commission, the European Data Protection Supervisor (EDPS) and the AI system 
providers); 
29 See Articles 65 (5) and 66 of the proposal; 
30 See Articles 56 – 58 of the proposal; 
31 See, for example, Access Now: https://www.accessnow.org/eu-minimal-steps-to-regulate-harmful-ai-
systems/. 



https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32020L1828&qid=1623135746284

https://eur-lex.europa.eu/legal-content/en/TXT/?qid=1608117147218&uri=COM%3A2020%3A825%3AFIN

https://www.accessnow.org/eu-minimal-steps-to-regulate-harmful-ai-systems/

https://www.accessnow.org/eu-minimal-steps-to-regulate-harmful-ai-systems/</pre>",POSITIVE
PyPDF2_2665624_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2665624.pdf,11,4,2665624,attachments/2665624.pdf#page=4,Is the line order correct?,"<pre>   
 
johner -institut.de    Johner Institut GmbH  
Reichenaustraße 1  
78467 Konstanz  
T +49 7531 94500 -20 
info@johner -institut.de  monitoring” or “serious 
inciden t 
A device is considered a high -
risk AI system, if the following 
two conditions are met (article 
6):  
(a) the AI system is intended to 
be used as a safety component 
of a product, or is itself a 
product, covered by the Union 
harmonization legislation listed 
in Annex II ; 
(b) the product whose safety 
component is the AI system, or 
the AI  system itself as a 
product, is required to 
undergo a third -party 
conformity assessment with a 
view to the placing on the 
market or putting into service 
of that product pursuant to the 
Union harmonization 
legislation listed in Annex II .  Medical devices a re covered 
by the regulations listed in 
Annex II, because the MDR 
and IVDR are mentioned. To 
fulfill the MDR, medical 
devices class IIa and higher 
must undergo a conformity 
assessment procedure.   
 
Does this mean all software 
as medical device using AI is 
considered to be a high -risk 
product ? 
 
MDR rule 11 classifies 
software, independent of risk, 
in most cases in class IIa or 
higher. Thus, the extensive 
requirements for high -risk 
products would apply for 
medical devices. The negative 
effects of rule 11 wou ld be 
amplified by the AI act . Recital (31) states: The 
classification of an AI system as 
high-risk pursuant to this 
Regulation should not 
necessarily mean that the 
product whose safety 
component is the AI system, or 
the AI system itself as a 
product, is c onsidered ‘high -
risk’ under the criteria 
established in the relevant 
Union harmonization 
legislation that applies to the 
product. This is notably the 
case for Regulation (EU) 
2017/745 of the European 
Parliament and of the 
Council47 and Regulation (EU) 
2017 /746 of the European 
Parliament and of the 
Council48, where a third -party 
conformity assessment is 
provided for medium -risk and 
high-risk products . 
→ This should be considered in 
the AI ac t 
In article 10 the AI act requires   
„training, validation, and 
testing data sets shall be 
relevant, representative, free of 
errors and complete. “ Real-world data is rarely “free 
of error” and “complete”. It is 
also unclear what “complete” 
means. Do all datasets need 
to be available (whateve r this 
means) or is the complete 
data of one dataset required ? This requirement should be 
annulled. More suitable seems 
the requirement, that 
manufactures must define 
quality standards and verify 
their compliance. Another 
possible requirement could be 
the claim that the definition of 
the quality standards has to be 
risk-based.   
Further, definitions i.a. “correct” 
are needed.   </pre>",NEGATIVE
PyPDF2_2665518_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665518.pdf,2,2,2665518,attachments/2665518.pdf#page=2,Is the line order correct?,"<pre> 
Page 2 of 2 
 The proposed regulation does not address the role of shop s tewards in ensuring that worker's  
rights are safeguarded  when artificial intelligence is introduced in the workplace . The best 
solutions are found t hrough  local dialogue  between management and worker's representatives. 
To ensure co -determination from employees, Negotia believes that the social dialogue  should 
have a clearly defined role in the regulation of artificial intelligence in the workplace. We also 
believe that the conformity assessment of artificial intelligence solutions that are intended to 
be used in the workplace should be performed by an independen t third party.  
In conclusion , Negotia believes the European Commission's proposal to regulate artificial 
intelligence,  is a great step in the right direction,  but that it has clear weaknesses when it 
comes to regulating technology in the workplace . Further  measures should be put in place to 
ensure workers ' rights , in the face of technological developments.  
We are grateful for the opportunity to provide feedback on the proposal ,and look forward to 
following  the legislative process going forward.  
 
 
Monica A. Paulsen  
President  of Negotia  
 
</pre>",POSITIVE
