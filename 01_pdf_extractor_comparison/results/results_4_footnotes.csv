id,user_type,filename,n_pages,page,source_id,source,task,content,result
fitz_2663289_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2663289.pdf,7,3,2663289,attachments/2663289.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>20210728 ‚Äì 1.0 </code><br><code>11.</code><code> </code><code>The following comments set out some of the ICO‚Äôs thoughts on the </code><br><code>European Commission (‚Äòthe Commission‚Äô) proposed AIA. Our analysis </code><br><code>centres around the implications of the AIA for data protection law as this is </code><br><code>our remit and area of expertise.  </code><br><code>An important step towards regulating AI  </code><br><code>12.</code><code> </code><code>We welcome the AIA‚Äôs ambition to regulate the use of AI so that it is safe </code><br><code>and respects existing law, fundamental rights and EU values. We agree with </code><br><code>the proposal‚Äôs view that legal certainty is paramount in facilitating </code><br><code>innovation and investment in this emerging technology. We believe such a </code><br><code>far-reaching regulatory framework should first and foremost serve the </code><br><code>public interest by creating an ecosystem that provides consistency and </code><br><code>certainty for the good players and enforces against the bad players in the </code><br><code>marketplace. </code><br><code>13.</code><code> </code><code>The ICO appreciates and supports the innovation and opportunities to </code><br><code>society that AI can bring. To build the trust that is necessary to realise that </code><br><code>potential value, we must be mindful of the fact that the standard practices </code><br><code>for developing and deploying AI may create data protection risks such as </code><br><code>non-compliance with GDPR‚Äôs data minimisation principle or individual </code><br><code>information rights, as well as harms such as unfair discrimination. </code><br><code>14.</code><code> </code><code>Following the UK‚Äôs exit from the EU, we remain committed to ensuring high </code><br><code>standards of data protection that protect individual rights while also </code><br><code>enabling data to be used responsibly to deliver social and economic </code><br><code>benefits. Continuing to engage with our EU partners remains critical for the </code><br><code>ICO and we support an approach to AI policy that respects and protects </code><br><code>fundamental rights while boosting innovation by enabling personal data to </code><br><code>be used responsibly and deliver social and economic benefits</code><code> </code><br><code>15.</code><code> </code><code>Data protection law is already playing an important role in AI regulation </code><br><code>and we commend the Commission‚Äôs intention for the AIA to be consistent </code><br><code>with existing legislation on data protection, consumer protection, nondiscrimination and gender equality. We believe regulatory coherence </code><br><code>between data protection law and the AIA will be vital for businesses to </code><br><code>innovate free of the impediments that legal uncertainty creates.  </code><br><code>Points of the AIA proposal the ICO supports  </code><br><code>16.</code><code> </code><code>We believe responsible development, testing, deployment and oversight of </code><br><code>AI can accelerate economic growth, build public trust in the technology </code><br><code>itself and lead to technological progress and human flourishing more </code><br><code>broadly.  </code><br><code>17.</code><code> </code><code>As the UK data protection regulator, the ICO has been playing its part in </code><br><code>helping businesses responsibly develop AI in ways that protect fundamental </code><br><code>rights, including privacy and the right to non-discrimination. We agree that </code><br><code>ensuring the public and consumers are protected while innovative </code><br><code>businesses are supported is better accomplished by putting in place both </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662780_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2662780.pdf,4,1,2662780,attachments/2662780.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Education International </code><br><code>Internationale de l'Education </code><br><code>Internacional de la Educaci√≥n </code><br><code> </code><br><code>http://www.ei-ie.org </code><br><code>EUROPEAN REGION- </code><br><code>ETUCE </code><br><code> </code><br><code>President </code><br><code>Larry FLANAGAN </code><br><code> </code><br><code>Vice-Presidents</code><code> </code><br><code>Odile CORDELIER  </code><br><code>Andreas KELLER </code><br><code>Trudy KERPERIEN </code><br><code>Dorte LANGE </code><br><code>Galina MERKULOVA  </code><br><code>Branimir STRUKELJ</code><code>  </code><br><code> </code><br><code> </code><br><code> </code><br><code>Boulevard Bischoffsheim, 15 </code><br><code>1000 Brussels, Belgium </code><br><code>Tel +32 2 224 06 91/92 </code><br><code>Fax +32 2 224 06 94 </code><br><code>secretariat@csee-etuce.org </code><br><code>http://www.csee-etuce.org</code><code> </code><br><code> </code><br><code>European Director </code><br><code>Susan FLOCKEN </code><br><code> </code><br><code>Treasurer </code><br><code>Joan DONEGAN </code><br><code> </code><br><code> </code><br><code> </code><br><code>ETUCE  </code><br><code>European Trade Union Committee for Education </code><br><code>EI European Region  </code><br><code> </code><br><code> </code><br><code> </code><br><code>ETUCE position on the EU Regulation on Artificial Intelligence </code><br><code>(Adopted by the ETUCE Bureau on 7 June 2021) </code><br><code>Background: </code><br><code>On 21 April 2021, the European Commission published a proposal for a </code><code>‚ÄúRegulation on a </code><br><code>European Approach for Artificial intelligence</code><code>‚Äù (the AI Regulation). With this proposal, the </code><br><code>European Commission follows up on its </code><code>White Paper on Artificial Intelligence</code><code> (February </code><br><code>2020), based on the results of a broad consultation process to which ETUCE </code><code>contributed</code><code>. </code><br><code>The aim of the initiative is to establish the first EU legal framework regulating the entire </code><br><code>lifecycle of the use of Artificial Intelligence (AI) in all sectors, including education.  </code><br><code>The AI Regulation </code><code>classifies</code><code> the use of Artificial Intelligence in various sectors based on the </code><br><code>risk that the AI tools have on the health and safety and the fundamental rights</code><code> of </code><br><code>individuals. Concerning education, the proposal considers the use of Artificial Intelligence </code><br><code>tools in </code><code>education as high-risk</code><code> as potentially harmful to the right to education and training </code><br><code>as well as the right not to be discriminated in education. For high-risk sectors, the AI </code><br><code>Regulation establishes </code><code>stricter horizontal legal requirements</code><code> to which AI tools must comply </code><br><code>before being authorised on the market. These include risk management system during the </code><br><code>entire lifecycle of the AI system.     </code><br><code>Following the publication of the proposal, on 26 April 2021, the European Commission </code><br><code>issued a </code><code>public consultation</code><code> that will run until 20 July 2021, accompanied by an </code><code>impact </code><br><code>assessment report</code><code>.  </code><br><code>The following text is the ETUCE response to the public consultation bringing the perspective </code><br><code>of teachers, academics and other education personnel on the sections of the AI Regulation </code><br><code>that touch upon the education sector.  </code><br><code> </code><br><code>ETUCE reply: </code><br><code>ETUCE welcomes the publication of the AI Regulation as it sets the ground for the first </code><br><code>comprehensive EU regulation on Artificial Intelligence to ensure a controlled development </code><br><code>of AI tools in education and address the risks connected to their use by teachers, academic, </code><br><code>other education personnel and students. While ETUCE recognises the potential of digital </code><br><code>technologies and Artificial Intelligence tools to bring about improvements in education, it </code><br><code>also underlines the </code><code>numerous ethical concerns</code><code> related to their trustworthiness, data </code><br><code>privacy, accountability, transparency and their impact on equality and inclusion in </code><br><code>education. ETUCE underlines that </code><code>further research</code><code> at national and European level is needed </code><br><code>to assess and address the risks connected to the use of Artificial Intelligence in education </code><br><code>with constant and meaningful consultation with education social partners.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2661384_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2661384.pdf,3,2,2661384,attachments/2661384.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>         2 </code><br><code>situation where the respect for core European values and freedoms, in particular in the setting of internet </code><br><code>standards and new technologies like artificial intelligence, blockchain, data or online platforms, is being </code><br><code>challenged. At the same time, setting international standards is key to promoting the EU‚Äôs strategic capacity </code><br><code>in areas like raw materials, space data, batteries, hydrogen or microchips. The coordination of EU </code><br><code>stakeholders and available resources should be improved to ensure that standards are set in line with EU </code><br><code>values.  </code><br><code>(5) The importance of standards is growing, while at European (and national) level there is almost no formal </code><br><code>education on or vocational training in standardisation. </code><br><code>(6)</code><code> </code><code>While services account for about 70% of the EU‚Äôs GDP, they only represent a mere 2% of standardisation </code><br><code>activities at EU level. In particular, for business services, standards can be a solution to removing barriers in </code><br><code>the single market. </code><code> </code><br><code>Basis for EU intervention (legal basis and subsidiarity check) </code><code> </code><br><code>The legal framework under which the Commission acts on European standardisation, while fully respecting the </code><br><code>distribution of competences between the Union and the Member States as laid down in the Treaties, concerns in </code><br><code>particular Articles 14, 151, 152, 153, 165, 166 and 168 of the Treaty on the Functioning of the European Union </code><br><code>(TFEU) and Protocol (No 26) on Services of General Interest annexed to the Treaty on European Union (TEU) </code><br><code>and to the TFEU. </code><code> </code><br><code>B. What does the initiative aim to achieve and how  </code><br><code>This initiative‚Äôs overarching goal is to consolidate and improve the EU standardisation system, so that it </code><br><code>continues to support a well-functioning single market and the competitiveness of EU industries and protects EU </code><br><code>citizens and the environment. The strategy will seek to do the following: </code><br><code>(1) Modernise and consolidate the European standardisation system, ensuring it is better oriented towards </code><br><code>meeting the EU‚Äôs main interests, policy priorities, core principles and values, notably the green/digital </code><br><code>industrial transition, in a timely manner. This could mean:  </code><br><code>ÔÇ∑</code><code> </code><br><code>finding ways to anticipate and define standardisation priorities at political level and with European </code><br><code>stakeholders. </code><br><code>ÔÇ∑</code><code> </code><br><code>addressing bottlenecks within the standardisation system, including procedural aspects and working on </code><br><code>the delivery mechanisms to ensure speed and quality. A joint task force between the Commission and </code><br><code>the European Standardisation Organisations (CEN/CENELEC and ETSI) has been set up for this </code><br><code>purpose, as a follow-up to the update of the industrial strategy.  </code><br><code>ÔÇ∑</code><code> </code><br><code>introducing more agile working methods and developing closer cooperation between national </code><br><code>standardisation bodies, the European industry, European Standardisation Organisations and the </code><br><code>European Commission and further improvements of governance and funding mechanisms of the </code><br><code>European Standardisation Organisations, while preserving the inclusiveness of the European </code><br><code>standardisation system (role of SMEs, civil society organisations and ‚Äòvertical‚Äô industries). </code><br><code>ÔÇ∑</code><code> </code><br><code>finding ways to incentivise coordination, efficiency and flexibility in the timely delivery of European </code><br><code>standards. </code><br><code>(2) Develop a more strategic and coordinated approach to global standards-setting in areas of strategic EU </code><br><code>interest, including through Member States, the European Parliament and European stakeholders, and </code><br><code>fostering strategic partnerships with like-minded trading partners.  </code><br><code>(3) Make full use of EU industrial resources to contribute to (pre-) standardisation activities, including research </code><br><code>and innovation activities. </code><br><code>(4) Address standards-related education, skills and expertise, both in the public and private sector. </code><br><code>As a follow-up to the </code><code>updated industrial strategy</code><code>, the Commission will assess whether amendments to the </code><br><code>Standardisation Regulation are required to achieve the objectives outlined above. Complementary to the </code><br><code>Commission‚Äôs announcement on an initiative for business services standards, the strategy will look at further </code><br><code>developing services standardisation activities. </code><br><code>C. Better regulation </code><br><code>Consultation of citizens and stakeholders </code><code> </code><br><code>Feedback is sought particularly on: 1) whether the current European standardisation system is fit for purpose to </code><br><code>support European strategic interests; 2) how the EU can leverage and promote global leadership in standardssetting; 3) whether changes in governance and working methods are required to improve the performance of the </code><br><code>European standardisation system. The main stakeholders of the consultation activities are EU industries, civil </code><br><code>society organisations, academia, trade unions and SMEs, Member States, European/national parliaments, </code><br><code>European/national standardisation bodies.  </code><br><code>The Commission will undertake several targeted consultations to ensure that all key stakeholders have an </code><br><code>opportunity to express their views: pro-active outreach (including through bilateral meetings) to industrial, civil </code><br><code>society and stakeholder organisations, in-depth discussions in relevant Commission sectoral expert groups and </code><br><code>committees, including the Committee on Standards and the European Multi-Stakeholder Platform on ICT </code><br><code>Standardisation, pro-active outreach to the European standardisation organisations and national standardisation </code><br><code>bodies via the European policy hub. </code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665562_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2665562.pdf,10,3,2665562,attachments/2665562.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>interact with;</code><br><code>(b)</code><code> For a given set of human-defined</code><br><code>objectives, generate outputs such</code><br><code>as content, predictions,</code><br><code>recommendations, or decisions</code><br><code>(c) Must be based on a rule base or</code><br><code>model,</code><br><code>(d)</code><code> Influences the environment that it</code><br><code>interacts with;</code><br><code>OFE is ready to further discuss and work on improvements. We would like to</code><br><code>encourage the European Commission to support some amendments - for instance in</code><br><code>the form as proposed by OFE ÓÇà in the context of the co-regulation process and the</code><br><code>discussions with Parliament and Council.</code><br><code>Scope should be clarified for open source developers</code><br><code>Open source developers should be free from provider obligations unless they provide a</code><br><code>fully functioning AI system for placing on the EU market. OFE believes that the</code><br><code>proposal is not intended to hamper open source developments, communities and open</code><br><code>source code hosting platforms in the sphere of AI systems; however, we suggest it be</code><br><code>clarified.</code><br><code>When an open source developer collaborates with fellow developers under established</code><br><code>OSI/FSF licensing on AI-related code, such developers should be free of obligations</code><br><code>under the regulation unless they place a fully functioning AI system on the EU market.</code><br><code>Uncertainty of possible obligations and liabilities could have a chilling effect on</code><br><code>innovation, particularly among European developers who are collaborating and sharing</code><br><code>ideas related to potentially break-through research and collaboration. Insofar as the</code><br><code>EU AI Act intends to support EU innovation, the Commission should clarify that sharing</code><br><code>open source AI code, proof of concept AI research, or simply experimenting with AI</code><br><code>models should be expressly excluded from the regulation. We suggest three changes</code><br><code>to clarify this scope:</code><br><code>First, the proposal‚Äôs recital 16 demonstrates the intention that the AI Act should</code><br><code>not curtail ‚ÄúResearch for legitimate purposes‚Äù on certain high-risk AI systems ‚Äúif</code><br><code>such research does not amount to use of the AI system in human-machine</code><br><code>relations‚Ä¶.‚Äù However, the definition by reference of ‚Äúprovider‚Äù--as a natural</code><br><code>person that develops an AI system ‚Äúwith a view to placing it on the market‚Äù,</code><br><code>defined as ‚Äúfirst making available‚Äù on the market, defined as ‚Äúsupply of an AI</code><br><code>system for distribution or use on the Union market in the course of a</code><br><code>commercial activity, whether in return for payment or free of charge‚Äù--may be</code><br><code>OpenForum Europe AISBL</code><br><code>Avenue des Arts 56 4C</code><br><code>Brussels 1000</code><br><code>OFE in the EU transparency register: 2702114689</code><br><code>3</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665441_6,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665441.pdf,8,6,2665441,attachments/2665441.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>SEMI Europe </code><code>| Rue de la Science 14, 1040 | EU Transparency Register: 402302029423-14                                                     </code><br><code>Tel.: +32 (0) 2 609 53 18 | www.semi.org/eu </code><br><code> </code><br><code>Page | 6 </code><br><code>for a general statement in the database directive that takes into account the protection of valuable own </code><br><code>generated data rather than only data collected by other sources. </code><br><code> </code><br><code> </code><br><code> </code><br><code>IV.</code><code> </code><br><code>Establishing standards for AI systems addressing potential risks and enabling innovation </code><br><code>In the global manufacturing ecosystem, standards play a key role in advancing the innovation of numerous </code><br><code>technologies and systems, enabling the integration of smart manufacturing solutions across industries.     </code><br><code>As businesses embrace AI, they increasingly look for standards to guide their efforts to improve </code><br><code>interoperability, communication, security, efficiency and productivity of existing and emerging </code><br><code>manufacturing technologies.  </code><br><code>European standardization efforts are an important step in guiding ethical research and development of </code><br><code>AI, addressing potential barriers and risks that could hinder efficient and secure operation of AI systems </code><br><code>and technologies. AI standards are a valuable asset to Europe‚Äôs industrial competitiveness reducing </code><br><code>system operation costs, easing emerging markets penetration, advancing the dissemination and </code><br><code>exploitation of  technological expertise among SMEs and start-ups. It is crucial for the EU and the </code><br><code>electronics manufacturing industry that standardization efforts keep pace with the global AI development, </code><br><code>so that Europe remains at the forefront of standardization of emerging technologies.  </code><br><code>According to Article 41 of the proposed regulation on AI, the Commission may by means of implementing </code><br><code>acts, adopt common specifications (for high-risk AI applications) where harmonized standards do not exist </code><br><code>or where the Commission considered that relevant harmonized standards are insufficient.  </code><br><code>SEMI Europe appreciates the European Commission‚Äôs efforts to drive standardization of AI, while </code><br><code>considering the need for addressing insufficient or missing standards. To generate reliable, innovationfriendly AI standards, a consensus-based approach among regional and international standardization </code><br><code>bodies, organizations and industry players is crucial. The collective effort should focus not only on the </code><br><code>improvement of existing AI standards, but also on areas where standards are limited or non-existent, </code><br><code>referring to standardization of categories of systems that are directly influenced by AI. Examples are </code><br><code>systems with impact on Medical procedures, impact on Smart mobility (Automotive, Aerospace), and </code><br><code>impact on Defense actions. These categories and more will have to be defined and standards (including </code><br><code>definition of overriding AI control) will have to be agreed upon.   </code><br><code>SEMI Europe would like to bring to the attention of the Commission the SEMI International Standards </code><br><code>Program. The initiative brings together industry experts, stakeholders and professionals, driving the </code><br><code>development of standards in regards to technological innovation and needs of the global industry.                  </code><br><code>Since the inception of the program, over 1000 developed standards have assisted the global </code><br><code>manufacturing industry in engaging the challenges of increased productivity, improved product reliability, </code><br><code>operations quality and resource efficiency.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662602_2,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662602.pdf,4,2,2662602,attachments/2662602.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Technische Hochschule Deggendorf </code><br><code>Dieter-G√∂rlitz-Platz 1 </code><br><code>94469 Deggendorf</code><code> </code><br><code> </code><br><code>Tel.: +49 991 3615-0 </code><br><code>Fax: +49 991 3615-297 </code><br><code> </code><br><code>www.th-deg.de </code><br><code>info@th-deg.de </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Schnittmengen mit anderen Gebieten, wie z.B. der Statistik und Signalverarbeitung. Diese </code><br><code>Problematik erschwert grunds√§tzlich jegliche Art von KI-Regulierung. </code><br><code> </code><br><code>Der Verordnungsvorschlag enth√§lt in </code><code>Article 3 (Definitions)</code><code> mit Verweis auf </code><code>Annex I </code><code>eine </code><br><code>eigene Definition von KI: Diese umfasst jede Software</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>, die Methoden des maschinellen </code><br><code>Lernens oder logikbasierte Verfahren nutzt. Sie schlie√üt jedoch auch jede Software ein, die </code><br><code>statistische Verfahren oder Such- und Optimierungsverfahren einsetzt. Hierdurch wird nahezu </code><br><code>jede bestehende und zuk√ºnftige Software ohne KI-Bezug als ‚ÄûKI‚Äú klassifiziert. Ein Beispiel </code><br><code>hierf√ºr ist jede Software, die den Mittelwert von Zahlen berechnet und somit ein ‚Äûstatistisches </code><br><code>Verfahren‚Äú anwendet. </code><br><code> </code><br><code>Da nahezu jede Software von diesem Regelwerk umfasst wird, w√ºrden somit f√ºr jedes Unternehmen unvorhersehbare Risiken entstehen, sobald es Software einsetzt oder entwickelt. </code><br><code> </code><br><code>2. Kein KI-spezifischer Regulierungsbedarf </code><br><code>Bestehende Regulierungen, Gesetze, Standards, Normen, etc. von Technologien sind in den </code><br><code>meisten F√§llen vertikal aufgebaut und betrachten dabei allgemein Systeme, die bestimmte </code><br><code>sicherheitskritische Anwendungsf√§lle adressieren, wie z.B. im Flugverkehr, im Stra√üenverkehr </code><br><code>oder der Betrieb von Atomkraftwerken. Der genauen Implementierung dieser Systeme durch </code><br><code>Hardware, Software oder einer Kombination daraus wird darin oft wenig bis keine Beachtung </code><br><code>geschenkt. </code><br><code> </code><br><code>KI-L√∂sungen sind meist ein kleiner Teil von gr√∂√üeren Software-/Hardwaresystemen. Der </code><br><code>Verordnungsvorschlag versucht diesen Anteil horizontal - und somit anwendungsfallunabh√§ngig - zu regulieren</code><code>2</code><code>. Dieser Ansatz erscheint aufgrund der insgesamt meist </code><br><code>unkritischen KI-Anwendungsf√§llen als nicht praktikabel. Es ist davon auszugehen, dass eine </code><br><code>zus√§tzliche horizontale Regulierung zu unklaren Zust√§ndigkeiten und Kompetenzstreitigkeiten </code><br><code>f√ºhren w√ºrde. Zus√§tzliche Regulierungen sollten daher nur neuartige Anwendungsf√§lle </code><br><code>adressieren, die von bestehenden Regulierungen noch nicht erfasst werden.  </code><br><code>  </code><br><code>Der KI Bundesverband spricht sich daf√ºr aus, bestehende anwendungsfallspezifische </code><br><code>Regulierungen anzuwenden und diese bei Bedarf KI-spezifisch anzupassen</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>. Der Bitkom ist </code><br><code>ebenfalls der Ansicht, dass selbst eine allgemeinere horizontale Regulierung von </code><br><code>algorithmischen Systemen nicht praktikabel sei</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code>.  </code><br><code> </code><br><code>3. Fehlende Abgrenzung zu bestehenden Regulierungen </code><br><code>Die in dem Verordnungsvorschlag in </code><code>Article 5</code><code> verbotenen KI-Anwendungsbereiche sind sehr </code><br><code>weit gefasst und f√ºhren aufgrund des entsprechend weiten Interpretationsspielraums</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> zu </code><br><code>Unsicherheiten f√ºr alle Beteiligten. Deutlich zielgerichteter w√§re die Definition von konkret </code><br><code>verbotenen Anwendungsf√§llen. Dabei sollte auch gepr√ºft werden, ob diese √ºberhaupt explizit </code><br><code>verboten werden m√ºssten oder ob dies schon heute durch andere Gesetze, wie z.B. dem </code><br><code>Strafgesetzbuch oder der Datenschutz-Grundverordnung (DSGVO), der Fall ist. Neue Verbote </code><br><code>sollten </code><br><code>zudem </code><br><code>die </code><br><code>entsprechenden </code><br><code>Anwendungsf√§lle </code><br><code>im </code><br><code>Allgemeinen </code><br><code>und </code><br><code>ohne </code><br><code> </code><br><code>1</code><code> </code><code>Angemerkt sei, dass KI grunds√§tzlich auch durch rein hardwarebasierte Ans√§tze ohne Software implementiert werden </code><br><code>kann. Der Verordnungsvorschlag l√§sst dies au√üer Acht und w√ºrde auf diesem Wege implementierte KIs grunds√§tzlich </code><br><code>nicht regulieren.</code><code> </code><br><code>2</code><code> </code><code>Viele g√§ngige KI-Verfahren k√∂nnen auch dem Fachgebiet der Signalverarbeitung zugeordnet werden. Es gab in der </code><br><code>Geschichte </code><br><code>der </code><br><code>Signalverarbeitung </code><br><code>jedoch </code><br><code>keine </code><br><code>entsprechend </code><br><code>breiten </code><br><code>anwendungsfallunabh√§ngigen </code><br><code>Regulierungsversuche, da diese nicht praktikabel w√§ren.</code><code> </code><br><code>3</code><code> </code><code>https://ki-verband.de/wp-content/uploads/2021/03/Final_Regulierung_compressed-1-1.pdf</code><code> </code><br><code>4</code><code> </code><code>https://www.bitkom.org/sites/default/files/2020-04/20200402_kurzfassung-bitkom-stellungnahme-zumabschlussbericht-der-dek.pdf</code><code> </code><br><code>5</code><code> </code><code>Beispielsweise k√∂nnte schon der Betrieb einer Suchmaschine auf einer potentiell nicht repr√§sentativen Datenbasis </code><br><code>durch </code><code>Article 5 (a</code><code>) aufgrund der m√∂glichen Auswirkungen der Suchergebnisse verboten sein.</code><code> </code>",FALSE_NEGATIVE
fitz_2661969_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2661969.pdf,3,3,2661969,attachments/2661969.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>3 (3) </code><br><code> </code><br><code>LO v√§lkomnar att AI-system i utbildning och yrkesutbildning samt </code><br><code>syssels√§ttning och arbetet klassas som h√∂g risk i f√∂rslaget. Dock verkar det </code><br><code>finns en diskrepans mellan de olika avsnitten i f√∂rslagets texter som handlar </code><br><code>om h√∂g risk inom omr√•dena syssels√§ttning och arbete. F√∂r LO √§r det av </code><br><code>st√∂rsta vikt att de omr√•den som n√§mns, inte minst i bilaga III ut√∂kas att </code><br><code>omfatta fler omr√•den som n√§mns i de inledande delarna av f√∂rslaget.  </code><br><code> </code><br><code>I f√∂rslaget till maskinf√∂rordning st√•r att l√§sa att ‚Äùen annan f√∂renklingsaspekt </code><br><code>√§r komplementariteten mellan lagstiftningsf√∂rslagen om AI och maskiner, </code><br><code>d√§r bed√∂mningen av √∂verensst√§mmelse delegeras till maskinf√∂rordningen </code><br><code>genom AI-f√∂rordningen, s√• att riskbed√∂mningen f√∂r hela maskinen med AIsystem endast g√∂rs enligt den framtida f√∂rordningen om maskinprodukter. </code><br><code>LO delar logiken i att utg√• fr√•n maskinf√∂rordningen men ser en stor </code><br><code>komplexitet i hur den f√∂reslagna f√∂rordningen ska matchas med redan </code><br><code>befintlig lagstiftning som indirekt eller direkt ber√∂r delar av det f√∂rslaget </code><br><code>avser att reglera. Det √§r av yttersta vikt att f√∂rslaget harmoniserar med √∂vrig </code><br><code>nationell och europeisk reglering. Det √§r ocks√• av stor vikt att d√§r ansvaret </code><br><code>delegeras till andra r√§ttliga regleringar att detta sker i enlighet med </code><br><code>f√∂rslagets reglering s√• att det inte i praktiken uppst√•r s√§rregleringar. </code><br><code>LO vill h√§r ocks√• passa p√• att lyfta de m√•nga standarder som ligger till </code><br><code>grund f√∂r m√•nga tekniska verktyg och som Kommissionen ocks√• h√§nvisar </code><br><code>till i sitt f√∂rslag. LO anser att det nogsamt b√∂r s√§kerst√§llas att arbetsgivaren </code><br><code>tillg√§ngligg√∂r standarder f√∂r skyddsombud och anst√§llda f√∂r att √∂ka insyn </code><br><code>och undvika risker vid anv√§ndningen.  </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Med v√§nlig h√§lsning </code><br><code>Landsorganisationen i Sverige </code><br><code> </code><br><code> </code><br><code> </code><br><code>Susanna Gideonsson </code><br><code>Linda Larsson </code><br><code>Handl√§ggare </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665634_7,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665634.pdf,13,7,2665634,attachments/2665634.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>AI Index: POL 30/4567/2021 </code><br><code> </code><br><code> 7/13 </code><br><code> </code><br><code>harm to multiple human rights and even democracy and the rule of law. </code><br><code>What is Amnesty International calling for? </code><br><code>The use of AI systems that condition and manipulate people into certain behaviour </code><br><code>without their knowing and having adverse effects such as material and/or immaterial </code><br><code>damage or a violation of fundamental rights, is unacceptable and should be banned. </code><br><code>2.</code><code> </code><code>Loopholes </code><br><code>The proposed AIA does not go far enough in protecting people‚Äôs human rights and </code><br><code>should be more ambitious to effectively protect them. In particular, the AIA contains </code><br><code>several ‚Äòloopholes‚Äô or ‚Äòmissed opportunities‚Äô. </code><br><code>(i)</code><code> </code><br><code>Self-assessment and standardisation </code><br><code>A fundamental concern with the AIA proposal is the over-reliance on the providers </code><br><code>of AI systems to self-assess their compliance with the regulation. Under Chapter 5 </code><br><code>of Title III, the vast majority of AI systems designated to be ‚Äòhigh-risk‚Äô are only </code><br><code>subject to a limited conformity assessment procedure based on internal control, </code><br><code>without any third-party conformity assessment. This amounts to a very weak </code><br><code>safeguard against human rights abuses, especially given the recognised high-risk </code><br><code>nature of such systems. This ranges from AI used for evaluating creditworthiness, </code><br><code>work performance or the eligibility for public benefits, to crime prediction AI and </code><br><code>AI used to examine visa applications. Such uses of AI pose real dangers to human </code><br><code>rights yet are left to self-certification.  </code><br><code>The only high-risk systems that do require a more stringent conformity assessment </code><br><code>including the involvement of an independent third party are AI systems intended to </code><br><code>be used as safety components, and AI systems for remote biometric identification. </code><br><code>In the latter case, given that as set out above such use should properly be subject </code><br><code>to an outright prohibition, the more stringent conformity assessment measures are </code><br><code>insufficient.  </code><br><code>Moreover, the requirements for high-risk AI against which the assessment must be </code><br><code>done, will be standardised through opaque processes of standardisation, which is </code><br><code>an industry dominated process. There is a risk this will obstruct the participation of </code><br><code>representatives of affected groups and other civil society organisations in rulemaking procedures that have highly consequential implications for human rights.</code><code>17</code><code>  </code><br><code> </code><br><code> </code><br><code>17</code><code> Michael Veale and Frederik Zuiderveen Borgesius, </code><code>Demystifying the Draft EU Artificial Intelligence Act, </code><code>July </code><br><code>2021  </code>",POSITIVE
fitz_2665266_29,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665266.pdf,47,29,2665266,attachments/2665266.pdf#page=29,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>‚Äú Often, we can be right next to </code><br><code>each other and they will still be </code><br><code>‚ÄòSnapchatting‚Äô each other.‚Äù </code><code>‚Äî Aged 15</code><br><code>29</code><br><code>4.2 Quality of relationships </code><br><code>The quality of social interactions diminishes </code><br><code>the more devices are used.</code><br><code>116</code><code> Brown, Manago </code><br><code>and Trimble (2016) found that the sheer volume </code><br><code>of people and events, and the emphasis on </code><br><code>popularity numerically quantified, lowered  </code><br><code>the quality of communication.</code><br><code>117</code><br><code>Dr. Caroline Fisher similarly argues that </code><br><code>pathological internet use affects an individual‚Äôs </code><br><code>sense of wellbeing and can lead to social </code><br><code>withdrawal, self-neglect, poor diet and  </code><br><code>family conflict.</code><br><code>118</code><code> </code><br><code>The PISA Wellbeing report found that children </code><br><code>who are </code><code>‚Äòextreme internet users‚Äô</code><code> (defined  </code><br><code>as more than six hours) were 15.1% less likely </code><br><code>than moderate users (1‚Äì2 hours) to report  </code><br><code>a sense of belonging, and more likely to  </code><br><code>report feeling lonely, at school.</code><br><code>119</code><code> Meanwhile  </code><br><code>a Common Sense Media report found that  </code><br><code>70% of American teenagers, aged between  </code><br><code>12 and 18 years, fight with their parents  </code><br><code>about their devices; 32% on a daily basis.</code><br><code>120</code><code>  </code><br><code>The Education Policy Institute evidence  </code><br><code>review found that excessive internet use  </code><br><code>is preventing young people from developing </code><br><code>strong relationships offline.</code><br><code>121</code><br><code>Online relationships can enrich a child‚Äôs social </code><br><code>and emotional life, especially those who may </code><br><code>be isolated in other settings. However, the </code><br><code>persistent demands to interact often diminish </code><br><code>the quality of relationships, levels of emotional </code><br><code>understanding and create conflict. </code><br><code>4.3 Opportunity cost</code><br><code>There is an undeniable truth that if you spend </code><br><code>(or lose) a great deal of time doing one </code><br><code>thing, something else must </code><code>‚Äògive‚Äô</code><code>. This is the </code><br><code>opportunity cost.</code><br><code>Creativity, autonomy, memory</code><code> </code><br><code>The potential to access information, creative </code><br><code>activities, undertake research or build and </code><br><code>maintain important relationships online must </code><br><code>not be ignored. But creative activities of UK </code><br><code>children only occupy around 3% of their total </code><br><code>time online,</code><br><code>122 </code><code>meanwhile UK teenagers</code><br><code>123</code><code> are </code><br><code>spending less time on informational, civic and </code><br><code>creative activities now, compared with a few </code><br><code>years ago.</code><br><code>124</code><br><code>‚ÄúI love reading, but by the time I‚Äôve spent  </code><br><code>an hour too long on my phone, I can no  </code><br><code>longer read my book.‚Äù  </code><br><code>Aged 17</code><br><code>MIT Professor Sherry Turkle notes: </code><br><code>‚ÄúThe capacity for boredom is the single most </code><br><code>important development of childhood. The </code><br><code>capacity to self-soothe, go into your mind, </code><br><code>go into your imagination. Children who are </code><br><code>constantly being stimulated by a phone don‚Äôt </code><br><code>learn how to be alone, and if you don‚Äôt teach </code><br><code>a child how to be alone, they will always be </code><br><code>lonely.‚Äù</code><br><code>125</code><br><code>Development of memory is another opportunity </code><br><code>cost. Dr. Benjamin Storm‚Äôs (2016) research </code><br><code>on internet use and memory found that when </code><br><code>participants were allowed to use Google to </code><br><code>answer questions, they used it even when they </code><br><code>already knew the answer.</code><br><code>126</code><code> He commented: </code><br><code>‚ÄúMemory is changing. Our research shows that </code><br><code>as we use the internet to support and extend </code><br><code>our memory we become more reliant on it. </code><br><code>Whereas before we might have tried to recall </code><br><code>something on our own, now we don‚Äôt bother.‚Äù</code><br><code>127</code><br><code>Chapter Four </code><br><code>Impact of persuasive technologies </code><br><code>on childhood</code>",FALSE_NEGATIVE
fitz_2665625_13,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665625.pdf,28,13,2665625,attachments/2665625.pdf#page=13,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>This conclusion affirms an investigation into exports to one territory, by</code><code> Amnesty International</code><code>,</code><br><code>which identified at least three cases of European companies exporting AI and surveillance</code><br><code>technology to China:</code><br><code>‚óè</code><br><code>France (Idemia/Morpho)</code><code>: in 2015, Morpho, which specialises in security and identity</code><br><code>systems, including facial recognition systems and other biometric identification products,</code><br><code>entered into a contract to supply facial recognition equipment directly to the Shanghai</code><br><code>Public Security Bureau;</code><br><code>‚óè</code><br><code>The Netherlands (Noldus)</code><code>: Noldus sold its ‚ÄúFaceReader‚Äù software, which is used for</code><br><code>automated analysis of facial expressions that convey anger, happiness, sadness,</code><br><code>surprise and disgust, to public security and law enforcement authorities in China.</code><br><code>FaceReader was also used in Chinese universities with links to the police, and the</code><br><code>Ministry of Public Security. Digital surveillance technology has also been sold by the</code><br><code>company to universities in China;</code><br><code>‚óè</code><br><code>Sweden (Axis Communications):</code><code> from 2012 to 2019, the company has been listed as</code><br><code>a ‚Äúrecommended brand‚Äù in Chinese state surveillance tender documents. Axis</code><br><code>Communications is specialised in security surveillance and remote monitoring, and has</code><br><code>supplied technology to the surveillance programme of Guilin, a city in the south of China,</code><br><code>in order to expand it from 8,000 cameras to 30,000.</code><br><code>Amnesty International argues that the companies took insufficient steps to satisfy themselves as</code><br><code>to whether sales to China's authorities were of significant risk. In doing so</code><code>,</code><code> Amnesty</code><br><code>International concludes, those European companies ‚Äú</code><code>totally failed in their human rights</code><br><code>responsibilities</code><code>.‚Äù</code><br><code>We agree with these concerns and encourage the Commission to include the export of AI</code><br><code>Systems in the scope of application of the AI Act. Article 2 should be amended to include</code><br><code>exporters based in the European Union, even if the AI systems they provide are deployed</code><br><code>outside the Union.</code><br><code>Additionally we believe the AI Act would be stronger if it incorporated the due diligence</code><br><code>obligations set forth in the</code><code> Regulation (EU) 2021/821</code><code> (‚ÄúDual Use Regulation‚Äù)</code><code style=""font-weight: 1000; background-color: #FF0000;"">32</code><code>, as Article 2,7.</code><br><code>As you will notice, the language we suggest is based on the wording of Article 5,2 of the Dual</code><br><code>Use Regulation, which establishes due diligence obligations for the export of cyber-surveillance</code><br><code>items, establishing:</code><br><code>‚óè</code><br><code>Prohibition of EU vendors to export AI systems that are prohibited by the AI Act;</code><br><code>‚óè</code><br><code>For all other AI systems, where an exporter is aware, according to its due</code><br><code>diligence findings, that AI systems which the exporter proposes to export are</code><br><code>intended, in their entirety or in part, for use in connection with internal repression</code><br><code>32</code><code> European Parliament.</code><code> Regulation (EU) 2021/821</code><code> of the European Parliament and of the Council of 20</code><br><code>May 2021 setting up a Union regime for the control of exports, brokering, technical assistance, transit and</code><br><code>transfer of dual-use items (recast). (2021)</code><br><code>13</code>",POSITIVE
fitz_2665469_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665469.pdf,3,2,2665469,attachments/2665469.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>V1.0 5. august 2021 KMD  </code><br><code>Side 2 af 3 </code><br><code> </code><br><code> </code><br><code>KMD suggests a more concrete definition of AI in the proposal similar to the UN definition of </code><br><code>AI</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> where AI operates by means of knowledge modelling and representation and by exploiting </code><br><code>data and calculating correlations. </code><br><code> </code><br><code>1.4</code><code> </code><br><code>Standardization </code><br><code>KMD welcomes the focus on standards, but finds it challenging that the EC in hindsight can </code><br><code>decide </code><code>that the relevant harmonised standards are insufficient. As a vendor, it should be </code><br><code>possible to place a product on the market if that product follows the harmonised standards </code><br><code>given at the time of development. </code><br><code> </code><br><code>1.5</code><code> </code><br><code>Quality measures </code><br><code>KMD acknowledges that it is possible to maintain very high data quality, as is also expressed in </code><br><code>the proposal, but it is not possible to guarantee that data is ‚Äúfree of errors and complete‚Äù.</code><code>  </code><br><code> </code><br><code>In general, the proposal assumes that AI systems can deliver predictions that are always </code><br><code>accurate, reliable, and transparent. These concepts seem rather subjective in their nature and </code><br><code>a subject for interpretation, which is why KMD recommends to specify the requirements for </code><br><code>how to measure these concepts. </code><br><code> </code><br><code> </code><br><code>2</code><code> </code><br><code>Clarification of roles and responsibilities </code><br><code> </code><br><code>2.1</code><code> </code><br><code>Unclear definition of roles </code><br><code>In the proposal, concepts like </code><code>user, provider,</code><code> and </code><code>distributor</code><code> are used with different </code><br><code>connotations in different contexts. For instance, the provider role shifts between the customer, </code><br><code>designer, developer, distributor, operator, company, and importer.  </code><br><code> </code><br><code>A consequence of the unclear roles and responsibilities regards infringement to the </code><br><code>requirements, e.g. when determining if the offender is a company or an individual. It is </code><br><code>complicated because the </code><code>provider </code><code>role lacks a unified definition. Moreover, it may be expected </code><br><code>that entities procuring IT-solutions may want to push the full responsibility of the AI solution to </code><br><code>the IT-supplier. A clearer role definition would define which responsibilities they cannot </code><br><code>prevent. </code><br><code> </code><br><code>It must be stressed that the entity putting the service into the market is not necessarily the </code><br><code>same legal entity as the one placing it on the market, and the proposal would benefit from </code><br><code>clarifying the roles in greater detail (Recital 53; Title I Article 3(2); Title III Article 18; Title III </code><br><code>Article 27). </code><br><code> </code><br><code>2.2</code><code> </code><br><code>Human oversight </code><br><code>Human oversight (Title III Article 14) is a means to increase trust in AI solutions and help </code><br><code>place responsibility to a legal person if the AI system fails. In the proposal, human oversight </code><br><code>‚Äú[‚Ä¶]shall aim at preventing or minimizing the risks to health, safety or fundamental rights that </code><br><code>may emerge[‚Ä¶]‚Äù. However, the requirements described are difficult to meet in a meaningful </code><br><code>way.  </code><br><code> </code><br><code>The individuals assigned to oversee the solutions are not required to have an expertise in the </code><br><code>field, but would that really be the right type of human oversight? It is not clear how the </code><br><code>provider may ensure that the user fully understands the AI solution and outcomes, nor who </code><br><code>defines if the system is ‚Äúeffectively overseen‚Äù. </code><br><code> </code><br><code>KMD suggests as an alternative to the human oversight described in the proposal a </code><br><code>combination of explainable AI and assignment of responsibility to a human legal entity.  </code><br><code> </code><br><code>1</code><code> Draft text of the Recommendation on the Ethics of Artificial Intelligence - UNESCO Digital Library </code>",POSITIVE
fitz_2665596_10,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665596.pdf,10,10,2665596,attachments/2665596.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>falsche Entscheidungen trifft oder falsche oder verzerrte Ergebnisse </code><br><code>hervorbringt.‚Äú  </code><br><code>Anforderungen bzgl. Bias-Effekten sind zu finden in Art. 10(2), dass in </code><br><code>Bezug auf die verwendeten Trainings-, Test- und Validierungsdaten </code><code>‚Äûeine </code><br><code>Beobachtung, Erkennung und Korrektur im Hinblick auf m√∂gliche </code><br><code>Verzerrungen (Bias)‚Äú </code><code>und gem√§√ü Art. 10(5) eine </code><code>‚ÄûErkennung und </code><br><code>Korrektur von Verzerrungen‚Äú</code><code> (</code><code>‚Äûbias monitoring, detection and </code><br><code>correction‚Äú</code><code>) bzgl. der Daten durchzuf√ºhren ist. </code><br><code>In Bezug auf kontinuierlich lernende Systeme soll zudem √ºberpr√ºft </code><br><code>werden, ob eine </code><code>‚Äûm√∂gliche Neigung zu einem automatischen oder </code><br><code>√ºberm√§√üigen Vertrauen in das von einem Hochrisiko-KI-System </code><br><code>hervorgebrachte Ergebnis (‚ÄûAutomatisierungsbias‚Äú)‚Äú</code><code> vorliegt (siehe Art. </code><br><code>14(4b)), und dass f√ºr diese Systeme </code><code>‚Äûauf m√∂glicherweise verzerrte </code><br><code>Ergebnisse, die durch eine Verwendung vorheriger Ergebnisse als </code><br><code>Eingabedaten f√ºr den k√ºnftigen Betrieb entstehen </code><br><code>(‚ÄûR√ºckkopplungsschleifen‚Äú), angemessen mit geeigneten </code><br><code>Risikominderungsma√ünahmen eingegangen wird‚Äú</code><code>.</code><code>  </code><br><code>lernenden Systemen angegeben. Eine Adressierung von Bias-Effekten in </code><br><code>den Resultaten eines KI-Systems fehlen.   </code><br><code>Der Artikel 64 der KI-Verordnung verlangt von den Herstellern den </code><br><code>Beh√∂rden einen vollst√§ndigen Remote-Zugriff zu den Trainings-, </code><br><code>Validierungs- und Testdaten zu verschaffen, sogar durch eine API.</code><code> </code><br><code>Vertrauliche Patientendaten √ºber einen Remote-Zugriff zugreifbar zu </code><br><code>machen, steht in einem gewissen Ma√üe im Konflikt mit der gesetzlichen </code><br><code>Forderung nach Data Protection by Design. Gesundheitsdaten z√§hlen zur </code><br><code>besonders sch√ºtzenswerten Kategorie personenbezogener Daten.  </code><br><code>Eine externe API zu den Trainingsdaten zu entwickeln und mit </code><br><code>entsprechenden Sicherheitsmechanismen bereitzustellen, bedeutet f√ºr </code><br><code>die Hersteller einen erheblichen Mehraufwand und erscheint </code><br><code>unverh√§ltnism√§√üig. Zudem entsteht durch derartige Backdoors immer </code><br><code>eine gewisse Sicherheitsgef√§hrdung bzgl. des Zugangs zu pers√∂nlichen </code><br><code>und vertrauensw√ºrdigen Daten. Bei anderen, oft sogar kritischeren Daten </code><br><code>und Informationen zum Design und zur Produktion von Produkten (z.B. </code><br><code>Source-Code oder CAD-Zeichnungen) w√ºrde niemand verlangen, dass die </code><br><code>Hersteller den Beh√∂rden einen Remote-Zugriff gew√§hren m√ºssen.  </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663276_76,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2663276.pdf,82,76,2663276,attachments/2663276.pdf#page=76,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>76 </code><br><code> </code><br><code>Bibliography </code><br><code>Louis Abraham. </code><code>In Algorithms We Trust</code><code>. ACPR (21 mars 2019). </code><br><code>Peter Addo, Dominique Gu√©gan, Bertrand Hassani. </code><code>Credit Risk Analysis using Machine and Deep </code><br><code>Learning models</code><code>. ffhalshs-01719983f (2018). </code><br><code>Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, Been Kim: </code><code>Sanity </code><br><code>Checks for Saliency Maps</code><code>. NeurIPS 2018: 9525-9536 (2018). </code><br><code>AEAPP. </code><code>Final Report on public consultation No. 19/270 on Guidelines on outsourcing to cloud </code><br><code>service providers</code><code>. EIOPA-BoS-20-002 (2020). </code><br><code>Daniel Felix Ahelegbey, Paolo Giudici, Branka Hadji-Misheva. </code><code>Latent Factor Models for Credit </code><br><code>Scoring in P2P Systems</code><code>. Physica A: Statistical Mechanics and its Applications No. 522 (10 February </code><br><code>2019): pp. 112-121 (2018). </code><br><code>Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing. </code><code>Contextual Explanation Networks.</code><code> </code><br><code>arXiv:1705.10301v3 [cs.LG] (2018).  </code><br><code>David Alvarez-Melis, Tommi S. Jaakkola. </code><code>Towards Robust Interpretability with Self-Explaining </code><br><code>Neural Networks.</code><code> arXiv:1806.07538v2 [cs.LG] (2018).  </code><br><code>0 David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, KlausRobert Muller. </code><code>How to Explain Individual Classification Decisions.</code><code> J. Mach. Learn. Res. 11: 18031831 (2009). </code><br><code>Marco Barreno, Blaine Nelson, Anthony D. Joseph, J.D. Tygar. </code><code>The security of machine learning.</code><code> </code><br><code>Mach Learn (2010) 81: 121‚Äì148 DOI 10.1007/s10994-010-5188-5 (2010). </code><br><code>Robert P. Bartlett, Adair Morse, Richard Stanton, Nancy Wallace. </code><code>Consumer-lending discrimination </code><br><code>in the FinTech era</code><code> (No. w25943). National Bureau of Economic Research (2019). </code><br><code>David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba. </code><code>Network Dissection: </code><br><code>Quantifying Interpretability of Deep Visual Representations</code><code>. CVPR 2017: 3319-3327 (2017). </code><br><code>Roland Berger. </code><code>The road to AI Investment dynamics in the European ecosystem</code><code>. AI Global Index </code><br><code>(2019). </code><br><code>Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh </code><br><code>Ruchir Puri, Jos√© M. F. Moura, Peter Eckersley. </code><code>Explainable Machine Learning in Deployment.</code><code> </code><br><code>arXiv:1909.06342 [cs.LG] (2019) </code><br><code>Or Biran, Courtenay V. Cotton. </code><code>Explanation and Justification in Machine Learning: A Survey</code><code> (2017). </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665296_9,other,../24212003_requirements_for_artificial_intelligence/attachments/2665296.pdf,9,9,2665296,attachments/2665296.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Data, AI and Robotics (DAIRO) aisbl </code><br><code>Avenue des Arts, 56 </code><code>‚ö´</code><code> 1000 Brussels </code><code>‚ö´</code><code> Belgium </code><br><code>E-Mail: </code><code>info@core.bdva.eu </code><code>‚ö´</code><code> Web: </code><code>www.bdva.eu</code><code>          </code><br><code>About this Document </code><br><code> </code><br><code>Main editors of this documentare (in alphabetical order): </code><br><code>‚Ä¢</code><code> </code><br><code>Natalie Bertels, Valorisation Manager & Researcher, imec/KULeuven, representing BDVA </code><br><code>TF5 ‚Äì Policy and Societal </code><br><code>‚Ä¢</code><code> </code><br><code>Freek Bomhof, Senior Data Science consultant, TNO, representing BDVA TF5 ‚Äì Policy and </code><br><code>Societal  </code><br><code>‚Ä¢</code><code> </code><br><code>Roberto Di Bernardo, Head of Open Government R&D Group, Engineering Ingegneria </code><br><code>Informatica S.p.A. representing BDVA TF7.SG8 Smart Governance and Smart Cities </code><br><code>‚Ä¢</code><code> </code><br><code>Ana Garc√≠a Robles, Secretary General BDVA/DAIRO </code><br><code>‚Ä¢</code><code> </code><br><code>Norbert Jastroch, Head of Research MET Communications, BDVA/DAIRO member </code><br><code>‚Ä¢</code><code> </code><br><code>Tjerk Timan, Researcher TNO, BDVA/DAIRO member </code><br><code>‚Ä¢</code><code> </code><br><code>Mattia Trino, Operations Manager BDVA/DAIRO </code><br><code>‚Ä¢</code><code> </code><br><code>Ray Walshe, Lecturer Dublin City University, representing BDVA/DAIRO TF6.SG6 </code><br><code>Standardisation  </code><br><code>‚Ä¢</code><code> </code><br><code>Katerina Yordanova, Researcher imec/KULeuven representing BDVA TF5 ‚Äì Policy and </code><br><code>Societal </code><br><code>‚Ä¢</code><code> </code><br><code>Sonja Zillner, Lead of Core Company Technology Module ‚ÄúTrustworthy AI‚Äù at Siemens AG, </code><br><code>SRIA Lead at BDVA/DAIRO </code><br><code>This paper is the result of a cooperative work that gather inputs from the almost 150 BDVA/DAIRO </code><br><code>members that participated in the following: </code><br><code>‚Ä¢</code><code> </code><br><code>Workshop with BDVA/DAIRO members ‚Äì Feedback to AI Regulation proposal (07th June </code><br><code>2021) </code><br><code>‚Ä¢</code><code> </code><br><code>Workshop on BDVA/DAIRO Position Paper on Industrial and Trustworthy AI (18th June 2021) </code><br><code>‚Ä¢</code><code> </code><br><code>Consolidation Workshop on BDVA/DAIRO Feedback to AI Regulation proposal (30th June </code><br><code>2021) </code><br><code> </code><br><code>The final document has been drafted during the month of July 2021, thanks to a join effort of the main </code><br><code>editors in coordination with the BDVA/DAIRO Secretariat.  </code><br><code> </code><br><code> </code><br><code>About BDVA/DAIRO </code><br><code>The Big Data Value Association ‚Äì BDVA, (from 2021, DAIRO - Data, AI and Robotics aisbl), is an </code><br><code>industry-driven international not‚Äìfor-profit organisation with more than 230 members all over Europe </code><br><code>and a well-balanced composition of large, small, and medium-sized industries as well as research and </code><br><code>user organizations. BDVA/DAIRO focuses on enabling the digital transformation of the economy and </code><br><code>society through Data and Artificial Intelligence by advancing in areas such as big data and AI </code><br><code>technologies and services, data platforms and data spaces, Industrial AI, data-driven value creation, </code><br><code>standardisation, and skills. BDVA/DAIRO has been the private side of the H2020 partnership Big Data </code><br><code>Value PPP, it is a private member of the EuroHPC JU and is also one of the founding members of the </code><br><code>AI, Data and Robotics Partnership. BDVA/DAIRO is an open and inclusive community and is always </code><br><code>eager to accept new members who share these ambitious objectives </code><br><code>Contact for further information: </code><code>info@core.bdva.eu</code><code>  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2660610_5,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2660610.pdf,8,5,2660610,attachments/2660610.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>‚Ä¢</code><code> (B). This is very well written and needs to no changes. It plays well into</code><br><code>existing anti discrimination legislation on a EU level.</code><br><code>‚Ä¢</code><code> (C). Banning the concept of ‚Äùtrustworthiness‚Äù or social scoring is necessary. This is not the case further into the paragraph of the article, which</code><br><code>is why it must be changed.</code><br><code>The distinction between being mistreated</code><br><code>because of data taken out of a different situation, versus when it is disproportional or unjust, is interesting and not followed up on elsewhere.</code><br><code>‚Ä¢</code><code> (D). This is not a prohibition.</code><br><code>This allows all EU member states to</code><br><code>use real-time remote identification biometric systems</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> </code><code>within many exceptions, and can theoretically lead to certain member states making use</code><br><code>of them as the rule instead of the exception.</code><br><code>The following changes may be worth considering.</code><br><code>It is suggested that (c) is changed to:</code><code> the placing on the market, putting</code><br><code>into service or use of AI systems by public authorities or on their behalf for the</code><br><code>evaluation or classification of the trustworthiness of natural persons regardless</code><br><code>of the method and whether or not the end result is social scoring;</code><code> The current</code><br><code>version has means of circumvention based on how the wording of (i) or (ii) is</code><br><code>read. They are therefore not worth keeping, as it prevents an actual prohibition.</code><br><code>Details about how these scores etc. are generated were removed, as these are</code><br><code>bound to change to follow any current trends, this makes the prohibition more</code><br><code>technology neutral and lets it effectively ban all means to do so</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code>. This follows</code><br><code>the oral presentation of the Commission as well (banning all social scoring). The</code><br><code>problem with that statement is yet again, that it does not ban private (including</code><br><code>in education) social scoring, but this can be changed later.</code><br><code>It is suggested that (d) is changed to:</code><code> the use of ‚Äòreal-time‚Äô remote biometric</code><br><code>identification systems in publicly accessible spaces</code><code> The 3 exceptions are far too</code><br><code>wide to allow to stay. And merely focusing on law enforcement seems to miss</code><br><code>the purpose of the Act, as private organisations can cause the same damage or</code><br><code>may be employed by the State to do the same work. Otherwise, allowing them</code><br><code>in general (outside of law enforcement) is not suitable. This is both down to the</code><br><code>chance of misuse, the severity of the personal data and profiling that is created,</code><br><code>and the proportionality - it is known from surveillance research in general, that</code><br><code>there will never be a 100 percent success on anything, and because this kind of</code><br><code>technology is employed en masse, the 10 percent or more that go through will</code><br><code>lead to hundreds of thousands of false positives/negatives and errors. Regardless</code><br><code>of this, it is acknowledged that the member states have interest (and private</code><br><code>entities) in using it, which is why narrower exceptions are suggested.</code><br><code>Furthermore,</code><code> in order to materially distort a person‚Äôs behaviour in a manner that causes or is likely to cause that person or another person physical or</code><br><code>psychological harm;</code><code> should be removed from Art. 5(1)(a) and (b), because they</code><br><code>6</code><code>These may not even fulfill the criteria to be AI, why include them here then?</code><br><code>7</code><code>Not for private entities though.</code>",POSITIVE
fitz_2665314_5,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2665314.pdf,6,5,2665314,attachments/2665314.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Side 5 </code><br><code> </code><br><code>It is a strength of Norwegian working life that employees have a great deal of influence </code><br><code>over their own professional practice, working day and conditions. This applies to both the </code><br><code>individual employee and not least the employees' union representatives and </code><br><code>organisations. </code><br><code> </code><br><code>Norwegian regulation in the area of workers' rights ensures workers' and their </code><br><code>organisations' influence when it comes to the introduction of new technologies in the </code><br><code>workplace.  </code><br><code> </code><br><code>Article 29 in the AI Act states that users' obligations to use high risk AI systems in </code><br><code>accordance with the instructions of use are 'without prejudice to other user obligations </code><br><code>under Union law or national law'. This means that existing EU and national regulations for </code><br><code>both consumers and workers apply to high risk AI systems. We would like to underline </code><br><code>the importance of the new AI regulation not overriding existing and well-functioning </code><br><code>regulation in these areas. We recommend clarifying in Article 1 of the AI Act that Union </code><br><code>legislation or national law shall apply in all areas that are not explicitly covered by the </code><br><code>regulation, such as consumer rights and workers' rights. </code><br><code>7 CONSEQUENCES FOR SMEs </code><br><code>Complex regulations may pose an undue burden to SMEs (Small and Medium </code><br><code>Enterprises). Even when their applications fall outside the scope of the regulation, fear of </code><br><code>non-compliance and the possibility of large fines, means that they often will 'err on the </code><br><code>side of caution' and drop potential projects. This may seriously stifle innovation in the field </code><br><code>of AI and favour large international companies that have the legal and financial muscles </code><br><code>to ensure compliance and provide all necessary documentation. </code><br><code> </code><br><code>SMEs would greatly benefit from more clarity, simpler language and clearer links to other </code><br><code>regulations such as the GDPR. It should be easy for SMEs to navigate the regulation and </code><br><code>understand what parts apply to them and their projects. It is also important that support </code><br><code>mechanisms such as standards, guidelines and checklists, regulatory sandboxes and </code><br><code>European Digital Innovation Hubs (EDIHs) are in place </code><code>before</code><code> the regulation comes into </code><br><code>effect.  </code><br><code>8 REGULATORY SANDBOXES </code><br><code>It is encouraging to see regulatory sandboxes as part of the EU proposal. The Norwegian </code><br><code>government established a regulatory sandbox for AI and personal data protection at the </code><br><code>Data Protection Authority in 2020. The goal is to promote the development of innovative </code><br><code>artificial intelligence solutions that are both ethical and responsible. We support </code><br><code>regulatory sandboxes as an instrument to stimulate innovation in areas that are complex </code><br><code>and challenging. We believe that innovation and data protection can be promoted through </code><br><code>a sandbox, especially for SMEs with limited resources. </code><br><code> </code><br><code>From the wording of Article 53, it is not quite clear what a regulatory sandbox will </code><br><code>encompass. The question arises whether the proposed regulatory sandbox includes an IT </code><br><code>infrastructure in each member state, with some additional legal grounds for further </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663380_3,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2663380.pdf,12,3,2663380,attachments/2663380.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>3 </code><br><code> </code><br><code>applications ‚Äì largely the same obligations for each type of data processing.  The </code><br><code>possibility in the GDPR to define different risk classes of data processing, which require </code><br><code>different legal bases, is not being used. </code><br><code>The regulation of data in AI model development and deployment in a balanced fashion requires </code><br><code>the process to be free of legacy prejudices, and Voss recognizes a prejudice against any </code><br><code>processing in the General Data Protection Regulation (‚ÄúGDPR‚Äù).   </code><br><code>It (GDPR) also wants to establish the view that processing of personal data is generally </code><br><code>regarded as a socially undesirable behaviour.  This approach is not only latently hostile </code><br><code>to progress.  The result is that even the processing of personal data that is protected by </code><br><code>fundamental rights or that is socially desirable for the protection of public interest comes </code><br><code>under constant pressure to justify itself.   </code><br><code>He reminds the reader that the GDPR requires balancing against the full range of fundamental </code><br><code>rights and does not focus just on privacy.  Specifically, the use of data to arrive at accurate </code><br><code>insights is not per se an infringement on dignity.  Infringements may come from actions taken </code><br><code>based on the processing, but the inclusion of data should make no judgment on the persons to </code><br><code>whom the data may pertain.  It is more likely that infringements on dignity could come from </code><br><code>inaccuracies resulting from insufficient training data.  The IAF agrees with Voss that the GDPR is </code><br><code>an important law and that privacy must be protected.  However, the GDPR should make </code><br><code>possible technology applications such as AI, and in the IAF‚Äôs view, there are unresolved tensions </code><br><code>between the AI Regulation and the GDPR. The IAF in these comments will suggest ways in </code><br><code>which the GDPR in general might be improved so that data serves people and so that the risk of </code><br><code>harm is more appropriately addressed. </code><br><code>Overview of the IAF‚Äôs Comments</code><code> </code><br><code> In the IAF‚Äôs view, the AI Regulation could be improved by addressing two key areas. </code><br><code>‚Ä¢</code><code> </code><code>The AI Regulation glosses over the tension with the GDPR.</code><code> When AI involves personal data, </code><br><code>the GDPR challenges the use of this data in AI.  Tensions and conflict with the GDPR need to </code><br><code>be resolved. The GDPR strictly interpreted makes AI in any of the high-risk cases challenging </code><br><code>to use. This result is contrary to the stated goals of the AI Regulation. </code><br><code>‚Ä¢</code><code> </code><code>The construct of a risk-based approach in the AI Regulation is unevenly developed and </code><br><code>applied.</code><code> First, by limiting the scope of the AI Regulation to select high risk areas, the AI </code><br><code>Regulation infers that many of the good risk-based data governance requirements included </code><br><code>in the AI Regulation will be ignored in other AI scenarios that could create similar risk to </code><br><code>individuals and society. Second, the detailed approach to the use of ‚Äúconformity‚Äù </code><br><code>assessments is limiting and does not reflect the growing body of literature and indeed </code><br><code>public policy that promotes a broader approach to AI risk management through AI Impact </code><br><code>Assessments (AIAs). </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662603_2,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662603.pdf,9,2,2662603,attachments/2662603.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>2</code><br><code>Patrick Glauner</code><br><code>includes most healthcare applications of AI. The proposal‚Äôs cover page is depicted</code><br><code>in Fig. 1.</code><br><code>Fig. 1</code><code> EU Proposal for a Regulation Laying Down Harmonised Rules on Artificial Intelligence</code><br><code>(Artificial Intelligence Act). Source: (European Commission, 2021).</code><br><code>Prior to that, the European Commission published an AI white paper in February</code><br><code>2020 (European Commission, 2020). The white paper intends to serve as an initial</code><br><code>step towards an AI strategy for the EU. However, the white paper lacks crucial points:</code><br><code>For example, it does not contrast its proposed actions to those of the contemporary AI</code><br><code>leaders. The term ‚ÄúChina"" does not appear in the white paper at all. Furthermore, the</code><br><code>proposed investments appear to be negligible compared to the Chinese investment.</code>",NO_FOOTNOTES_ON_PAGE
fitz_2662780_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2662780.pdf,4,3,2662780,attachments/2662780.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>3 </code><br><code>preserving the professional and pedagogical autonomy and academic freedom of teachers </code><br><code>and academics. </code><br><code> </code><br><code>Transparency and AI literacy and CPD of teachers on AI: </code><br><code>ETUCE welcomes that the proposal of AI Regulation requires that </code><code>users of AI tools</code><code> (who </code><br><code>include students, teachers, academic and education staff for the education sector) must be </code><br><code>adequately informed</code><code> about the intended purpose, level of accuracy, residual risks of AI </code><br><code>tools. Nevertheless, ETUCE highlights that providing </code><code>information is not sufficient</code><code> to ensure </code><br><code>the transparency of the AI tools when users miss the adequate digital skills and data and AI</code><code> </code><br><code>literacy</code><code> to interpret it. Therefore, it is of utmost importance to improve the importance of </code><br><code>digital skills, AI literacy and data literacy</code><code> in educational curricula and raise awareness on</code><code> </code><br><code>the</code><code> risks</code><code> related to the use of AI tools in education. It is also essential to ensure that </code><br><code>infrastructures</code><code> of education institutions are adequately equipped for digital education as </code><br><code>well as to provide </code><code>equal access to digital technologies and ICT tools</code><code> to all teachers and </code><br><code>students, with particular attention to the most disadvantaged groups. To these purposes, </code><br><code>sustainable public investment should be provided by national governments and the </code><br><code>European Commission should provide financial support through European funding such as </code><br><code>Horizon Europe, Digital Europe and in the framework of National Recovery and Resilience </code><br><code>Facility. </code><br><code>While the AI Regulation blandly mention to the possibility of providing users with </code><code>training </code><br><code>on Artificial Intelligence</code><code>, ETUCE emphasises that it is crucial that sustainable public funding </code><br><code>are provided at national and European level to ensure that teachers, trainers, academics </code><br><code>and other education personnel receive </code><code>up-to-date and free of charge continuous training </code><br><code>and professional development on the use of AI tools</code><code> in accordance with their professional </code><br><code>needs.  </code><br><code> </code><br><code>EdTech expansion and issues of intellectual property rights, data privacy of teachers</code><code>:  </code><code> </code><br><code>ETUCE points out that the development of the use of Artificial Intelligence in education has </code><br><code>been accompanied by </code><code>the expansion of Ed-tech companies</code><code> that are progressively </code><br><code>increasing their influence in the education sector, especially under the pressure of </code><br><code>emergency online teaching and learning during the COVID-19 pandemic. ETUCE reminds </code><br><code>that education is a human right and public good whose value needs to be protected. ETUCE </code><br><code>calls for further public responsibility from national governments that should not limit their </code><br><code>scope to regulating the EdTech sector and should develop and implement public platforms </code><br><code>for online teaching and learning to protect the public value of education. In addition, public </code><br><code>platforms should be implemented in full respect of professional autonomy of teachers and </code><br><code>education personnel as well as academic freedom and autonomy of education institutions, </code><br><code>without creating pressure on teachers and education personnel regarding the education </code><br><code>material and pedagogical methods they use. It is also essential to </code><code>protect the accountability </code><br><code>and transparency</code><code> in the governance of public education systems from the influence of </code><br><code>private and commercial interests and actors.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2636017_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2636017.pdf,6,2,2636017,attachments/2636017.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>2 </code><br><code> </code><br><code>b)</code><code> </code><code>AI systems used in employment (recitals 27, 36, articles 6-8, Annex III) and </code><br><code>fundamental and workers‚Äô rights (recital 28</code><code>)</code><code> </code><br><code>-</code><code> </code><br><code>The Commission focuses in its approach to the use of AI above all on the innovative </code><br><code>aspects of new technologies. Though we welcome the beneficial potential of AI systems, </code><br><code>we think that the general debate on AI and labour neglects the fact that AI also creates </code><br><code>low quality employment. AI systems fuel the growth of ‚Äúghost workers‚Äù, workers that </code><br><code>carry out repetitive tasks such as labelling, editing and moderating data needed in AI </code><br><code>systems. Ghost work is often characterized by low pay, stress and is uncredited. When </code><br><code>regulating AI in the area of employment, it is important to pay sufficient attention to the </code><br><code>negative impact that the use of AI driven technology has on working conditions.</code><code> </code><br><code> </code><br><code>-</code><code> </code><br><code>Though the draft regulation mentions high-risk AI-systems in the field of employment, </code><br><code>workers management and access to self-employment (Article 6.2 and Annex III.4), the list </code><br><code>of applications covered is limited. The regulation only mentions systems used for </code><br><code>algorithmic management especially in HR (recruitment, selection of candidates, </code><br><code>advertising of vacancies, screening applications, interviews) and decision- making </code><br><code>(promotion and termination of employees, performance /behaviour evaluation and </code><br><code>monitoring) and for task allocation. </code><br><code>-</code><code> </code><br><code>However, other AI applications with possible consequences for employees do not fall </code><br><code>under this scope. We believe that the list in Annex III is not complete as future AI systems </code><br><code>that allow for an extended algorithmic management might not be covered by the list in </code><br><code>Annex III.  </code><br><code>-</code><code> </code><br><code>Given the huge impact of high-risk applications (especially for surveillance) in the </code><br><code>employment area regarding health & safety and fundamental rights, we think that such a </code><br><code>limitation of the scope of high-risk applications is not sufficient. We are especially </code><br><code>concerned about high-risk applications to   </code><br><code>o</code><code> </code><code>Infringe privacy/data protection rights as employer can access workers‚Äô data,  </code><br><code>o</code><code> </code><code>Allow surveillance to take place outside of company premises and outside of </code><br><code>working hours as it invades workers‚Äô homes, </code><br><code>o</code><code> </code><code>Cause bias, incorrect and discriminating AI decisions due to limited data. </code><br><code> </code><br><code>-</code><code> </code><br><code>New systems currently tested and researched often combine image-based and voice </code><br><code>analysis regarding emotion recognition software, personality analysis software and lie </code><br><code>detection software. All of these are currently highly unreliable, especially in an ethnically </code><br><code>and/or culturally diverse environment. This software is not prohibited so it might end up </code><br><code>in the high-risk category, but it is unclear under which category exactly. </code><code>(E.g., a Belgian </code><br><code>Bank considers the further training and development of an algorithm to ‚Äòdigitally assist‚Äô </code><br><code>the bank employees to work more efficiently.) </code><code> </code><br><code>-</code><code> </code><br><code>Though it will be possible to add certain AI systems to the scope of Annex III (art. 7), this </code><br><code>can only be updated within the areas already covered. The Commission lays down criteria </code><br><code>for this assessment explicitly mentioning harm to health, impact on fundamental rights </code><br><code>and a position of imbalance of power and social economic circumstances. This is certainly </code><br><code>the case for any AI system impacting on workers given the structural power imbalances </code><br><code>between employer and employees, and also the risk to health and safety involved. Adding </code><br><code>a high-risk AI system ex post after because it has already caused harm, is too late. Instead, </code><br><code>the precautionary principle should be applied and any AI system that is intended for </code><br><code>implementation at work should be classified as high-risk. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662109_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662109.pdf,3,1,2662109,attachments/2662109.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>HOPE POSITION</code><code> </code><br><code>                                                                                                      </code><code>May 2021</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>HOPE ‚Äì European Hospital and Healthcare Federation </code><br><code>www.hope.be</code><code>  </code><br><code> 1 | </code><code>P a g e</code><code>  </code><br><code>HOPE Position Paper </code><br><code>on Artificial Intelligence </code><br><code> </code><br><code> </code><br><code>AI technology starts to be used across the healthcare sector but still not as routine. This can be to </code><br><code>assist research by better matching patients to clinical trials, to support the planning of care for patients </code><br><code>with complex needs, etc. Prospective uses of artificial intelligence can be found also in robot care </code><br><code>support and speech recognition for medical documentation. </code><br><code> </code><br><code>Artificial Intelligence (AI) is a complex phenomenon that is interfering with the way medical research </code><br><code>is conducted, the biomedical data is used, and the healthcare professions and healthcare </code><br><code>organisations are regulated.  </code><br><code> </code><br><code>Health data is recognised as a special category of data under the General Data Protection Regulation </code><br><code>due to its sensitivity. </code><br><code> </code><br><code>AI uses in the healthcare field would then also requires a specific regulatory approach, in addition to </code><br><code>a strong horizontal cross-sector regulation of AI. </code><br><code> </code><br><code>HOPE key recommendations to ensure that the application of AI in healthcare benefit patients and </code><br><code>consumers are as follows:   </code><br><code> </code><br><code> </code><br><code>We need to agree at European level on a definition of Artificial intelligence for health care </code><code>as a basis </code><br><code>for further discussion from an ethical or legal perspective or to be used to determine requirements </code><br><code>for the quality of AI.</code><code> </code><br><code> </code><br><code> </code><br><code>We need to build action on AI on clear citizens‚Äô rights (and not only when they are patients): </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>right to transparency, explanation and objection;  </code><br><code>‚ñ™</code><code> </code><br><code>right to accountability and control;  </code><br><code>‚ñ™</code><code> </code><br><code>right to fairness;  </code><br><code>‚ñ™</code><code> </code><br><code>right to non-discrimination;  </code><br><code>‚ñ™</code><code> </code><br><code>right to safety and security;  </code><br><code>‚ñ™</code><code> </code><br><code>right to access to justice;  </code><br><code>‚ñ™</code><code> </code><br><code>right to reliability and robustness.  </code><br><code> </code><br><code>The well-being and autonomy of the AI user should have priority. AI should support the user but not </code><br><code>restrict the user‚Äôs autonomy. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665433_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665433.pdf,5,1,2665433,attachments/2665433.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>1 </code><br><code> </code><br><code> </code><br><code>05/08/2021 </code><br><code>EU ARTIFICIAL INTELLIGENCE ACT ‚Äì SPLUNK POSITION </code><br><code> </code><br><code>The development of artificial intelligence has major potential to improve business processes and </code><br><code>services for citizens, but there are a number of complex issues associated with its </code><br><code>adoption. Splunk supports a flexible policy framework that builds confidence and trust in AI </code><br><code>systems, encourages investment in research and development, strengthens cybersecurity and </code><br><code>privacy protections, and takes into account different types of AI and machine learning to avoid </code><br><code>hampering innovation unnecessarily.   </code><br><code> </code><br><code>With its strong risk-based approach, </code><code>the EU‚Äôs proposal for an AI Act</code><code> is largely in line with these </code><br><code>principles. We welcome the focus on regulating high-risk AI systems and the proposal‚Äôs approach </code><br><code>to conformity assessments (done through internal checks for all high-risk AI except remote </code><br><code>biometric identification). We do however have some concerns about the sharing of </code><br><code>responsibilities between AI providers and AI users and about the exact nature of requirements </code><br><code>introduced for high-risk AI systems.   </code><br><code> </code><br><code> </code><br><code>1.</code><code> </code><code>A risk-based approach to regulating Artificial Intelligence is the right way forward </code><br><code>The EU proposal aims to regulate </code><code>uses of AI</code><code>, rather than AI itself as a technology. On this basis, the </code><br><code>proposal rightly aims for a risk-based approach to regulation. Some limited AI uses will be prohibited </code><br><code>due to the unacceptable risk they present; some high-risk uses will be subject to compliance </code><br><code>requirements before their placing on the market; other low-risk AI systems will only face some </code><br><code>transparency obligations. This tiered structure of the Regulation is welcome and should </code><br><code>appropriately address the most pressing concerns around AI, for example the safety of human </code><br><code>beings or the protection of their fundamental rights. We think that such a risk-based approach will </code><br><code>be able to generate trust in the technology amongst citizens and stimulate AI innovation and the </code><br><code>development of new AI use cases.  </code><br><code>Definition of AI </code><br><code>Whilst the overall approach is correct, we do however have some questions about the definition of </code><br><code>Artificial Intelligence in the proposal. The Regulation‚Äôs definition of AI is very broad, and so is the list </code><br><code>of techniques listed under Annex I.  </code><br><code>We understand the EU‚Äôs desire to provide a future-proof definition, but we are concerned that this </code><br><code>broad definition could cover techniques that are not </code><code>always</code><code> AI. For example, under point (c), we </code><br><code>would argue that there are lots of ‚Äústatistical approaches‚Äù and ‚Äúsearch and optimization methods‚Äù </code><br><code>that are not AI.  </code><br><code>To avoid overregulating non-AI techniques, Annex I could be tightened to focus only on Machine </code><br><code>Learning approaches covered under (a). Other public authorities in the world have opted for such a </code><br><code>targeted definition, such as the ICO in the UK</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>, focusing on supervised, unsupervised, and </code><br><code>reinforcement learning.  </code><br><code> </code><br><code>1</code><code> </code><code>AI definitions, Information Commissioner‚Äôs Office (ICO)</code><code> </code>",POSITIVE
fitz_2662226_8,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2662226.pdf,8,8,2662226,attachments/2662226.pdf#page=8,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>ANEC is supported financially by the European Union & EFTA </code><br><code>This document may be quoted and reproduced, provided the source is given. </code><br><code>This document is available in English upon request from the ANEC Secretariat  </code><br><code>or from the ANEC website at www.anec.eu ¬© Copyright ANEC 2021 </code><br><code>Designed by AdGrafics.eu </code><br><code>ANEC is the European consumer voice in </code><br><code>standardisation, defending consumer interests </code><br><code>in the processes of technical standardisation </code><br><code>and the use of standards, as well as related </code><br><code>legislation and public policies. </code><br><code>ANEC was established in 1995 as an </code><br><code>international non-profit association under </code><br><code>Belgian law and is open to the representation </code><br><code>of national consumer organisations in 34 </code><br><code>countries. </code><br><code>ANEC is funded by the European Union and </code><br><code>EFTA, with national consumer organisations </code><br><code>contributing in kind. Its Secretariat is based in </code><br><code>Brussels. </code><br><code>European association for the coordination of </code><br><code>consumer representation in standardisation aisbl </code><br><code>Rue d‚ÄôArlon 80 </code><br><code>B-1040 Brussels, Belgium </code><br><code>+32 2 743 24 70 </code><br><code> </code><br><code>anec@anec.eu </code><br><code>www.anec.eu </code><br><code> </code><br><code>EC Register of Interest Representatives: </code><br><code>Identification number 507800799-30 </code><br><code>BCE 0457.696.181     </code><br><code>@anectweet </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662611_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662611.pdf,7,3,2662611,attachments/2662611.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>The proposal for an EU AI Act of 21 April 2021 </code><br><code>Hildebrandt commentary 19 July 2021 </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Under a and b</code><code> the prohibition concerns (a) manipulation or (b) exploitation of vulnerabilities </code><br><code>(b) that results in </code><code>physical or psychological harm to a natural person</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Under (c)</code><code> the prohibition concerns social credit scoring by or on behalf of governments that </code><br><code>results in </code><code>detrimental or unfavourable treatment of a natural person</code><code> (with some additional </code><br><code>conditions). </code><br><code>‚Ä¢</code><code> </code><br><code>Under (d)</code><code> the prohibition concerns </code><code>the use of a specific technology by law enforcement </code><br><code>with some exceptions. No individualised negative results are required for the prohibition to </code><br><code>apply.  </code><br><code>High risk AI systems</code><code> </code><br><code>This refers to AI systems that are: </code><br><code>- a product, or a safety component of a product, covered by legislation in Annex II (this mainly </code><br><code>concerns health and safety threats) </code><br><code>- a system referred to in Annnex III (this mainly concerns fundamental rights threats) </code><br><code>Note that the distinction between prohibited practices, high risk systems and ‚Äòcertain systems‚Äô with </code><br><code>extra transparency obligations </code><code>does not refer to mutually exclusive systems</code><code>. A system that is not </code><br><code>high risk may nevertheless be part of a prohibited practice, and a system with extra transparency </code><br><code>obligations may also be part of a prohibited practice or qualify as a high risk system if e.g. used for </code><br><code>recruitment. This is potentially confusing.  </code><br><code>2</code><code> </code><code>Issues </code><br><code>2.1</code><code> </code><code>Prohibited practices </code><br><code>Three of the prohibited practices</code><code> are based on a combination of a certain </code><code>intent</code><code> (manipulation, </code><br><code>exploitation of vulnerable persons, social credit scoring by government) and a certain </code><code>result</code><code> </code><br><code>individualised harm/detrimental or unfavorable treatment (see above).  </code><br><code>A.</code><code> </code><code>I think that requiring identifiable individualised harm/detrimental or unfavourable treatment </code><br><code>is highly problematic. First, because the negative impacts of manipulation, exploitation or </code><br><code>social credit scoring are not limited to the level of individual harm; it will often play out at </code><br><code>the level of democratic processes (e.g. disrupting the public sphere) and diminish public </code><br><code>goods such as freedom of expression, human autonomy and fair treatment despite the fact </code><br><code>that individual harm cannot be identified (e.g. chilling effect of certain types of surveillance). </code><br><code>Second, tort liability law has demonstrated that it is next to impossible to substantiate and </code><br><code>prove such individual harm even when it is probable (such proof will be a requirement for a </code><br><code>fine ex art. 71.3(a) of the Act and for private law liability). Though the latter can be ‚Äòresolved‚Äô </code><br><code>by imputing strict liability the former (damage to democracy and public goods) cannot be </code><br><code>resolved by way of private law remedies.  </code><br><code>B.</code><code> </code><code>The fourth prohibited practice</code><code> concerns the use of a </code><code>specific technology</code><code> (remote real time </code><br><code>biometric identification systems) if used </code><code>by law enforcement</code><code> in </code><code>publicly accessible space</code><code> </code><br><code>(with the specified exceptions). The exceptions may seem reasonable, considering their </code><br><code>narrowly defined scope, except for  </code><br><code>- the third (art. 5.1(d) under iii), that allows such technologies to be used in the context of an </code><br><code>arrest- and surrender order it concerns offences to which maximum punishments of 3 years </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665448_5,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665448.pdf,5,5,2665448,attachments/2665448.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>5 </code><br><code> </code><br><code>Article 69 ‚Äì Codes of Conduct </code><br><code> </code><br><code> </code><br><code>Europe TPC welcomes the proposal to create codes of conduct for developers and </code><br><code>deployers of AI systems and commends two recent Europe TPC policy products to the Commission‚Äôs attention in this context: its </code><code>Joint Statement on Algorithmic Transparency and </code><br><code>Accountability,</code><code> and </code><code>When Computers Decide: European Recommendations on MachineLearned Automated Decision Making</code><code> white paper co-authored with Informatics Europe.  </code><br><code> </code><br><code> </code><br><code>We also urge the Commission to consult for relevant precepts and practices the </code><code>ACM </code><br><code>Code of Ethics and Professional Conduct</code><code>. Revised in 2018 after a three-year and highly </code><br><code>collaborative international process, ACM‚Äôs benchmark Code and its precursors have guided </code><br><code>the work of professionals in all aspects of computing for 55 years. </code><br><code> </code><br><code>Conclusion </code><br><code> </code><br><code> </code><br><code>ACM‚Äôs Europe Technology Policy Committee, and its thousands of expert European </code><br><code>members, stand ready to assist the Commission at any point in its further consideration of </code><br><code>the Proposal or otherwise with respect to technical matters implicating artificial intelligence, </code><br><code>machine learning, and all other aspects of computing. To request such technical, apolitical </code><br><code>and non-lobbying input, please contact ACM‚Äôs Director of Global Policy & Public Affairs, </code><br><code>Adam Eisgrau, at </code><code>acmpo@acm.org</code><code> or +1 202.580.6555. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665289_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2665289.pdf,4,1,2665289,attachments/2665289.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>ETSI Contribution to the consultation on the </code><br><code>draft AI Regulation  </code><br><code>Preamble </code><br><code>ETSI welcomes the draft Regulation on AI presented by the European Commission (EC) at </code><code>https://eurlex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206</code><code>. The draft Regulation builds on the </code><br><code>processes of the New Legislative Framework (NLF) putting harmonised European Standards into the </code><br><code>focus of demonstrating compliance with the regulatory requirements. This is in line with the view of </code><br><code>ETSI that ""The NLF should be used for technical regulation in new areas including AI and data."" </code><br><code>ETSI has analysed the draft AI Regulation in an activity involving the ETSI Board as well as ETSI's expert </code><br><code>group on AI, set up  across all technical committees and groups working on AI, the OCG-AI group. ETSI </code><br><code>has many activities in the area of AI, focusing on applications in or using ICT. A concise overview from </code><br><code>our working groups is available on the OCG-AI's web site at </code><code>https://portal.etsi.org//TBSiteMap/OCG/OCG-AI-Co-ordination</code><code>. </code><br><code>With this document ETSI provides feedback on the draft Regulation based on the broad spectrum of </code><br><code>expert knowledge available in ETSI. This includes in depth knowledge of AI technologies and systems </code><br><code>as well as expertise present in ETSI on European standardisation, e.g. from the work on harmonised </code><br><code>standards in support of other technical regulation and on the basis of ETSI's ENAP process for the </code><br><code>development and adoption of harmonised standards. </code><br><code>ETSI also thanks the EC for the established close dialogue and exchange on the draft AI Regulation. </code><br><code>This dialogue is very helpful for ETSI in its role as a European Standardisation Organisation to </code><br><code>prepare for the upcoming tasks of developing harmonised European standards on AI.  </code><br><code>1.</code><code> </code><code>On the technical aspects of the draft Regulation on AI </code><br><code>Overall, with the Regulation of AI, in a style similar to the NLF, Europe enters a new area in technical </code><br><code>regulation by applying the NLF model to software. While there are some examples that this works </code><br><code>well and that also the life-cycle of a product can be addressed well in that way, it is still a new area </code><br><code>which also means that all parties will be on a learning curve. </code><br><code>ETSI has expertise - in particular with the EMCD, RED and Accessibility and their respective </code><br><code>harmonised standards -  ETSI is ready to leverage this expertise to achieve successful development </code><br><code>of harmonised European standards in support of the future Regulation on AI.  </code><br><code>Definition of AI </code><code> </code><br><code>It has been noted that the current definition of AI as proposed by the EC in TITLE 1 Article 3(1) and in </code><br><code>Annex I may be misinterpreted insofar as even the use of a spreadsheet for calculating some statistics </code><br><code>may fall under the ANNEX 1 list of techniques at point (c) </code><br><code>We welcome the clarification made verbally by the EC that this is not what is intended and appreciate </code><br><code>the readiness of the EC to provide written clarification of their definition of AI for the purposes of </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665638_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665638.pdf,4,1,2665638,attachments/2665638.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Page 1/4</code><code> </code><br><code>Tour & Taxis Building - Avenue du Port 86c - Box 302 - B-1000 Brussels - T‚Äâ+‚Äâ32 2 203 38 03 - info@wecglobal.org - </code><code>www.weceurope.org</code><code> </code><br><code> </code><br><code>A Proportional Regulation for AI Innovation & Application </code><br><code>WEC-Europe‚Äôs response to the proposed EU Artificial Intelligence Act  </code><br><code>In this paper World Employment Confederation ‚Äì Europe (WEC-Europe), the European trade association for Private </code><br><code>Employment Services, provides its input on the Proposal for a regulation laying down harmonised rules on Artificial </code><br><code>Intelligence (hereafter: the AI Act). </code><br><code> </code><br><code>Labour market efficiency, inclusiveness, and support in times of economic disruption </code><br><code>Members of WEC-Europe provide private employment services including recruitment, temporary agency work and career </code><br><code>guidance. Through these services, they contribute to (1.) efficient matching of labour market demand and supply, (2.) </code><br><code>labour market inclusivity and (3.) a stepping-stone to quality and sustainable employment. In this time of increased </code><br><code>economic uncertainty and the subsequent labour market dynamic, private employment services are a crucial part of the </code><br><code>European ecosystem for labour market support for business and workers. Of course, like any industry, the services are </code><br><code>evermore enhanced and improved by new (digital) technologies, including Artificial Intelligence (hereafter: AI). </code><br><code>Private Employment Services get workers ready for an AI enhanced workplace </code><br><code>Annually, </code><code>private employment services touch the lives of 11,5 million workers</code><code> as they transition on(to) the labour market. In </code><br><code>this they play a key role to prepare and re-/upskill workers and job seekers for new workplaces that require new skills, </code><br><code>competences, and ways of working. This is exactly the difference half a million labour markets consultants and recruiters in </code><br><code>Main points: </code><br><code>-</code><code> </code><code>WEC-Europe welcomes the creation of a regulatory framework of AI that will improve predictability and </code><br><code>a level playing field for the application of AI.  </code><br><code>-</code><code> </code><code>Following its Code of Conduct, WEC-Europe is dedicated to improving labour market inclusiveness and </code><br><code>fighting (un)conscious human bias from the recruitment process, irrespective of the software used in its </code><br><code>services.  </code><br><code>-</code><code> </code><code>WEC-Europe emphasizes that AI is a tool that can be programmed to identify these (un)conscious </code><br><code>human biases and minimize them in recruitment procedures. As such, it welcomes the opportunities in </code><br><code>the proposal to do so. It highlights that in contrast to this ambition, ‚Äòusers‚Äô are unable to do so in the </code><br><code>proposal. This prevents users ability to countercheck providers and/or enhance de-biasing </code><br><code>methodologies. Moreover, this shapes market dependencies that will not drive innovation and </code><br><code>diversity. </code><br><code>-</code><code> </code><code>WEC-Europe warns that the current definitions in the proposal of high-risk AI as well as recruitment is </code><br><code>so broad it covers all software used in recruitment (and employment), including those that do not </code><br><code>involve any automated decision-making or machine-learning that impact the risk the proposal seeks to </code><br><code>mitigate. As such, the proposal‚Äôs definitions diminish the overall high-risk approach.  </code><br><code>-</code><code> </code><code>WEC-Europe brings forward that this broad definition and the administrative requirements of the </code><br><code>proposal will benefit large providers that are able to meet these. Thereby raising issues of AI market </code><br><code>concentration, competition, innovation, and adoption. </code><br><code>-</code><code> </code><code>WEC-Europe welcomes the decentral oversight mechanisms but emphasizes the need for clarity for </code><br><code>business on the competent authority. </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2661969_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2661969.pdf,3,1,2661969,attachments/2661969.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Remiss av Europeiska kommissionens f√∂rslag </code><br><code>till f√∂rordning om harmoniserade regler f√∂r </code><br><code>artificiell intelligens </code><br><code> </code><br><code>LO har i detta skede gjort en f√∂rsta bed√∂mning av f√∂rslaget till AIf√∂rordning och har valt att l√§mna n√•gra √∂vergripande kommentarer p√• </code><br><code>f√∂rslaget.  </code><br><code> </code><br><code>LO ser positivt p√• ett europeiskt initiativ som syftar till att reglera AI. Det √§r </code><br><code>dock viktigt att en f√∂rordning som inneb√§r en fullharmonisering inte verkar </code><br><code>som ett tak utan som ett golv f√∂r andra regleringar s√•v√§l nationellt som </code><br><code>internationellt som direkt eller indirekt syftar till att reglera effekter eller </code><br><code>processer r√∂rande AI-system. Det √§r innefattar ocks√• de nationella </code><br><code>partssystemen och dess regleringar.   </code><br><code> </code><br><code>LO v√§lkomnar f√∂rordningens syften och hoppas dessutom att f√∂rslaget kan </code><br><code>bidra till att √∂ka tydligheten och likv√§rdigheten p√• den inre marknaden inom </code><br><code>detta omr√•de vilket gynnar innovation och r√§ttvis konkurrens inom unionen. </code><br><code> </code><br><code>F√∂rslaget till f√∂rordning lyfter ocks√• vikten av uppf√∂ljning efter en </code><br><code>implementering av f√∂rordningen. LO anser att det √§r av st√∂rsta vikt att </code><br><code>Kommissionen och medlemsl√§nderna gemensamt f√∂ljer utvecklingen och </code><br><code>effekterna av f√∂rordningen.  </code><br><code> </code><br><code>Det system som f√∂resl√•s f√∂r marknads√∂vervakning- och regelefterlevnad </code><br><code>genom offentliga organ p√• s√•v√§l nationell som EU-niv√• ter sig komplext. </code><br><code>Det framg√•r heller inte tydligt av f√∂rslaget vilken relation de olika organen </code><br><code>p√• nationell respektive europeisk niv√• kommer ha. Om utformningen av </code><br><code>marknads√∂vervakning- och regelefterlevnad blir som f√∂rlaget √§r det viktigt </code><br><code>att dessa rollf√∂rdelningar blir tydliga och att det framf√∂r allt p√• nationellt </code><br><code>plan s√§kras tillr√§cklig kompetens och resurser f√∂r verksamheten. Annars </code><br><code>riskerar f√∂rordningen bli tandl√∂s.  </code><br><code>ENHET </code><br><code>Enheten f√∂r Avtalsfr√•gor </code><br><code>DATUM </code><br><code>2021-07-07 </code><br><code>DIARIENUMMER </code><br><code>20210182 </code><br><code>HANDL√ÑGGARE </code><br><code>Linda Larsson </code><br><code>ERT DATUM </code><br><code>Ert datum</code><code> </code><br><code>ER REFERENS </code><br><code>Er referens</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Europeiska kommissionen  </code>,NO_FOOTNOTES_ON_PAGE
fitz_2665168_4,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665168.pdf,5,4,2665168,attachments/2665168.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>¬ß1. In the AI regulatory sandbox personal data lawfully collected for other purposes shall be </code><br><code>processed for the purposes of developing and testing certain innovative AI systems in the </code><br><code>sandbox under the following conditions: </code><code> </code><br><code>(a)  the innovative AI systems shall be developed for safeguarding substantial public interest </code><br><code>in one or more of the following areas: (i) ‚Ä¶ criminal offences prevention ‚Ä¶; (ii) ‚Ä¶ public </code><br><code>safety/health; (iii) ‚Ä¶ environment protection ‚Ä¶  </code><br><code>Article 54(1): While the idea of regulatory sandboxes is applauded, we ask the </code><br><code>commission to not restrict the areas of application.  </code><br><code>Article 64 </code><code>‚Äì Access to data and documentation </code><br><code>¬ß1. Access to data and documentation in the context of their activities, the market </code><br><code>surveillance authorities shall be granted full access to the training, validation and testing </code><br><code>datasets used by the provider, including through application programming interfaces (‚ÄòAPI‚Äô) </code><br><code>or other appropriate technical means and tools enabling remote access.</code><code> </code><br><code>Article 64(1): This provision would imply full storage of all data, which may lead to </code><br><code>prohibitively large data storage capacity. </code><br><code> </code><br><code>Article 64(2): Giving access to the full data and/or code of an application is not always </code><br><code>possible.  In particular, when 3</code><code>rd</code><code>-party pretrained models are used, data or source </code><br><code>code may not be available. Not being able to deploy such systems will lead to a </code><br><code>considerable competitive disadvantage.  </code><br><code>DEFINITION OF AI </code><br><code>The definition of AI as laid down in Annex I is very broad, as it includes almost all software; it </code><br><code>may cover traditional control algorithms as well as any piece of software which is based on </code><br><code>statistical approaches.  </code><br><code>At the same time, it is applauded that the definition does not refer to undefined concepts </code><br><code>such as ‚Äúintelligence‚Äù.  </code><br><code>Probably the key issue is that we are talking about parameterised methods where the </code><br><code>parameters are determined based on data, using non-convex parameter optimisation ‚Äì that </code><br><code>means that, there is no guaranteed optimal solution.  </code><br><code>It is well understood that the definition is only relevant in combination with high-risk </code><br><code>methodologies as defined in Annex II.  It may be worth pointing that out explicitly. </code><br><code>There are two issues. First, the exemplary character of this definition is not to be </code><br><code>underestimated. As the AI Act will serve as a basis for many other regulatory frameworks, it </code><br><code>is likely that this definition will be adopted by those. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663361_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2663361.pdf,2,1,2663361,attachments/2663361.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>[Status]</code><code> </code><br><code>EIT Health AI consultation response  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>EIT Health welcomes the proposed AI Regulation and its ambition to create horizontal </code><br><code>legislation overseeing AI and to maximise the safety and trustworthiness of this technology. </code><br><code>We welcome the creation of a risk-classification of AI and prohibition of certain AI in </code><br><code>alignment with the OECD, as this type of risk management is vital to ensuring uptake of this </code><br><code>important technology. </code><br><code>ÔÇ∑</code><code> </code><code>With the importance of AI in the healthcare context in mind, the AI Act should support ‚Äì not </code><br><code>supress ‚Äì innovation. Additional requirements and conformity assessments imposed by the </code><br><code>foreseen legislation should therefore be combined or simplified to the greatest extent </code><br><code>possible, and accompanied by clear guidance and support to innovators of all sizes to </code><br><code>provide certainty and ensure they are not inhibited by additional regulatory requirements. </code><br><code>ÔÇ∑</code><code> </code><code>To avoid delays both in terms of innovations coming to market and patient access to new </code><br><code>technologies and treatments, notified bodies and other authorities should be supported by </code><br><code>the EU to ensure they are able to assess AI coming to market quickly whilst complying with </code><br><code>the necessary safety assessments.  </code><br><code>ÔÇ∑</code><code> </code><code>We welcome the European AI Board but stress the need for cohesion with other EU level </code><br><code>boards to ensure complete harmonisation of legislation to reduce regulatory burden. The </code><br><code>proposed independent expert group is a positive inclusion, and EIT Health would welcome </code><br><code>the opportunity to participate to provide insight and real-world evidence generated from its </code><br><code>network of health innovators.   </code><br><code>ÔÇ∑</code><code> </code><code>Other stakeholders within the healthcare ecosystem need to be supported in the uptake </code><br><code>and use of AI, notably patients and healthcare professionals. Appropriate education and </code><br><code>upskilling on all the relevant aspects of AI and its utility in healthcare should be incorporated </code><br><code>within existing and future curricula, and continued professional education for healthcare </code><br><code>professionals is a vital foundational step. EIT Health‚Äôs education pillar has trained or </code><br><code>educated 43,000 graduates and professionals, through activities including Summer Schools, </code><br><code>Fellowships and Master‚Äôs/PhD programmes. On AI specifically, the EIT Health project HelloAI </code><br><code>RES Online (https://eithealth.eu/project/helloairis/) was designed to introduce medical </code><br><code>professionals to AI and equip them with the skills needed to allow them to benefit from the </code><br><code>ongoing evolution of the healthcare field. </code><br><code>ÔÇ∑</code><code> </code><code>The proposed Regulation must have appropriate flexibility to accommodate sector specific </code><br><code>issues, notably in health, where the risks are greater and the level of regulation already high. </code><br><code>The proposal finds itself overlapping with many health sector specific and horizontal </code><br><code>legislation (DGA, EHDS, MDR, IVDR, EHDS) and appropriate measures must be taken to </code><br><code>ensure that health innovation is still allowed to thrive whilst of course ensuring patient </code><br><code>safety.  </code><br><code>ÔÇ∑</code><code> </code><code>Further clarity is needed on how healthcare AI outside of the scope of the MDR and IVDR </code><br><code>will be regulated, notably when used for research purposes. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663339_8,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663339.pdf,19,8,2663339,attachments/2663339.pdf#page=8,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>www.acea.auto </code><br><code>7</code><code> </code><br><code>However, the following aspects under the current AI Act would benefit from further </code><br><code>clarification on Commission side (DG CNECT and GROW): </code><br><code> </code><br><code>‚Ä¢ </code><br><code>Interaction of the technical mandatory requirements for high-risk AI </code><br><code>systems in the AI Act with Type Approval</code><code> </code><br><code>With reference to Article 80 in the AI Act</code><code> </code><code>(Amendment to Regulation (EU) 2018/858): </code><br><code>In Article 5 of Regulation (EU) 2018/858 the following paragraph is added:  ‚Äò4. When </code><br><code>adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence </code><br><code>systems which are safety components in the meaning of Regulation (EU) YYY/XX [on </code><br><code>Artificial Intelligence] of the European Parliament and of the Council *, the </code><br><code>requirements set out in Title III, Chapter 2 of that Regulation shall be taken into </code><br><code>account.‚Äô </code><code> </code><br><code>, We note that the </code><code>‚Äòshall be taken into account‚Äô </code><code>phrasing leaves much leeway to EC on </code><br><code>how to accomplish that.</code><code> </code><code>It is unclear whether the requirements set out in the AI </code><br><code>Regulation will be adapted to the specific situation of motor vehicles or if a simple </code><br><code>reference to the requirements in Chapter 2 Title III will be made when adopting </code><code>any</code><code> </code><br><code>future delegated acts under the Type Approval.  </code><br><code>In this regard, we stress that only those few Type Approval regulations that are relevant </code><br><code>for AI should include AI provisions when amended. This should be limited to a minimum </code><br><code>of safety-critical regulations and not include e.g. mechanical requirements, emissions </code><br><code>and noise.  </code><br><code>A gap analysis to assess existing legislation in the automotive sector and potential </code><br><code>gaps relating to AI is encouraged. This should be carried out within the remits of DG </code><br><code>GROW and the industry should partake in this process. </code><br><code> </code><br><code>‚Ä¢ </code><br><code>Timeline for the application of the requirements to motor vehicles</code><code> </code><br><code>When Regulation (EU) No. 2018/858 (Type Approval) will be amended and future </code><br><code>delegated act thereunder adopted, the application of the new AI requirements should </code><br><code>consider the development cycle length of automotive products. Vehicles with </code><br><code>automated functions that will be on the roads in the next few years are already being </code><br><code>trained now.  </code><br><code>Hence, vehicle manufacturers need to be given a suitable timeframe to comply with </code><br><code>any new requirements: at least for the first amended Type Approval regulation that </code><br><code>include AI, a prolonged timeframe will be required. A phased implementation / lead </code><br><code>time is needed for new vehicle and all vehicle types to comply with the requirements. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665502_10,other,../24212003_requirements_for_artificial_intelligence/attachments/2665502.pdf,10,10,2665502,attachments/2665502.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>10 </code><br><code>require a huge time and resource investment from their side. Hence, they may need to be </code><br><code>financially encouraged to participate. While a reduction of the cost of third-party conformity </code><br><code>assessment for small scale providers and users is welcome, this should be further extended to </code><br><code>specific funding for organizations who undergo a self-assessment procedure. In Belgium and </code><br><code>elsewhere, we see that start-ups, scale-ups and SMEs develop many of the most innovative AI </code><br><code>applications. Without adequate incentives, this regulation may put them at a competitive </code><br><code>disadvantage compared to bigger companies who already have a lot of experience with </code><br><code>compliance mechanisms. </code><br><code>10)</code><code> More attention should be given to the impact of AI on societal interests and the </code><br><code>environment </code><code> </code><br><code>While the proposed AI regulation certainly advances citizens‚Äô protection against AI‚Äôs adverse </code><br><code>impact by countering some of the risks posed thereby, many have raised that the attention to </code><br><code>collective and societal interests is fairly limited (as opposed to risks to individual interests). For </code><br><code>instance, the adverse effects that the use of AI can have on the rule of law or on the integrity of </code><br><code>the democratic process, does not seem to be tackled through this regulation, nor is the </code><br><code>environmental impact of AI systems. The focus is currently placed on individual human rights, yet </code><br><code>AI might also affect groups of people, not only according the traditional criteria (race, political, </code><br><code>religious or philosophical opinions) but also based on less traditional profiling criteria, as well as </code><br><code>society at large. </code><br><code>There is hence further scope left to provide attention to these aspects, and grant the public a </code><br><code>greater role to counter these risks. This can be done, for instance, by allowing individuals or </code><br><code>associations to file complaints with supervisory authorities, providing for collective redress </code><br><code>mechanisms, and stimulate public interest litigation against uses of AI that may breach public </code><br><code>values ‚Äì for instance through the cumulative effects of the use of AI systems on a wider scale. In </code><br><code>addition, stakeholder consultations should be organized at a frequent basis in order to determine </code><br><code>the extent to which the regulation needs periodic revision to effectively counter the risks of </code><br><code>problematic AI practices. </code><br><code>Some members noted that this attention to societal risks of AI systems should be reflected in the </code><br><code>list of high-risk AI systems (e.g. through the inclusion of AI systems present on the web that </code><br><code>recommend content or that moderate content and generate risks for freedom of expression and </code><br><code>information and therefore for democracy, as reflected in the EP resolution on an ethical framework </code><br><code>for AI (2020/2012(INL))). In the same vein, some suggested this should be reflected in the list of </code><br><code>prohibited AI systems. Notably, in their joint comment on the proposed regulation, the </code><br><code>EBDP/EDPS advocated that all remote biometric identification in public spaces should be </code><br><code>prohibited as it generates extremely high risks for non-democratic intrusion into individuals‚Äô private </code><br><code>life. Similarly, emotion recognition systems might also pose an unduly high risk to societal </code><br><code>interests that may not be justified in a democratic society. </code><br><code>Evidently, countering AI‚Äôs societal (and individual) impact necessitates not only protective </code><br><code>measures on the supply side, but also broader awareness and education initiatives to ensure that </code><br><code>citizens are well-equipped to understand the potential risks that accompany AI‚Äôs opportunities. </code><br><code>Such increased awareness will also contribute to the enforcement of the proposed regulation.</code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665515_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665515.pdf,5,1,2665515,attachments/2665515.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>C</code><code>OMMENTARY TO </code><code>AI</code><code> </code><code>A</code><code>CT</code><code> </code><br><code>INTRODUCTION </code><br><code>The key goal of the AI Act is to ensure that AI systems placed on the EU market ‚Äúare safe and respect existing </code><br><code>law on fundamental rights and existing values‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>. The proposal lays down a risk methodology for ‚Äúhigh-risk‚Äù AI </code><br><code>systems, which ‚Äúhave to comply with a set of horizontal mandatory requirements for trustworthy AI and follow </code><br><code>conformity assessment procedures‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>. </code><br><code>With respect to ‚ÄúAI systems that relate to products that are covered by relevant Old Approach legislation (e.g., </code><br><code>aviation, cars), the AI Act does not directly apply; however, the ex-ante essential requirements for high-risk AI </code><br><code>‚Ä¶ will have to be taken into account when adopting relevant ‚Ä¶ legislation.</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> </code><br><code>For non-high-risk AI systems, only very limited transparency obligations are imposed, e.g., ‚Äúflag the use of an AI </code><br><code>system when interacting with humans‚Äù</code><code> </code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code>.  For high-risk AI systems, ‚Äúthe requirements of high-quality data, </code><br><code>documentation and traceability, transparency, human oversight, accuracy and robustness, are strictly necessary‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>. </code><br><code>The EC ‚Äúwill establish a system for registering stand-alone high-risk AI applications in a public EU-wide </code><br><code>database.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> AI providers must ‚Äúinform national ‚Ä¶ authorities about serious incidents or malfunctioning‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code>. </code><br><code>Volkswagen AG very much welcomes the AI Act and its risk-based approach, and understands and supports the </code><br><code>unique opportunity that the European Commission has in advancing the safe and trustworthy use of AI.  We </code><br><code>believe that this approach will give Europe a competitive advantage on the long-term development and </code><br><code>deployment of AI systems. </code><br><code>Besides our below feedback we clearly indicate our support of the feedback to the AI Act given by the European </code><br><code>Automobile Manufacturers Association (ACEA). </code><br><code>Out feedback includes the following. </code><br><code>D</code><code>EFINITION OF SAFETY COMPONENT</code><code> </code><br><code>In this regard, we ask clarification on the following aspects: </code><br><code>‚Ä¢</code><code> </code><br><code>Which AI systems can be considered high-risk safety components in motor vehicles and which ones are </code><br><code>not?  </code><br><code>‚Ä¢</code><code> </code><br><code>Which ones would fall in the scope of the AI Act (not type-approved)?  </code><br><code>‚Ä¢</code><code> </code><br><code>How is a safety component characterised? A module embedded in a safety critical chain does not </code><br><code>necessarily bears safety requirement. </code><br><code>                                                                 </code><br><code>1</code><code> Explanatory memorandum 1.1, p3. </code><br><code>2</code><code> Explanatory memorandum 1.1, p3. </code><br><code>3</code><code> Explanatory memorandum 1.1, p3. </code><br><code>4</code><code> Explanatory memorandum 2.3, p7. </code><br><code>5</code><code> Explanatory memorandum 2.3, p7. </code><br><code>6</code><code> Explanatory memorandum 5, p12. </code><br><code>7</code><code> Explanatory memorandum 5, p12. </code>",POSITIVE
fitz_2665579_3,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665579.pdf,4,3,2665579,attachments/2665579.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>techUK response to the Commission‚Äôs proposed Artificial Intelligence Act </code><br><code>To address this, we believe that the co-legislators should thus reassess the responsibilities and roles </code><br><code>of providers and users to better reflect the reality of designing an AI system, compared to operating </code><br><code>it. Ultimately, the AI Act should offer flexibility to allocate responsibilities to the actors that can most </code><br><code>appropriately ensure compliance, notably by ensuring the freedom of the parties to allocate </code><br><code>responsibilities through contractual obligations.</code><code> </code><code> </code><br><code>The EU‚Äôs approach must also take adequate account of the long and complex supply chains for AI </code><br><code>products and services. It‚Äôs important that we find a way of ensuring that the company which puts </code><br><code>the final AI product on the market has trust in the components which have gone into the product. </code><br><code>techUK would welcome a simplified explanation of the concerned parties and how they interact. </code><br><code>Members have mapped out how they believe various parties such as the Provider, Notifying Body, </code><br><code>Notified Bodies, National Competent Authority, etc. will interact, but this is complex. Although </code><br><code>relationships between users and providers should be left to commercial contracts, it would be </code><br><code>helpful to see a diagram depicting how the Commission envisages these different parties working </code><br><code>together.  </code><br><code>Finally, in order to determine responsibility in the supply chain for carrying out the risk-assessment, </code><br><code>it would be helpful to have a clearer guidance on what constitutes a ‚Äòsubstantial modification‚Äô in </code><br><code>Recital 66.  </code><br><code>Areas of further clarification  </code><br><code> </code><br><code>We support the proposal‚Äôs attempt at a risk-based approach, recognising that certain applications or </code><br><code>contexts will not pose risks to fundamental rights and freedoms. Although there are a number of </code><br><code>areas where we would welcome further clarity. For example, Article 5(1) (a), there is a lack of clarity </code><br><code>on which systems and uses could potentially be in scope, with no apparent link to the risk </code><br><code>management process for High-Risk use cases. It would therefore be helpful to provide a clear </code><br><code>definition of what is meant by ‚Äú</code><code>Subliminal techniques beyond a person‚Äôs consciousness in order to </code><br><code>materially distort behaviour</code><code>‚Äù and secondly, guidance on what legal standard or process would be </code><br><code>used to determine and enforce this provision, particularly in relation to demonstration of </code><br><code>‚Äòpsychological harm‚Äô</code><code>.  For these prohibited AI applications further clarity is needed on how this will </code><br><code>be applied in practice. For example, how will this apply to existing applications such as a social media </code><br><code>recommendation system, if</code><code> </code><code>an algorithm was to recommend extremist content?  </code><br><code>Article 10(3) requires that ‚Äútraining, validation and testing data sets shall be relevant, representative, </code><br><code>free of errors and complete.‚Äù Although we do not disagree with the spirit of the requirement, it </code><br><code>demands a level of perfection that is not technically feasible. This requirement must reflect feasible, </code><br><code>best practice standards and be capable of adaptation - for example the requirements relating for </code><br><code>datasets used in a medical context and/or relying on biometric data will be quite different to those </code><br><code>used to automate routine form completion or improve a retail function. The risk is that all AI </code><br><code>producers will be in violation of the requirement. The Regulation could include some form of </code><br><code>qualification that would maintain the spirit of the requirement (‚ÄúProviders shall make reasonable </code><br><code>efforts to ensure that training, validation and testing data are relevant‚Äù, etc‚Ä¶), while being </code><br><code>implementable in practice. Alternatively, techUK believes a better approach would be to allow </code><br><code>industry to continue to develop data standards, such as via data labelling which would provide users </code><br><code>of AI systems with an understanding of the quality and qualities of the training data. techUK believes </code><br><code>industry is best placed to develop these technical standards. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665425_15,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665425.pdf,27,15,2665425,attachments/2665425.pdf#page=15,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>15</code><code> l 27 </code><br><code>Verbraucherzentrale Bundesverband e.V.</code><code> </code><br><code>Artificial intelligence needs real world regulation</code><code> </code><br><code>atic adult gamers with a propensity for game addiction into in-game purchases: ‚ÄúSystems also pair in-game purchase offers with known triggers for an individual player or </code><br><code>known triggers for similar players.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">33</code><code> </code><br><code>True, also a human salesperson can try to spot customers‚Äô weaknesses and exploit </code><br><code>them. But, in contrast to the human, AI-driven marketing systems can draw on large </code><br><code>amounts of granular data from various areas in real time to influence consumer decision making: this includes data on individual consumers as well as behavioural patterns </code><br><code>of others consumers. Providers can scale up AI systems and systematically exploit individuals‚Äô weaknesses and manipulate many consumers individually</code><code>34</code><code>. Marketers already </code><br><code>use AI systems for real time analysis of consumer behaviour to influence individual decision making along the entire value chain. This ranges from targeted advertisement, to </code><br><code>individually curated content and personal rebates influencing consumers‚Äô evaluation </code><br><code>and purchase decision</code><code style=""font-weight: 1000; background-color: #FF0000;"">35</code><code>. The trend for individualisation of AI-driven marketing will increase even further the existing imbalance of power and knowledge between consumers and providers.  </code><br><code>Examples for this AI-driven ‚Äúhyper-personalisation‚Äù of marketing are already in use today are various: E-commerce, websites use AI-driven tools, like Prudsys</code><code>36</code><code>, to analyse </code><br><code>user browsing behaviour in real time in order to offer them personalised prices in the </code><br><code>form of personalised discounts. In order to push consumers to make purchases, e-Spirit </code><br><code>provides AI-driven E-commerce tools to personalise multiple sales channels‚Äô layout, </code><br><code>menu bars, displayed ads, pop-ups, text and CTAs (so-called ‚ÄúCalls to action‚Äù, meaning </code><br><code>designs or phrases intended to prompt an immediate sale</code><code style=""font-weight: 1000; background-color: #FF0000;"">37</code><code>). Personalisation can be </code><br><code>based on each consumer‚Äôs online behaviour or profile. The Canadian car insurer Kanetix used integrate.AI</code><code>38</code><code> systems to detect and target undecided consumers and increased convergence rate by 13%</code><code>39</code><code>. </code><br><code>Firms can use these tools to exploit consumers‚Äô vulnerabilities like illnesses, exhaustion </code><br><code>or other personal difficulties people are struggling with. This can lead to welfare losses </code><br><code>when marketers use this technology to overcharge consumers or manipulate them to </code><br><code>make purchases they would not do otherwise. </code><br><code>The AIA should in general </code><code>prohibit </code><code>AI systems from </code><code>exploiting </code><code>weaknesses and </code><br><code>vulnerabilities </code><code>of </code><code>consumers</code><code>. This protection should not be limited to young, old </code><br><code>and persons with disabilities, but include all consumers, even if their weaknesses or </code><br><code>vulnerabilities are temporary or ‚Äúsituational‚Äù.  </code><br><code>___________________________________________________________________________________________</code><code> </code><br><code>33</code><code> Markle, Tracy; Kennedy, Brett: In-Game Purchases: How Video Games Turn Players into Payers. in: Digital Media </code><br><code>Treatment (2021), URL: https://digitalmediatreatment.com/in-game-purchases/ [Access: 21.07.2021]. </code><br><code>34</code><code> Kietzmann, Jan; Paschen, Jeannette; Treen, Emily (see FN. 31). </code><br><code>35</code><code> Davenport, Thomas u. a.: How Artificial Intelligence Will Change the Future of Marketing 48 (2020), in: Journal of the </code><br><code>Academy of Marketing Science, H. 1, p. 24‚Äì42, URL: https://link.springer.com/content/pdf/10.1007/s11747-019-006960.pdf [Access: 08.07.2021]; Kietzmann, Jan; Paschen, Jeannette; Treen, Emily (see FN. 31); Prudsys: Price </code><br><code>Optimization: Intelligent and Personalized Couponing (2021), URL: https://prudsys.de/en/case/priceoptimization_promotion-pricing/ [Access: 22.07.2021]. </code><br><code>36</code><code> Prudsys (see FN. 35). </code><br><code>37</code><code> Call to action (marketing) - Wikipedia. in: Wikipedia (2021), URL: </code><br><code>https://en.wikipedia.org/w/index.php?oldid=1033452251 [Access: 23.07.2021]. </code><br><code>38</code><code> Integrate.ai (2021), URL: https://integrate.ai/ [Access: 26.07.2021]. </code><br><code>39</code><code> Adriano, Lyle: Kanetix leverages AI technology to optimize consumer experience (2018), URL: </code><br><code>https://www.insurancebusinessmag.com/ca/news/digital-age/kanetix-leverages-ai-technology-to-optimize-consumerexperience-93703.aspx [Access: 21.07.2021]. </code>",FALSE_NEGATIVE
fitz_2663324_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2663324.pdf,7,1,2663324,attachments/2663324.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>08 September 2021  </code><br><code> </code><br><code> </code><br><code>Rue Marie de Bourgogne, 58 ‚Äì 1000 Brussels ‚Äì Tel 0032 2 842 69 82 - www.eucope.org  </code><br><code> </code><br><code>AI IN HEALTHCARE: A EUROPEAN </code><br><code>OPPORTUNITY </code><br><code>1. INTRODUCTION </code><br><code>As demonstrated by the fight against COVID-19, Artificial Intelligence (AI) has the potential to drive change </code><br><code>and improve efficiency and accessibility in healthcare</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>.  </code><br><code>From compounds design</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code> in medicines development to faster patient screening and diagnosis to hospital </code><br><code>management (to name but a few applications), </code><code>AI-based solutions can help</code><code> </code><code>upgrade our healthcare </code><br><code>systems towards more sustainability, while helping to address patients‚Äô unmet needs</code><code>. AI represents </code><br><code>a historic opportunity to redesign healthcare in Europe, but realizing its full potential will require some </code><br><code>structural, behavioural and societal adjustments. It is undeniable that digital applications and innovations in </code><br><code>health have not experienced the massive uptake seen in other sectors (e.g. security, finance, automotive). </code><br><code>As AI‚Äôs potential in health attracts interests and enthusiasm, its limitations due to its adaptive nature, data </code><br><code>availability and appropriate regulatory framework have also come to the fore. In the EU, the COVID-19 </code><br><code>pandemic has accelerated reflections about the role of policy in enabling companies to lead globally in </code><br><code>digital health and innovation. Leaders need to ensure small to medium-sized companies (SMEs) can access </code><br><code>capital, data and skills</code><code> to develop AI-solutions in healthcare.  </code><br><code>As a follow-up to its White Paper on Artificial Intelligence</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>, the European Commission released its Data </code><br><code>Strategy</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> which aims to boost healthcare data sharing and interoperability through the creation of a </code><br><code>European Health Data Space</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> ‚Äì which will enable ‚Äì and further foster ‚Äì the effective use of AI-related </code><br><code>technologies in healthcare. Moreover, the European Medicines Agency explicitly highlights the value and </code><br><code>need for enhanced regulatory clarity for AI in the Regulatory Science Strategy for 2025</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> and the HMA-EMA </code><br><code>Joint Big Data Taskforce Recommendations</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code>. Hence, it appears certain that a </code><code>fit-for-purpose regulatory </code><br><code>                                                      </code><br><code>1</code><code> J</code><code>in, C., Chen, W., Cao, Y. </code><code>et al.</code><code> Development and evaluation of an artificial intelligence system for COVID-19 </code><br><code>diagnosis. </code><code>Nat Commun</code><code> 11</code><code>, </code><code>5088 (2020). https://doi.org/10.1038/s41467-020-18685-1 </code><br><code>2</code><code> </code><code>For example: screening of compounds, automatic design of chemical compounds, appropriate synthetic route to </code><br><code>make compounds, models to predict efficacy / ADMET properties of compounds</code><code> </code><br><code>3</code><code> </code><code>https://ec.europa.eu/info/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en</code><code>  </code><br><code>4</code><code> </code><code>https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age/european-data-strategy_en</code><code>  </code><br><code>5</code><code> </code><code>https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=COM:2018:0232:FIN</code><code>  </code><br><code>6</code><code> </code><code>https://www.ema.europa.eu/en/documents/regulatory-procedural-guideline/ema-regulatory-science-2025-strategicreflection_en.pdf</code><code> </code><br><code>7</code><code> </code><code>https://www.ema.europa.eu/en/documents/other/hma-ema-joint-big-data-taskforce-phase-ii-report-evolving-datadriven-regulation_en.pd</code><code>f</code><code> </code>",POSITIVE
fitz_2665432_7,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665432.pdf,7,7,2665432,attachments/2665432.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>7 </code><br><code>-</code><code> </code><br><code>Providers of high risk AI systems are only obliged to report serious incidents or </code><br><code>malfunctioning which constitute a breach of Union law intended to protect fundamental </code><br><code>rights. This leaves out breaches of fundamental rights stemming from non-high risk AI </code><br><code>systems, as well as any serious incidents or malfunctioning which are not directly </code><br><code>related to fundamental rights. For example, a faulty AI used in an energy distribution </code><br><code>grid can have a serious impact on the finances and wellbeing of consumers by </code><br><code>inadvertently cutting off their energy supply or overcharging them. This would not be </code><br><code>covered under the proposal. </code><br><code> </code><br><code>It is key to have a clear and coherent oversight and enforcement structure to guarantee </code><br><code>an effective and consistent protection of consumers all across the EU. </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665537_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665537.pdf,3,1,2665537,attachments/2665537.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>FNA   9‚Äì11 Avenue Michelet          F‚Äì93583  Saint-Ouen Cedex          aliou.sow@fna.fr        </code><code>ÔÄß</code><code> (+33) 1 40 11 99 04  -  (+33) 6 62 79 62 12</code><code> </code><code> </code><br><code>Federation of Craft Businesses in the automotive sector and in mobility services (FNA) </code><code>would like to thank the authors of the Proposal </code><br><code>of European Regulation laying down harmonized rules on Artificial Intelligence (AI) to inform citizens and stakeholders about the </code><br><code>Commission's work in order to allow them to provide feedback on the intended initiative and to participate effectively in future consultation </code><br><code>activities. FNA representatives also express their thanks for being given the opportunity of once again making submissions on the </code><br><code>Consultation. They make the following comments. </code><br><code>Access to Automotive data and information on AI </code><br><code>Findings of new undue hardship on advanced technologies </code><br><code> </code><br><code>- Electric vehicles</code><code> </code><br><code>1. Advanced technologies win the automotive industry: on the one hand, the electric car revolutionizes maintenance methods; on the other </code><br><code>hand, the increasing connectivity of new </code><code>""smart""</code><code> vehicles tends to channel diagnostic, maintenance and repair services through the </code><br><code>manufacturer's network. </code><br><code>2. While the classic vehicle has thousands of moving parts in the engine, the electric car is less than ten: maintenance is therefore simpler </code><br><code>and 30 to 40% cheaper. The battery, which is managed by the on-board electronics, warns the driver of the level of his capacity being </code><br><code>affected by the charge and discharge cycles. In order to carry out the maintenance of the vehicle, it must be temporarily deactivated via a </code><br><code>safety connector. The specificity of the maintenance relies precisely on the safety instructions and data to be applied: the work is done under </code><br><code>tension, the electrical intensities are strong. </code><br><code>3. Difficulties in access to the maintenance and repair market, which have already been identified for the combustion engine, are focused </code><br><code>on the electrical system: many safety data and the needed equipment. While the new technique is designed to reduce the cost of </code><br><code>maintenance and repair, vehicle manufacturers tend to reserve the operation to their network, at the expense of consumers‚Äô choice. </code><br><code>3.1. For example, Mrs Johanne Berner Hansen, lawyer of Dansk Bilbrancher√•d (DBR) reported that TESLA representatives are able to </code><br><code>‚Äúbypass‚Äù the car. They can make it ‚Äúunsupported‚Äù which means that TESLA can remove the capability of fast charge. This makes the car </code><br><code>useless in many ways. TESLA has an intern notification that states why a car is being ‚Äúunsupported‚Äù. There is no opportunity to reverse, i.e. </code><br><code>the car cannot be supported again. This is a major issue for consumers as TESLA is in this way keeping competition out. </code><br><code>3.2. In the same way TESLA removes the entire warranty of the car if electric components (for instance tail-or front-gate) have been fitted </code><br><code>together by others than TESLA network. In order for the car owner to get the warranty back, he needs to get the entire wiring harness in the </code><br><code>car displaced ‚Äì of course at TESLA and of course by charge.  </code><br><code>3.3. TESLA and their way of doing business is a major issue all around Europe and right know, Danish DBR and French FNA fear that </code><br><code>TESLA way of business will spread out to other car manufacturers - only because </code><code>""it works"".</code><code>  </code><br><code>As a result, it is difficult for car owners to make a choice regarding repairer, since no independent repairer is entitled to repair TESLA cars.  </code><br><code> </code><br><code>- Connected vehicles </code><br><code>4. The same observation can be made for the connected vehicle: data exchanges are supposed to offer more comfort and to be a source of </code><br><code>savings for the consumer: remote prognosis of the vehicle condition, use, support services, smart electric vehicle recharging, traffic </code><br><code>management, connection to infrastructure and other vehicles for cooperative and highly automated driving. But the exclusive and permanent </code><br><code>connection of manufacturers with motorists entails the risk of favoring their network. </code><br><code>5. Moreover, this risk has been highlighted by the European Commission, after the study carried out by the Cooperative Intelligent Transport </code><br><code>Systems (C-ITS) platform it has set up. Members warned the European Parliament and the Council of Ministers that these new challenges </code><br><code>have raised concerns about the potential exclusion of independent operators and the monitoring of their activities by manufacturers that are </code><br><code>in competition with them. The role of European legislation is fundamental to ensure that key conditions are fulfilled, in particular those </code><br><code>identified by the C-ITS platform: </code><br><code>-a prior consent of the person concerned (driver or vehicle owner); </code><br><code>-a fair and undistorted competition; </code><br><code>-data protection and privacy; </code><br><code>-non-falsifiable access and liability; </code><br><code>-data saving. </code><br><code>These conditions should be met in light of the need to protect the potential intellectual property</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>. </code><br><code> </code><br><code> </code><br><code>                                                           </code><br><code>1</code><code>European Commission - Report from the Commission to the European Parliament and the Council on the functioning of the system for access to information </code><br><code>on repair and maintenance of vehicles established by Regulation (EC) No 715/2007 - COM (2016) 782 Final of 9 December 2016, paragraph 5.6 page 11. </code><br><code>Chapter 8 of the report of the Cooperative Intelligent Transport Systems (C-ITS) platform on access to embedded data and resources: </code><br><code>http://ec.europa.eu/transport/themes/its/doc/c-its-platform-final-report-january-2016.pdf</code><code>. </code>",POSITIVE
fitz_2665397_11,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665397.pdf,15,11,2665397,attachments/2665397.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>10 of 14</code><code> </code><br><code> </code><br><code>There are parts of Annex III that provide for certain usages of emotion recognition to be </code><br><code>treated as high-risk AI systems. Point 6 of Annex III lists high-risk AI systems in law </code><br><code>enforcement, such as: ‚Äú(b) AI systems intended to be used by law enforcement authorities as </code><br><code>polygraphs and similar tools or to detect the emotional state of a natural person.‚Äù Point 7 </code><br><code>provides a list of high-risk AI systems in migration, asylum and border control management, </code><br><code>and also mentions: ‚Äú(a) AI systems intended to be used by competent public authorities as </code><br><code>polygraphs and similar tools or to detect the emotional state of a natural person.‚Äù </code><br><code>Linking the use of emotion recognition systems to a specific type of activity, usually reserved </code><br><code>for public authorities, means less scrutiny for private actors in the market. Given the </code><br><code>sensitivity of the issue, the possible exploitation in other private areas of one‚Äôs life, especially </code><br><code>in working places, and the intrusive nature of the technology, the transparency obligations </code><br><code>set out in Article 52(2) of the Draft AI Act might not be enough to protect individuals and their </code><br><code>fundamental rights. Therefore, we would suggest for consideration a general ban of largescale emotion recognition systems in publicly accessible places</code><code>26</code><code>, and the inclusion of </code><br><code>emotion recognition systems as an additional category of high-risk AI systems in all the other </code><br><code>cases. </code><br><code> </code><br><code>3.2.2. Requirements for high-risk AI systems and conformity assessment </code><br><code> </code><br><code>The core of the Draft AI Act focuses on the systems falling within the high-risk category, which </code><br><code>will have to comply with the requirements in Chapter 2. </code><br><code>We welcome, for instance, Article 13, which establishes a transparency obligation towards </code><br><code>the users. The latter should receive ‚Äúconcise and clear information‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">27</code><code> that allows them to </code><br><code>‚Äúinterpret the system‚Äôs output and use it appropriately‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">28</code><code>. In Recital 47, the Draft AI Act </code><br><code>recognises the importance to address the opacity of the systems which makes them  </code><br><code>""incomprehensible or too complex for natural persons‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">29</code><code>. Therefore, we wonder why an </code><br><code>equivalent transparency obligation is not placed onto the user vis-a-vis the individuals who </code><br><code>are subject to the AI system.  This measure will encompass the technology's whole life cycle, </code><br><code>including all the relevant subjects involved. </code><br><code>Before placing high-risk AI systems on the market or putting them into service, providers will </code><br><code>have to assess the conformity with the requirements set in Chapter 2. To this end, there are </code><br><code>three possible options: the conformity can be presumed if the AI system complies with </code><br><code>existing CEN and CENELEC standards (Article 40). Alternatively, where harmonised standards </code><br><code>do not exist or are not sufficient, the Commission can adopt commons specifications (Article </code><br><code>41). Also in this case, compliance with the specifications creates a presumption of conformity. </code><br><code> </code><br><code>26</code><code> See, above Section 3.1., point 5. </code><br><code>27</code><code> Recital 47 Draft AI Act. </code><br><code>28</code><code> Article 13 Draft AI Act. </code><br><code>29</code><code> Recital 47 Draft AI Act. </code>",FALSE_NEGATIVE
fitz_2665481_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665481.pdf,20,2,2665481,attachments/2665481.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>2 </code><br><code> </code><br><code>sectoral legislation and existing regimes, but also commensurate with the evolving and </code><br><code>innovative nature of the technology itself.  While CTA is encouraged by the Commission‚Äôs use </code><br><code>of a risk-based framework in the Proposed Rules, ambiguities in the Proposed Rules as written </code><br><code>could undermine or impair further innovation and development of this transformative </code><br><code>technology.  To address these concerns, CTA recommends greater clarity by the Commission, </code><br><code>specifically regarding the portions of the Proposed Rules discussed herein.    </code><br><code>CTA applauds the Commission for considering the potential costs of complying with this </code><br><code>broad new regulatory framework, but CTA is concerned that the full impact of such costs may </code><br><code>not be fully appreciated.  Indeed, a recent study by the Center for Data Innovation indicates that </code><br><code>such costs are likely to be much greater than the Commission estimates.  The Commission must </code><br><code>recognize that if compliance costs outweigh the benefits of the Proposed Rules, promising AI </code><br><code>products and services may not be developed at all if the Proposed Rules force companies to </code><br><code>assume exorbitant compliance costs in connection with conducting conformity assessments, </code><br><code>recordkeeping, reporting and other duties.  The Commission must be mindful of the potential </code><br><code>impact not only on small- and medium-sized businesses, but on the EU market as whole.   </code><br><code>Respectfully, CTA urges the Commission to address several ambiguities in the Proposed </code><br><code>Rules.  For instance, CTA recommends clarification of the definitions of ‚Äúartificial intelligence‚Äù </code><br><code>and ‚Äúsafety component.‚Äù  Also, clarifying the meaning of ‚Äúprovider‚Äù is critical for legal </code><br><code>certainty, given the potential extra-territorial reach such ambiguity presents.  Further, it is also </code><br><code>imperative that the Commission review and revise the scope of certain requirements imposed, </code><br><code>including those related to data governance, human oversight, recordkeeping and reporting. As </code><br><code>discussed more fully below, these changes are necessary to ensure that the Commission‚Äôs </code><br><code>proposal leads to a balanced, risk-based framework that does not hamper innovation or the </code><br><code>potential of this truly life changing technology. </code><br><code>------------------ </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2635975_7,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2635975.pdf,9,7,2635975,attachments/2635975.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>7 </code><br><code>fact, EU companies are still relatively small in comparison to firms in the US and China in key </code><br><code>online sectors, such as search, social, and e-commerce.  </code><br><code> </code><br><code>A long and complicated relationship </code><br><code> </code><br><code>For a long time, people have had a complicated relationship with artificial intelligence. The </code><br><code>relationship between artificial intelligence and society has been marked by a succession of AI </code><br><code>‚Äúsummers‚Äù and ‚Äúwinters.‚Äù(</code><code>14</code><code>) AI summers are periods of intense excitement about the potentials </code><br><code>of artificial intelligence. They are usually brought by breakthroughs, such as those embodied by </code><br><code>recent advances in machine learning (</code><code>15</code><code>‚Äì</code><code>20</code><code>). But these summers are usually followed by AI </code><br><code>winters. These are periods of desilusion and backlash, brought by frustration and limitations with </code><br><code>the new technology in combination with other social and economic forces.  </code><br><code> </code><br><code>Just like the AI summer that started in the 1950s had become a winter by the 1970s, today we </code><br><code>are seeing a similar transition. But this time is also different. AI has left the lab. It is now closer to </code><br><code>the end of a road that starts on ideas and ends on products. If that is truly the case, the effort to </code><br><code>regulate AI comes at the right time. It is now a mature technology, similar to the one found in other </code><br><code>tightly regulated industries, such as pharmaceuticals and automotive. But if the technology is yet </code><br><code>too immature, the effort to regulate it may limit avenues for learning and development that require </code><br><code>rapid prototyping and testing.  </code><br><code> </code><br><code>Time will tell us whether the time was right, as we continue to look at the development of AI </code><br><code>technology in and out of Europe. At this time, we welcome the effort by the EU to kickstart a </code><br><code>thoughtful discussion on the future of AI regulation. </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2635987_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2635987.pdf,5,2,2635987,attachments/2635987.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>DIN and DKE, the German national standardization bodies and members of the European </code><br><code>Standardization Organizations (ESOs) CEN, CENELEC and ETSI welcome the European </code><br><code>Commission's (EC) proposal for a regulation that focuses on trustworthiness and product </code><br><code>safety of high-risk artificial intelligence (AI)</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> for the European Single Market. AI technology will </code><br><code>only be used across the board if it is trustworthy and finds acceptance in society. Standards </code><br><code>play an essential role in helping to achieve a broader acceptance of AI systems and bolster </code><br><code>trust in them. For example, they can define criteria for quality, explainability, fairness, safety, </code><br><code>security, privacy, and transparency. We therefore welcome the fact that the Commission assigns an essential role to standards in the safe placing of AI technology on the European Single </code><br><code>Market. We further believe that the AIA proposal paves a way for European experts to significantly shape global standards on AI. </code><br><code> </code><br><code>The New Legislative Framework is the right regulatory approach for the AIA</code><code> </code><br><code>We strongly support the EC‚Äôs approach to regulate high-risk AI according to the New Legislative </code><br><code>Framework (NLF). Following the NLF approach, policymakers determine legal requirements </code><br><code>(so-called ‚Äúessential requirements‚Äù) and refer to standards to specify the technical details. </code><br><code>These standards are developed by the ESOs CEN, CENELEC and ETSI and become ‚Äòharmonized standards‚Äô through the publication of their reference in the Official Journal of the European </code><br><code>Union (OJEU) by the EC. Harmonized European standards provide the presumption of conformity with the respective EU legislation. This enables all market participants to meet the essential requirements of the respective EU legislation, provides state-of-the-art solutions, eases </code><br><code>the burden on the legislator to provide detailed technical regulation and helps to maintain a </code><br><code>flexible legal framework. Hence, by deciding to issue standardization requests to the ESOs in </code><br><code>accordance with Regulation (EU) 1025/2012 on European Standardization to specify technical </code><br><code>details resulting from the requirements under Chapter 2 of the AIA, the EC lays the foundation </code><br><code>for a harmonized digital single market for AI: European standards adopted by the ESOs are </code><br><code>identically adopted by all national standardization bodies. Competing national standards must </code><br><code>be withdrawn at the same time, thus creating a level playing field across Europe. Thus, the NLF </code><br><code>has been a well proven concept to consolidate the European single market for over 30 years</code><code>.</code><code>  </code><br><code> </code><br><code>Strengthening and supporting existing artificial intelligence standardization  </code><br><code>In standardisation, AI has already been recognized as an important work item. The international standardization committee </code><code>ISO/IEC JTC1/SC 42 ‚ÄúArtificial Intelligence‚Äù</code><code> currently addresses  several topics such as foundational standards, management system standard, data </code><br><code>quality, trustworthiness, ethics, risk management, explainability and governance implications </code><br><code>of AI, as examples of its comprehensive </code><code>work program</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>.  </code><br><code>At the European level, CEN and CENELEC have established the Joint Technical Committee </code><br><code>CEN-CENELEC/JTC 21 ‚ÄúArtificial Intelligence‚Äù. Its tasks are to identify and adopt international </code><br><code>standards already available, produce standardization deliverables addressing the European </code><br><code>market and societal needs, as well as underpinning EU legislation, policies, principles and </code><br><code>                                                      </code><br><code>1</code><code> </code><code>ISO/IEC  DIS 22989:2021 ‚ÄúInformation Technology ‚Äî Artificial Intelligence ‚Äî Artificial Intelligence Concepts and Terminology‚Äù defines artificial intelligence as a ‚Äúset of methods or automated entities that together build, optimize and apply </code><br><code>a model (3.1.26) so that the system can, for a given set of predefined tasks (3.1.37), compute predictions (3.2.12), recommendations, or decisions‚Äù.</code><code>  </code><br><code>2</code><code> </code><code>Standards by ISO/IEC JTC 1/SC 42 ‚ÄúArtificial intelligence‚Äù: </code><code>https://www.iso.org/committee/6794475/x/catalogue/p/0/u/1/w/0/d/0</code><code>  </code>",POSITIVE
fitz_2665167_6,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665167.pdf,6,6,2665167,attachments/2665167.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Associa√ß√£o de Utilidade P√∫blica  </code><br><code>ONG ‚Äì Organiza√ß√£o N√£o Governamental </code><br><code> </code><br><code> </code><br><code>Rua Alexandre Cabral, 2C ‚Äì Loja A </code><br><code>1600-803 Lisboa ‚Äì Portugal </code><br><code>URL: www.apdsi.pt </code><br><code>Tel.: (+351) 217 510 762 </code><br><code>Fax: (+351) 217 570 516 </code><br><code>E-mail: secretariado@apdsi.pt</code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Patrocinadores Globais da APDSI </code><br><code> </code><br><code> </code><br><code>   </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665252_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665252.pdf,2,1,2665252,attachments/2665252.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>As a trade union focused on education, we view this initiative from a set of core values (quality </code><br><code>education for every child, good working conditions for teachers  - working conditions are learning </code><br><code>conditions -, teachers as autonomous professionals, and education as a public good in a free </code><br><code>democratic society) that constitute our organization. Our remarks on the EC initiative and the </code><br><code>response by ETUCE is based on these cornerstone values, and is as follows: </code><br><code>- </code><br><code>Education plays a key role in our society. It entails striving for a broad formation for every </code><br><code>student and has an immense innovative and emancipating strength. Every child should be entitled to </code><br><code>a high quality education that aims to bring out the best in every student and learner. Introducing AI in </code><br><code>our education systems can provide benefits for our teachers and students, as it opens up new </code><br><code>possibilities and has the potential to make teaching and learning more personal. But introducing AI in </code><br><code>education doesn‚Äôt come without risks. Therefore it is of utmost importance that legislation is put in </code><br><code>place so that every actor in our educational environment, and with extension our entire society, is </code><br><code>protected and can fall back on a legislative framework. A good legislative framework will ensure a level </code><br><code>playing field for all actors, and offer transparency for everyone involved. Introducing AI in education </code><br><code>cannot undermine the equal access to quality education, which is a human right. As the ongoing </code><br><code>Corona-pandemic has shown, digitalization has enhanced the inequalities in education (OECD, 2021), </code><br><code>so this risk is real, not potential or plausible. Therefore we believe the EC should take the lead in </code><br><code>developing clear and binding measures, which should include but not be limited to ethical guidelines </code><br><code>that address the risks that AI poses concerning transparency, accountability, intellectual property </code><br><code>rights, data privacy, cyber-safety, equality and environmental protection. </code><br><code>- </code><br><code>The introduction of AI in education will signify a change in the work environment and the </code><br><code>working conditions of teachers. Since we, as an organization, sincerely believe that the working </code><br><code>conditions of teachers are of the utmost importance and form an integral part of the learning </code><br><code>conditions of children and students, we think it is necessary to regard teachers as autonomous </code><br><code>professionals. Teachers should be respected as masters of their craft, who know better than anyone </code><br><code>else what is needed in education. The Corona-crisis has shown again that there is a big social value to </code><br><code>education, and this social part is an integral segment of the quality of education. AI cannot and should </code><br><code>not replace teachers, but should be seen as an aid to teachers. Consequently, it is important that </code><br><code>teachers are included in the development of AI in all its aspects regarding education, and that they </code><br><code>are represented by their representative organizations in this process. Therefore the social partners </code><br><code>have to be included in the governance regarding AI in education. As teachers are the ones who have </code><br><code>to work with it and implement it in their every practices, involving them as equal partners from the </code><br><code>start will only improve both the transition to a more digital educational environment and the quality </code><br><code>of the digital tools that will be used. It will also make teachers ‚Äì as end-users ‚Äì more familiar and at </code><br><code>ease in using AI in their teaching practice.  </code><br><code>- </code><br><code>To ensure that teachers can work as autonomous professionals, the guidelines regarding AI in </code><br><code>education need to have a segment embedded that focusses on the need and realization of extended </code><br><code>professionalization of the digital skills of teachers. We see this professionalization as twofold: it is clear </code><br><code>that teachers need the necessary technical digital competences, and these should be part of the </code><br><code>further professionalization of teachers, but we also find it of the utmost importance to include ethical </code><br><code>awareness and critical thinking. Teachers need to have the tools to make conscious, deliberate choices </code><br><code>when including AI in their teaching. This means that teachers should be involved in the process, and </code><br><code>not be reduced to the role of executors of what others decide. Teachers get to decide how and when </code><br><code>to use AI and digital tools, not the other way around. </code><br><code>-  </code><br><code>Education, as a human right, is a public service that should provide quality formation for every </code><br><code>student, regardless of means and family situation, and therefore needs to be funded with public </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663289_7,other,../24212003_requirements_for_artificial_intelligence/attachments/2663289.pdf,7,7,2663289,attachments/2663289.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>20210728 ‚Äì 1.0 </code><br><code>rights, facilitates the development of lawful, safe and trustworthy AI and </code><br><code>ensures legal certainty for businesses.  </code><br><code>34.</code><code> </code><code>We support the Commission‚Äôs broader view that it is important to balance </code><br><code>the flow and wide use of data, while preserving high privacy, security, </code><br><code>safety and ethical standards.</code><code style=""font-weight: 1000; background-color: #FF0000;"">21</code><code> </code><br><code>35.</code><code> </code><code>We will monitor any further developments from the EU Commission </code><br><code>regarding this proposal and related frameworks and will contribute when </code><br><code>appropriate. </code><br><code> </code><br><code>21</code><code> EU Commission (2020). Communication from the Commission to the European Parliament, the Council, the </code><br><code>European Economic and Social Committee and the Committee of the Regions: A European Strategy for Data. </code><br><code>COM(2020) 66 final, 19 February 2020. Available at: </code><br><code>https://ec.europa.eu/info/sites/default/files/communication-european-strategy-data-19feb2020_en.pdf</code><code> </code>",POSITIVE
fitz_2665299_2,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665299.pdf,5,2,2665299,attachments/2665299.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>2 </code><br><code> </code><br><code>platforms.  On the other hand, the ‚ÄòRegulation for a European Approach for </code><br><code>Artificial Intelligence‚Äô apparently deals with ‚Äòhigh risk‚Äô AI used at ‚Äòhigh risk‚Äô </code><br><code>activities.  According to the risk-based approach of the Regulation, AI systems </code><br><code>are classified as creating ‚Äòunacceptable risk‚Äô, ‚Äòhigh risk‚Äô, and ‚Äòminimal risk‚Äô.</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>  The </code><br><code>only section of the Regulation that applies to AI systems other than ‚Äòhigh risk‚Äô </code><br><code>AI is Title IV on the ‚ÄòTransparency Obligations for Certain AI systems‚Äô, which is </code><br><code>essentially the obligation of AI systems to inform natural persons that they are </code><br><code>interacting with an AI system.</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>  Other important prerequisites including </code><br><code>requirements,</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> </code><br><code>obligations </code><br><code>of </code><br><code>providers </code><br><code>and </code><br><code>users</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> </code><br><code>and </code><br><code>conformity </code><br><code>assessments</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> only apply to ‚Äòhigh risk‚Äô AI.   </code><br><code> </code><br><code>4. Thus, the effective regulation for AI classified as ‚Äòlow or minimal risk‚Äô AI may </code><br><code>appear not sufficiently addressed and/or missing in the expert and/or public </code><br><code>eye.</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code>  Such AI arguably includes smart assistants, such as Google Assistant, </code><br><code>Apple‚Äôs Siri and Amazon‚Äôs Alexa‚Äôs, which run on millions of devices daily.  By </code><br><code>leaving the regulation for the development of non-high-risk AI allegedly and/or </code><br><code>primarily to self-regulation or other modes of decentralised governance, it is </code><br><code>argued that a huge market of AI is left essentially unclassified.  A question that </code><br><code>arises from the risk-based approach of the Regulation is the gap between ‚Äòhigh </code><br><code>risk‚Äô AI and ‚Äòlow risk‚Äô AI, as no ‚Äòaverage risk‚Äô AI is created/expressly referred to.  </code><br><code>Thus, the current Regulation appears to have loopholes, which hardware and </code><br><code>software AI manufacturers and/or other actors may use to circumvent certain </code><br><code>requirements and maximise business and/or industry practices involving data </code><br><code>collection. To the extent that the risk-based approach is the chosen method of </code><br><code>regulation, we are therefore calling on the European Commission to take further </code><br><code>steps towards the enlargement of the risk-based approach for AI, and to </code><br><code>introduce similar/equivalent/proportionate standards for ‚Äòaverage‚Äô and ‚Äòlow or </code><br><code>minimal risk‚Äô AI in the current Regulation and/or by virtue of a new legal </code><br><code>instrument of secondary legislation or otherwise, at the EU level, specifically for </code><br><code>                                                           </code><br><code>1</code><code> Regulation Of The European Parliament And Of The Council Laying Down Harmonised Rules On Artificial </code><br><code>Intelligence (Artificial Intelligence Act) And Amending Certain Union Legislative Acts (2021/0106), 12 </code><br><code>2</code><code> Ibid, Article 52 </code><br><code>3</code><code> Ibid, Chapter 2 </code><br><code>4</code><code> Ibid, Chapter 3 </code><br><code>5</code><code> Ibid, Chapter 5 </code><br><code>6</code><code> Ibid, 12 </code>",POSITIVE
fitz_2665646_11,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665646.pdf,15,11,2665646,attachments/2665646.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>to have been collected starting from well before the introduction of the AI system, so that a proper </code><br><code>pre- and post- analysis comparison can be made.  </code><br><code> </code><br><code>3.</code><code> </code><code>THE AI ACT MUST ENSURE FAR GREATER TRANSPARENCY AND EXPLAINABILITY </code><br><code>The AI Act must require more openness, transparency, and explainability of AI systems and their </code><br><code>use, the decisions that are made, and significantly, must focus not just on ensuring transparency to </code><br><code>the </code><code>users</code><code> of the systems, but also to those individuals impacted by AI or AI-assisted decisions. </code><br><code>Transparency is a fundamental aspect of an adversarial process that underpins the right to a fair trial, </code><br><code>and human rights standards require that as a general rule defendants should be given unrestricted </code><br><code>access to their case-file,</code><code style=""font-weight: 1000; background-color: #FF0000;"">60</code><code> and to be given the opportunity to comment on the evidence used against </code><br><code>them.</code><code style=""font-weight: 1000; background-color: #FF0000;"">61</code><code> The procedural requirement of an adversarial process is not one that is limited to substantive </code><br><code>criminal proceedings ‚Äì it also applies in the context of pre-trial decision-making processes, especially </code><br><code>for decisions on the deprivation of liberty.</code><code style=""font-weight: 1000; background-color: #FF0000;"">62</code><code> </code><br><code>However, the AI Act falls far short of ensuring sufficient transparency that would ensure that the right </code><br><code>to a fair trial and the right to liberty can be exercised effectively. In particular, it is disappointing that </code><br><code>while the AI Act recognises the importance of transparency, it prioritises the interests of the users of </code><br><code>AI systems, and commercial interests, over the fundamental rights of those affected by these systems.  </code><br><code>AI systems must be transparent for both users and affected individuals </code><br><code>Article 13 provides for the transparency and provision of information to users in relation to high-risk </code><br><code>AI systems. This includes that they must be designed and developed in such a way to ensure that </code><code>‚Äútheir </code><br><code>operation is sufficiently transparent‚Äù</code><code> and to </code><code>‚Äúenable users to interpret the system‚Äôs output and use it </code><br><code>appropriately‚Äù</code><code>. Information must also be provided on its intended purpose, the ""</code><code>level of accuracy </code><br><code>robustness</code><code>‚Äù and ‚Äú</code><code>accuracy</code><code>‚Äù, potential fundamental rights issues and human oversight measures, and </code><br><code>details about input data and training data used.  </code><br><code>This information provision is a positive measure, but this attempted transparency safeguard fails at a </code><br><code>key juncture ‚Äì ensuring meaningful transparency and explainability to those </code><code>impacted</code><code> by the system, </code><br><code>not just the user, so that individuals subjected to AI systems‚Äô decisions are given information about </code><br><code>the decision, are able to understand that information, and are able to use that in any contestation or </code><br><code>challenge to that decision.  </code><br><code>By contrast, the requirements set out in Article 60 and Annex VIII for what information is to be </code><br><code>provided on the publicly-accessible EU database for stand-alone high-risk AI systems, do not replicate </code><br><code>the same level of transparency as that which is intended to be provided to AI system users. The </code><br><code>database does not require the same level of information to be publicly available, such as information </code><br><code>on the level of accuracy, performance and intended subjects, potential risks, fundamental rights </code><br><code>issues, human oversight measures and the training and testing data sets used.  </code><br><code> </code><br><code>60</code><code> ECtHR, </code><code>Beraru v Romania</code><code> App. No. 40107/04 (Judgment of 18 March 2014) </code><br><code>61</code><code> ECtHR, </code><code>Kuopila v Finaland</code><code>, App. No. 27752/95 (Judgment of 27 April 2000) </code><br><code>62</code><code> Access to Information Directive, Article 7(1); ECtHR, </code><code>Wloch v Poland</code><code>, App. No. 27785/95 (Judgment of 19 </code><br><code>October 2000) </code>",POSITIVE
fitz_2663398_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2663398.pdf,2,1,2663398,attachments/2663398.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>ResMed consultation response </code><br><code>Artificial Intelligence Act </code><br><code> </code><br><code>General legal framework:</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed welcomes the proposal for horizontal legislation overseeing AI, the proposed efforts </code><br><code>to ensure the safety and trustworthiness of AI, and the creation of an ecosystem to support </code><br><code>innovation and uptake of AI within the EU and beyond. To support this ambition, ResMed </code><br><code>urges that the rules surrounding AI are clear and workable, and the appropriate guidelines are </code><br><code>provided by regulators to implement the proposed requirements across the AI value chain, in </code><br><code>particular as regards interaction with existing sectoral legislation. </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed welcomes the Commission defining AI in coordination with the OECD, prohibiting </code><br><code>certain practices and taking a risk-based approach to regulating AI systems. </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed calls upon the Commission to ensure the legislative framework is developed with </code><br><code>future innovations in mind, to ensure that new developments are supported and encouraged </code><br><code>rather than overregulated.   </code><br><code> </code><br><code>Ensuring harmonisation with the wider regulatory framework: </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed welcomes the effort for harmonisation with other existing legislation but urges that </code><br><code>the legislation reduces overlap and simplifies compliance with other related legislation to the </code><br><code>greatest extent possible, of particular note are the Medical Devices Regulation (MDR), the In </code><br><code>Vitro Diagnostics Regulation (IVDR), the General Data Protection Regulation (GDPR), the Data </code><br><code>Governance Act (DGA) and the European Health Data Space (EHDS). </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed welcomes the streamlining of additional requirements through the existing </code><br><code>conformity assessment process involving notified bodies and urges that the proposed </code><br><code>legislation does not create duplication or overregulation. Whilst, looking forward to more </code><br><code>clarity on how the AIA and MDR/IVDR conformity assessments will function in practice, how </code><br><code>overlap of technical documentation can be reduced and how the competent authorities will </code><br><code>be supported in overseeing and evaluating AI systems. </code><br><code>‚Ä¢</code><code> </code><br><code>In order to minimise any challenges which will arise due to the ex-ante conformity assessment, </code><br><code>ResMed calls for the Commission to clarify the interplay between legislation which overlaps </code><br><code>with the AI regulation and encourage an approach that relies on self-regulation and industry </code><br><code>standardisation. </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed looks forward to further clarity on how healthcare AI products or the safety </code><br><code>components of products which fall outside of the scope of Rule 11 of the MDR will be </code><br><code>regulated, in particular those involved in research. </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed looks forward to further clarity on post-market surveillance obligations, the types of </code><br><code>data and experiences to be collected, and how such data will need to be stored and protected, </code><br><code>such that the proposed legislation does not create duplication or overregulation with </code><br><code>MDR/IVDR post-market requirements. </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2660159_4,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2660159.pdf,10,4,2660159,attachments/2660159.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>June 2021 </code><br><code>Page 4 of 10 </code><br><code>Issues to be clarified:</code><code> </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><br><code>Legally sound terminology for ‚Äúclassification‚Äù </code><br><code> </code><br><code>In Article 7, the regulation grants the EU Commission the additional authority to supplement the list of </code><br><code>high-risk systems in Annex III on the basis of vaguely defined criteria. Both the criteria and the </code><br><code>classification to be applied here require a transparent and democratically monitored procedure. </code><br><code> </code><br><code>The regulation of ‚Äúprohibited‚Äù AI applications in Article 5 contains many undefined legal terms, </code><br><code>leaving much room for interpretation. For example, there is the question of whether it is possible to </code><br><code>determine on a legally sound basis what exactly is meant by ‚ÄúAI systems‚Äù that exploit any of the </code><br><code>vulnerabilities of a specific group of persons due to their physical disability in order to influence their </code><br><code>behaviour with the result of psychological harm. It is important to ensure that evaluation of union </code><br><code>activity in public spaces using AI as a union-busting strategy is prevented. </code><br><code> </code><br><code>The following areas are of further relevance to the world of work and thus to the classification as high </code><br><code>risk: Social law (Recital 37), ‚Äúlaw enforcement authorities‚Äù (Recital 38), border control management </code><br><code>(Recital 39) and administration of justice and democratic processes (Recital 40). It is to be welcomed </code><br><code>that AI systems intended for administration or for democratic processes are assessed as high risk. With </code><br><code>regard to works council elections, AI systems should only be permissible under very narrow conditions. </code><br><code> </code><br><code>We appreciate that the rules are also to apply to applications established outside the EU. This already </code><br><code>is the case of many systems used in the world of work. The exception of Article 2(4) is problematic due </code><br><code>to complete exclusion of authorities from third countries and international bodies that are active in the </code><br><code>area of law enforcement. Particularly in law enforcement and police operations, unlimited and </code><br><code>unregulated use of AI can lead to massive violations of the law under certain circumstances. </code><br><code> </code><br><code>It is unclear whether and which mechanism shall take effect if a high-risk application develops the </code><br><code>criteria of a ‚Äúprohibited‚Äù system in the course of ‚Äúlearning‚Äù (that is, the data-driven further </code><br><code>development of its properties), i.e. if, on the basis of the collected data, the conduct of a person can be </code><br><code>subliminally influenced. In this regard, neither Title II (prohibited applications) nor Title III (high-risk </code><br><code>applications) regulations contain resolution mechanisms. </code><br><code> </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><br><code>Rollout of biometric real-time monitoring ‚Äúthrough the back door‚Äù ‚Äì including the working life </code><br><code> </code><br><code>The exception to the regulation under Article 5(1)(d), which corresponds to the general exception </code><br><code>under Article 2(4) is particularly problematic as it allows the use of remote biometric identification </code><br><code>systems in ‚Äúreal time‚Äù in publicly accessible spaces for the purpose of law enforcement, such as for the </code><br><code>prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons </code><br><code>or a terrorist threat. This legitimises expansion of surveillance options using the collection of biometric </code><br><code>real-time data, such as using the argument of a constant threat to physical safety in the context of a </code><br><code>pandemic, for example. Employees working in the public realm ‚Äì such as the police, emergency </code><br><code>services, street cleaners or local public transport ‚Äì would also be subject to surveillance possibilities in </code><br><code>the employment relationship that go further than the surveillance permitted in the employment </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665582_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2665582.pdf,3,1,2665582,attachments/2665582.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>1 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>S</code><code>haping t</code><code>h</code><code>e </code><code>e</code><code>thical dimensions of smart information </code><br><code>systems‚Äì a Eu</code><code>r</code><code>o</code><code>p</code><code>e</code><code>a</code><code>n perspective (SHERPA)</code><code> </code><br><code> </code><br><code> </code><br><code>Feedback to the  </code><br><code>European Commission on its  </code><br><code> </code><br><code>Proposal for a legal act of the European Parliament and the </code><br><code>Council laying down requirements for Artificial Intelligence </code><br><code> </code><br><code>Based on research from the SHERPA project </code><br><code> </code><br><code> </code><br><code> </code><br><code>August 6, 2020</code><code>	</code><br><code> </code><br><code> </code><br><code> </code><br><code>This project has received funding from the </code><br><code>European Union‚Äôs Horizon 2020 Research and Innovation Programme </code><br><code>Under Grant Agreement no. 786641</code>",NO_FOOTNOTES_ON_PAGE
fitz_2662602_3,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662602.pdf,4,3,2662602,attachments/2662602.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Technische Hochschule Deggendorf </code><br><code>Dieter-G√∂rlitz-Platz 1 </code><br><code>94469 Deggendorf</code><code> </code><br><code> </code><br><code>Tel.: +49 991 3615-0 </code><br><code>Fax: +49 991 3615-297 </code><br><code> </code><br><code>www.th-deg.de </code><br><code>info@th-deg.de </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Bezug zu KI adressieren, da sie (in Zukunft) ggf. auch ohne den expliziten Einsatz von KIMethoden implementiert werden k√∂nnten. </code><br><code> </code><br><code>Die Definition von konkreten Anwendungsf√§llen ohne expliziten Bezug zu KI w√§re aus den </code><br><code>gleichen Gr√ºnden daher auch in </code><code>Article 6 (Classification rules for high-risk AI systems)</code><code> f√ºr </code><br><code>eine pr√§zisere Definition von ‚ÄûHochrisikoanwendungen‚Äú hilfreich</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code>. Es ist zudem aus dem </code><br><code>Verordnungsvorschlag unklar, was die Europ√§ische Kommission konkret unter ‚Äûsicheren‚Äú </code><br><code>Anwendungen versteht. Dar√ºber hinaus ist offen, wie die ‚ÄûSicherheit‚Äú in einzelnen </code><br><code>Anwendungsf√§llen √ºberhaupt hergestellt werden k√∂nnte, insbesondere da viele Methoden des </code><br><code>maschinellen Lernens den Faktor ‚ÄûUnsicherheit‚Äú fest in ihrer Funktionsweise integriert haben. </code><br><code> </code><br><code>Auch droht durch den Verordnungsvorschlag in Verbindung mit bestehenden Regulierungen </code><br><code>die Gefahr von widerspr√ºchlichen und doppelten Anforderungen, u.a. bei der automatisierten </code><br><code>Kreditvergabe</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code>. </code><br><code> </code><br><code>4. Unerf√ºllbare Anforderungen an ‚ÄûHochrisikoanwendungen‚Äú</code><code style=""font-weight: 1000; background-color: #FF0000;"">8</code><code> </code><br><code>Neben der in </code><code>Article </code><code>6 sehr breit gefassten Definition von ‚ÄûHochrisikoanwendungen‚Äú sieht der </code><br><code>Verordnungsvorschlag u.a. in </code><code>Article 11 (Technical documentation)</code><code> entsprechende Dokumentationspflichten, in </code><code>Article 60 (EU database for stand-alone high-risk AI systems)</code><code> </code><br><code>Registrierungspflichten, sowie in </code><code>Article 62 (Reporting of serious incidents and of </code><br><code>malfunctioning)</code><code> Meldepflichten vor. Diese Anforderungen an die Entwicklung oder den Einsatz </code><br><code>von KI in sicherheitskritischen Anwendungsbereichen sind vergleichbar mit dem Betrieb von </code><br><code>Atomkraftwerken oder der Entwicklung von Flugzeugen. Sie erscheinen daher als </code><br><code>innovationshemmend und unverh√§ltnism√§√üig. </code><br><code> </code><br><code>Zur Umsetzung der in </code><code>Article 64 (Access to data and documentation)</code><code> beschriebenen Verfahren </code><br><code>m√ºsste </code><br><code>die </code><br><code>Europ√§ische </code><br><code>Kommission </code><br><code>praktisch </code><br><code>die </code><br><code>EU-weite </code><br><code>KI-Kompetenz </code><br><code>aller </code><br><code>Unternehmen, Hochschulen und Expertinnen und Experten in den entsprechenden Beh√∂rden </code><br><code>b√ºndeln und Hunderte Milliarden Euro in eine eigene Infrastruktur investieren.</code><code> </code><code>Die </code><br><code>Anforderungen an die in </code><code>Article 53 (AI regulatory sandboxes)</code><code> beschriebenen Sandboxtests </code><br><code>erfordern, dass das gesamte aus Daten und KI bestehende geistige Eigentum mit den </code><br><code>entsprechenden Beh√∂rden geteilt werden m√ºsste. Dies erscheint als unverh√§ltnism√§√üig und </code><br><code>nicht realisierbar. Zudem bestehen offene Fragen zur Haftung, falls durch Sandboxtests Dritte </code><br><code>Zugriff auf das geistige Eigentum erhalten sollten.</code><code> </code><br><code> </code><br><code>Aus diesen Gr√ºnden w√ºrde der Verordnungsvorschlag die Entwicklung oder den Einsatz von </code><br><code>entsprechenden sicherheitskritischen KI-Anwendungen, wie z.B. jegliche sicherheitskritischen </code><br><code>Assistenzsysteme in Fahrzeugen, in der Europ√§ischen Union nahezu unm√∂glich machen.  </code><br><code> </code><br><code>Bemerkenswert ist zudem, dass der finale Verordnungsvorschlag im Vergleich zu den </code><br><code>vorherigen Entw√ºrfen mit Hinblick auf den Einsatz von KI in sozialen Netzwerken deutlich </code><br><code>entsch√§rft wurde. Darin werden soziale Netzwerke nun gar nicht mehr konkret adressiert, </code><br><code>obwohl gerade hier eine realistische Gefahr f√ºr die Beeinflussung durch KI liegt</code><code style=""font-weight: 1000; background-color: #FF0000;"">9</code><code>. </code><br><code> </code><br><code> </code><br><code> </code><br><code>6</code><code> </code><code>https://www.politico.eu/article/6-key-battles-europes-ai-law-artificial-intelligence-act/</code><code> </code><br><code>7</code><code> </code><code>https://amp2.handelsblatt.com/politik/international/kuenstliche-intelligenz-neue-eu-regeln-fuer-ki-koennten-fuereuropa-zum-nachteil-werden-sorge-ueber-ausbleibende-investitionen/27113210.html</code><code> </code><br><code>8</code><code> </code><code>Ich m√∂chte mich insbesondere bei Tobias Manthey von der EvoTegra GmbH f√ºr die umfangreichen Diskussionen </code><br><code>zum Thema ‚ÄûHochrisikoanwendungen‚Äú w√§hrend der Verfassung dieser Stellungnahme bedanken. </code><br><code>9</code><code> </code><code>https://www.cnbc.com/2020/09/19/2020-presidential-election-facebook-and-information-manipulation.html</code><code> </code>",POSITIVE
fitz_2662219_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2662219.pdf,2,2,2662219,attachments/2662219.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>At this stage, EuroGeograpphics members are both, AI users and providers of data that are very widely </code><br><code>feeding AI services and business. AI is used for processing geospatial information, in particular for </code><br><code>increasing the efficiency of their quality assurance and management ‚Äì the calling card of NMCA data. </code><br><code>For example, some of our members have been combining high-resolution elevation analysis with deep </code><br><code>learning techniques, to provide policy-makers with vital information for the transition to solar energy, </code><br><code>exploring how to use AI for a wide range of activities including land cover mapping and the creation </code><br><code>of 3D data is high on the agenda. </code><br><code> </code><br><code>Furthermore, our members have fully embraced the European strategy for data, and they already </code><br><code>contribute substantially to make it a success story. The crucial contribution is primarily by </code><br><code>implementing the Open data and PSI directive, which will establish trusted mechanisms and services </code><br><code>for the re-use, sharing and pooling of data that are essential for the development of data-driven AI </code><br><code>models of high quality.</code><code>   </code><br><code>The Regulation proposal confirms that </code><code>some areas of AI deployment will fall under existing legislation </code><br><code>and will be fully coherent with the Commission‚Äôs overall digital strategy. This is a strong stir for our </code><br><code>members, since the stable governance principles and data protection established thereof, encourages </code><br><code>our members to create and test high‚Äìrisk AI systems, for example in the transport field. This possibility </code><br><code>will provide many opportunities for NMCAs to contribute to the digital single market. The </code><br><code>development of programmes, research, projects, or support, that will allow the growing of the </code><br><code>diffusion of AI within our member organizations, will be highly welcomed. </code><br><code> </code><br><code>We are aware that this Regulation applies to providers placing on the market or putting into service </code><br><code>AI systems and we are not there yet. Nevertheless, our members' authoritative data, high quality </code><br><code>datasets, are back bones to many of the critical infrastructures as defined in the proposed Regulation. </code><br><code>In recent years, we have seen an explosion in location-based AI systems. Some of our members are </code><br><code>already in the testing phase of geospatial data-based AI applications, and this regulatory framework </code><br><code>is the supportive direction for that, which is considered a good start. </code><br><code>The Regulation proposal has positively raised awareness of AI and is intended to balance very sensitive </code><br><code>issues such as fundamental rights, public safety, and innovation. It is highly desirable to have </code><br><code>definitions in a flexible manner, not to hamper innovations.  </code><br><code>Legal certainty, risk assessment levels, regulated use, and concept of trust by increasing the trust for </code><br><code>EU made AI products are the main benefits of the proposal from Eurogeographics members‚Äô point of </code><br><code>view. </code><br><code>We express our interest in further development of this legal debate. If you need any further </code><br><code>clarifications, please do not hesitate to contact: </code><code>lea.bodossian@eurogeographics.org</code><code>  </code><br><code> </code><br><code>L√©a Bodossian</code><code> </code><br><code>Secretary General & Executive Director</code><code> </code><br><code>w. </code><code>www.eurogeographics.org</code><code> </code><br><code>e. </code><code>lea.bodossian@eurogeographics.org</code><code> </code><br><code>m. + 32 485 68 79 05  </code><br><code> </code><br><code>EuroGeographics Head Office  </code><br><code>Rue du Nord 76/Noordstraat 76 </code><br><code>1000 Brussels, BELGIUM </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665642_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665642.pdf,31,1,2665642,attachments/2665642.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>An approach for a fundamental rights</code><br><code>impact assessment to automated</code><br><code>decision-making</code><br><code>Heleen L. Janssen*</code><br><code>Introduction</code><br><code>Automated decision-making (ADM) is already used</code><br><code>across a variety of societal contexts,</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> </code><code>from simplistic</code><br><code>models that help online service providers to carry out</code><br><code>operations on behalf of their users, for instance for billing</code><br><code>purposes, or to create a better functioning social</code><br><code>network</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>‚Äîto more complex profiling algorithms that</code><br><code>filter systems for targeted advertisements, credit scoring,</code><br><code>recommender systems, IoT-applications, insurance proposals, health care applications, or examination to enter</code><br><code>education or training.</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><br><code>Meanwhile, dynamics of ADM may collide with and</code><br><code>exert pressure on fundamental rights.</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> </code><code>For instance,</code><br><code>Kramer</code><br><code>and</code><br><code>others</code><br><code>described</code><br><code>how</code><br><code>Facebook</code><br><code>Key Points</code><br><code>ÔøΩ</code><code> Companies expect great and promising benefits</code><br><code>from automated decision-making with personal</code><br><code>data; however, scientific research indicates that</code><br><code>legal uncertainty exists among private controllers</code><br><code>with the interpretation of provisions relevant to</code><br><code>automated decision-making under the General</code><br><code>Data Protection Regulation (GDPR).</code><br><code>ÔøΩ</code><code> Article 35 GDPR obliges private controllers to execute a Data Protection Impact Assessment</code><br><code>(DPIA) prior to deploying automated decisions</code><br><code>on humans. Assessing potential fundamental</code><br><code>rights impacts is part of that DPIA.</code><br><code>ÔøΩ</code><code> The objective of this article is to provide private</code><br><code>controllers with a practical approach for a DPIA</code><br><code>to automated decision-making to detect potential</code><br><code>impacts on fundamental rights. The approach</code><br><code>indicates levels of impacts and types of measures</code><br><code>a controller should consider to achieve an appropriate risk management.</code><br><code>ÔøΩ</code><code> The impact assessment is based on four benchmarks: (i) to identify fundamental rights potentially at risk; (ii) to identify risks occurring in</code><br><code>their ADM systems at design stages and during</code><br><code>operation; (iii) to balance fundamental rights</code><br><code>risks and controller interests involved; and (iv) to</code><br><code>establish to what extent data subjects exercise</code><br><code>control over data processing.</code><br><code>ÔøΩ</code><code> By responding to the benchmarks, controllers</code><br><code>identify risk levels that indicate the type of measures that should be considered to achieve fundamental rights compliant ADM.</code><br><code>ÔøΩ</code><code> This approach enables controllers to give account</code><br><code>towards data subjects and supervisory authorities</code><br><code>about envisaged risk management to potential</code><br><code>impacts on fundamental rights. The proposed</code><br><code>approach seeks to foster compliant, fair, and</code><br><code>transparent automated decision-making.</code><br><code>*</code><br><code>Department of Computer Science and Technology, Compliant and</code><br><code>Accountable Systems Research Group, University of Cambridge,</code><br><code>Cambridge, UK. E-mail: heleen.janssen@cl.cam.ac.uk. The author</code><br><code>acknowledges the support of Engineering and Physical Sciences Research</code><br><code>Council, UK (Grant number EP/P024394/01). The author wishes to thank</code><br><code>the editor of the journal, the reviewers, as well as John Morijn, Janneke</code><br><code>Gerards and Jatinder Singh for their useful comments on this article.</code><br><code>1</code><br><code>Information Commission Officer, ‚ÄòWhat is Automated Individual</code><br><code>Decision-making and Profiling‚Äô (2018)</code><code> <</code><code>https://ico.org.uk/media/fororganisations/guide-to-data-protection/guide-to-the-general-data-protec</code><br><code>tion-regulation-gdpr/automated-decision-making-and-profiling-1-1.pdf</code><code>></code><br><code>accessed 18 February 2020.</code><br><code>2</code><br><code>Hojung Kim, Joseph Giacomin and Robert Macredie, ‚ÄòA Qualitative</code><br><code>Study of Stakeholders‚Äô Perspectives on the Social Network Service</code><br><code>Environment‚Äô (2012) 30 International Journal of Human-Computer</code><br><code>Interaction 965.</code><br><code>3</code><br><code>Mireille Hildebrandt and Serge Gutwirth, ‚ÄòGeneral Introduction and</code><br><code>Overview‚Äô in Mireille Hildebrandt and Serge Gutwirth (eds),</code><code> Profiling the</code><br><code>European Citizen</code><code> (Springer, Dordrecht 2008) 1.</code><br><code>4</code><br><code>For a comprehensive overview of ADM impacts on fundamental rights,</code><br><code>see eg Rinie van Est, Joost Gerritsen, Linda Kool, ‚ÄòHuman rights in</code><br><code>the robot age: Challenges arising from the use of robotics, artificial</code><br><code>intelligence, and virtual and augmented reality‚Äô, Expert report written</code><br><code>for the Committee on Culture, Science, Education and Media of the</code><br><code>V</code><br><code>C</code><code> The Author(s) 2020. Published by Oxford University Press. All rights reserved.</code><br><code>For permissions, please email: journals.permissions@oup.com</code><br><code>76</code><br><code>ARTICLE</code><br><code>International Data Privacy Law,</code><code> 2020, Vol. 10, No. 1</code><br><code>Downloaded from https://academic.oup.com/idpl/article/10/1/76/5788543 by University Library, University of Amsterdam user on 19 June 2021</code>",POSITIVE
fitz_2665321_8,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665321.pdf,9,8,2665321,attachments/2665321.pdf#page=8,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Page 8 / 9 </code><br><code> </code><br><code>There would be a high risk that, as a result of the bureaucratic requirements, </code><br><code>many innovations benefitting customers and the society might be stifled and </code><br><code>that European insurers, for instance, would be placed at a competitive disadvantage to foreign providers. </code><br><code> </code><br><code>Furthermore, it is crucial for the design of the supervisory structures that </code><br><code>harmonised law enforcement is being ensured within the EU. The AI Regulation operates with numerous uncertain legal terms which must be interpreted in a harmonised way across Europe. This is particularly challenging </code><br><code>with regard to cross-border matters. In addition to the national supervisory </code><br><code>authorities, the ‚ÄúArtificial Intelligence Board‚Äù will play a major role in this </code><br><code>respect. The establishment and structure of the Board, as provided for in </code><br><code>the Regulation by the EU Commission, is welcomed. In its function as </code><br><code>guardian of the Treaties, the EU Commission pursues the fundamental objective of harmonized Union law enforcement. It is therefore only logical that </code><br><code>essential tasks of AI regulation are in the Commission‚Äôs hands and that the </code><br><code>Commission is being assisted and advised by the Board in carrying out </code><br><code>these tasks. From the point of view of the insurance industry, it is therefore </code><br><code>the right approach that the Board is not given any power to issue guidelines </code><br><code>or monitor them. The supporting and advisory role of the Board is evident </code><br><code>from the fact that it is authorised to issue opinions, recommendations or </code><br><code>written contributions on matters related to the implementation of the Regulation. </code><br><code> </code><br><code>It is crucial in this context that the ‚ÄúArtificial Intelligence Board‚Äù does not </code><br><code>follow the blueprint of the European Data Protection Board (EDPD). According to recital 139 GDPR, the European Data Protection Board should also </code><br><code>‚Äúcontribute to the consistent application of this Regulation throughout the </code><br><code>Union (‚Ä¶) and promoting cooperation of the supervisory authorities </code><br><code>throughout the Union‚Äù. Pursuant to Article 70 GDPR, the Board can issue a </code><br><code>whole series of ‚Äúguidelines, recommendations, and best practices‚Äù for this </code><br><code>purpose. While the decisions by the EU Commission are directly subject to </code><br><code>judicial control, the powers of the EDPB have a quasi-binding effect. However, they are not subject to direct judicial review. Furthermore, the Board </code><br><code>interprets the GDPR very broadly, sometimes contrary to the text of the law </code><br><code>and without taking account of what is relevant in practice, and to the detriment of controllers. </code><br><code> </code><br><code>The governance structure of the Board provided in the Regulation is therefore appreciated. Due to the fact that the EU Commission chairs the Board, </code><br><code>it is ensured that the results of the Board are based on common decisionmaking. The governing function of the EU Commission is also reflected by </code><br><code>the fact that the Board‚Äôs rules of procedure require the Commission‚Äôs approval. This way it is being prevented that the Board becomes independent, </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665235_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665235.pdf,5,3,2665235,attachments/2665235.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>right to deletion of the data subject (or the so-called right be forgotten) must be solved and balanced </code><br><code>with the framework of the GDPR. </code><br><code>High-risk AI systems </code><br><code>‚ñ™</code><code> </code><br><code>We believe that </code><code>‚Äúthe management and operation of critical infrastructure‚Äù</code><code> currently defined as a highrisk category is too broad and against the objective of focusing only on well-defined areas of risk. Such </code><br><code>a broad category could entail a wide range of back-office AI solutions (with regards to procurement, </code><br><code>invoicing, payments etc.) in the scope that do not cause any harms to the safety of individuals. Instead, </code><br><code>we recommend narrowing down the high-risk category to a complete list of affected sectors and </code><br><code>affected operational areas excluding commercial and administrative departments that are not related </code><br><code>to the underlying risks. </code><br><code>‚ñ™</code><code> </code><br><code>Based on the definition of remote biometric identification systems in the proposal, it is not clear whether </code><br><code>unlocking devices or having remote access to machines, systems with the usage of biometric data </code><br><code>would be considered as high-risk AI systems. We believe further clarification in this high-risk category </code><br><code>will be essential as not all remote biometric identification systems in all contexts pose a risk to the </code><br><code>fundamental rights and safety of individuals. Also, not all biometric identification systems are AI </code><br><code>systems. This would lead to a different (legal) treatment for some biometric identification systems </code><br><code>falling under the regulation and others not, depending on which underlying technology they use. </code><br><code>‚ñ™</code><code> </code><br><code>The high-risk category of </code><code>‚Äúaccess to and enjoyment of essential private services and public services </code><br><code>and benefits‚Äù</code><code> was left very broad that could include the identification of a wide range of AI use cases </code><br><code>used in the private and public sector. The future extension of this category could have detrimental </code><br><code>effect on AI adoption in the public services. We would recommend EU decision-makers to narrow </code><br><code>down the category to concrete use cases based on the classification of high-risk AI systems outlined </code><br><code>in the proposal.  </code><br><code>‚ñ™</code><code> </code><br><code>With regards to the revision and extension of the high-risk AI systems list (Annex III), we recommend </code><br><code>the further clarification of the overall process for the sake of legal certainty. We believe that clear </code><br><code>criteria for the evaluation of the definition of a High-risk AI system are essential for implementing </code><br><code>privacy by design and for designing enhanced technologies to meet the high level of protection of the </code><br><code>fundamental rights of concerned individuals rather than listing certain AI systems.    </code><br><code>‚ñ™</code><code> </code><br><code>Currently, the legislation does not provide any information on who could initiate the extension process, </code><br><code>the potential involvement of industry, a transition period for providers and users to comply with the </code><br><code>changes, the role of the EU AI Board and an independent Expert Group to provide technical input. We </code><br><code>believe that all these details should be explicitly included in the proposal. The process to periodically </code><br><code>review the list of high-risk AI systems should be clear, transparent, based on evidence and take place </code><br><code>in consultation with involved stakeholders. </code><br><code>Mandatory Requirements </code><br><code>‚ñ™</code><code> </code><br><code>For the Artificial Intelligence Act to be effective and able to be implemented and enforced, there are </code><br><code>several mechanisms that should be taken into consideration by EU decision-makers. First and </code><br><code>foremost, there should be a common understanding on the concept and meaning of several ethical </code><br><code>principles (i.e. bias). Also, they should only be included in the act if there is a concrete idea of how </code><br><code>they can be enforced through a regulatory process. It will be crucial to clarify these concepts in order </code><br><code>to set up operational requirements for businesses as they should not be in a position to interpret ethical </code><br><code>concepts that often have several valid subjective meanings, even between the various EU member </code><br><code>states. </code><br><code>‚ñ™</code><code> </code><br><code>When it comes to the operationalization of mandatory requirements, we strongly support the European </code><br><code>Commission to work with standardization bodies instead of focusing on the preparation of technical </code><br><code>specifications. This will enable engagement by public and private sectors to develop a flexible and </code><br><code>innovation-friendly regulatory framework at EU level and to define the most appropriate standards for </code><br><code>the current technology and markets. For the sake of legal certainty, to drive innovations and to ensure </code><br><code>competitiveness of companies it will be essential to establish a timeline for the set-up of standards.  </code><br><code>Data Governance </code><br><code>‚ñ™</code><code> </code><br><code>We encourage the European Commission to build upon existing EU legislation while also allowing for </code><br><code>measures that will help promote trustworthy AI. In case of data governance related requirements, it </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665501_6,other,../24212003_requirements_for_artificial_intelligence/attachments/2665501.pdf,10,6,2665501,attachments/2665501.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>6 </code><br><code>the fact that the AI systems meet the required standards. Moreover, this could also improve legal </code><br><code>certainty by further reducing the risk of liability claims for unintended consequences of these highrisk AI systems. In such case, specific provisions about the status of auditors would be welcome. </code><br><code>Other members, however, disagreed on this point. They consider that the application of thirdparty conformity assessments would go against the goal of taking a risk-based approach. They </code><br><code>also noted that the use of third-party conformity assessment for only one high-risk AI system </code><br><code>makes sense because that system, in different circumstances, is prohibited. </code><code> </code><br><code>6) The value of the proposed CE marking is questioned </code><br><code>The CE-marking for AI applications in Europe can help to create trust and assurance in technology </code><br><code>that is complex to understand for users. However, with the CE-marking, certain risks arise such </code><br><code>as a potential false flag for unreliable products being marketed in Europe. To achieve the </code><br><code>trustworthiness of such a CE-marking, there is a big need to have standards to support </code><br><code>organizations to comply with the requirements. As it is, these standards do not yet exist, and it </code><br><code>will take a huge effort to ensure that they are published in time before this regulation comes into </code><br><code>effect.  </code><br><code>In this regard, when setting up these standards or best practices, it is also important to consider </code><br><code>the views of various stakeholders, including civil society, so as to ensure that compliance with the </code><br><code>requirements is not only claimed but can also be demonstrated and lead to </code><code>earned</code><code> trust. </code><br><code>Moreover, the CE-marking has been developed for ‚Äòhardware‚Äô and it is unclear how it would work </code><br><code>for software, from the basics ‚Äì of how it can be affixed ‚Äì to the more complicated issue of how it </code><br><code>would work in practice. This provision seems telling of the Commission‚Äôs approach to AI coming </code><br><code>from a ‚Äòproduct liability‚Äô perspective, which can present serious limitations in certain instances, </code><br><code>given the nature of AI. Indeed, many of the obligations set out in the Regulation seem most </code><br><code>appropriate for the products covered in Annex II, such as the requirement to comply with </code><br><code>harmonized standards or to undergo a new conformity assessment every time there is a </code><br><code>substantial modification. These obligations may, however, be less suitable for AI services, since </code><br><code>these services can often be deployed in an almost unlimited range of scenarios, and are already </code><br><code>frequently updated.  </code><br><code>7) There is a need for guidance on how to implement the various requirements </code><br><code>As noted above, many of the requirements contain vague language, and will hence need to be </code><br><code>clarified through guidance from the European Commission in order to ensure their </code><br><code>implementation. In this regard, lessons should be drawn from the implementation of the GDPR, </code><br><code>where a lack of guidance in combination with high potential fines prompted organizations to </code><br><code>massively seek consent from data subject, leading to overflooding citizens with messages and </code><br><code>legal uncertainty on how to comply. To avoid the difficulties in interpretation that seem to be </code><br><code>prevalent with the GDPR, it is important that guidance be provided at the European level (including </code><br><code>through the European Artificial Intelligence Board). Providers, importers and users of AI operate </code><br><code>in different countries and do business across the EU ‚Äì they hence need uniform guidance to </code><br><code>implement these requirements, regardless of the member state in which they primarily operate. </code><br><code>Attention should also be given to the way in which different types of AI work, such as platform and </code><br><code>general-purpose systems. These systems may not be high-risk in the form initially placed on the </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663064_4,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2663064.pdf,11,4,2663064,attachments/2663064.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>4 </code><br><code> </code><br><code>non ambigu√´s, ce qui signifie que m√™me les √©tiqueteurs humains les plus experts ne seront pas </code><br><code>d'accord et feront des erreurs. Une norme sans erreur rendrait l'apprentissage supervis√© </code><br><code>impossible et exclurait certaines des avanc√©es les plus prometteuses de la recherche en IA. On </code><br><code>peut notamment citer l‚Äôexemple de la sant√©.  </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>Plus g√©n√©ralement, les obligations de l‚Äôarticle 10 sont peu claires. A titre d‚Äôexemple, l‚Äôobligation </code><br><code>que les datasets d‚Äôentrainement ¬´ poss√®dent les propri√©t√©s statistiques appropri√©es ¬ª est </code><br><code>difficilement compr√©hensible.</code><code> </code><code>Une approche plus r√©aliste et pragmatique consisterait √† ce que </code><br><code>la proposition de r√®glement demande aux fournisseurs de faire des efforts raisonnables pour </code><br><code>atteindre les objectifs de l'article 10.   </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>En outre, l‚Äôarticle 10(5) indique que ¬´ dans la mesure o√π cela est strictement n√©cessaire pour </code><br><code>assurer la surveillance, la d√©tection et la correction des biais en relation avec les syst√®mes d'IA </code><br><code>√† haut risque, les fournisseurs de ces syst√®mes peuvent traiter les cat√©gories particuli√®res de </code><br><code>donn√©es √† caract√®re personnel vis√©es ¬ª. Sur ce point, il conviendrait d‚Äôinclure explicitement </code><br><code>dans le texte une base l√©gale pour le traitement des cat√©gories sp√©ciales de donn√©es </code><br><code>personnelles pour la surveillance des biais des syst√®mes d'IA, afin de clarifier que ce traitement </code><br><code>est autoris√© par le RGPD.   </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>L‚Äôobligation de tenue d‚Äôune documentation technique avant toute mise en service est </code><br><code>impraticable pour des startups et des jeunes soci√©t√©s, qui ont par d√©finition besoin de tester </code><br><code>leur produit pour it√©rer dessus avant ouverture plus large au march√© (article 11). </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>De la m√™me mani√®re, l‚Äôarticle 14 impose le recrutement tr√®s t√¥t dans le d√©veloppement d‚Äôune </code><br><code>soci√©t√© de personnel hautement qualifi√©, peu compatible avec le statut de startup.  </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>Les menaces de cybers√©curit√© √©tant en perp√©tuelle √©volution, il est inconcevable de garantir </code><br><code>qu‚Äôun syst√®me informatique, quel qu‚Äôil soit, est inviolable. Il convient d‚Äôassouplir les formulations </code><br><code>de l‚Äôarticle 15, notamment en s‚Äôinspirant de l‚Äôarticle 32 du RGPD.  </code><br><code> </code><br><code>La proposition de r√®glement pr√©voit que les autorit√©s de surveillance des march√©s puissent acc√©der aux </code><br><code>ensembles de donn√©es de formation, de validation et de test utilis√©s par le fournisseur. Lorsque cela </code><br><code>est n√©cessaire et sur demande motiv√©e pour √©valuer la conformit√© du syst√®me d'IA √† haut risque avec </code><br><code>les exigences de la proposition de r√®glement, les autorit√©s de surveillance peuvent se voir accorder </code><br><code>l'acc√®s au code source du syst√®me d'IA. A ce stade, cela soul√®ve un certain nombre de questions.  </code><br><code>‚ñ™</code><code> </code><br><code>√âtant donn√© qu‚Äôun mod√®le commercial sp√©cifique peut √™tre r√©glement√© par diverses autorit√©s </code><br><code>de march√© lorsqu‚Äôune entreprise op√®re sur plusieurs march√©s simultan√©ment, il sera essentiel </code><br><code>de clarifier quelle autorit√© de surveillance sera autoris√©e √† superviser l'IA d√©velopp√©e pour un </code><br><code>type d‚Äôactivit√©s donn√©. De plus, les sanctions devraient √™tre impos√©es sur la base d'un </code><br><code>catalogue clair, bas√© sur des infractions sp√©cifiques √©num√©r√©es. </code><br><code>‚ñ™</code><code> </code><br><code>Nous nous interrogeons sur la justification technique d‚Äôacc√©der aux codes sources du </code><br><code>fournisseur dans le cadre de la proposition de r√®glement. Si l‚Äôacc√®s aux codes sources constitue </code><br><code>un risque pour le fournisseur lui-m√™me, il peut aussi constituer un r√©el frein √† l‚Äôinnovation en </code><br><code>mati√®re d‚ÄôIntelligence Artificielle. Il semble difficile de mettre √† disposition un code source m√™me </code><br><code>√† une autorit√© publique ou √† un organisme notifi√©. Cette mesure ne semble pas respecter les </code><br><code>dispositions en mati√®re de cybers√©curit√© ainsi que le principe de proportionnalit√© au regard de </code><br><code>l‚Äôutilit√© de cette d√©marche. Il n‚Äôest pas certain que les autorit√©s de surveillance auront m√™me les </code><br><code>capacit√©s techniques pour √©valuer les codes sources. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2661982_3,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2661982.pdf,6,3,2661982,attachments/2661982.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>3 </code><br><code> </code><br><code>Definitions / delimitations </code><br><code>How AI is defined will determine whether a specific product or service falls under the </code><br><code>regulation, alternatively whether this is open for interpretation. A clear and concise </code><br><code>definition that leaves few grey areas is therefore of critical importance. The approach </code><br><code>taken is to base the definition on the techniques and methods being used, rather than </code><br><code>basing it on the features of the product or service. We believe this is a good approach </code><br><code>that leaves less ambiguity than many of the alternatives.  </code><br><code> </code><br><code>The proposed legal framework is meant to be future proof in its fundamental choices. </code><br><code>However, this appears to be an overly optimistic view in the light of the rapid </code><br><code>developments in the field. With respect to the approach taken to define AI, the </code><br><code>consequence is that frequent updates to Annex I will be needed as the field progresses </code><br><code>and new techniques are developed. </code><br><code> </code><br><code>The risk-based approach </code><br><code>The fact that the draft regulation has a risk-based approach, allows for different levels of </code><br><code>intervention for different AI applications, based on the associated risk. We welcome this </code><br><code>approach. Moreover, we appreciate that the draft regulation bans certain use cases of AI </code><br><code>that are not aligned with fundamental and shared values in Europe. The number of risk </code><br><code>categories and the orientation of each category seem appropriate.  </code><br><code> </code><br><code>The high-risk category includes AI applications with potential to bring societal benefits, </code><br><code>economic growth and enhanced innovation and global competitiveness. A balanced </code><br><code>regulation will increase the level of trust by citizens in such products and services, and it </code><br><code>will reduce uncertainty and risk for companies.  </code><br><code> </code><br><code>The regulation identifies two main categories of high-risk AI systems; firstly, those that </code><br><code>use AI systems as safety components of products subject to third party conformity </code><br><code>assessment, according to already existing regulations; secondly, AI application areas with </code><br><code>implications for the fundamental rights of citizens, in which a set of eight application areas </code><br><code>are listed. For seven of these, the regulation mandates a self-assessment scheme for the </code><br><code>ex-ante conformity assessment. The latter of the eight application areas, remote biometric </code><br><code>identification systems, shall be subject to third party conformity assessment.  </code><br><code> </code><br><code>It is expected that safety components based on AI technology will play an important role </code><br><code>in a wide range of products in the future. We find the draft regulation vague in its </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663375_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663375.pdf,2,1,2663375,attachments/2663375.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>28</code><code>th</code><code> July 2021 ‚Äì Secure Identity Alliance </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Facial Recognition Technologies: What‚Äôs at Stake and Why </code><br><code> </code><br><code>On April 21, 2021, the European Commission proposed </code><code>new rules and actions</code><code> on the development </code><br><code>and use of artificial intelligence (AI) systems with the stated objective to ‚Äúturn Europe into the global </code><br><code>hub for trustworthy artificial intelligence‚Äù.   </code><br><code> </code><br><code>Following a risk-based approach, the proposals will ban those AI systems considered a clear threat </code><br><code>to the safety, livelihoods and rights of people. Others will be rated on a risk scale from high to </code><br><code>minimal. In particular, all remote biometric identification systems are considered high risk and </code><br><code>subject to strict requirements. </code><br><code> </code><br><code>Secure Identity Alliance (SIA), whose members are European and global leaders in this fastemerging space, are supportive of the principles to ensure excellence and trust in AI systems, and </code><br><code>believe a fit-for-purpose framework has the potential to become an adopted standard around the </code><br><code>world ‚Äì as has been the case with the General Data Protection Regulation.  </code><br><code> </code><br><code>However, SIA believes that the rules governing the development and provision of AI systems should </code><br><code>be proportionate.  </code><br><code> </code><br><code>As the proposals stand today, there is a concern that some rules threaten to stifle innovation and </code><br><code>therefore undermine the stated goals of the Commission and the position of Europe as a leader in </code><br><code>this critical field of technology.  </code><br><code> </code><br><code>Ex-ante conformance assessment threat to IP & sovereignty   </code><br><code> </code><br><code>AI systems intended to be used for real time, remote biometric identification are classified as highrisk under the proposals and would require an ex-ante conformance assessment. This, SIA believes, </code><br><code>represents a threat to Intellectual Property if not properly managed with trusted third parties ‚Äì a </code><br><code>concept to be clarified. </code><br><code> </code><br><code>This may have a negative impact on the levels of AI system functionality and performance available </code><br><code>to European users and government agencies such as national security and law enforcement </code><br><code>agencies as developers may choose to bring lower performance, less IP-sensitive versions of their </code><br><code>technology to the EU in order to avoid the risk of IP leakage. </code><br><code> </code><br><code>In addition:  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>The rules imply a requirement to grant assessors access to sensitive data that are covered </code><br><code>by non-disclosure agreements, and would seem to run contrary to GDPR compliance  </code><br><code>‚Ä¢</code><code> </code><br><code>Similarly, the rules include a clause granting access through APIs. This represents a </code><br><code>significant vulnerability in terms of IT security, and a significant expense for developers  </code><br><code> </code><br><code>The rules lack critical definitions when it comes to identifying what a ‚Äòrelevant, representative, </code><br><code>free of errors and complete‚Äô training dataset is. At the same time, there is little scientific evidence </code><br><code>that simply balancing training datasets removes the need for ex-post performance assessment, </code><br><code>(ex post assessment being the state of the art practice for performance but also since a few </code><br><code>years for bias measurement as well).The EU should facilitate the access to high data quality for </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665548_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665548.pdf,4,3,2665548,attachments/2665548.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>- 3 - </code><br><code>6 August 2021 </code><br><code> </code><br><code> </code><br><code> </code><br><code>      </code><br><code> </code><br><code>Hogan Lovells </code><br><code> </code><br><code> </code><br><code>User obligations </code><br><code> </code><br><code>In accordance with Article 29(1) of the Draft Regulation, the primary obligation that applies to users </code><br><code>of high-risk AI systems is for them to use such systems in accordance with the instructions of use </code><br><code>that are supplied by the provider. </code><br><code> </code><br><code>By delegating the requirement to specify the precise obligations that need to be complied with to </code><br><code>the provider, the Commission is potentially creating a significant degree of uncertainty for users. </code><br><code>This is particularly true, given that it seems likely that both the content and quality of the instructions </code><br><code>that will be made available to users will vary significantly depending on the nature and </code><br><code>sophistication of the provider. It is also likely to create difficulties for providers, who will be forced </code><br><code>to consider what restrictions and requirements they should impose on their own customers when </code><br><code>using their products. </code><br><code> </code><br><code>Taking these considerations into account, we believe it will be easier for all parties to understand </code><br><code>and comply with the regulatory expectations placed upon them, if all of the obligations that apply </code><br><code>to users are explicitly set out within the Draft Regulation.  </code><br><code> </code><br><code>In addition, the Commission should consider which provisions of the Draft Regulation are intended </code><br><code>to apply to organisations that both use and develop high-risk AI systems in-house. Where this </code><br><code>scenario arises, it is currently unclear whether the organisation would be considered a user, a </code><br><code>provider, or both. </code><br><code> </code><br><code>Data governance standards </code><br><code> </code><br><code>Article 10(3) of the Draft Regulation states that providers are expected to use training, validation </code><br><code>and testing data sets that are ‚Äúrelevant, representative, free of errors and complete‚Äù.  </code><br><code> </code><br><code>While we acknowledge the importance of providers implementing appropriate data governance </code><br><code>measures in the development of high-risk AI systems, the current standard appears to be </code><br><code>excessively onerous and impractical to comply with. </code><br><code> </code><br><code>We suggest that the standard of data governance that providers are expected to adhere to should </code><br><code>be determined with reference to the primary objectives of this obligation. Namely that proportionate </code><br><code>steps should be taken by providers to ensure the training, validation and testing data sets are </code><br><code>created in such a manner so to ensure that AI systems are: (i) developed to an appropriate degree </code><br><code>of accuracy; (ii) mitigate the risks of algorithmic bias; and (iii) sufficiently representative of the </code><br><code>population or environment that the AI system is looking to model. </code><br><code> </code><br><code>Interrelationship with the GDPR </code><br><code> </code><br><code>As outlined in the </code><code>joint opinion</code><code> of the European Data Protection Board and European Data </code><br><code>Protection Supervisor, there is currently a lack of clarity on the relationship between the Draft </code><br><code>Regulation and EU‚Äôs data protection framework, particularly the GDPR. </code><br><code> </code><br><code>High-risk AI systems will often be heavily reliant on large volumes of personal data in their training, </code><br><code>testing and use. We ask the Commission to particularly consider how the concepts of controller </code><br><code>and processor under the GDPR are meant to align with roles of user and provider and ensure that </code><br><code>the obligations and expectations under these two regulatory frameworks are consistent and do not </code><br><code>result in conflicts. </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665491_9,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665491.pdf,9,9,2665491,attachments/2665491.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>while using AI-based tools. The European Commission</code><code> White Paper on AI</code><code> already identified</code><br><code>risks to data protection and privacy rights by</code><code> e.g.</code><code> using AI to retrace or de-anonymise data, even</code><br><code>for data sets which do not, as such, include personal data.</code><br><code>Other elements of attention - No√©mie Krack</code><br><code>The place of research in AI</code><br><code>It is crucial to clarify the situation of research when it comes to the AI act. There is a strong call</code><br><code>for more provisions, information concerning the role of research and an extension of the</code><br><code>research exemption.</code><br><code>European Data sets</code><br><code>The development of an EU sovereignty in data and ethical use of the data is crucial. There is</code><br><code>the need to have European data sets fulfilling ethical and legal principles and rules in order to</code><br><code>base ethical and legally compliant progress in research and commercial application of AI</code><br><code>systems.</code><br><code>Transparency</code><br><code>Transparency has different faces and multiple notions can be understood as transparency, there</code><br><code>is a need to clarify the legal vocabulary between transparency, explainability, interpretability,</code><br><code>record keeping, communication to the end-user. Some elements present in previous policy text</code><br><code>on AI such as the HLEG guidelines are not taken in the proposal and more consistency on this</code><br><code>aspect will be beneficial to transparency in general.</code><br><code>In addition, making use of signs, symbols could be a way forward and to develop an easy</code><br><code>understanding for the consumers. The text is complex and hard to read for non-lawyers and</code><br><code>might create problems of enforcement and transparency towards the population already</code><br><code>suffering from information asymmetry.</code><br><code>Consumers and society: what role do they have?</code><br><code>There is a strong presence/mention of fundamental rights in this internal market proposal</code><br><code>(Article 114 TFEU as legal basis) : 80 mentions through the all document and a clear will to</code><br><code>establish a text with positive impact for citizens. However, in the text there is no concrete</code><br><code>redress and action mechanisms for citizens, civil society or consumer rights associations to act</code><br><code>against non-compliant providers or users. Specific provisions should be established especially</code><br><code>on the burden of proof, as there is already an asymmetry of information context. It is important</code><br><code>to have provisions helping victims for lodging complaints to the competent authorities and</code><br><code>asking for redress.</code><br><code>9</code>",NO_FOOTNOTES_ON_PAGE
fitz_2661333_1,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2661333.pdf,2,1,2661333,attachments/2661333.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Region V√§stra G√∂taland </code><br><code>2021-07-05 </code><br><code>Region V√§stra G√∂taland‚Äôs position paper on EU‚Äôs </code><br><code>Artificial Intelligence Act </code><br><code> </code><br><code>Key messages: </code><br><code>‚Ä¢</code><code> </code><code>Welcomes that the regulation is based on citizens‚Äô fundamental rights. The </code><br><code>development and use of AI must be guided by democratic and ethical </code><br><code>principles. </code><br><code>‚Ä¢</code><code> </code><code>Welcomes that the regulation uses a risk-based approach to create a </code><br><code>structured division between different types of AI systems.  </code><br><code>‚Ä¢</code><code> </code><code>Welcomes that the regulation highlights the importance of test and </code><br><code>development opportunities, in controlled forms, to achieve increased </code><br><code>application and development. The importance of test environments and </code><br><code>regulatory sandboxes for testing new solutions in a safe environment is </code><br><code>crucial for the development of AI. </code><br><code>‚Ä¢</code><code> </code><code>Highlights the importance of ensuring that public actors, such as regions </code><br><code>and municipalities, have the tools to make an adequate risk assessment of </code><br><code>often complex AI value chains. </code><br><code>‚Ä¢</code><code> </code><code>Welcomes the Coordinated Plan on Artificial Intelligence 2021 Review as </code><br><code>an important part in the work to create a European approach to Artificial </code><br><code>Intelligence. </code><br><code> </code><br><code>Artificial intelligence (AI) has the potential to contribute significantly in several areas such as </code><br><code>increased economic growth as well as solutions to environmental and social challenges. Already </code><br><code>today, there are examples of AI enabling better diagnoses of diseases, reducing energy use, </code><br><code>reducing traffic accidents, creating new services, streamlining industrial production, developing </code><br><code>new drugs, and shortening red tape. At the same time, the risks of AI need to be considered. It is </code><br><code>therefore positive that the proposal uses a risk-based approach to create a structured division </code><br><code>between different types of AI systems and its use. As in the reply to the Public Consultation on the </code><br><code>White Paper on AI, Region V√§stra G√∂taland highlights that the development and the use of AI need </code><br><code>to be guided by norms and ethical principles and be based on the principle of human rights. The </code><br><code>overall goal should be sustainable AI, meaning that AI applications should be ethical, secure, </code><br><code>reliable, and transparent. Ethical and safety considerations cannot be an afterthought in AI </code><br><code>applications but must be an integral part of the early design work. </code><br><code>The use of AI has the potential to create significant benefits in several areas through increased </code><br><code>economic growth as well as solutions to environmental and societal challenges. It is therefore </code><br><code>important that there is a balance between the consideration of risk on the one hand and development </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665488_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2665488.pdf,5,4,2665488,attachments/2665488.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>4 / 5 </code><br><code>Der Aufwand der Konformit√§tspr√ºfung ist bei der kommerziellen Entwicklung von Hochrisiko-KI-Systemen angemessen, da diese von vielen Betreibern genutzt werden und so </code><br><code>ein kumulatives Risiko von Ihnen ausgeht. Entwickelt oder modifiziert ein Unternehmen </code><br><code>hingegen eine KI f√ºr den Eigengebrauch, steht das Risiko in einem g√§nzlich anderen Verh√§ltnis zu dem Aufwand der Konformit√§tspr√ºfung. F√ºr kommunale Unternehmen w√ºrde </code><br><code>eine solche Pr√ºfung schlimmstenfalls ein weiteres Hemmnis in der fl√§chendeckenden Nutzung von KI-Systemen darstellen, da der Aufwand der Konformit√§tspr√ºfung abschreckend </code><br><code>wirken k√∂nnte. Gleichzeitig w√ºrde dies einen Anreiz schaffen, KI-Systeme nicht selbst zu </code><br><code>entwickeln oder zu modifizieren, sondern als fertige L√∂sung einzukaufen. Langfristig w√ºrden so die Innovationskraft kommunaler Unternehmen gehemmt und die Abh√§ngigkeit </code><br><code>von Drittanbietern erh√∂ht. Vielmehr sollte die eigene Entwicklung von KI-Systemen, sowie </code><br><code>die Anpassung eingekaufter Systeme an die eigenen Bed√ºrfnisse gef√∂rdert werden, da </code><br><code>sich hierdurch betriebsspezifisches Know-how bildet. </code><code>Daher spricht sich der VKU daf√ºr </code><br><code>aus, die Entwicklung und Modifizierung von Hochrisiko-KI-Systemen f√ºr den Eigengebrauch von den versch√§rften Anforderungen f√ºr Anbieter auszunehmen. Gegebenenfalls k√∂nnte hier eine vereinfachte Konformit√§tspr√ºfung einen geeigneten Mittelweg </code><br><code>bilden. </code><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665321_7,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665321.pdf,9,7,2665321,attachments/2665321.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Page 7 / 9 </code><br><code> </code><br><code>However, for these data, in particular, using APIs poses some risks that </code><br><code>need to be adequately considered. Attacks or manipulation attempts regarding AI applications through so-called ‚Äúadversarial examples‚Äù show how </code><br><code>important it is to protect training, validation and testing data as well as the </code><br><code>source code of the AI application. Knowledge of the data or the source code </code><br><code>makes it possible to deliberately manipulate the predictions or decisions of </code><br><code>AI applications. Against this background, each transfer of such data and </code><br><code>every interface which enables direct access to these data pose an additional </code><br><code>safety risk. Furthermore, implementation and maintenance efforts will be </code><br><code>required with regard to the demanded application programming interfaces, </code><br><code>which would place a heavy burden on small and medium-sized companies, </code><br><code>in particular. </code><br><code> </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><br><code>No weakening of the level playing field in regulatory sandboxes </code><br><code> </code><br><code>In order to ensure innovation-friendly framework conditions and to encourage innovation it is fundamental to prevent overregulation of AI applications </code><br><code>and legal uncertainty. In addition, measures to encourage innovation may </code><br><code>also play an important role. Numerous countries within the EU have already </code><br><code>established innovation hubs or regulatory sandboxes such as the ‚ÄúEuropean Forum for Innovation Facilitators‚Äù as a platform to share information </code><br><code>and coordinate efforts at European level to facilitate innovation in the financial sector. The supervisory infrastructure and expertise built in this context </code><br><code>should also be used to encourage innovation in the area of AI applications. </code><br><code> </code><br><code>With regard to measures to encourage innovation it is crucial that a level </code><br><code>playing field continues to be ensured for all providers, following the principle </code><br><code>of ‚Äúsame business, same risk, same rules‚Äù. This is the only way to allow for </code><br><code>fair competition in innovation between different types of market participants </code><br><code>(e. g. traditional providers and start-ups). Providing small and medium-sized </code><br><code>companies with priority access to regulatory sandboxes, as proposed by the </code><br><code>EU Commission under Article 55, would mean an unreasonable discrimination against large-scale companies and is therefore to be rejected. </code><br><code> </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><br><code>Preventing duplication in the design of governance structures </code><br><code> </code><br><code>The possible use of existing supervisory structures, where available, and </code><br><code>the consideration of national distributions of competences are welcomed. </code><br><code>As a result, well-functioning structures will be preserved and a duplication </code><br><code>of supervisory activities will be prevented. For industries which are already </code><br><code>subject to regulation and comprehensive oversight, such as the insurance </code><br><code>industry, additional regulation and additional supervision would not seem </code><br><code>justified, considering the efforts to be required and the benefits to be gained. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665314_6,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2665314.pdf,6,6,2665314,attachments/2665314.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Side 6 </code><br><code> </code><br><code>processing, or whether it merely organizes access to regulatory expertise and guidance. </code><br><code>In our experience, requiring the authority to provide IT infrastructure will be raising the </code><br><code>threshold for establishing sandboxes, due to the technical expertise and costs that this </code><br><code>would entail. </code><br><code> </code><br><code>Article 53, section 3 states that the sandbox does not affect supervisory and corrective </code><br><code>powers. This is a useful clarification. However, we also see a need for some guidance on </code><br><code>how competent authorities can strike a good balance between being a supervisory </code><br><code>authority on the one hand and giving detailed guidance through a sandbox on the other. </code><br><code>We propose that the AI Act specifies that participation in a sandbox does not constitute a </code><br><code>stamp of approval, and that the organization/controller is still accountable for its </code><br><code>processing of personal data. </code><br><code> </code><br><code>Article 53, section 6 describes that the modalities and conditions of the operation of the </code><br><code>sandboxes shall be set out in implementing acts. It is important that specific guidelines be </code><br><code>produced in order to ensure some consistence and help in the establishment and </code><br><code>operation of sandboxes. However, binding implementing acts could limit each Member </code><br><code>State‚Äôs ability to customise the sandbox according to local needs. </code><br><code> </code><br><code>It is our experience so far that operating a sandbox is resource intensive, and it is to be </code><br><code>expected that only a small number of businesses would get the opportunity to participate. </code><br><code>It shall be noted that participating in the sandbox could constitute a competitive </code><br><code>advantage for participants that are able to further process data for new purposes, as </code><br><code>reflected in Article 54. Therefore, the opportunity for further processing, as set out in </code><br><code>Article 54, would require careful consideration of how to select participants to the </code><br><code>sandbox in order to ensure fair competition.  </code><br><code>9 GOVERNANCE STRUCTURE AND AI BOARD </code><br><code>The Norwegian government foresees to designate a national supervisory authority, in </code><br><code>accordance with the proposal. Norway, as an EEA member state, should be represented </code><br><code>in the European Artificial Intelligence Board (EAIB) to be established in accordance with </code><br><code>title VI, chapter I of the proposal, equivalent to our participation in the European Data </code><br><code>Protection Board. This will be essential to ensure consistent application of the AIregulation across the internal market.  </code><br><code> </code><br><code>The Proposal foresees to give a predominant role to the Commission, with the EAIB in an </code><br><code>advisory role. This contrasts with the need for a European AI body independent from any </code><br><code>political influence. We would recommend giving more autonomy to the EAIB. This will </code><br><code>give the Board a better possibility to ensure the consistent application of the regulation </code><br><code>across the single market. We note that no power is conferred to the Board regarding the </code><br><code>enforcement of the proposed regulation. Yet, considering the spread of AI systems </code><br><code>across the single market and the likelihood of cross-border cases, there is a crucial need </code><br><code>for a harmonized enforcement and a proper allocation of competence between national </code><br><code>supervisory authorities. We therefore recommend that the cooperation mechanisms </code><br><code>between national supervisory authorities be specified in the forthcoming regulation. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665507_4,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665507.pdf,4,4,2665507,attachments/2665507.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>4 </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Ensure the right technical tests and legal tests are carried out by suppliers, and enable </code><br><code>the required transparency between business partners in order for all value chain actors to </code><br><code>have equivalent levels of information.  </code><br><code> </code><br><code>In the context of compliance for low risk AI systems, the framework would </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Help translate company-specific codes of conduct for low-risk AI systems into something </code><br><code>tangible, standardised and usable by suppliers and customers. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Where transparency requirements apply, ensure all actors have equivalent levels of </code><br><code>information (e.g. system integrators‚Äô automatic disclaimers solutions, deployers, custommade notifications panel, etc.) </code><br><code> </code><br><code>The framework would help cost-effectively satisfy post-market surveillance authorities requests </code><br><code>as well as strengthen conformity assessments (whether 3</code><code>rd</code><code> party or self-performed). It would </code><br><code>also help actors be more confident that other actors involved in the value chain are fulfilling their </code><br><code>regulatory obligations. By providing a one-stop solution, it would help ensure a coherent selfenforcement mechanism to be adopted alongside official harmonized standards, reducing compliance costs significantly.</code><code>  </code><br><code>About Global Digital Foundation </code><br><code>Global Digital Foundation is a platform for dialogue between policymakers, stakeholders and </code><br><code>scholars in support of evidence-based policy for the digital society. It is also a source of briefings for all who have a personal or professional interest in policy that affects the development </code><br><code>and use of digital technology.  </code><br><code>www.globaldigitalfoundation.org</code><code> </code><br><code> </code><br><code> </code><br><code>paul.macdonnell@globaldigitalfoundation.org </code><br><code>5</code><code>th</code><code> August 2021</code><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662802_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662802.pdf,4,1,2662802,attachments/2662802.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>FIM position on the Artificial intelligence draft Regulation   </code><br><code>Auteur : </code><br><code>Roxana Turcanu </code><br><code>Date de publication : </code><br><code>21/07/2021 </code><br><code> </code><br><code>rturcanu@fimeca.org - + 33 (0)1 47 17 64 87 </code><br><code> </code><br><code> </code><br><code> </code><br><code>As a preamble, one has to acknowledge that the implementation of software techniques and approaches commonly </code><br><code>referred to as ""Artificial Intelligence"" is not a novelty, as self-driving machinery is already placed on the market and </code><br><code>put into service especially in the industrial and agricultural sectors. </code><br><code> </code><br><code>These advanced techniques also contribute to improve the reliability of components, to implement predictive monitoring and maintenance, to increase the lifespan of machinery, to optimise energy efficiency and to adapt production </code><br><code>to customer demand. </code><br><code> </code><br><code>The deployment of these techniques represents a cornerstone for the competitiveness of the European Union, and </code><br><code>it is imperative to foster innovation in this sector. </code><br><code>Context and challenges  </code><br><code>In the context of the Artificial intelligence draft Regulation, FIM supports the risk-based approach. </code><br><code> </code><br><code>Nevertheless, it appears that this text has been developed without considering that many regulated products incorporating AI as a safety function, notably machinery subject to the provisions of Directive 2006/42/EC, are currently </code><br><code>used in several sectors. Currently, the AI is considered in the framework of risk analysis, thus allowing manufacturers </code><br><code>to take appropriate protection measures. Therefore, the articulation between this draft Regulation and the revision </code><br><code>of Directive 2006/42/EC raises questions.  </code><br><code> </code><br><code>Furthermore, the IA draft Regulation establishes obligations, particularly in terms of risk management and conformity assessment, which are already covered by the Machinery Directive, and which go well beyond what the Machinery </code><br><code>Directive requires, notably concerning Chapters II and III of Title III.  Today, the methodology adopted, for example </code><br><code>by the Machinery Directive, is to carry out a risk analysis, to determine the applicable Health and Safety Requirements </code><br><code>and to take the appropriate safety measures. In this draft Regulation, the provisions of Chapters II and III must be </code><br><code>applied in an absolute manner. </code><br><code> </code><br><code>Finally, we make ours the title of the book written by Siri's co-creator, the Franco-American engineer Luc Julia: ""Artificial intelligence does not exist"". </code><br><code> </code><br><code>While this draft text seems to address legitimate concerns about the use of AI software, prohibiting certain uses, it </code><br><code>is not at all adapted to the use in industrial products, especially if the industrial products are already regulated by a </code><br><code>New Approach Regulation.  </code><br><code> </code><br><code>This act will have a disproportionate impact on the companies‚Äô competitiveness and will limit the European Union's </code><br><code>capacity for innovation in this area. </code><br><code> </code><br><code>In this context, FIM would like to make comments and to contribute to the improvement of the proposal. </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665627_11,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665627.pdf,11,11,2665627,attachments/2665627.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>[15] European Commission, Proposal for a Regulation of the European Parliament and of the </code><br><code>Council on machinery products, COM(2021)202, 21 April 2021</code><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665586_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665586.pdf,3,3,2665586,attachments/2665586.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>3 </code><br><code> </code><br><code>o</code><code> </code><code>Transitional provisions/Entry into force and application (Article 85):</code><code> Experience with the </code><br><code>transition from the Medical Device Directives to the EU MDR showed that sufficient </code><br><code>transition time is crucial not only for medical device manufacturers but for the entire </code><br><code>regulatory system. Putting the necessary infrastructure in place, ensuring compliance with </code><br><code>the requirements and guidance documents, as well as adequate notified body capacity </code><br><code>needs sufficient time. </code><code>The transitional period should be therefore extended by at least </code><br><code>two years, i.e. to 48 months to allow for all necessary elements to be in place in time </code><br><code>before the date of application.</code><code> </code><br><code>If the abovementioned misalignments are not properly addressed, there is a definite risk of duplications </code><br><code>and additional unnecessary administrative burden resulting in increased complexity, legal uncertainty, and </code><br><code>higher implementation costs not only for the manufacturers but the entire healthcare system, including </code><br><code>patients. We are concerned that the increased complexity will accelerate the already existing trend of </code><br><code>privileging other geographies for first placing of innovative medical software on the market. This in turn </code><br><code>will have a huge impact on the EU‚Äôs competitiveness and innovation resulting in delayed patient access to </code><br><code>innovative digital health products and solutions. </code><br><code>We look forward to further engaging with the EU institutions in the legislative process. </code><br><code>About Royal Philips </code><br><code>Royal Philips (NYSE: PHG, AEX: PHIA) is a leading health technology company focused on improving people's </code><br><code>health and well-being, and enabling better outcomes across the health continuum ‚Äì from healthy living </code><br><code>and prevention, to diagnosis, treatment and home care. Philips leverages advanced technology and deep </code><br><code>clinical and consumer insights to deliver integrated solutions. Headquartered in the Netherlands, the </code><br><code>company is a leader in diagnostic imaging, image-guided therapy, patient monitoring and health </code><br><code>informatics, as well as in consumer health and home care. Philips generated 2020 sales of EUR 17.3 billion </code><br><code>and employs approximately 77,000 employees with sales and services in more than 100 countries. </code><br><code>*** </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2661982_2,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2661982.pdf,6,2,2661982,attachments/2661982.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>2 </code><br><code> </code><br><code>3. Businesses will not be able to act on the regulation before harmonized international </code><br><code>standards exist. Top priority must therefore be given to developing such standards </code><br><code>in parallel with the development of the regulation itself. </code><br><code>Trustworthy AI as the norm </code><br><code>We support the ambition set out in the proposed legislative framework, that AI should be </code><br><code>a tool for people and a force for good in society, with the ultimate goal to increase human </code><br><code>well-being. We agree that a legal framework for trustworthy AI is a necessity in order to </code><br><code>develop an AI ecosystem of trust in Europe. A lack of trust from citizens and unclear legal </code><br><code>conditions for companies could slow down the development and uptake of AI </code><br><code>technologies, and hence reduce the competitiveness of Europe.  </code><br><code>Disparate regulatory responses by national authorities would risk a fragmentation of the </code><br><code>internal market. Hence such regulation clearly needs harmonization at the European </code><br><code>level. Regulation is needed to ensure that AI systems, products, and services deployed </code><br><code>in Europe comply with European norms and values. We therefore welcome the draft </code><br><code>regulation.  </code><br><code>The regulatory landscape</code><code> </code><br><code>As the technology progresses further, AI will play an important role in an ever-increasing </code><br><code>number of systems, products, and services. The draft Artificial Intelligence Act aims to put </code><br><code>down fundamental principles to govern such use. However, we find it challenging to </code><br><code>understand the connection between the proposed regulation and already existing policy </code><br><code>provisions, specifically with respect to the use of AI in products and services that are </code><br><code>governed by already existing regulations such as for cars, marine equipment, etc.  </code><br><code>The draft Artificial Intelligence Act also needs to be seen in conjunction with other relevant </code><br><code>and recent regulations. In particular, emphasis must be given to harmonizing the Artificial </code><br><code>Intelligence Act and the recently proposed Data Governance Act as well as the General </code><br><code>Data Protection Regulation. </code><br><code>Companies that are developing or integrating AI in systems, products, or services rely on </code><br><code>harmonized standards as a means of verifying that their offerings meet relevant </code><br><code>regulations and directives. It is therefore critical for continued innovation that harmonized </code><br><code>standards are developed with the maximum priority and minimum delay from the release </code><br><code>of the new Artificial Intelligence Act. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663380_10,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2663380.pdf,12,10,2663380,attachments/2663380.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>10 </code><br><code> </code><br><code>could be separate from GDPR assessments, they could be incorporated where AI involves </code><br><code>personal data. The area of AIA‚Äôs has been explored in detail in the IAF‚Äôs report on </code><code>The-Road-toExpansive-Impact-Assessments.pdf (secureservercdn.net)</code><code> </code><br><code>The IAF believes that risk assessments, based on the concept of adverse outcomes, should be </code><br><code>broadly applied and that oversight and potentially ex ante reviews also should be risk based. </code><br><code>This approach ties to a related recommendation. As the AI Regulation is narrow in scope and </code><br><code>only applies to named high risk scenarios, the implication is that all of the good practice </code><br><code>requirements listed in the AI Regulation could be ignored by another AI application that was </code><br><code>not initially high risk, as defined, but still could have significant impact and risk to an individual. </code><br><code>While the AI Regulation may have been designed to take an ‚Äúimmediate but limited‚Äù approach, </code><br><code>in the case of AI, such an approach is not consistent with the GDPR generally (legal or similar </code><br><code>impact on an individual) or specifically (risky processing triggering the need for a DPIA under </code><br><code>Article 35). An example is a broad-based AI system that results in a segmented approach to </code><br><code>access to key services or pricing. It appears that highly impactful AI scenarios, such as the racial </code><br><code>bias found in </code><code>Airbnb‚Äôs 2015 Smart Pricing algorithm</code><code> or the </code><code>bias in online targeted  advertising</code><code> </code><br><code>targeting, would not be covered by the AI Regulation. </code><br><code>AIAs would address all requirements and facilitate risk and adverse outcomes reviews. They </code><br><code>would identify higher risks (by potential impact, not by scenario) and facilitate appropriate </code><br><code>oversight. The oversight system needs to be more grounded than one that is based on ex ante </code><br><code>reviews by third parties that are driven by established standards in an arena where there are no </code><br><code>AI standards. </code><br><code> </code><br><code>The IAF respectfully makes the following suggestions: </code><br><code>The IAF‚Äôs core suggestion to improve the AI Regulation is to adapt the AI Regulation so that it </code><br><code>applies to all AI application scenarios that would be risk assessed as opposed to applying only </code><br><code>elements of the AI Regulation to a narrow set of defined high-risk scenarios. This change would </code><br><code>allow for the tailoring of requirements to those applications that have the potential to create </code><br><code>higher risk. More specifically: </code><br><code>‚Ä¢</code><code> </code><code>A requirement to assess the likely risk of an AI application would allow for the extension of </code><br><code>the very good data governance requirements to be extended to all AI applications. While </code><br><code>many of these included in the AI Regulation should arguably be applied to all areas of AI </code><br><code>development relative to data, this would allow for a tailoring of requirements based on risk.  </code><br><code>‚Ä¢</code><code> </code><br><code>Broad based AIAs should be a mainstay requirement for all AI applications and should </code><br><code>include the assessment of risk. Conformity assessment requirements as currently outlined </code><br><code>in the AI Regulation could be incorporated into an AIA as appropriate as could aspects of a </code><br><code>DPIA as required by Article 35 of the GDPR. The concept of broader impact assessments has </code><br><code>been explored in the IAF‚Äôs recent report </code><code>The Road to Expansive Impact Assessments.</code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665323_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665323.pdf,3,3,2665323,attachments/2665323.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Page </code><code>3</code><code> / </code><code>3</code><code> </code><br><code>Laboratoire national de m√©trologie et d‚Äôessais </code><br><code>Article 43.3 and article 38‚Äì Areas covered by existing notified bodies </code><br><code>In order to facilitate the development of the competence of the notified bodies that will be involved </code><br><code>in the high-risk AI systems to which the legal acts listed in Annex II, Section A, apply, and to </code><br><code>harmonise the assessments to be carried out by these bodies, we recommend the establishment of </code><br><code>assessment guides. </code><br><code>Similarly, there should be communication between the sectoral group of notified bodies referred to </code><br><code>in Article 38 and the sectoral groups of notified bodies provided for in the legal acts listed in Annex II, </code><br><code>Section A. </code><br><code> </code><br><code>Article 43.4 ‚Äì Lifelong learning </code><br><code>It is proposed in the current version of the AI Act that high-risk AI system modifications resulting </code><br><code>from post-deployment learning are not considered substantial (i.e. do not require a compliance </code><br><code>reassessment) if they were predetermined by the provider at the time of the initial assessment (cf. </code><br><code>Article 43(4) and Whereas 66). However, for a system with an incremental or continuous learning </code><br><code>capability after deployment, this notion of predetermined modification is unclear to us, while the </code><br><code>risks of degrading the performance of such systems are significant. We therefore recommend that </code><br><code>this point be clarified and that a compliance assessment process be developed for systems with </code><br><code>lifecycle learning.   </code><br><code> </code><br><code>Article 69 (Title IX) - Codes of conduct </code><br><code>With regard to Article 69, we wish to inform the European Commission of the creation by the LNE of </code><br><code>a process certification for AI. The document on which this certification is based defines criteria for </code><br><code>the design, development, evaluation and maintenance processes of artificial intelligence systems. </code><br><code>This document was developed thanks to the reflections of a working group bringing together </code><br><code>developers, evaluators and users of all sizes and from various backgrounds (AXIONABLE, ARCURE, IRT </code><br><code>RAILENIUM, KICKMAKER, ORANGE, PROXINNOV - A2D, SCHNEIDER ELECTRIC, SCORTEX, THALES, </code><br><code>TOSIT, etc.) </code><br><code>  </code><br><code>This document is intended to evolve to take into account the state of the art, new regulations or </code><br><code>standards in the field. </code><br><code>  </code><br><code>We therefore suggest that Article 69 should not limit the possibility of setting up codes of good </code><br><code>conduct to individual suppliers of AI systems or organisations representing them. </code><br><code>  </code><br><code>For more details on LNE certification: https://www.lne.fr/fr/service/certification/certificationprocessus-ia </code><br><code>To download the standard: </code><code>https://www.lne.fr/fr/recevoir-referentiel-certif-IA</code><code> </code><br><code> </code><br><code>Annex II section A </code><br><code>The list of legislative texts in section A of Annex II could be completed by Directives 2014/31/EU and </code><br><code>2014/32/EU. Indeed, the use of AI systems in regulated measuring instruments is now envisaged by </code><br><code>the manufacturers of these products, with implications in particular in the field of energy metering or </code><br><code>measurements of pollutant emissions from vehicles.</code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665416_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665416.pdf,9,3,2665416,attachments/2665416.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>3</code><br><code>technological development or otherwise increasing the cost of placing AI solutions on </code><br><code>the market</code><code>‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code>. The Commission also aims to ‚Äú</code><code>[strengthen] Europe‚Äôs competitiveness and </code><br><code>industrial basis in AI</code><code>‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>. </code><br><code>3.5</code><br><code>NEC recognises that balancing the need to encourage innovation and competitiveness </code><br><code>in the field of AI, whilst at the same time protecting the public from the possible harm </code><br><code>that AI can cause, is a difficult exercise. However, as a broad comment, NEC is </code><br><code>concerned that the Draft Regulation does not strike the right balance between these </code><br><code>competing interests and that, in its current form, it is likely to disincentivise AI innovation </code><br><code>and weaken the EU‚Äôs competitiveness and global position in fostering the development </code><br><code>and use of AI.</code><br><code>3.6</code><br><code>In particular, and as is described in more detail below:</code><br><code>(A)</code><br><code>The requirements of Title III of the Draft Regulation (relating to high-risk AI </code><br><code>systems) are very onerous. They are likely to require significant work and a large </code><br><code>financial burden, particularly for ‚Äúproviders‚Äù of high-risk AI systems.</code><br><code>(B)</code><br><code>This burden is likely to stifle AI innovation and discourage firms from placing AI </code><br><code>systems on the EU market. The EU is likely to be seen as a market with high </code><br><code>barriers to entry, which will ultimately reduce its competitiveness on a global </code><br><code>scale.</code><br><code>(C)</code><br><code>This burden is also likely to disproportionately affect SMEs (particularly those </code><br><code>involved in the development of AI technologies), given that they are unlikely to </code><br><code>have available to them the resources needed to comply with these onerous</code><br><code>requirements. This will inevitably have an anti-competitive effect on the AI </code><br><code>developers market, which will be dominated by larger tech firms that are better </code><br><code>able to comply with these requirements. </code><br><code>3.7</code><br><code>NEC hopes that, during the legislative process of the Draft Regulation, there will be an </code><br><code>opportunity for the Commission (and also the European Parliament and Council) to </code><br><code>reflect on the points noted above and the balance that the Draft Regulation seeks to </code><br><code>strike between protection, innovation and competition. </code><br><code>3.8</code><br><code>NEC would be happy to engage in further dialogue with the Commission regarding this </code><br><code>point (and the Draft Regulation more generally). In light of the potentially significant </code><br><code>impact that the Draft Regulation is likely to have on organisations (globally) involved in </code><br><code>AI development and use, NEC respectfully suggests that further discussions with </code><br><code>relevant stakeholders regarding the Draft Regulation would benefit the legislative </code><br><code>process and further iterations of this important legal framework.</code><br><code>4</code><code> Paragraph 1.1, Explanatory Memorandum. </code><br><code>5</code><code> Paragraph 2.2, Explanatory Memorandum.</code>",POSITIVE
fitz_2665600_6,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665600.pdf,6,6,2665600,attachments/2665600.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Page </code><code>5</code><code> of </code><code>5</code><code> </code><br><code> </code><br><code>                                                                                                                                                                                           </code><br><code>kritikusait-es-magyar-ujsagirokat-is-celba-vettek-vele/</code><code> . Nikolov, K (2021), </code><code>Bulgarian secret services suspected of eavesdropping </code><br><code>opposition politicians</code><code>. Euractiv, available at: </code><code>https://www.euractiv.com/section/politics/short_news/bulgarian-secret-servicessuspected-of-eavesdropping-opposition-politicians/</code><code>  </code><br><code>7</code><code> Such as those adopted by the Council of Europe in Convention for the protection of individuals with regard to processing of </code><br><code>personal data (Convention 108+). </code><br><code>8</code><code> In Chapters 2 and 3 of Title III. </code><br><code>9</code><code> To that effect, for example, Wachter, S., Mittelstadt, B. (2019) </code><code>A Right to Reasonable Inferences:Re-thinking Data Protection Law </code><br><code>in the Age of Big Data and AI</code><code>. Columbia Business Law Review, available at: </code><br><code>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3248829</code><code>  </code><br><code>10</code><code> Wachter, S., Mittelstadt, B. (2019), supra. O‚ÄôNeil, C. (2016). </code><code>Weapons of Math Destruction</code><code>. Crown </code><br><code>11</code><code> In Chapters 2, 3 and 5 of Title III. </code><br><code>12</code><code> Empirical evidence to that effects available in Jia, J. et all (2019). </code><code>GDPR and the Localness of Venture Investment</code><code>. American </code><br><code>Economic Association, available at: </code><code>https://www.aeaweb.org/conference/2020/preliminary/paper/7dfztbb4</code><code> . </code><br><code>13</code><code> E.g. pharma, with respect to more complex solutions (e.g. innovative treatments). </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665626_7,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665626.pdf,7,7,2665626,attachments/2665626.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>updates. This could easily be done by establishing an anonymised database, based </code><br><code>on the serious incidents reported under Article 62. </code><br><code>‚Ä¢</code><code> </code><br><code>EU institutions and Member States funding more work (within government and academia) </code><br><code>on AI monitoring and assessment. </code><br><code>o</code><code> </code><br><code>Successful implementation of the regulation will depend on innovation in both </code><br><code>methods to monitor the evolving AI risk landscape and methods for assessing the </code><br><code>impacts of AI systems. EU institutions and Member States should explore ways to </code><br><code>directly fund this work in order to ensure it is as aligned with the goals of the </code><br><code>regulation as possible. </code><br><code>‚Ä¢</code><code> </code><br><code>Ensuring that the Commission can update Annex VIII</code><code> (information to be submitted for the </code><br><code>public database). </code><br><code>o</code><code> </code><br><code>The public database will be an important way to attribute harms, seek redress, </code><br><code>enable post-market monitoring, and identify new sources of harm from AI systems. </code><br><code>It will be especially important for users, regulators, academia, and civil society. The </code><br><code>database needs to be kept up to date in light of technical progress - in coming years </code><br><code>new forms of information may become important. This could include, for example: </code><br><code>parameter count; a measure of the computational resources used to develop, train, </code><br><code>test and validate the AI system; or measures of hyperparameters, such as </code><br><code>‚Äòtemperature‚Äô. However as the Act is currently drafted, Annex VIII (information to be </code><br><code>submitted for the public database) cannot currently be updated. This seems like an </code><br><code>oversight - it is important that the Commission can keep this, like other Annexes, up </code><br><code>to date. This could be easily fixed by adding to Article 60: ‚Äú6. The Commission is </code><br><code>empowered to adopt delegated acts in accordance with Article 73 for the purpose of </code><br><code>updating Annex VIII in order to introduce additional required information that </code><br><code>becomes necessary in light of technical progress.‚Äù, and adding ‚Äúand Article 60(6)‚Äù to </code><br><code>the lists in 2, 3 and 5 in Article 73. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663486_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2663486.pdf,8,1,2663486,attachments/2663486.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Education International </code><br><code>Internationale de l'Education </code><br><code>Internacional de la Educaci√≥n </code><br><code> </code><br><code>http://www.ei-ie.org </code><br><code>EUROPEAN REGION- </code><br><code>ETUCE </code><br><code> </code><br><code>President </code><br><code>Larry FLANAGAN </code><br><code> </code><br><code>Vice-Presidents</code><code> </code><br><code>Odile CORDELIER  </code><br><code>Andreas KELLER </code><br><code>Trudy KERPERIEN </code><br><code>Dorte LANGE </code><br><code>Galina MERKULOVA  </code><br><code>Branimir STRUKELJ</code><code>  </code><br><code> </code><br><code> </code><br><code> </code><br><code>Boulevard Bischoffsheim, 15 </code><br><code>1000 Brussels, Belgium </code><br><code>Tel +32 2 224 06 91/92 </code><br><code>Fax +32 2 224 06 94 </code><br><code>secretariat@csee-etuce.org </code><br><code>http://www.csee-etuce.org</code><code> </code><br><code> </code><br><code>European Director </code><br><code>Susan FLOCKEN </code><br><code> </code><br><code>Treasurer </code><br><code>Joan DONEGAN </code><br><code> </code><br><code> </code><br><code> </code><br><code>ETUCE  </code><br><code>European Trade Union Committee for Education </code><br><code>EI European Region  </code><br><code> </code><br><code> </code><br><code> </code><br><code>ETUCE position on the EU Regulation on Artificial Intelligence </code><br><code>(Adopted by the ETUCE Bureau on 7 June 2021) </code><br><code>Background: </code><br><code>On 21 April 2021, the European Commission published a proposal for a </code><code>‚ÄúRegulation on a </code><br><code>European Approach for Artificial intelligence</code><code>‚Äù (the AI Regulation). With this proposal, the </code><br><code>European Commission follows up on its </code><code>White Paper on Artificial Intelligence</code><code> (February </code><br><code>2020), based on the results of a broad consultation process to which ETUCE </code><code>contributed</code><code>. </code><br><code>The aim of the initiative is to establish the first EU legal framework regulating the entire </code><br><code>lifecycle of the use of Artificial Intelligence (AI) in all sectors, including education.  </code><br><code>The AI Regulation </code><code>classifies</code><code> the use of Artificial Intelligence in various sectors based on the </code><br><code>risk that the AI tools have on the health and safety and the fundamental rights</code><code> of </code><br><code>individuals. Concerning education, the proposal considers the use of Artificial Intelligence </code><br><code>tools in </code><code>education as high-risk</code><code> as potentially harmful to the right to education and training </code><br><code>as well as the right not to be discriminated in education. For high-risk sectors, the AI </code><br><code>Regulation establishes </code><code>stricter horizontal legal requirements</code><code> to which AI tools must comply </code><br><code>before being authorised on the market. These include risk management system during the </code><br><code>entire lifecycle of the AI system.     </code><br><code>Following the publication of the proposal, on 26 April 2021, the European Commission </code><br><code>issued a </code><code>public consultation</code><code> that will run until 20 July 2021, accompanied by an </code><code>impact </code><br><code>assessment report</code><code>.  </code><br><code>The following text is the ETUCE response to the public consultation bringing the perspective </code><br><code>of teachers, academics and other education personnel on the sections of the AI Regulation </code><br><code>that touch upon the education sector.  </code><br><code> </code><br><code>ETUCE reply: </code><br><code>ETUCE welcomes the publication of the AI Regulation as it sets the ground for the first </code><br><code>comprehensive EU regulation on Artificial Intelligence to ensure a controlled development </code><br><code>of AI tools in education and address the risks connected to their use by teachers, academic, </code><br><code>other education personnel and students. While ETUCE recognises the potential of digital </code><br><code>technologies and Artificial Intelligence tools to bring about improvements in education, it </code><br><code>also underlines the </code><code>numerous ethical concerns</code><code> related to their trustworthiness, data </code><br><code>privacy, accountability, transparency and their impact on equality and inclusion in </code><br><code>education. ETUCE underlines that </code><code>further research</code><code> at national and European level is needed </code><br><code>to assess and address the risks connected to the use of Artificial Intelligence in education </code><br><code>with constant and meaningful consultation with education social partners.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665536_10,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665536.pdf,19,10,2665536,attachments/2665536.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>10 </code><br><code>Technical Documentation </code><br><code>Legislators should take into consideration the feasibility of compliance with prescriptive </code><br><code>documentation requirements in article 11 and annex IV for the variety of systems in scope. </code><br><code>The requirements in point 2 of Annex IV to provide a detailed description of the general logic </code><br><code>of the algorithm, or extensive information on datasets could lead to the revealing of </code><br><code>potentially sensitive information. At the same time, such burdensome reporting requirements </code><br><code>may be difficult for smaller providers to comply with and thus may risk disincentivizing </code><br><code>innovation.  </code><br><code> </code><br><code>The proposal should focus on feasible documentation appropriate to the use case, rather than </code><br><code>rigorous proofs of quality. Since AI systems are software processes and not products, a checkbox exercise of requirements is unlikely to adequately serve the purpose of a future-proof </code><br><code>framework. Data quality and data provenance vary greatly, and thorough documentation is </code><br><code>not always possible or necessary in light of risks. We recommend allowing for more flexibility </code><br><code>in demonstrating compliance with the AI Act, using relevant documentation that could come </code><br><code>from a company‚Äôs internal practices as well as from documentation required by international </code><br><code>standards.  </code><br><code> </code><br><code>Record Keeping  </code><br><code>The record keeping requirements in article 12 are also based on a prescriptive approach, and </code><br><code>present difficulties for uniform implementation by the variety of providers in scope of the AI </code><br><code>Act. Article 13(1) for instance mandates an automatic recording of logs on high-risk AI </code><br><code>systems. As mentioned, there should be flexibility on </code><code>how </code><code>to achieve some of the goals, to </code><br><code>ensure that the regulation is as future-proof as possible and allows the necessary flexibility </code><br><code>for providers to find the most appropriate and efficient solutions for their specific AI product. </code><br><code>In this sense, a prescriptive mandatory requirement to have automated log-recording in place </code><br><code>would place a significant burden on providers.</code><code> </code><code>We thus suggest avoiding mandatory recordkeeping requirements on an ongoing basis as provided by article 16(d) and article 20, as some </code><br><code>providers may have capacity problems in storing and maintaining these large amounts of </code><br><code>data. </code><br><code> </code><br><code>Looking at previous experiences, requirements imposing record keeping, for instance for </code><br><code>medical devices for a period of time equivalent to the design and expected life of the device </code><br><code>can generate high administrative burden and investments for an outcome that is uncertain. </code><br><code>Specific, measured requirements for enumerated high-risk AI applications should be </code><br><code>considered instead. Often, a better solution is to support the development of standardised </code><br><code>testing and require specific types of testing for high-risk AI applications. The EU should </code><br><code>establish (or fund/support establishment of) these standards and benchmarks, including </code><br><code>tests, for high-risk scenarios, and ensure AI systems that will be used in those scenarios meet </code><br><code>these standards. This would be a far more effective method of addressing these concerns </code><br><code>than requiring years of recordkeeping and making proprietary data/programming/algorithms </code><br><code>available. </code><br><code> </code><br><code>Transparency </code><br><code>Transparency is an important aspect of and helps facilitating trust in AI systems. Still, it does </code><br><code>not automatically equate to better control of automated decisions by the user. For example, </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665532_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665532.pdf,4,2,2665532,attachments/2665532.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>www.</code><code>medtecheurope</code><code>.org</code><code> </code><br><code>Page 2 of 4</code><code> </code><br><code>similar to the requirements in MDR/IVDR but differ in terms of details, which may lead to complex </code><br><code>interpretation issues. Although we acknowledge that duplication is not the Commission‚Äôs intended vision, </code><br><code>there are concerns that the AIA would in effect create the need for manufacturers to </code><code>undertake </code><br><code>duplicative certification / conformity assessment, via</code><code> </code><code>two Notified Bodies, and maintain two sets of </code><br><code>technical documentation, should misalignments between AIA and MDR/IVDR not be resolved.</code><code> </code><br><code>Duplication of this kind would lead to unnecessary overlaps in the regulatory approval of AI as/in medical </code><br><code>technology, which could have a negative effect on the timely access of citizens and patients to highly </code><br><code>innovative and fairly priced AI medical technology in the EU.  </code><br><code> </code><br><code>Some examples of misalignment between the AIA and the MDR/IVDR include: </code><br><code> </code><br><code>‚Ä¢ </code><code>Risk Classification</code><code>: Medical technologies can be assigned to a range of risk classes under the </code><br><code>MDR/IVDR. While the AIA is not meant to change sectoral classification, it would put most cases of AI </code><br><code>in/as a medical technology in the highest risk class under the Act. This is a clear deviation from how </code><br><code>medical technologies are regulated in Europe and around the world. It would drive confusion among </code><br><code>regulators and manufacturers and would create additional, unnecessary complexity in the regulatory </code><br><code>approval process that could hinder highly innovative technology to reach citizens in a timely manner.  </code><br><code>‚Ä¢ </code><code>Definitions</code><code>: Terms such as ‚Äúprovider‚Äù, ‚Äúimporter‚Äù, ‚Äúserious incident‚Äù, ‚Äúputting into service‚Äù and ‚Äúuser‚Äù in </code><br><code>the AIA do not match those under the MDR/IVDR. The definition of ‚Äòrisk‚Äô which is the main concept of </code><br><code>conformity assessment is missing in the proposal. </code><br><code>‚Ä¢ </code><code>Certification</code><code>: The AI Act requirements for designation of Notified Bodies, conduct of conformity </code><br><code>assessments, and issuance of certificates differ in substance from the corresponding requirements of </code><br><code>the MDR/IVDR. In particular, MedTech Europe has concerns regarding the lack of clarity on the </code><br><code>following matters, which collectively pose the risk of dual/duplicative certification of AI in/as medical </code><br><code>technology: </code><br><code>‚Ä¢ </code><code>Designation of Notified Bodies for AI-specific competencies. </code><br><code>‚Ä¢ </code><code>Time and capacity required for the designation of Notified Bodies with AI and medical technology</code><code> </code><br><code>competencies to be ramped up. </code><br><code>‚Ä¢ </code><code>Roles and responsibilities for Notified Bodies conducting conformity assessments on AI as/in medical </code><br><code>technology.  </code><br><code>‚Ä¢ </code><code>The very possible scenario where an MDR/IVDR-designated Notified Body is not successfully </code><br><code>designated for AI-specific competencies: should this occur, the risk of needing two separate technical </code><br><code>documentation submissions, leading to two separate conformity assessments and certifications, must </code><br><code>be avoided. </code><br><code>‚Ä¢ </code><code>Vigilance and post-market surveillance reporting</code><code>: The future European Database for Medical </code><br><code>Devices (EUDAMED) should remain the system used for these purposes to ensure aligned, </code><br><code>streamlined, efficient non-duplicative market surveillance of AI in/as medical technologies.  </code><br><code>‚Ä¢ </code><code>Change control</code><code>: The question of what constitutes ‚Äòchanges‚Äô in AI systems and the requirements for </code><br><code>new submissions in case substantial changes are made is an area of considerable unclarity and </code><br><code>urgently needs additional clarification in order to not hinder innovative AI medical technologies to enter </code><br><code>the market. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662219_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662219.pdf,2,1,2662219,attachments/2662219.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>1 </code><br><code> </code><br><code> </code><br><code> </code><br><code>EuroGeographics feedback on EU </code><br><code>Artificial Intelligence Act proposal</code><code> </code><br><code> </code><br><code>Ref no.2021-00037.LT </code><br><code>TR 51080067776-74 </code><br><code>Brussels, 2021</code><code> </code><br><code> </code><br><code>Key points: </code><br><code>-</code><code> </code><br><code>Legal certainty, risk assessment levels, regulated use, and concept of trust by increasing </code><br><code>the trust for EU made AI products are the main benefits of the proposal from </code><br><code>Eurogeographics members‚Äô point of view. </code><br><code>-</code><code> </code><br><code>The coherence of this Regulation proposal with the Commission‚Äôs overall digital and data </code><br><code>strategy is very much appreciated. </code><br><code>-</code><code> </code><br><code>EuroGeographics members‚Äô authoritative data, high quality datasets, are back bones to </code><br><code>many of the critical infrastructures as defined in the proposed Regulation.  </code><br><code>-</code><code> </code><br><code>This Regulation represents a good start for regulated development and deployment of AI </code><br><code>systems within geo spatial community. </code><br><code>-</code><code> </code><br><code>We understand that the Commission intends to revise the Liability legal framework by </code><br><code>taking into account innovative technologies including AI, and we will be reviewing these </code><br><code>with a specific interest. </code><br><code> </code><br><code>EuroGeographics is an independent international not-for-profit organisation representing Europe‚Äôs </code><br><code>National Mapping, Cadastral and Land Registration Authorities (NMCAs). We are a passionate </code><br><code>advocate for European geospatial data from official trusted sources, in particular when it is </code><br><code>harmonised to standard specifications. We</code><code> </code><code>are providers of a range of products and services and </code><br><code>expertise which supports navigation, automated vehicles management, emergency response, a </code><br><code>reliable and secure land and property market, fiscal and many more government and business </code><br><code>decisions and services. </code><br><code> </code><br><code>NMCAs developments of AI systems is still very much at proof-of-concept stage. However, our sector </code><br><code>has proved its ability to embrace new technologies and we reckon that what is new now will certainly </code><br><code>go mainstream soon. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663398_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2663398.pdf,2,2,2663398,attachments/2663398.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Trust</code><code>: </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed urges that the legislation builds trust in AI, including among public health </code><br><code>practitioners and patients, by ensuring appropriate protections are in place, such as </code><br><code>maintaining data for monitoring performance, and limiting the claims that can be made about </code><br><code>AI systems. Additionally, education, training and up/re-skilling will be important to ensure that </code><br><code>healthcare professionals and patients are comfortable using AI technologies. </code><br><code> </code><br><code>Access to data:</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed calls attention to the importance of the secondary use of healthcare data for </code><br><code>innovation and urges the legislative framework to allow for the reuse of health data for AI </code><br><code>development purposes as well as for research insights. </code><br><code>‚Ä¢</code><code> </code><br><code>In order to support the EU in realizing its vision for AI in Europe, ResMed urges the Commission </code><br><code>to clarify the roles of controller, processor and joint controller under the GDPR to enable a </code><br><code>competitive and innovation friendly environment for healthcare AI. </code><br><code> </code><br><code>AI value chain:</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Whilst additional safety provided by looking at the entire AI value chain is welcomed, ResMed </code><br><code>urges for this not to add extensive additional requirements which would stymie innovation </code><br><code>and the free flow of data -- hindering trade, including digital trade, which is key to bringing AI </code><br><code>systems to the EU market and, equally, allowing EU companies to fully participate in global </code><br><code>efforts to innovate. </code><br><code> </code><br><code>Foreseen European AI Board:</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed welcomes the proposed European AI Board but calls for the Commission to look to </code><br><code>lessons learned from the GDPR enforcement model and looks forward to more clarity on how </code><br><code>national authorities will be supported in the necessary up/re-skilling. Additionally, the </code><br><code>European AI Board should ensure it works collaboratively with and does not duplicate or </code><br><code>undermine the work of the European Data Protection Board or the European Data Innovation </code><br><code>Board foreseen under the DGA.  </code><br><code>‚Ä¢</code><code> </code><br><code>ResMed commends the proposal for an independent expert group to assist the European AI </code><br><code>Board in its work. Encourages the Commission to ensure that the group take into account and </code><br><code>adequately represent the specificities of the healthcare sector including public authorities, </code><br><code>clinical research, health practitioners and patients, and private companies via consultative </code><br><code>processes.  </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663263_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663263.pdf,2,2,2663263,attachments/2663263.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>- </code><br><code>KI-Systeme haben das </code><code>Potenzial</code><code>, die Produktivit√§t von Unternehmen und gleichzeitig </code><br><code>das Wohlbefinden der Arbeitskr√§fte zu steigern, etwa durch eine effektive </code><br><code>Aufgabenteilung zwischen Mensch und Maschine, durch die Bereitstellung von Tools zur </code><br><code>Kompetenzentwicklung und durch den Zugang zu besseren Arbeitsbedingungen, </code><br><code>insbesondere im Bereich Gesundheit und Sicherheit.   </code><br><code>- </code><br><code>Dar√ºber hinaus gibt die autonome </code><code>Vereinbarung der europ√§ischen Sozialpartner</code><code> </code><code>zur </code><br><code>Digitalisierung</code><code> bereits einige Richtungen und Grunds√§tze vor, wie und unter welchen </code><br><code>Umst√§nden KI in die Arbeitswelt eingef√ºhrt wird. </code><br><code>- </code><br><code>Eine zu weit gefasste Definition w√ºrde vor allem KI-Anwendungen im </code><code>Personal- und </code><br><code>Bildungswesen</code><code> ausbremsen, die ihre Effektivit√§t und Sicherheit bei der Verbesserung </code><br><code>des Bewerbenden- und Mitarbeitenden-Erlebnisses sowie bei der Steigerung der </code><br><code>Effizienz unter Beweis gestellt haben. </code><br><code>- </code><br><code>Aus diesen Gr√ºnden empfehlen wir, die in Anhang III aufgef√ºhrten Bereiche auf </code><br><code>spezifischere Anwendungsf√§lle</code><code> mit hohem Risiko einzugrenzen.  </code><br><code> </code><br><code>Aktualisierung der Liste von KI-Systemen mit hohem Risiko schafft Unberechenbarkeit </code><br><code>- </code><br><code>Die Europ√§ische Kommission wird erm√§chtigt, die Liste der eigenst√§ndigen KI-Systeme </code><br><code>mit hohem Risiko durch </code><code>delegierte Rechtsakte</code><code> (Art. 7) zu aktualisieren.  </code><br><code>- </code><br><code>Diese </code><code>dynamische Anpassung des Geltungsbereichs</code><code> kann zu gro√üer </code><br><code>Unberechenbarkeit f√ºr den Markt f√ºhren.  </code><br><code>- </code><br><code>KI-Anbieter w√ºrden aufgrund der </code><code>unvorhersehbaren Entwicklung des </code><br><code>Anwendungsbereichs der Verordnung</code><code> in den n√§chsten Jahren davon abgehalten, </code><br><code>innovative KI-L√∂sungen zu entwickeln.  </code><br><code>- </code><br><code>Die </code><code>Kriterien</code><code> (Art. 7, Abs. 2), die die Kommission erm√§chtigen, die Liste in Anhang III zu </code><br><code>aktualisieren, indem sie KI-Systeme mit hohem Risiko unter bestimmten Bedingungen </code><br><code>hinzuf√ºgt, sind </code><code>zu vage</code><code>. </code><br><code>- </code><br><code>Um die Rechtssicherheit und die Vorhersehbarkeit des Marktes zu unterst√ºtzen, </code><br><code>begr√º√üen wir weitere Klarheit √ºber die genauen Kriterien, die es der Kommission </code><br><code>erm√∂glichen w√ºrden, die Liste der Hochrisiko-KI-Systeme zu aktualisieren. </code><br><code>- </code><br><code>Die Einf√ºhrung expliziter Bestimmungen f√ºr die </code><code>Beteiligung der Anbieter und der </code><br><code>betrieblichen Nutzer</code><code> an jedem zuk√ºnftigen Prozess zur Aktualisierung der Liste ‚Äì etwa </code><br><code>durch die Ausweitung des Mandats der hochrangigen Expertengruppe f√ºr KI ‚Äì w√§re </code><br><code>sinnvoll. </code><br><code> </code><br><code>Verpflichtungen f√ºr Anbieter und Anwender nachbessern  </code><code> </code><br><code>- </code><br><code>Im Verordnungsvorschlag bleibt die </code><code>Verantwortungsverteilung</code><code> zwischen Anbietern und </code><br><code>Anwendern </code><code>unklar</code><code>.  </code><br><code>- </code><br><code>Es scheint nicht ber√ºcksichtigt zu werden, dass generelle KI-Anwendungen durch den </code><br><code>Nutzer f√ºr einen </code><code>bestimmten Zweck konfiguriert</code><code> werden k√∂nnen. In diesem Fall hat der </code><br><code>Anbieter keine Kontrolle √ºber die Anwendung.  </code><br><code>- </code><br><code>Derzeit gelten die Pflichten Dritter inklusive Nutzer (Art. 28) nur f√ºr Systeme, die bereits </code><br><code>von Anfang an als Hochrisiko-Systeme klassifiziert werden.  </code><br><code>- </code><br><code>Hier sollte der Geltungsbereich auch auf Nutzer ausgeweitet werden, die den </code><br><code>Verwendungszweck eines sich bereits </code><code>auf dem Markt befindlichen</code><code> oder in Betrieb </code><br><code>genommenen KI-Systems so ver√§ndern, dass ein Hochrisiko-System geschaffen wird.  </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665646_5,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665646.pdf,15,5,2665646,attachments/2665646.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>areas that have historically been targeted by law enforcement.</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> The same is often true for </code><br><code>prosecutorial or judicial records. </code><br><code>It is well documented that there is significant discrimination in criminal justice systems in the EU.</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> The </code><br><code>EU Fundamental Rights Agency (FRA) has found clear disparities in stop and search practices against </code><br><code>people from ethnic and national minority groups in several Member States in studies conducted over </code><br><code>5 years in both 2010 and 2018.</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code> Across Europe, there are also clear and widespread racial and ethnic </code><br><code>disparities in pre-trial detention. In Council of Europe states, 40% of all ‚Äòforeign nationals‚Äô in prison </code><br><code>were being held in pre-trial detention, compared to 25% of all prisoners. People from certain ethnic </code><br><code>or racial groups face worse outcomes in their cases: they face longer prison sentences and are not </code><br><code>granted non-custodial sanctions such as fines,</code><code style=""font-weight: 1000; background-color: #FF0000;"">20</code><code> and data from across Europe shows that they are also </code><br><code>disproportionately over-represented in prison, relative to the percentage of the population they </code><br><code>represent.</code><code style=""font-weight: 1000; background-color: #FF0000;"">21</code><code>  </code><br><code>When law enforcement and criminal justice data representing such disparities is used in predictive, </code><br><code>profiling and risk-assessment AI systems in criminal justice, it will inevitably flag up people, whose </code><br><code>profiles fit those who are over-represented in that data, as being higher risk. AI built and operated on </code><br><code>data embedded with such biases, which then assists or informs, law enforcement or criminal justice </code><br><code>decisions, can also result in a re-enforcement and re-entrenchment of those biases.</code><code style=""font-weight: 1000; background-color: #FF0000;"">26</code><code> This leads to a </code><br><code>self-fulfilling prophecy whereby the predictions become true because police target those groups </code><br><code>following the prediction or risk-assessment, which leads the system to strengthen its correlation </code><br><code>between those groups and criminal justice outcomes, leading to self-perpetuating ‚Äòfeedback loops‚Äô </code><br><code>which reinforce patterns of inequality.</code><code style=""font-weight: 1000; background-color: #FF0000;"">27</code><code> </code><br><code>These AI systems are increasingly used in European and EU Member States‚Äô law enforcement and </code><br><code>criminal justice systems. For example, in 2015 the Amsterdam Municipality started the ‚ÄòTop 400‚Äô, a </code><br><code> </code><br><code>5</code><code> ibid. </code><br><code>6</code><code> Justicia European Rights Network ‚ÄòDisparities in Criminal Justice Systems for Individuals of Different Ethnic, </code><br><code>Racial, and National Background in the European Union‚Äô (November 2018); and Namoradze, Zaza and Pacho, </code><br><code>Irmina ‚ÄòWhen It Comes to Race, European Justice Is Not Blind‚Äô (2018) Open Society Justice Initiative, </code><br><code>https://www.justiceinitiative.org/voices/when-it-comes-race-european-justice-not-blind</code><code>. </code><br><code>7</code><code> Fundamental Rights Agency, ‚ÄòEU-MIDIS II Second European Union Minorities and Discrimination Survey‚Äô (2018); </code><br><code>and European Union Agency for Fundamental Rights ‚ÄòData in Focus Report ‚Äì Police Stops and Minorities‚Äô (2010), </code><br><code>https://fra.europa.eu/sites/default/files/fra_uploads/1132-EU-MIDIS-police.pdf</code><code>. </code><br><code>20</code><code> Fair Trials, Disparities and Discrimination in the European Union‚Äôs Criminal Legal Systems, January 2021. </code><br><code>https://www.fairtrials.org/sites/default/files/publication_pdf/Disparities-and-Discrimination-in-the-EuropeanUnions-Criminal-Legal-Systems.pdf</code><code>  </code><br><code>21</code><code> Hilde Wermink, Sigrid van Wingerden, Johan van Wilsem & Paul Nieuwbeerta, Studying Ethnic Disparities in </code><br><code>Sentencing: The Importance of Refining Ethnic Minority Measures, in Handbook on Punishment Decisions 239264 (L Ulmer & M Bradley 1ed 2019); Samantha Bielen, Peter Grajzl and Wim Marneffe, Blame Based on One's </code><br><code>Name? Extralegal Disparities in Criminal Conviction and Sentencing, SSRN Electronic Journal, (2008); and Virginie </code><br><code>Gautron & Jean-No√´l Reti√®re, La d√©cision judiciaire : jugements p√©naux ou jugements sociaux ?, 88 Mouvements </code><br><code>11-18 (2016). </code><br><code>26 Lum, Kristian and William, Isaac ‚ÄòTo Predict and Serve?‚Äô (2016) Significance 13 (5): 14‚Äì19 </code><br><code>https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2016.00960.x. </code><code>  </code><br><code>27 ibid; Ensign, Danielle et al. ‚ÄòRunaway Feedback Loops in Predictive Policing‚Äô (2017) Cornell University Library, </code><br><code>29 June 2019, https://arxiv.org/abs/1706.09847; Bennett Moses, Lyria and Chan, Janet, ‚ÄòAlgorithmic prediction </code><br><code>in </code><br><code>policing: </code><br><code>assumptions, </code><br><code>evaluation, </code><br><code>and </code><br><code>accountability‚Äô </code><br><code>(2016) </code><br><code>Policing </code><br><code>and </code><br><code>Society </code><br><code>28:7, </code><br><code>https://www.tandfonline.com/doi/10.1080/10439463.2016.1253695. </code><code> </code>",POSITIVE
fitz_2665170_11,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665170.pdf,12,11,2665170,attachments/2665170.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Hub France IA ‚Äì Groupe de Travail Banques et Auditabilit√© </code><br><code> </code><br><code>p 11/ 12  </code><br><code>1.</code><code> </code><code>Est-ce que cela englobe tout type de cr√©dit ou uniquement les cr√©dits ¬´ essentiels ¬ª ? Le p√©rim√®tre </code><br><code>d‚Äôapplication devrait √™tre d√©fini plus pr√©cis√©ment.  </code><br><code>2.</code><code> </code><code>Ensuite, est-ce que cela inclut √©galement le monitoring du cr√©dit ou est-ce qu‚Äôon reste uniquement au niveau </code><br><code>de l‚Äôoctroi d√®s lors que le monitoring ne supprime pas de service essentiel ? Qu‚Äôen est-il des mod√®les </code><br><code>d‚Äô√©valuation de la solvabilit√© d‚Äôune personne qui peuvent ensuite √™tre utilis√©s pour √©tablir un score d‚Äôoctroi </code><br><code>ou pour suivre le niveau de risque associ√© √† un pr√™t existant ? </code><br><code>3.</code><code> </code><code>Question sur l‚Äôemploi du terme ¬´ personne physique ¬ª.  Il est fait mention ici de la personne physique, pas </code><br><code>du citoyen. Dans le cas des soci√©t√©s unipersonnelles (personne morale) repr√©sent√©es par une seule personne </code><br><code>physique, les auto-entrepreneurs, doit-on consid√©rer que la demande de cr√©dit de la personne morale est </code><br><code>classifi√©e comme √† haut risque ?   </code><br><code>4.</code><code> </code><code>L‚ÄôIA √©tant dans le document d√©finie de mani√®re tr√®s large/vaste, est-ce que des solutions bas√©es sur des </code><br><code>syst√®mes de r√®gles m√©tiers ou m√™me parfois des calculs fait sur des fichiers Excel et qui font de l‚Äô√©valuation </code><br><code>de ¬´ solvabilit√© ¬ª vont rentrer sous le coup de la r√©gulation ? </code><br><code>USAGES PAR LES AUTORITES REPRESSIVES </code><br><code>Les dispositifs bas√©s sur l‚ÄôIntelligence Artificielle mis en ≈ìuvre par les banques √† ce titre peuvent concerner : </code><br><code>a)</code><code> </code><code>la lutte contre le blanchiment d‚Äôargent et le terrorisme,  </code><br><code>b)</code><code> </code><code>lutte contre la fraude interne et externe,  </code><br><code>c)</code><code> </code><code>devoir de diligence pour la connaissance client. </code><br><code>Ces syst√®mes pourraient √™tre consid√©r√©s comme des composantes de la s√©curit√© (au sens de Art.6 1.) des </code><br><code>op√©rations financi√®res et semblent relever de l‚ÄôAnnexe III.6.g et seraient donc consid√©r√©s comme √† haut risque </code><br><code>ou pourraient le devenir √† la faveur d‚Äôune modification telle que pr√©vue √† l‚Äôarticle 7.  Est-ce bien le cas ? Dans </code><br><code>l‚Äôaffirmative, les d√©lais induits par les obligations pr√©vues au chapitre 3 ou la transparence requise au Titre IV </code><br><code>n‚Äôiront-elles pas √† l‚Äôencontre des obligations r√©glementaires vis√©es ? La clause 6.g peut-elle √™tre pr√©cis√©e ? </code><br><code>Recommandation 13</code><code> : exclure explicitement les syst√®me IA d√©di√©s √† la s√©curisation des op√©rations financi√®res, </code><br><code>√† la lutte anti-fraude, anti blanchiment ou anti-terroriste de la liste des syst√®mes √† haut risque. </code><br><code>BIOMETRIE </code><br><code>Les cas d‚Äôusage connus √† date de l‚Äôusage de la biom√©trie sont potentiellement les suivants : </code><br><code>‚Ä¢</code><code> </code><br><code>Authentification client par reconnaissance faciale ou par reconnaissance d‚Äôempreinte digitale dans nos </code><br><code>applications mobiles pour acc√©der √† l‚Äôapplication et pour effectuer les op√©rations sensibles (enregistrement </code><br><code>nouveau RIB, souscription produit) </code><br><code>‚Ä¢</code><code> </code><br><code>Authentification client dans les centres d‚Äôappels par reconnaissance d‚Äôempreinte vocale </code><br><code>‚Ä¢</code><code> </code><br><code>Authentification par biom√©trie dans les ATM (</code><code>https://www.americanbanker.com/payments/news/howbiometric-atms-are-entering-mainstream-use</code><code>) </code><br><code>‚Ä¢</code><code> </code><br><code>Authentification par empreinte digitale pour s√©curiser des paiements directement sur les cartes de paiement </code><br><code>(</code><code>https://www.thalesgroup.com/fr/europe/france/dis/banque/cartes/biometrique-emv</code><code>  </code><br><code>‚Ä¢</code><code> </code><br><code>Authentification par empreinte digitale ou reconnaissance d‚ÄôIRIS des collaborateurs (ou de prestataires) </code><br><code>accr√©dit√©s √† p√©n√©trer dans des locaux sensibles (type datacenter) </code><br><code>‚Ä¢</code><code> </code><br><code>Usage de la reconnaissance faciale pour comparer le visage du client √† la photo de sa pi√®ce d'identit√© lors du </code><br><code>processus d'onboarding √† distance (exemple fintech https://www.ubble.ai/fr/accueil/#contact-ubble) </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665596_4,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665596.pdf,10,4,2665596,attachments/2665596.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>vorherigen Punkt diskutiert wurde. Insgesamt geht es darum, auf Basis </code><br><code>einer soliden Begr√ºndung den Umfang der umzusetzenden </code><br><code>Anforderungen dem Risiko entsprechend anpassen zu k√∂nnen. In den </code><br><code>grundlegenden Entscheidungsoptionen zur Vorbereitung der Verordnung </code><br><code>scheint eine solche Option nicht wirklich vorhanden gewesen zu sein.     </code><br><code>In der Einleitung wird von Ma√ünahmen gesprochen, mit denen bei KISystemen </code><code>‚Äûsowohl der Nutzen als auch die Risiken der KI auf Unionsebene </code><br><code>angemessen geregelt werden‚Äú</code><code>. Auch in dem White Paper der ‚ÄûHLEG AI‚Äú </code><br><code>wird einem ausgewogenen Verh√§ltnis von Risiken und Nutzen eine </code><br><code>wichtige Rolle zugeordnet. Die KI-Verordnung selbst betrachtet nur die </code><br><code>Seite der Risiken und erlaubt keine Abw√§gung gegen√ºber dem </code><br><code>potenziellen Nutzen, der sich aus einem System ergibt. </code><br><code>In der EU-Medizinprodukteverordnung (MDR) ist gezielt der folgende </code><br><code>Punkt mit aufgenommen: </code><code>‚Äû‚Ä¶ wobei etwaige Risiken im Zusammenhang </code><br><code>mit ihrer Anwendung gemessen am Nutzen f√ºr den Patienten vertretbar </code><br><code>und mit einem hohen Ma√ü an Gesundheitsschutz und Sicherheit vereinbar </code><br><code>sein m√ºssen.‚Äú</code><code> (Anhang I ‚Äì </code><code>Grundlegende Sicherheits- und </code><br><code>Leistungsanforderungen</code><code>, Kap. 1, Pos. 1). Das w√ºrde zu Inkonsistenzen </code><br><code>f√ºhren, da die KI-Verordnung eine solche Abw√§gung nicht erlaubt. </code><br><code>Gerade in Zeiten der CoViD-Pandemie ist deutlich geworden, dass </code><br><code>manchmal Risiken akzeptiert werden m√ºssen, um einen bestimmten </code><br><code>Nutzen erreichen zu k√∂nnen, siehe z.B. beschleunigte Zulassung von </code><br><code>Impfstoffen. Auch wenn es sich in dem Beispiel um ein Pharma- und nicht </code><br><code>um ein Medizinprodukt handelt, zeigt es, wie wichtig eine solche </code><br><code>Gegen√ºberstellung von Risiken und Nutzen ist. Andere Beispiele, die in </code><br><code>den Bereich KI hineinreichen, w√§ren Systeme zur Vorhersage und zum </code><br><code>Management der Ausbreitung der Infektionen.  </code><br><code>Es w√§re sinnvoll, zu erlauben, eine solche Risiko-Nutzen-Absch√§tzung </code><br><code>machen zu k√∂nnen, wenn entsprechende Begr√ºndungen/Nachweise f√ºr </code><br><code>den Nutzen dargelegt werden k√∂nnen ‚Äì alleine schon um Konsistenz mit </code><br><code>der MDR zu erreichen. Der Ansatz, hier √ºber Ausnahmegenehmigungen </code><br><code>gehen zu m√ºssen und dabei die Notwendigkeit eines </code><br><code>Konformit√§tsbewertungsverfahren aussetzen zu k√∂nnen (siehe </code><br><code>Erw√§gungsgrund (68</code><code>): ‚ÄûEs ist daher angebracht, dass die Mitgliedstaaten </code><br><code>aus au√üergew√∂hnlichen Gr√ºnden der √∂ffentlichen Sicherheit, des Schutzes </code><br><code>des Lebens und der Gesundheit nat√ºrlicher Personen und des Schutzes des </code><br><code>gewerblichen und kommerziellen Eigentums das Inverkehrbringen oder </code><br><code>die Inbetriebnahme von KI-Systemen, die keiner Konformit√§tsbewertung </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665421_6,company,../24212003_requirements_for_artificial_intelligence/attachments/2665421.pdf,7,6,2665421,attachments/2665421.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>                                                                                                                                             </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>6 </code><br><code> </code><br><code>account of the progress of artificial intelligence systems, there remains the issue of continued work on </code><br><code>defining ‚Äúinteraction with natural persons‚Äù. </code><br><code>  </code><br><code>On the other hand, para. 52 section 3 imposes the obligation to inform on changing, by the user of AI, </code><br><code>the objective reality, labelling this action as ‚Äúdeep fake‚Äù. Using the phrase ‚Äúdeep fake‚Äù in this context </code><br><code>does not seem justified as this term suggests intentional misinformation of the recipient with regard to </code><br><code>the identity of the presented object, frequently due to foul or even illegal motives. Obviously, such </code><br><code>practices should be stigmatised. Making changes to the presented reality may, however, have a polelike motivation - artistic, useful, explanatory, educational, quotative or polemical. At the same time it </code><br><code>should be assumed that these are the situations constituting a majority of cases consisting in </code><br><code>‚Äúmanipulating‚Äù with the real picture. Imposition, in every case, of the obligation to inform on the change </code><br><code>introduced even to the least extent, could materially hinder running journalist, artistic or, broadly </code><br><code>speaking, artistic activity. The second section of the item referred to above eliminates this risk to a lesser </code><br><code>extent. The legislator should specify whether every change should be labelled. </code><br><code> </code><br><code> </code><br><code>9. Disturbed balance of obligations among suppliers, implementing parties and users of high-risk </code><br><code>artificial intelligence </code><br><code> </code><br><code>In their current wording, laws make no difference between the obligations imposed on the AI user if </code><br><code>they perform the role of a party implementing a given use of artificial intelligence system and the </code><br><code>obligations of ‚Äúartificial intelligence system supplier‚Äù towards the customer. </code><br><code> </code><br><code>The party implementing the use of artificial intelligence should be ultimately a principal subject of the </code><br><code>assessment as enterprises offering tools based on artificial intelligence systems are not able, in the end, </code><br><code>to verify final use to which their systems are applied or additional data which may be introduced into </code><br><code>the system. Suppliers of solutions relying on artificial intelligence systems may and should deliver any </code><br><code>and all information that is necessary for the implementing parties to perform self-assessment. It is very </code><br><code>important for a supplier of API solutions/ interfaces of which the supplier of solutions relying on artificial </code><br><code>intelligence systems does not take over control when users give their consent to allowing customers/ </code><br><code>users for access to the solutions at their discretion. </code><br><code> </code><br><code>10. Exemptions pertaining to multi-task systems/ open source tools </code><br><code> </code><br><code>The obligations to observe requirements pertaining to artificial intelligence systems should be vested in </code><br><code>legal entities or natural persons using the tools of open source type, such as TensorFlow or AutoML as </code><br><code>they have a final control over their objective and using artificial intelligence implementations. Imposition </code><br><code>of obligations on open source tools suppliers would, to a large extent, discourage from providing such </code><br><code>technologies which support the entire eco-system of innovativeness.</code><code> </code><br><code> </code><br><code>We are also in favour of exempting from obligation to publish fundamental research as the publication </code><br><code>of fundamental research is not qualified as ‚Äúplacing on the market‚Äù or ‚Äúhanding in for use‚Äù. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662381_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662381.pdf,4,4,2662381,attachments/2662381.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>  </code><br><code> </code><br><code> </code><br><code>  </code><br><code> </code><br><code>Pagina 4 van 4 </code><br><code>  </code><br><code>mogelijk ook meerdere lidstaten bevoegd zijn ten aanzien van √©√©n </code><br><code>provider? </code><br><code>‚Ä¢</code><code> </code><br><code>Is jurisdictie verbonden aan de vestigingsplaats van de provider, </code><br><code>producent of gebruiker? Welke lidstaat is bevoegd om tot handhaving over </code><br><code>te gaan, is nog onvoldoende scherp. Meer duiding of aanscherping is </code><br><code>gewenst.  </code><br><code> </code><br><code>6.</code><code> </code><code>Co√∂rdinatie tussen toezichthouders </code><br><code>‚Ä¢</code><code> </code><br><code>Gedeelde verantwoordelijkheden betekent dat in de verschillende fasen </code><br><code>van het toezicht de bevoegde autoriteit zaakkennis moet overdragen of </code><br><code>dat een organisatie te maken krijgt met meerdere toezichthouders. Hoe </code><br><code>wordt informatiedeling en co√∂rdinatie van toezichtsactiviteiten voorzien? </code><br><code>En wat is de juridische basis? </code><br><code>‚Ä¢</code><code> </code><br><code>Daarnaast vraagt effectief markttoezicht in Europa om naast nationaal </code><br><code>toezicht een Europees netwerk op te richten waar aan gezamenlijke </code><br><code>toezichtsactiviteiten, onderzoeken en handhaving wordt gewerkt.   </code><br><code> </code><br><code>7.</code><code> </code><code>Rechtsbescherming burgers  </code><br><code>Er is onvoldoende voorzien in de mogelijkheid voor burgers om individuele </code><br><code>problemen met AI te rapporteren, waardoor de bescherming tekortschiet. Op </code><br><code>basis van de AI Act kijkt een toezichthouder naar het AI-systeem en de werking in </code><br><code>het geheel, niet naar de gevolgen voor een individueel geval. Om de burgers </code><br><code>voldoende rechtsbescherming te bieden moeten zij kunnen aangeven dat zij zijn </code><br><code>benadeeld door de toepassing van AI.   </code><br><code> </code><br><code>Het maatschappelijk vertrouwen in AI-toepassingen is afhankelijk van </code><br><code>rechtsbescherming, effectief toezicht, transparante markttoegang en </code><br><code>standaardisatie.  </code><br><code> </code><br><code>i</code><code> NISD. </code><code>Directive (EU) 2016/1148 of the European Parliament and of the Council of </code><br><code>6 July 2016 concerning measures for a high common level of security of network </code><br><code>and information systems across the Union.</code><code> </code><br><code>ii</code><code> eIDAS. </code><code>VERORDENING (EU) Nr. 910/2014 VAN HET EUROPEES PARLEMENT EN DE </code><br><code>RAAD van 23 juli 2014 betreffende elektronische identificatie en vertrouwensdiensten </code><br><code>voor elektronische transacties in de interne markt.</code><code> </code><br><code>iii</code><code> RED. </code><code>Directive 2014/53/EU of the European Parliament and of the Council of 16 </code><br><code>April 2014 on the harmonisation of the laws of the Member States relating to the </code><br><code>making available on the market of radio equipment.</code><code> </code><br><code>iv</code><code> CSA. </code><code>VERORDENING (EU) 2019/881 VAN HET EUROPEES PARLEMENT EN DE RAAD </code><br><code>van 17 april 2019 zake ENISA (het Agentschap van de Europese Unie voor </code><br><code>cyberbeveiliging), en inzake de certificering van de cyberbeveiliging van informatie- </code><br><code>en communicatietechnologie.</code><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665627_9,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665627.pdf,11,9,2665627,attachments/2665627.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>proposed regulation, which could have a negative effect on product safety and trust but yield </code><br><code>positive effects in terms of investments and job opportunities on national level. </code><br><code> </code><br><code>Since it is the national competent authorities that decide on the usage of regulatory sandboxes </code><br><code>it is also important that each authority is aware of who can establish a regulatory sandbox for </code><br><code>which AI system.  </code><br><code> </code><br><code>Raising our heads </code><br><code>The task of analyzing the proposed regulation and its possible effects is daunting. There are </code><br><code>interactions with other regulations, either as they stand today [4,8] or might be formulated in </code><br><code>the future [11,12,13,15], across diverse activities such as the production of toys, social </code><br><code>interaction on digital platforms and public governance. To grasp the full picture and describe </code><br><code>the most important issues at hand requires both time and resources.  </code><br><code> </code><br><code>It is therefore difficult to determine what the impact of the regulation will be in terms of </code><br><code>innovation, public governance, and trust. There are aspects on how SMEs will fare in relation </code><br><code>to large and established actors where the regulation could serve to hamper innovation but also </code><br><code>lead to new kinds of partnerships and business opportunities. There are aspects of how the </code><br><code>regulation can hamper open access to public assets, the usage of digital services and products </code><br><code>in public governance as well as the relationship between national governance and the EU </code><br><code>institutions. If these aspects are not resolved in a satisfactory manner there can be </code><br><code>implications on public trust, not necessarily towards AI as technology but towards the </code><br><code>regulation as such and the intensions to regulate AI as technology. </code><br><code> </code><br><code>We have so far mainly focused on the content of the proposed regulation. If we instead look </code><br><code>at what is not in the proposition, there is one global aspect missing ‚Äì sustainable </code><br><code>development. The impact of AI on sustainable development is substantial. If one wants to, </code><br><code>you can see an attempt to address some of the social aspects in the current proposal, such as </code><br><code>in article 5 and annex III. Some of the economical sides can also be identified by a willing </code><br><code>eye in terms of annex II and article 6. There are of course more examples of where </code><br><code>sustainable development can be induced from the proposition.  </code><br><code> </code><br><code>However, it is far more difficult to see any attempt to regulate the climate impact of AI. Still, </code><br><code>there is an opportunity to align the visions of leading responsible digitalization and </code><br><code>sustainable development. The question is why the opportunity has not been grasped? Is it </code><br><code>because it is too difficult to determine the amount of resources used for AI development and </code><br><code>how to regulate that consumption? Or is it too sensitive? There might be many, and more, </code><br><code>reasons, so stating a</code><code> </code><code>clear intent in terms of how AI and sustainable development are to be </code><br><code>aligned within the harmonized market would reduce the uncertainty. Regulating how AI is </code><br><code>used within sustainable development is a complex task but if the regulation is going to serve </code><br><code>its objectives and fit in with the needs of today and future generations it needs to be </code><br><code>addressed. If not now, when?  </code><br><code> </code><br><code>References </code><br><code>[1] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE </code><br><code>COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE </code><br><code>(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION </code><br><code>LEGISLATIVE ACTS COM/2021/206 final </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665519_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665519.pdf,3,2,2665519,attachments/2665519.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Page | 2  </code><br><code> </code><br><code>body.  Similar to auditors for corporate governance, this new, complex and evolving environment calls </code><br><code>for the establishment of independent external auditors who examine the processes and procedures </code><br><code>put in place by the provider when developing the AI system. External auditors could provide an </code><br><code>accurate and fair understanding of the technical documentation released by the provider, helping to </code><br><code>generate trust among the public at large. A specific provision should be included in this regard. </code><br><code> </code><br><code>3.</code><code> </code><code>Transparency and provision of information to users - Article 13 </code><br><code>CPME fears that the information provided to users will not allow appropriate understanding of the AI </code><br><code>system. Particularly in healthcare, transparency requires that the information provided to users is clear </code><br><code>and understandable for non it-specialists. Moreover, an independent authority or third party should </code><br><code>have access to the algorithm in case of complaints or questions, taking due account for copyright, </code><br><code>privacy and commercial sensitivities. An open source should be allowed for certain AI systems, where </code><br><code>specialist organisations can test the algorithm to ensure that there is no bias.  </code><br><code> </code><br><code>4.</code><code> </code><code>Human oversight ‚Äì Article 14 </code><br><code>CPME recommends that the </code><code>human oversight</code><code> is of ‚Äòhigh quality‚Äô, meaning that the individual needs to </code><br><code>have the necessary competences to guarantee an adequate oversight, and the provider is </code><br><code>appropriately resourced for the effective performance of the task. Paragraph 3a: the function for the </code><br><code>human oversight should be an integral part of the High Risk AI System which enables effective human </code><br><code>oversight with high usability. The insertion ‚Äúwhen technically feasible‚Äù should be deleted. </code><br><code> </code><br><code>5.</code><code> </code><code>Quality management system ‚Äì Article 17(1)(i) and Article 62 </code><br><code>CPME supports a clear obligation to audit and to quality control with regularly statutory reporting </code><br><code>obligation to the regulator. CPME further recommends full disclosure of serious incidents and </code><br><code>malfunctions by providers/developers of AI systems to patients and users. In addition, medical </code><br><code>obligations need to be supervised by medical regulators, such as the health inspectorate, to guarantee </code><br><code>the quality of healthcare. Agreements and collaborations will be required on who ensures oversight </code><br><code>over what. </code><br><code> </code><br><code>6.</code><code> </code><code>Conformity assessment ‚Äì Article 43 </code><br><code>CPME supports the inclusion under Article 43 of the Proposal of an </code><code>ex-ante</code><code> third-party conformity </code><br><code>assessment to be carried out for high-risk AI.2 </code><br><code> </code><br><code>7.</code><code> </code><code>CE marking of conformity ‚Äì Article 49 </code><br><code>CPME believes that the CE marking of conformity should only be given to those AI systems that comply </code><br><code>with EU law, including the General Data Protection Regulation (GDPR). The compliance of the latter </code><br><code>should be a requirement under Chapter II and audited by a third party before the CE marking is affixed. </code><br><code>This would ensure alignment with the rules and principles of data protection, in particular the </code><br><code>accountability principle pursuant to Article 5(2) of the GDPR.</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>  </code><br><code> </code><br><code> </code><br><code>2</code><code> In this sense, please see point 37 of the </code><code>EDPB-EDPS Joint Opinion 5/2021</code><code>, 18 June 2021. </code><br><code>3</code><code> In this sense, please see point 23 of the </code><code>EDPB-EDPS Joint Opinion 5/2021</code><code>, 18 June 2021.</code><code> </code>",FALSE_NEGATIVE
fitz_2665299_3,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665299.pdf,5,3,2665299,attachments/2665299.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>3 </code><br><code> </code><br><code>this type of AI. This would enable the Commission to produce a complete EU </code><br><code>AI strategy. </code><br><code> </code><br><code>AI and its impact in the field of competition </code><br><code>5. Another point which the Commission could turn its further attention to is the </code><br><code>competition rules in the digital sphere.  It is well known and documented that </code><br><code>certain technology giants have influential power in all aspects of technology.  </code><br><code>An example on point is the penalty the Commission imposed on Google, for </code><br><code>promoting its own services in search results, while reducing the rankings of </code><br><code>competing services through its algorithm.</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code>  Considering the increased use of AI </code><br><code>in services, it is evident that AI and algorithms have a far-reaching effect </code><br><code>nowadays. As such, they may enable large tech companies to gain unfair </code><br><code>competition advantages/dominance over rivals and/or may modify the </code><br><code>intellectual property and other business rights landscape in the EU and beyond. </code><br><code>This can be done through AI generated creations</code><code style=""font-weight: 1000; background-color: #FF0000;"">8</code><code> and search engines using </code><br><code>algorithms of their own to prioritise certain results. It is emphasised again that </code><br><code>the proposed Regulation scope should be expanded as elaborated in the </code><br><code>previous section. The Commission should take steps to discourage and/or </code><br><code>further frame such practices and thereby create a pluralistic digital environment </code><br><code>for tech companies and consumers alike, while balancing citizens‚Äô interests in </code><br><code>an inclusive digital world.   </code><br><code> </code><br><code>Introduction of new rights </code><br><code>6. In the European digital public legal order, the Commission should also consider </code><br><code>the introduction of new rights in the Regulation and/or wider regulatory </code><br><code>framework, to ensure that </code><code>in effect</code><code> ‚ÄòAI is safe, lawful and in line with EU </code><br><code>                                                           </code><br><code>7</code><code> Antitrust: Commission fines Google ‚Ç¨2.42 billion for abusing dominance as search engine by giving illegal </code><br><code>advantage to own comparison shopping  service (European Commission, 27 June 2016) </code><br><code>https://ec.europa.eu/commission/presscorner/detail/en/IP_17_1784</code><code>  </code><br><code>8</code><code> European Parliament resolution of 20 October 2020 on intellectual property rights for the development of </code><br><code>artificial intelligence technologies (2020/2015(INI), 20 October 2020) </code><br><code>14https://www.europarl.europa.eu/doceo/document/TA-9-2020-0277_EN.html </code>",POSITIVE
fitz_2665421_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665421.pdf,7,3,2665421,attachments/2665421.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>                                                                                                                                             </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>3 </code><br><code> </code><br><code>pertaining to fundamental rights a strong and competitive-against-supradomestic-institutions role is </code><br><code>played by courts and tribunals of constitutional nature which reserve their right of priority for the </code><br><code>assessment of issues related to the protection of fundamental rights. A good example is the Republic of </code><br><code>Germany where from the </code><code>Solange I </code><code>ruling of 1974 through subsequent rulings pertaining to </code><br><code>constitutionality of the Community treaties to the ruling of 2010 on </code><code>Honeywell</code><code>, the German </code><br><code>Bundesverfassungsgericht was persistently emphasising its superior role for matters related to </code><br><code>fundamental rights. </code><code> </code><br><code> </code><br><code>The abovementioned term should be made materially precise by virtue of specification of the </code><br><code>provision text or adding a theme which would narrow down directions for interpretation.  </code><br><code> </code><br><code> </code><br><code>4. Possibility for data training even for some high risk applications </code><br><code> </code><br><code>We support the concept assuming that for some internal procedures of artificial intelligence system </code><br><code>efficient acts be taken up in order to counteract potential threats and create prudential framework for </code><br><code>conducting internal declaration of conformity. A good example is the assessment of creditworthiness. A </code><br><code>majority of artificial intelligence applications for the assessment of creditworthiness is managed </code><br><code>internally with no need to assess the conformity externally and register in the databases managed </code><br><code>externally. This process turned out to be relatively resistant to market turbulence. In the meantime, </code><br><code>delegation of the conformity assessment for such implementations of artificial intelligence to an external </code><br><code>regulator may result in a ‚Äúbottleneck‚Äù and significantly </code><code>slow down the creation of new enhancements </code><br><code>for models used in artificial intelligence systems and, as a consequence, create a precipice between </code><br><code>EU markets and external ones.</code><code> </code><br><code> </code><br><code>What is more, </code><code>testing some artificial intelligence systems, such as credit risk assessment, for example, </code><br><code>to a lesser extent, with no need to treat them as high risk ones, </code><code>may still be favourable for consumers </code><br><code>with simultaneous limiting the risk related to artificial intelligence. This is a common practice allowing </code><br><code>for the selection of the best solutions relying on artificial intelligence available for concrete </code><br><code>implementations.  </code><br><code> </code><br><code>Let us also draw your attention to imprecise and general rules of classification of artificial intelligence </code><br><code>systems referred to in the Appendix 3 in conjunction with para. 6 of the draft Act. </code><code>The proposed </code><br><code>classification may result in covering even the simplest programs used with the term of high-risk artificial </code><br><code>intelligence systems. For example, in the case of item 4 pertaining to the employment, staff </code><br><code>management and access to self-employment, a description of artificial intelligence systems which should </code><br><code>be deemed high risk systems is so general that virtually all systems used in the recruitment process may </code><br><code>be deemed high-risk artificial intelligence systems.</code><code> </code><br><code> </code><br><code>We welcome the risk-based approach. However, the risk level should be determined by the criticality of </code><br><code>the  application itself and not by a sector approach. The criteria proposed to be considered are: </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>analysis of unwanted consequences </code><br><code>ÔÇß</code><code> </code><br><code>side effects </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665462_13,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665462.pdf,31,13,2665462,attachments/2665462.pdf#page=13,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Access Now‚Äôs submission to the European Commission‚Äôs adoption</code><br><code>consultation on the AI Act</code><br><code>The shortcomings of Art. 5, paragraphs 1(a), 1(b), and 1(c)</code><br><code>As pointed out in the EDPS-EDPB Joint Opinion, rather than safeguarding fundamental rights, the</code><br><code>current formulation of the prohibitions in Article 5 risks paying mere lip service to fundamental rights,</code><br><code>and limits their scope ‚Äúto such an extent that it could turn out to be meaningless in practice.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">32</code><br><code>Let us begin with Art.5, 1(a) to see why this is the case. This is a prohibition on the use of an AI system</code><br><code>‚Äúthat deploys subliminal techniques beyond a person‚Äôs consciousness in order to materially distort a</code><br><code>person‚Äôs behaviour</code><code> in a manner that causes or is likely to cause that person or another person</code><br><code>physical or psychological harm.</code><code>‚Äù In our reading, and the reading of other commentators, the entire</code><br><code>33</code><br><code>bolded section of this prohibition is not only unnecessary and vague, but actively undermines the</code><br><code>prohibition itself.</code><br><code>It should be abundantly clear that we need to prohibit the use of an AI system ‚Äúthat deploys subliminal</code><br><code>techniques beyond a person‚Äôs consciousness in order to materially distort a person‚Äôs behaviour.‚Äù There</code><br><code>is no way to materially distort a person‚Äôs behaviour that is not a gross violation of fundamental rights,</code><br><code>including the right to freedom of thought, conscience and religion, and human dignity, which,</code><br><code>according to the EU Charter of Fundamental Rights, is inviolable and ‚Äúis not only a fundamental right</code><br><code>in itself but constitutes the real basis of fundamental rights.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">34</code><br><code>Adding the further condition that such a distortion must be done ‚Äúin a manner that causes or is likely</code><br><code>to cause that person or another person physical or psychological harm‚Äù suggests, erroneously, that a</code><br><code>person‚Äôs behaviour can be materially distorted in a way that benefits them.</code><code> Whether or not some net</code><br><code>benefit can be argued to result from this type of manipulation, the fact remains that the person</code><br><code>was thereby robbed of their agency and dignity in a manner that undermines any potential</code><br><code>benefit they may have received.</code><br><code>Furthermore, the burden of proof for such manipulation or distortion of behaviour must not fall on the</code><br><code>victim. Indeed, if the manipulation is very successful, the victim may not even be aware that they were</code><br><code>manipulated. We therefore recommend modifying the language of Article 5, paragraph 1(a), as follows:</code><br><code>-the placing on the market, putting into service or use of an AI system that deploys subliminal</code><br><code>techniques beyond a person‚Äôs consciousness in order to materially distort a person‚Äôs</code><br><code>behaviour in a manner that causes or is likely to cause that person or another person physical</code><br><code>or psychological harm</code><code>, including when such techniques may distort a person‚Äôs behaviour</code><br><code>Subliminal techniques in advertising are already prohibited under Article 3e, paragraph 1(b) of</code><br><code>Directive 2007/65/EC which states that ‚Äúaudiovisual commercial communications shall not use</code><br><code>34</code><code> EU Charter of Fundamental Rights, Article 1</code><br><code>33</code><code> https://osf.io/preprints/socarxiv/38p5f</code><br><code>32</code><code> EDPS-EDPB Joint Opinion, p.10</code><br><code>13</code>",FALSE_NEGATIVE
fitz_2665624_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665624.pdf,11,3,2665624,attachments/2665624.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>johner-institut.de  </code><br><code> </code><br><code> </code><code>Johner Institut GmbH </code><br><code>Reichenaustra√üe 1 </code><br><code>78467 Konstanz </code><br><code>T </code><code>+49 7531 94500-20 </code><br><code>info@johner-institut.de </code><br><code>Otherwise they would not be </code><br><code>allowed to apply the AI.</code><code> </code><br><code>and has only inadequately </code><br><code>addressed it with the MDCG </code><br><code>document 2019-11. </code><code> </code><br><code>Article 14 of the AI act states: </code><br><code>High-risk AI systems shall be </code><br><code>designed and developed in </code><br><code>such a way, including with </code><br><code>appropriate human-machine </code><br><code>interface tools, that they can </code><br><code>be effectively overseen by </code><br><code>natural persons during the </code><br><code>period in which the AI system </code><br><code>is in use.</code><code> </code><br><code>This requirement rules out </code><br><code>the use of AI in situations in </code><br><code>which humans can no longer </code><br><code>react quickly enough. Yet it is </code><br><code>precisely in these situations </code><br><code>that the use of AI could be </code><br><code>particularly helpful. If we have </code><br><code>to put a person next to each </code><br><code>device to ""effectively </code><br><code>supervise"" the use of AI, that </code><br><code>will mean the end of most AIbased products. It is possible </code><br><code>that the regulation meant </code><br><code>something else. But at least </code><br><code>there is a risk of </code><br><code>misinterpretation</code><code> </code><br><code> </code><br><code>A precise definition of the term </code><br><code>‚Äúhuman oversight‚Äù has to be </code><br><code>added. The AI regulation </code><br><code>should not require that a </code><br><code>natural person can intervene at </code><br><code>any time and during any </code><br><code>application. In addition, the </code><br><code>duty to supervise should be </code><br><code>risk-based. </code><code> </code><br><code> </code><br><code>The requirement could be that </code><br><code>the manufacturer needs to </code><br><code>assess during the risk </code><br><code>management if interference or </code><br><code>oversight by a person is a </code><br><code>suitable measure for risk </code><br><code>control.</code><code> </code><br><code>In article 3 (14) the regulation </code><br><code>defines ‚Äòsafety component‚Äô by </code><br><code>using the not defined term </code><br><code>‚Äòsafety function‚Äô:</code><code> </code><br><code> </code><br><code>‚Äòsafety component of a </code><br><code>product or system‚Äô means a </code><br><code>component of a product or of </code><br><code>a system which fulfils a safety </code><br><code>function for that product or </code><br><code>system or the failure or </code><br><code>malfunctioning of which </code><br><code>endangers the health and </code><br><code>safety of persons or property;</code><code> </code><br><code> </code><br><code>Also, other terms‚Äô definitions </code><br><code>are not in accordance with the </code><br><code>MDR, e.g. ‚Äúpost-market </code><br><code>This will cause controversy </code><br><code>about what is a safety </code><br><code>function. It could be for </code><br><code>example a feature that risks </code><br><code>patients‚Äô safety, when it does </code><br><code>not behave according to </code><br><code>specifications. But it could </code><br><code>also mean a feature that </code><br><code>implements a risk reducing </code><br><code>measure.</code><code> </code><br><code> </code><br><code>Non-aligned definitions </code><br><code>increase the effort required </code><br><code>by manufacturers to </code><br><code>understand and align the </code><br><code>various concepts and </code><br><code>associated requirements</code><code> </code><br><code>The terms ‚Äúfunction‚Äù and </code><br><code>‚Äúsafety functions‚Äù should be </code><br><code>defined. In the process the </code><br><code>definitions in the IEC 60601-1 </code><br><code>and the ISO 14971 should be </code><br><code>considered.</code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665532_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665532.pdf,4,1,2665532,attachments/2665532.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>www.</code><code>medtecheurope</code><code>.org</code><code> </code><br><code>Page 1 of 4</code><code> </code><br><code>Proposal for an Artificial Intelligence Act (COM/2021/206) </code><br><code> </code><br><code>6 August 2021 </code><br><code> </code><br><code>MedTech Europe response to the open public consultation </code><br><code> </code><br><code>MedTech Europe, the European trade association representing the medical technology industry including </code><br><code>diagnostics, medical devices and digital health, would like to provide its response to the European </code><br><code>Commission‚Äôs adoption consultation on the proposed Artificial Intelligence Act (AIA). Artificial Intelligence </code><br><code>(AI) technology is increasingly used in healthcare and in recent years has been greatly enhancing the </code><br><code>workflows and decision-making processes of healthcare providers. New medical technologies employing AI </code><br><code>are being developed and introduced on the market to bring improvements for citizens and patients, </code><br><code>healthcare providers, payers, and society at large. One prominent example is the deployment of medical </code><br><code>technologies using AI software to support in the fight against the COVID-19 pandemic.  </code><br><code> </code><br><code>The medical technology industry would like to stress the importance of a robust regulatory framework, </code><br><code>which provides </code><code>legal coherence, certainty, and clarity</code><code> to all actors. In particular, </code><code>interpretation issues </code><br><code>of the new rules for AI that comprises, or is incorporated in, a medical technology, should be </code><br><code>addressed.</code><code> We call for particular attention to be paid to </code><code>misalignment</code><code> </code><code>between provisions in the AI Act </code><br><code>and the Medical Device Regulation (MDR)</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> and In-Vitro Diagnostics Regulation (IVDR)</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code> as well as the </code><br><code>General Data Protection Regulation (GDPR)</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>. Addressing this misalignment is essential to ensure the </code><br><code>legal coherence, certainty and clarity needed to foster innovation, citizen access to quality care and </code><br><code>competitiveness of industry. </code><br><code> </code><br><code>1. Definition/Technical Scope</code><code> </code><br><code>MedTech Europe would like to point out that the proposed broad definition of AI and risk classification will </code><br><code>result in any medical device software (placed on the market or put into service as a stand-alone product or </code><br><code>component of hardware medical device) falling in the scope of the AI Act and being considered a high-risk </code><br><code>AI system, since most medical device software needs a conformity assessment by a Notified Body. </code><br><code> </code><br><code>2. Misalignment between AIA and MDR/IVDR </code><code> </code><br><code>Duplication and potential conflicts arising from </code><code>misalignment between the AIA and existing obligations </code><br><code>under MDR/IVDR must be avoided</code><code> in order to ensure legal coherence, certainty and clarity. The sectoral </code><br><code>regulations MDR/IVDR lay down some of the most stringent rules in the world on the safety and </code><br><code>performance of medical technologies, including those medical technologies that comprise, or incorporate </code><br><code>AI. These include, for instance, dedicated rules on risk management, quality management, technical </code><br><code>documentation, and conformity assessment with Notified Bodies. Obligations in the AIA are thematically </code><br><code> </code><br><code>1</code><code> </code><code>Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices.</code><code> </code><br><code>2</code><code> </code><code>Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro </code><br><code>diagnostic medical devices.</code><code> </code><br><code>3</code><code> </code><code>Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection </code><br><code>of natural persons with regard to the processing of personal data and on the free movement of such data </code><br><code>(General Data Protection Regulation).</code><code> </code>",POSITIVE
fitz_2665452_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665452.pdf,4,2,2665452,attachments/2665452.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><code>Press release </code><br><code>Date </code><code> </code><code>12 October 2020 </code><br><code>Subject </code><br><code> </code><code>BMW Group code of ethics for artificial intelligence. </code><br><code>Page </code><br><code> </code><code>2 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Corporate Communications </code><br><code>Seven principles covering the development and application of artificial </code><br><code>intelligence at the BMW Group: </code><br><code>‚Ä¢</code><code> </code><br><code>Human agency and oversight.  </code><br><code>The BMW Group implements appropriate human monitoring of decisions made by </code><br><code>AI applications and considers possible ways that humans can overrule algorithmic </code><br><code>decisions. </code><br><code>‚Ä¢</code><code> </code><br><code>Technical robustness and safety.  </code><br><code>The BMW Group aims to develop robust AI applications and observes the applicable </code><br><code>safety standards designed to decrease the risk of unintended consequences and </code><br><code>errors.</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Privacy and data governance.</code><code>  </code><br><code>The BMW Group extends its state-of-the-art data privacy and data security </code><br><code>measures to cover storage and processing in AI applications. </code><br><code>‚Ä¢</code><code> </code><br><code>Transparency.</code><code>  </code><br><code>The BMW Group aims for explainability of AI applications and open communication </code><br><code>where respective technologies are used. </code><br><code>‚Ä¢</code><code> </code><br><code>Diversity, non-discrimination and fairness. </code><code> </code><br><code>The BMW Group respects human dignity and therefore sets out to build fair AI </code><br><code>applications. This includes preventing non-compliance by AI applications. </code><br><code>‚Ä¢</code><code> </code><br><code>Environmental and societal well-being.  </code><br><code>The BMW Group is committed to developing and using AI applications that promote </code><br><code>the well-being of customers, employees and partners. This aligns with the BMW </code><br><code>Group‚Äôs goals in the areas of human rights and sustainability, which includes climate </code><br><code>change and environmental protection.</code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665484_4,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665484.pdf,15,4,2665484,attachments/2665484.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>4 </code><br><code> </code><br><code>multiple exceptions may apply even where all factors are present, severely weakening the value </code><br><code>of the prohibition.</code><code style=""font-weight: 1000; background-color: #FF0000;"">15</code><code> The prohibition applies to use of (i) real time (ii) remote (iii) biometric </code><br><code>identification systems (iv) in publicly accessible spaces (v) for the purposes of law </code><br><code>enforcement.</code><code style=""font-weight: 1000; background-color: #FF0000;"">16</code><code> These systems are NOT prohibited if their use is considered ‚Äústrictly necessary‚Äù </code><br><code>for targeted search for specific potential victims of crime; prevention of a specific, substantial, </code><br><code>and imminent threat to the life or safety of natural persons or a terrorist attack; or pursuit of a </code><br><code>suspect of a dizzying array of crimes (ranging from human trafficking, terrorism, and murder to </code><br><code>computer crime, racism, corruption, and fraud).</code><code style=""font-weight: 1000; background-color: #FF0000;"">17</code><code>  </code><br><code>The AIA describes a balancing test weighing the potential harm if the system is not used </code><br><code>against the potential impacts on the rights and freedoms of affected individuals if the system is </code><br><code>used, stating that this test should be applied prior to use of a system for any of the listed </code><br><code>exceptions and that necessary and appropriate safeguards should be applied.</code><code style=""font-weight: 1000; background-color: #FF0000;"">18</code><code> Prior authorization </code><br><code>is generally required to rely on of one of the listed exceptions‚Äîhowever, this requirement is </code><br><code>waived until after use in ‚Äúurgent‚Äù situations.</code><code style=""font-weight: 1000; background-color: #FF0000;"">19</code><code> In addition, while competent authorities may be </code><br><code>pre-authorized for ‚Äúindividual use‚Äù of a biometric system, it is unclear whether an ‚Äúindividual </code><br><code>use‚Äù may be for thematic (i.e. broadly applicable to organizations, places, or purposes) or </code><br><code>individual purposes, which would significantly broaden the pre-authorization scope.</code><code style=""font-weight: 1000; background-color: #FF0000;"">20</code><code> </code><br><code>The extensive exemptions detailed above for real-time remote biometric tracking‚Äî</code><br><code>combined with the option for Member States to authorize use of these systems in broad </code><br><code>terms</code><code style=""font-weight: 1000; background-color: #FF0000;"">21</code><code>‚Äîseverely weaken the protections against biometric surveillance. Setting aside the </code><br><code>numerous exceptions listed, the phrasing of the prohibition presents a veneer of limiting </code><br><code>biometric surveillance while functionally allowing biometric tracking practices to continue with </code><br><code>only minor inconvenience. The ‚Äúreal-time‚Äù stipulation allows for European law enforcement </code><br><code>agency use of recognition software services like Clearview AI or Poland-based PimEyes on </code><br><code>previously recorded footage or images to identify individuals, track their movements, and </code><br><code>attempt to link their behavior to certain social categories.</code><code style=""font-weight: 1000; background-color: #FF0000;"">22</code><code> The regulation also permits use of </code><br><code>biometric identification systems to recognize sensitive characteristics, such as an individual‚Äôs </code><br><code>gender, sexuality, race, or ethnicity, leaving open the possibility of perpetuating existing harms </code><br><code> </code><br><code>15</code><code> </code><code>Id.</code><code> at Title II, Article 5(1)(d)(i-iii). </code><br><code>16</code><code> </code><code>Id.</code><code> at Title II, Article 5(1)(d). </code><br><code>17</code><code> </code><code>Id.</code><code> at Title II, Article 5(1)(d)(i-iii); Council Framework Decision of 13 June 2002 on the European arrest </code><br><code>warrant and the surrender procedures between Member States, Article 2(2), 2002/584/JHA. </code><br><code>18</code><code> </code><code>Commission Proposal for a Regulation of the European Parliament and of the Council Laying Down </code><br><code>Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union </code><br><code>Legislative Acts</code><code>, at Title II, Article 5(2), COM (2021) 206 final (Apr. 21, 2021). </code><br><code>19</code><code> </code><code>Id.</code><code> at Title II, Article 5(3). </code><br><code>20</code><code> </code><code>See</code><code> Veale and Zuiderveen Borgesius, </code><code>supra</code><code> note 12</code><code>, </code><code>at 8 (Giving examples of individual purposes that may </code><br><code>be pre-authorized, including ‚Äúbiometrics related to all those on a missing children list or subject to a European </code><br><code>Arrest Warrant.‚Äù). </code><br><code>21</code><code> </code><code>Commission Proposal for a Regulation of the European Parliament and of the Council Laying Down </code><br><code>Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union </code><br><code>Legislative Acts</code><code>, at Title II, Article 5(4), COM (2021) 206 final (Apr. 21, 2021). </code><br><code>22</code><code> </code><code>See </code><code>Amba Kak, </code><code>Regulating Biometrics: Global Approaches and Urgent Questions</code><code>, AI Now Institute (Sept. </code><br><code>2020), https://ainowinstitute.org/regulatingbiometrics.pdf. </code>",POSITIVE
fitz_2661333_2,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2661333.pdf,2,2,2661333,attachments/2661333.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Region V√§stra G√∂taland‚Äôs position paper on EU AI Act</code><code> </code><code>|</code><code> </code><code> 2021-07-055 </code><br><code>2</code><code> </code><br><code>and innovation on the other. Thus, it is central that the proposal highlights the use of so-called </code><br><code>regulatory sandboxes. It is an important part in promoting innovation and streamline regulatory </code><br><code>compliance for future AI systems. Test and experimental environments, as well as regulatory </code><br><code>sandboxes, are central to the development of reliable AI.  </code><br><code>For AI to contribute with desired effects, data needs to be made available. It is with the help of large </code><br><code>amounts of data that AI can be used, for example, in healthcare diagnoses and contribute to </code><br><code>preventive health. A prerequisite is the possibility to share and use data in a secured manner. </code><br><code>Furthermore, data needs to be of high quality. </code><br><code>In the proposal, parts of the activities under regional responsibility falls under areas of possible </code><br><code>high-risk AI applications. It is therefore important that the right conditions are created to ensure that </code><br><code>the public administrations have the right tools to make an adequate risk assessment of often </code><br><code>complex AI value chains. It is of great importance that the consequences of the proposal for both </code><br><code>public and private activities are analysed in detail. The possible increased administrative burden for </code><br><code>regions and municipalities also needs to be analysed. </code><br><code>Furthermore, digital skills and competences are key factors. A prerequisite for sustainable </code><br><code>introduction and application of trusted AI systems is the understanding and commitment to AI </code><br><code>development. The need for digital skills is growing but there is simultaneously a shortage of digital </code><br><code>skills and excellence in many parts of the EU, for example in Sweden. This is a challenge, not least </code><br><code>for the development of AI. The proposal for a new AI regulation can be expected to further increase </code><br><code>the need for digital competence in various businesses and industries. Resources need to be invested </code><br><code>in skills development in both public and private sectors. Investments in lifelong learning, efforts to </code><br><code>attract international talent and improved measures to match the supply and demand in the labour </code><br><code>market are needed in the EU. Region V√§stra G√∂taland therefore welcomes the European </code><br><code>Commission's new coordinated plan on AI. It is important to work strategically on measures for </code><br><code>skills supply, digital skills and increased investment, and to make use of the already existing </code><br><code>structures at local, regional, national and European level. Regions have long experience of </code><br><code>supporting and collaborating with actors in different ecosystems and infrastructures at regional </code><br><code>level. These already existing forms of cooperation and ecosystems should be considered in the </code><br><code>future work in the field of digitalisation and AI.</code><br><code> </code><br><code>Region V√§stra G√∂taland  </code><br><code>Region V√§stra G√∂taland, governed by democratically elected politicians, has around 50 000 employees and is in </code><br><code>addition to regional development also responsible for providing health care and public transport for all </code><br><code>inhabitants in V√§stra G√∂taland. V√§stra G√∂taland is home to 1.7 million inhabitants. As a large procuring </code><br><code>organisation and employer, Region V√§stra G√∂taland has the ambition to act as a forerunner within sustainable </code><br><code>development and as test bed for new ideas and innovations. Together with the 49 municipalities, trade and </code><br><code>industry, organisations and academia, we drive development with V√§stra G√∂taland‚Äôs best interests as objective. </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665205_4,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665205.pdf,4,4,2665205,attachments/2665205.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>4 </code><br><code>AI tools storing a vast amount of data cause inevitable risks </code><code>on data protection, privacy</code><code> and </code><br><code>intellectual property rights</code><code> of teachers and academics and other education personnel. </code><br><code>ETUCE highlights that ensuring data protection and privacy of  teachers and students should </code><br><code>be a priority of the AI Regulation and calls on the EU Commission and the Member States </code><br><code>to develop appropriate</code><code> data-retention policies</code><code> applicable to Artificial Intelligence in </code><br><code>education,  in the respect of national competencies in education.    </code><br><code> </code><br><code>Equality and inclusion in the design and use of AI in education:  </code><br><code>As enshrined in the EU Pillar of Social Rights and the EU Charter of Fundamental Rights, nondiscrimination in education is a fundamental principle of our society. In this regard, the EU </code><br><code>Commission‚Äôs proposal states that the AI regulation ‚Äú</code><code>will minimise the risks of erroneous or </code><br><code>biased AI-assisted decision on education and training</code><code>‚Äù. In this context, ETUCE recognizes </code><br><code>that the use of Artificial Intelligence has the potential to advance the quality of life and </code><br><code>inclusion of teachers and students in education. Nonetheless, the persistent </code><code>lack of </code><br><code>diversity </code><code>and underrepresentation of women, ethnic minorities, Black People and </code><br><code>disadvantaged groups in the population of professionals responsible for designing, testing </code><br><code>and training the algorithms and data of AI tools translate in the presence of biases in AI </code><br><code>tools, leading to a </code><code>detrimental impact on inclusion and equality in education</code><code>. Therefore, </code><br><code>ETUCE calls on the European Commission and Member States to provide adequate public </code><br><code>investment to </code><code>encourage more diversity in the STEAM sector</code><code> and ensure that </code><code>AI tools are </code><br><code>designed and used with the full representation of the wide society.</code><code> </code><br><code>Besides, </code><code>research</code><code> shows that </code><code>cyber-violence, cyber-bullying and cyber-harassment</code><code> have </code><br><code>increased with the development of digitalisation in education. ETUCE underlines that it is </code><br><code>important to further explore how Artificial Intelligence systems can act as supporting tools </code><br><code>to detect and counter cyber-violence, cyber-bullying and cyber-harassment.  </code><br><code> </code><br><code> </code><br><code>*The European Trade Union Committee for Education (ETUCE) represents 127 Education </code><br><code>Trade Unions and 11 million teachers in 51 countries of Europe. ETUCE is a Social Partner </code><br><code>in education at the EU level and a European Trade Union Federation within ETUC, the </code><br><code>European Trade Union Confederation. ETUCE is the European Region of Education </code><br><code>International, the global federation of education trade unions. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665648_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665648.pdf,8,1,2665648,attachments/2665648.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>CHAI Position Paper on the EU ArtiÔøΩcial IntelligenceAct</code><br><code>University of California, Berkeley</code><br><code>The Center for Human-Compatible AI (CHAI) is a research group based at UC Berkeley, with</code><br><code>academic aÔøΩliates at a variety of other universities. CHAI‚Äôs goal is to develop the conceptual and</code><br><code>technical wherewithal to reorient the general thrust of AI research towards provably beneÔøΩcial systems.</code><br><code>CHAI is led by Prof. Stuart Russell.</code><br><code>Recommendations</code><br><code>Make the regulation future-proof and prepare for higher-risksystems</code><br><code>Update the regulation to address increasingly generalized AI systems that have multiple</code><br><code>purposes, such as OpenAI‚Äôs Generative Pre-trained Transformer 3 and DeepMind‚Äôs AlphaFold</code><br><code>system</code><br><code>Article 3(13) ‚Äúreasonably foreseeable misuse‚Äù and ‚Äúinteraction with other systems‚Äù: Explicitly</code><br><code>consider the issue of high-risk and societal-scale consequences stemming from the interaction</code><br><code>of many low-risk systems</code><br><code>Article 6 classiÔøΩcation rules for high-risk systems: Include recommender systems in the</code><br><code>classiÔøΩcation rules for high-risk systems</code><br><code>Article 6 classiÔøΩcation rules for high-risk systems: Include the requirements to document</code><br><code>perceptual inputs, action outputs, objectives, and the operational environment for high-risk</code><br><code>systems</code><br><code>Article 6 classiÔøΩcation rules for high-risk systems: Include the requirements to document</code><br><code>time-of-sale properties of systems</code><br><code>List of high-risk categories in Annex III: Add categories</code><br><code>Protect people from psychological harm</code><br><code>Article 5 (1) (a) on psychological manipulation: Consider expanding the current deÔøΩnition of</code><br><code>‚Äúsubliminal techniques beyond a person‚Äôs consciousness‚Äù</code><br><code>Section 3.5 of the Explanatory Memorandum: Include the protection of mental integrity in</code><br><code>the Explanatory Memorandum</code><br><code>Article 53 (AI regulatory sandboxes): Recognize the limit of sandboxes</code><br><code>1</code>",NO_FOOTNOTES_ON_PAGE
fitz_2662904_1,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662904.pdf,7,1,2662904,attachments/2662904.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Draft of the EU Commission on a European AI regulation </code><br><code>(Artificial Intelligence Act) 21 April 2021 </code><br><code> </code><br><code> </code><br><code> </code><br><code>Established in 1947, the </code><code>Commission nationale consultative des droits de l‚ÄôHomme</code><code> </code><br><code>(CNCDH) [National Advisory Commission on Human Rights] is a national institution which </code><br><code>promotes and protects human rights in France, as defined by the United Nations. It is </code><br><code>accredited with A status by the United Nations, which attests to its compliance with the Paris </code><br><code>Principles. The CNCDH is thus recognised as an independent institution with a pluralist </code><br><code>operation and broad mandate for promoting and protecting human rights. Through its </code><br><code>opinions, studies and recommendations, it independently plays an advisory role and makes </code><br><code>proposals to Government and Parliament in matters of human rights, international </code><br><code>humanitarian law and humanitarian action. It makes a major contribution to international </code><br><code>mechanisms for monitoring France‚Äôs international commitments, interacting with United </code><br><code>Nations bodies and those of the Council of Europe. It participates in monitoring and </code><br><code>evaluating a number of public policies relating to rights protected by European and </code><br><code>international human rights conventions. </code><br><code> </code><br><code>For several years, the CNCDH has been interested in certain artificial intelligence </code><br><code>applications. It recently published an </code><code>opinion on preventing online hate</code><code>, which includes </code><br><code>developments on how moderation algorithms work. Next November, it will give a more </code><br><code>general opinion on the impact of artificial intelligence on human rights. </code><br><code> </code><br><code>The CNCDH welcomes the proposal for a regulation from the European Commission </code><br><code>establishing a regulatory framework for artificial intelligence. It has developed considerably in </code><br><code>recent years, as much in the private as in the public sector, with no specific and appropriate </code><br><code>legal framework. In order to remedy this, the draft regulation sets a number of requirements </code><br><code>‚Äú</code><code>to foster the development, use and uptake of artificial intelligence in the internal market that </code><br><code>at the same time meets a high level of protection of public interests, such as health and </code><br><code>safety and the protection of fundamental rights, as recognised and protected by Union law</code><code>‚Äù </code><br><code>(recital 5). </code><br><code>On reading the document and its annexes, the primary objective of the text is to help </code><br><code>develop the European AI market, primarily targeting economic and government actors </code><br><code>(suppliers, distributors, users). It lays down requirements for them that vary according to the </code><br><code>level of risk posed by the AI applications considered. Regarding the guarantees granted to </code><br><code>individuals in relation to an AI system ‚Äì a user or person targeted by an algorithm, the draft </code><br><code>regulation refers to other EU texts, in particular the General Data Protection Regulation </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665329_6,other,../24212003_requirements_for_artificial_intelligence/attachments/2665329.pdf,10,6,2665329,attachments/2665329.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>     </code><code> </code><br><code>Document title </code><br><code>: Feedback on the AI Act  </code><br><code>Prepared by </code><br><code>           : NEN Medical Device / AI Expert Group</code><code>  </code><br><code>Feedback on the AI Act - Dutch Medical Device / AI expert group </code><br><code>     Page </code><code>6</code><code> of </code><code>10</code><code> </code><br><code>2)</code><code> </code><code>Topic: Certification </code><br><code> </code><br><code>Clause AI Act</code><code> </code><br><code>Clause MDR</code><code> </code><br><code>Analysis</code><code> </code><br><code>Article 11: </code><br><code>Paragraphs 1, </code><br><code>2, 3 </code><br><code> </code><br><code>Annex IV: </code><br><code>Full annex </code><br><code> </code><br><code>Annex VII: </code><br><code>Full Annex</code><code> </code><br><code>Article 10: </code><br><code>Paragraph 4 </code><br><code> </code><br><code>Annex II & III: </code><br><code>Full annexes </code><br><code> </code><br><code>Annex XIV: </code><br><code>Paragraph 1 </code><br><code> </code><br><code>Annex IX, Annex XI </code><br><code> </code><br><code>and: </code><br><code>MDCG 2020-1</code><code> </code><br><code>Process of setting up Technical Documentation and control of the Technical Documentation for high-risk AI is identical to Medical devices. </code><br><code> </code><br><code>Requirements from Annex IV (AI Act) are more extensive than those in An</code><code>nex II (MDR), however, these gaps are required for software medical de</code><code>vices under the IEC 62304:2006 / IEC 82304-1:2016 and Clinical Evaluation </code><br><code>requirements for medical devices (appraisal of the available data pertaining to the performance).  </code><br><code> </code><br><code>Annex IV paragraph 2(d) (AI Act) additionally requests describing training </code><br><code>methodologies and techniques, and 2(e) describing the level of human </code><br><code>oversight.  </code><br><code> </code><br><code>Paragraph 8 (Annex IV of the AI act) is more extensively covered for medical devices in the MDR (Annex III). </code><br><code> </code><br><code>Annex VII further introduces restrictive certification routes for high-risk AI </code><br><code>devices, and heavy change management requirements. </code><code> </code><br><code> </code><br><code> </code><br><code>Definition  </code><br><code>The definition of AI under the AI act (Article 3 & Annex I) includes techniques, such as logic and knowledge based </code><br><code>approaches and expert systems (under Annex I (b)), which bring many software medical devices, not using modern </code><br><code>AI technologies (such as machine learning, and deep learning) into the scope of the AI Act. Such devices, which </code><br><code>have been properly governed under the MDR, would be exposed to unnecessary, additional requirements.  </code><br><code> </code><br><code>Technical Documentation </code><br><code>The requirements regarding Technical Documentation in the current proposal of the AI Act are mostly already </code><br><code>captured throughout the various requirements applicable to medical devices, such as the Annex II, III and XIV, and </code><br><code>the various MDCG documents. There are additional requirements in the AI Act that are currently not explicitly </code><br><code>covered in the MDR (e.g. paragraph 2(d) and (e) of annex IV).  </code><br><code> </code><br><code>There is currently no standard that describes the type of documentation to be provided, but these aspects are part </code><br><code>of the lifecycle of the AI based medical device and could therefore be covered under the existing IEC 62304:2006 </code><br><code>and IEC 82304-1:2016. Model training description should be described as part of the clinical documentation of a </code><br><code>device, and should form part of the clinical evaluation requirements, and therefore should be integrated into </code><br><code>MDCG 2020-1.  </code><br><code> </code><br><code>Conformity Assessment </code><br><code>Both the AIA and the MDR require a conformity assessment. The MDR only requires Notified Body involvement for </code><br><code>Class I (s/m), Class IIa and higher, where the AI Act requires Notified Body involvement for all AI based medical </code><br><code>devices. The Notified Body will require review of the QMS and Technical Documentation.  </code><br><code> </code><br><code>This means in practice, that a manufacturer needs to select one Notified Body for the certification of the medical </code><br><code>device components, and one for the certification of the AI components. Ideally, this would be the same Notified </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665249_7,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665249.pdf,7,7,2665249,attachments/2665249.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>industriAll Europe Trade Union </code><br><code> </code><br><code> </code><br><code>Page 7 of 7 </code><br><code> </code><br><code> </code><br><code>Feedback to the Public Consultation on the </code><br><code>Proposal for an Artificial Intelligence Act</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>sufficient means to timely and thoroughly meet their responsibilities (similar to the provisions </code><br><code>discussed in the proposal for a Regulation on Machinery Products, Article 28);</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Address clearly the fact that regulatory sandboxes (Article 53) are a tool to help develop technical </code><br><code>solutions in the narrow sense. AI systems operating in the employment context should not be </code><br><code>allowed to run in regulatory sandboxes, as their development needs a broad and interdisciplinary </code><br><code>approach in which social and racial, as well as gender aspects, are taken into account. Regulatory </code><br><code>sandboxes will not be fit for such a purpose;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Address ‚Äòbias‚Äô in more detail, and clearly define categories such as ‚Äòrepresentative‚Äô or ‚Äòcomplete‚Äô </code><br><code>when discussing data sets;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Address the challenges of AI literacy and how the necessary digital skills can be acquired;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Clearly define ‚Äúintended use‚Äù when discussing high-risk AI applications, e.g. in Annex III, or find a </code><br><code>more suitable wording, i.e. when discussing ‚ÄúAI systems intended to be used for...‚Äù, as the current </code><br><code>formulation creates too many loopholes; this revised wording should also take into account that </code><br><code>the ‚Äúintended‚Äù use may evolve over time and that the initially ‚Äúintended‚Äù use does not prevent </code><br><code>data from being used for other purposes. The nature of the data (potentially) being collected </code><br><code>should therefore be taken into account as well, and any impact assessment should take the </code><br><code>potential further development of the system into account;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Include a provision that makes sure that the right of workers to information, consultation and </code><br><code>participation is respected on the introduction of any kind of AI at the workplace: there must be </code><br><code>‚Äònothing about us, without us‚Äô;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Clearly define and strengthen the role of the European Artificial Intelligence Board, and invite </code><br><code>social partners to join the Board as full members. </code><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665517_11,company,../24212003_requirements_for_artificial_intelligence/attachments/2665517.pdf,11,11,2665517,attachments/2665517.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,<code>11 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Herausgeber: </code><br><code>ZVEI - Zentralverband Elektrotechnik- und Elektronikindustrie e.V. </code><br><code>Abteilung Innovationspolitik </code><br><code>Lyoner Str. 9 </code><br><code>60528 Frankfurt am Main </code><br><code>Verantwortlich:  </code><br><code>Nils Scherrer </code><br><code>Telefon: +49 30 306960 28 </code><br><code>E-Mail: </code><code>nils.scherrer@zvei.org</code><code> </code><br><code> </code><br><code>Franziska Wirths </code><br><code>Telefon: +49 30 306960 17 </code><br><code>E-Mail: </code><code>franziska.wirths@zvei.org</code><code> </code><br><code> </code><br><code>www.zvei.org </code><br><code>August 2021 </code>,NO_FOOTNOTES_ON_PAGE
fitz_2665292_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665292.pdf,4,1,2665292,attachments/2665292.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>www.bitkom.org </code><br><code>Position paper </code><br><code> </code><br><code> </code><br><code>Bitkom </code><br><code>Bundesverband  </code><br><code>Informationswirtschaft, </code><br><code>Telekommunikation  </code><br><code>und Neue Medien e.V.  </code><br><code> </code><br><code>Lukas Klingholz </code><br><code>Head of Cloud, Gaia-X & AI </code><br><code>T +49 30 27576 101 </code><br><code>l.klingholz@bitkom.org </code><br><code> </code><br><code>David Adams </code><br><code>Manager EU Public Affairs </code><br><code>T +32 471 927890 </code><br><code>d.adams@bitkom.org </code><br><code> </code><br><code> </code><br><code>Albrechtstra√üe 10 </code><br><code>10117 Berlin </code><br><code> </code><br><code>Pr√§sident </code><br><code>Achim Berg </code><br><code> </code><br><code>Hauptgesch√§ftsf√ºhrer </code><br><code>Dr. Bernhard Rohleder </code><br><code> </code><br><code> </code><br><code>Bitkom principles for the Artificial Intelligence (AI) Act </code><br><code>04. August 2021 </code><br><code> </code><br><code>General Remarks </code><br><code>Bitkom welcomes the Commission proposal‚Äôs risk-based approach of the AI Act presented </code><br><code>in April 2021.</code><code> In order to achieve the intended results it needs to be more precise as out-</code><br><code>lined below in this paper. </code><br><code>We welcome that the Commissions‚Äôs proposal is cleary linked to existing horizontal </code><br><code>and vertical regulatory dossiers (Such as the NLF at the horizontal level which is well </code><br><code>known, established and has a already demonstrated its ability to support future proof </code><br><code>legislation. In addition existing sector-specific and application-related regulations at </code><br><code>the vertical level).</code><code> A clear, lean and coherent legal framework should be at best enable </code><br><code>and incentivize the integration and application of AI systems in Europe, which is needed </code><br><code>to stay competitive on a global level</code><code>. At the same time, the Proposal should cater for </code><br><code>the particularities of AI and, where necessary, make necessary changes. Concretely, </code><br><code>while extending the NLF to AI-systems embedded in products makes sense, the limits of </code><br><code>adopting a product-safety based approach to stand-alone and foundational AI-systems </code><br><code>should be further reflected upon. </code><code>Policy makers regularly emphasise the overarching </code><br><code>strategic goal of their AI policy: The creation of a European ecosystem of excellence in </code><br><code>AI that is closely linked to an environment of trust in the use of AI. This should be the </code><br><code>benchmark for the further evaluation of the present proposal. </code><br><code>The central question for companies that want to develop and produce AI systems is </code><br><code>how the process of market access, ongoing operation and market monitoring for high-</code><br><code>risk AI applications will look in the future concretely.</code><code> The use of artificial intelligence in </code><br><code>high-risk application areas in the sense of the AI Act is highly desirable from a social </code><br><code>and economic policy perspective and will increase steadily over time. Therefore, the reg-</code><br><code>ulatory framework has to be future proof allowing for the seamless integration of AI </code><br><code>technology across all industries for companies of all sizes, while being flexible enough </code><br><code>to address current and future challenges alike. The goal must therefore be to create a </code><br><code>framework in which European excellence in trustworthy AI is encouraged and enabled </code><br><code>in high-risk areas, which also means that the requirements and obligations laid out un-</code><br><code>der the proposed framework should be proportionate and should enable both public </code><br><code>and private sector in integrating and applying trustworthy high-risk AI applications by </code><br><code>taking into account the context how the technology is used.  </code><code>The main success criterion </code><br><code>for the AI Act is therefore to make the ethical and technical requirements underlying </code>",NO_FOOTNOTES_ON_PAGE
fitz_2660134_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660134.pdf,4,4,2660134,attachments/2660134.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Esta propuesta sirve para lograr facilitar la consecuci√≥n de los cuatro objetivos del borrador: </code><br><code>-</code><code> </code><br><code>garantizar que los sistemas de IA introducidos y usados en el mercado de la UE sean </code><br><code>seguros y respeten la legislaci√≥n vigente en materia de derechos fundamentales y </code><br><code>valores de la Uni√≥n permitiendo la identificaci√≥n de titulares de los sistemas; </code><br><code>-</code><code> </code><br><code>garantizar la seguridad jur√≠dica para facilitar la inversi√≥n e innovaci√≥n en IA a trav√©s de </code><br><code>la utilizaci√≥n de las herramientas de financiaci√≥n actuales reforzadas con la inscripci√≥n </code><br><code>en los registros correspondientes;  </code><br><code>-</code><code> </code><br><code>mejorar la gobernanza y la aplicaci√≥n efectiva de la legislaci√≥n vigente en materia de </code><br><code>derechos fundamentales y los requisitos de seguridad aplicables a los sistemas de IA; </code><br><code>facilitar el desarrollo de un mercado √∫nico para hacer un uso legal, seguro y fiable de </code><br><code>las aplicaciones de IA y evitar la fragmentaci√≥n del mercado empleando sistemas de </code><br><code>interconexi√≥n de registros como BRIS ya en funcionamiento. </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665249_6,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665249.pdf,7,6,2665249,attachments/2665249.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>industriAll Europe Trade Union </code><br><code> </code><br><code> </code><br><code>Page 6 of 7 </code><br><code> </code><br><code> </code><br><code>Feedback to the Public Consultation on the </code><br><code>Proposal for an Artificial Intelligence Act</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>agreement, a competent public authority should, again, be the contracting party. Part of </code><br><code>such an agreement should be: </code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(a) the nature of the data being collected on workers, the frequency of its </code><br><code>collection and the duration of its storage;</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(b) the explicit algorithms or the machine-learning system used to process this </code><br><code>data;</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(c) the metrics used to evaluate work and the performance values required from </code><br><code>workers;</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(d) the teaching data, its biases and the means implemented to overcome them;</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(e) the reliability and accuracy statistics of any implemented machine learning </code><br><code>system;</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(f) the acceptable means to supervise work and to detect, store and process </code><br><code>circumstances of non-compliance with work prescriptions;</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>(g) the procedeures for workers or their representatives to detect errors or unfair </code><br><code>treatment in this automated processing, report them and obtain redress.</code><code> </code><br><code>o</code><code> </code><code>Every worker should be aware of the exact nature of such a system monitoring their </code><br><code>performance, and of the parameters used to evaluate them;</code><code> </code><br><code>o</code><code> </code><code>Works councils should be provided with the means to hire software engineers to support </code><br><code>them in their analyses of the AI/ML systems;</code><code> </code><br><code>o</code><code> </code><code>Explainability must be guaranteed by using a language that is understood by those </code><br><code>subjected to AI/ML systems;</code><code> </code><br><code>o</code><code> </code><code>Consent to the processing of worker-related data should only be given collectively; </code><br><code>individual consent should not be considered sufficient in a situation of employment or of </code><br><code>dependent work;</code><code> </code><br><code>o</code><code> </code><code>It should be clarified that platform workers should also be covered by the relevant </code><br><code>collective agreements. </code><code> </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Introduce a provision to include new types of ‚Äòunacceptable‚Äô AI to the list included under Title II of </code><br><code>the draft Regulation;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Clearly define how AI applications that have been falsely labelled as ‚Äòlow-risk‚Äô can be re-classified;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Clearly define the questions of liability and redress in accidents and incidents involving AI systems. </code><br><code>The current general rule, whereby the employer is by default liable for any accident in the </code><br><code>workplace (in the absence of any wrongdoing by the worker) should remain;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Ban all types of emotional recognition software, as they are highly unreliable and their outcomes </code><br><code>have the potential to be more harmful than helpful;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Any AI application dealing with personal data, with workers‚Äô data and/or which affects working </code><br><code>conditions should be classified as ‚Äòhigh-risk‚Äô and subject to a third-party conformity assessment. </code><br><code>Collective agreements should be fostered to further regulate the processing of personal data in </code><br><code>the employment context (similar to Article 88 of the GDPR);</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Mandate third-party assessment for all high-risk AI applications, instead of self-assessment </code><br><code>procedures;</code><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Provide for independent and competent notified bodies at national level assigned with conducting </code><br><code>the third-party assessment; these should be capable to advise the user, to test and examine the </code><br><code>AI/ML application and to accept complaints. Those notified bodies should be equipped with </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665521_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665521.pdf,5,2,2665521,attachments/2665521.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>    </code><br><code> </code><br><code>  </code><br><code>datainnovation.org </code><br><code>up embedded in many different social and commercial products and services, and thus is not easily </code><br><code>amenable to singular regulation. It is impossible to predict the future impact of AI today and attempts </code><br><code>to impose broad </code><code>a priori</code><code> rules on the technology will most likely create deadweight losses, </code><br><code>opportunity costs, second-order effects, and other kinds of deleterious impacts on the rate and reach </code><br><code>of digital progress in Europe.  </code><br><code>The Center for Data Innovation recommends a regulatory approach for AI based on the idea of </code><br><code>algorithmic accountability: the principle that an algorithmic system should employ a variety of </code><br><code>controls to ensure the operator (the party responsible for deploying the algorithm) can verify it acts in </code><br><code>accordance with its intentions, as well as identify and rectify harmful outcomes. Adopting this </code><br><code>framework would both promote the vast benefits of algorithmic decision-making and minimize </code><br><code>harmful outcomes.</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code> This regulatory approach focuses on evidence-based action against documented </code><br><code>harms, rather than an </code><code>ex ante</code><code> approach that tries to foresee how a new technological stack is used. </code><br><code>Algorithmic legislation should focus on the idea that the operators of such tools can demonstrate </code><br><code>they had controls to ensure the system was acting as intended. This approach allows for the </code><br><code>regulatory framework to change over time as market forces, social norms, new technologies, and </code><br><code>other factors shape the use of algorithms in society.  </code><br><code>Beyond suggesting a different overall approach to regulating AI, we have the following concrete </code><br><code>recommendations on how to improve the AIA. </code><br><code>EXPAND AI REGULATORY SANDBOXES  </code><br><code>One of the costliest negative externalities the AIA creates is the reduction in the opportunities for AI </code><br><code>development and use in Europe. Because the AIA applies horizontally, it dampens the prospects of </code><br><code>European startups and small and medium enterprises (SMEs) developing and using AI. The AIA‚Äôs </code><br><code>regulatory sandbox provision is a sandbox in name only: It promises close regulatory oversight </code><br><code>without any meaningful concessions in return. A sandbox should allow companies to experiment with </code><br><code>new technologies free from some or all of an existing regulatory framework, while regulators monitor </code><br><code>the impact. The sandbox acts as a two-way street: if regulators see that suspending certain rules or </code><br><code>requirements does not lead to adverse outcomes, they conclude that these rules are redundant. A </code><br><code>better approach for the AIA‚Äôs sandbox would thus be to lift certain legal or regulatory requirements in </code><br><code>exchange for closer supervision of outcomes by regulators. This would allow businesses to creatively </code><br><code>experiment with new AI systems and see what impact various requirements have or do not have in </code><br><code>the real world.  </code><br><code> </code><br><code>2</code><code> For more, see Joshua New and Daniel Castro, ‚ÄúHow Policymakers Can Foster Algorithmic Accountability,‚Äù May 21, </code><br><code>2018, </code><code>https://www2.datainnovation.org/2018-algorithmic-accountability.pdf</code><code>.  </code>",POSITIVE
fitz_2665488_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665488.pdf,5,1,2665488,attachments/2665488.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Verband kommunaler Unternehmen e.V. ¬∑</code><code> Invalidenstra√üe 91 ¬∑ 10115 Berlin  </code><br><code>Fon +49 30 58580-0 ¬∑ Fax +49 30 58580-100 ¬∑ info@vku.de ¬∑ </code><code>www.vku.de</code><code> </code><br><code> </code><br><code>Der VKU ist mit einer Ver√∂ffentlichung der Stellungnahme einverstanden.  </code><br><code>Sofern Kontaktdaten von Ansprechpartnern enthalten sein sollten, bitten wir, diese vor einer Ver√∂ffentlichung zu schw√§rzen. </code><br><code>Der Verband kommunaler Unternehmen (VKU) vertritt rund 1.500 Stadtwerke und kommunalwirtschaftliche Unternehmen in den Bereichen Energie, Wasser/Abwasser, Abfallwirtschaft sowie Telekommunikation. Mit mehr als </code><br><code>275.000 Besch√§ftigten wurden 2018 Umsatzerl√∂se von rund 119 Milliarden Euro erwirtschaftet und mehr als 12 </code><br><code>Milliarden Euro investiert. Im Endkundensegment haben die VKU-Mitgliedsunternehmen gro√üe Marktanteile in </code><br><code>zentralen Ver- und Entsorgungsbereichen: Strom 62 Prozent, Erdgas 67 Prozent, Trinkwasser 90 Prozent, W√§rme </code><br><code>74 Prozent, Abwasser 44 Prozent. Sie entsorgen jeden Tag 31.500 Tonnen Abfall und tragen durch getrennte </code><br><code>Sammlung entscheidend dazu bei, dass Deutschland mit 67 Prozent die h√∂chste Recyclingquote in der Europ√§ischen Union hat. Immer mehr kommunale Unternehmen engagieren sich im Breitbandausbau. 190 Unternehmen </code><br><code>investieren pro Jahr √ºber 450 Mio. EUR. Sie steigern j√§hrlich ihre Investitionen um rund 30 Prozent. Beim Breitbandausbau setzen 93 Prozent der Unternehmen auf Glasfaser bis mindestens ins Geb√§ude. </code><br><code> </code><code>STELLUNGNAHME</code><code> </code><br><code>zum Verordnungsentwurf der Europ√§ischen Kommission f√ºr den Artificial Intelligence Act vom 21.April </code><br><code>2021 </code><br><code> </code><br><code>Berlin, 06. August 2021 </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665504_9,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665504.pdf,11,9,2665504,attachments/2665504.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>III. Recommendation to move forward</code><br><code>For all the reasons exposed above, we recommend to withdraw this project of regulation on AI, </code><br><code>and rather proceed to something that would be at the same time more effective, easier and </code><br><code>future-proof. </code><br><code>We argue that AI as such should not be regulated in an independent (‚Äúhorizontal‚Äù) manner, but </code><br><code>rather, EU should make sure that laws and regulations on what actually matters, i.e. </code><br><code>applications, are robust enough to encompass those applications that embed data-driven </code><br><code>and/or ‚ÄúAI‚Äù components. For the EU citizen, a ‚Äúsafe AI‚Äù does not mean much. To trust cars, toys, </code><br><code>medical devices, to be ensured that decisions are made in a fair way, without discrimination, </code><br><code>etc, that is what likely matters for the citizens.</code><br><code>The good news is that this exercice is easier, and even more: it has already been made in </code><br><code>practice. Let us take the example of EU regulation 2017/746 on In Vitro Diagnostics medical </code><br><code>devices (https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX</code><br><code>%3A32017R0746&qid=1627907459494). In the introduction, we find paragraph (17) : </code><br><code>It is necessary to clarify that software in its own right, when specifically intended by the </code><br><code>manufacturer to be used for one or more of the medical purposes set out in the definition of an </code><br><code>in vitro diagnostic medical device, qualifies as an in vitro diagnostic medical device, while </code><br><code>software for general purposes, even when used in a healthcare setting, or software intended </code><br><code>for well-being purposes is not an in vitro diagnostic medical device. The qualification of </code><br><code>software, either as a device or an accessory, is independent of the software's location or the </code><br><code>type of interconnection between the software and a device.</code><br><code>That makes the context and the spirit clear: an application for medical diagnosis integrating </code><br><code>software components will not elude the regulation, and a solution for in vitro diagnostics based</code><br><code>solely on software will also be considered as a medical device for diagnostics. This is later on </code><br><code>formalized in, Article 2 (2), with the definiton of ‚Äúin vitro diagnostic medical device‚Äù:</code><br><code>‚Äòin vitro diagnostic medical device‚Äô means any medical device which is a reagent, reagent </code><br><code>product, calibrator, control material, kit, instrument, apparatus, piece of equipment, software or </code><br><code>system, whether used alone or in combination, intended by the manufacturer to be used in vitro</code><br><code>for the examination of specimens, including blood and tissue donations, derived from the </code><br><code>human body, solely or principally for the purpose of providing information on one or more of the</code><br><code>following:</code><br><code>(a) concerning a physiological or pathological process or state;</code><br><code>(b) concerning congenital physical or mental impairments;</code><br><code>(c) concerning the predisposition to a medical condition or a disease;</code><br><code>Copyright @T.Helleputte ‚Äì 2021</code><br><code>Page 9/11</code>",NO_FOOTNOTES_ON_PAGE
fitz_2660510_4,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660510.pdf,5,4,2660510,attachments/2660510.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>PM </code><br><code>Sida 4 (5) </code><br><code> </code><br><code>Feedback on the EU consultation on the </code><br><code>Artificial Intelligence Act </code><br><code> </code><br><code>6. How is ‚Äúcommon normative standards‚Äù defined? </code><br><code>The proposal states: </code><code>In order to ensure a consistent and high level </code><br><code>of protection of public interests as regards health, safety and </code><br><code>fundamental rights, common normative standards for all high-risk </code><br><code>AI systems should be established. </code><br><code>How does the commission define ‚Äúcommon normative standards‚Äù </code><br><code>and how will this be followed up? </code><br><code> </code><br><code>7. AI and the citizens </code><br><code>There is also an ethical aspect of AI. It is important to clarify which </code><br><code>authority will hold the responsibility to communicate about AI and </code><br><code>answer the citizens‚Äô questions concerning data collection, which </code><br><code>algorithms that are behind the AI decisions etc. Many citizens are </code><br><code>sceptical of data collection and see it as an intrusion into their lives. </code><br><code>Proactive information campaigns on how AI works should be </code><br><code>planned. </code><br><code> </code><br><code>8. AI, innovation and European cloud services </code><br><code>The City also welcomes the initiative on encouraging member states </code><br><code>to establish artificial intelligence regulatory sandboxes. </code><br><code>Development and usage of AI brings about new challenges linked to </code><br><code>new digital technology. It is therefore important to encourage </code><br><code>innovation and to make it possible to try new ideas. A question that </code><br><code>arises is ‚Äì will this also include the area of cloud services?  Will the </code><br><code>AI Act encourage the creation of European cloud service to test AI </code><br><code>and AI-complience, which currently to a great extent is dominated </code><br><code>by American solutions? </code><br><code> </code><br><code>AI Act ‚Äì elaboration </code><code>in Swedish</code><code> from a legal perspective </code><br><code> </code><br><code>Otydlighet och eventuell regelkonflikt </code><br><code>F√∂rslaget √•l√§gger en ‚Äùprovider‚Äù av AI flera skyldigheter, bl. a tillse </code><br><code>regelefterlevnad, transparenskrav och uppr√§ttande av </code><br><code>dokumentation och instruktioner f√∂r ‚Äùusers‚Äù. </code><code> </code><code>Liknande skyldigheter </code><br><code>√•ligger enligt dataskyddsf√∂rordningen den personuppgiftsansvarige, </code><br><code>inte personuppgiftsbitr√§det som enligt f√∂rslaget ben√§mns </code><br><code>‚Äùprovider‚Äù. Den som upphandlar AI √§r vanligen </code><br><code>personuppgiftsansvarig enligt dataskyddsf√∂rordningen och har </code><br><code>d√§rmed en ansvarsskyldighet f√∂r de behandlingar av </code><br><code>personuppgifter som sker i verksamheten. Personuppgiftsansvarig </code><br><code>√§r skyldig att endast anlita personuppgiftsbitr√§den som har en </code><br><code>tillr√§cklig s√§kerhet och garanti, vidare en skyldighet att √•l√§gga </code><br><code>bitr√§det instruktioner f√∂r behandlingen av personuppgifter f√∂r sin </code><br><code>r√§kning etcetera. F√∂rslaget √• andra sidan √•l√§gger ‚Äùuser‚Äù en </code><br><code>skyldighet att efterleva den tekniska dokumentationen och de </code><br><code>instruktioner som en ‚Äùprovider‚Äù tillhandah√•ller, ‚Äùuser‚Äù ska √§ven     </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665438_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665438.pdf,2,2,2665438,attachments/2665438.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>  </code><br><code>2 of 2 </code><br><code>abstract terms, or variables that depend on subjective interpretation. To name a few examples: </code><br><code>the definition of AI in Annex I seems to be extremely broad. Restrictions such as ‚Äúfree of errors‚Äù </code><br><code>in a dataset seem unnecessary and, in some cases, impossible to fulfill (Title III, Chapter 2, Article </code><br><code>10). The quality of the AI has to be assessed on the final outcome since even a perfect data set </code><br><code>does not guarantee perfect algorithmic outcomes. In terms of logging, some aspects remain </code><br><code>ambiguous and/or unclear on how to implement. For example, the term ‚ÄúReference database‚Äù </code><br><code>may not be applicable for many systems (Title III, Chapter 2, Article 12). Additionally, the </code><br><code>attributes required for the log need careful consideration in order to follow the principle of data </code><br><code>minimization and avoid records subject to data privacy regulation in the general log data. When </code><br><code>considering high-risk, the act points towards the annex list, which brings additional uncertainty </code><br><code>that the annex could change constantly. Predictability is crucial to direct medium and long-term </code><br><code>investments.</code><code>  </code><br><code>Moreover, in regulated markets, it is essential to avoid uncertainty by duplicating the regulatory </code><br><code>authority. Regulations for AI applications in the area of medical devices or in-vitro diagnostic </code><br><code>tools, for example, should be integrated into those existing frameworks and rely on wellestablished authorities. </code><br><code>Lastly, since use cases are the basis of the risk assessment, strong AI may not fall under the </code><br><code>regulation. Therefore, even though there is still a considerable way to reach strong AI, it would </code><br><code>be relevant to have harmonized rules that are more futureproof and ready for the next pipeline </code><br><code>of breakthroughs. </code><br><code> </code><br><code>We are happy to exchange more deeply on the aspects outlined above and share our experiences </code><br><code>on AI applications and compliance with ethical standards.  </code><br><code> </code><br><code> </code><br><code>Sources </code><br><code>[1] Merck Code of Digital Ethics  </code><br><code>https://www.merckgroup.com/company/responsibility/us/products-businesses/CoDE-Code_of_Digital_Ethics.pdf</code><code>   </code><br><code> </code><br><code>[2] Mrowiec et al. (2020); Digital pathology to evaluate PD-L1 IHC scoring as a predictor of outcome with second-line </code><br><code>avelumab treatment in patients with non-small cell lung cancer (NSCLC); Journal of Clinical Oncology; Vol. 38; No. 15 </code><br><code>https://ascopubs.org/doi/abs/10.1200/JCO.2020.38.15_suppl.e21539</code><code>  </code><br><code> </code><br><code>[3] Schlaps et al. (2020); Automation of Unstructured Data Transformation for Regulatory and Identification of Medicinal </code><br><code>Products - Text Mining for Merck Pharma Regulatory Intelligence; Die Pharmazeutische Industrie; Vol. 82; P. 1354 </code><br><code>https://www.ecv.de/beitrag/pharmind/Automation_of_Unstructured_Data_Transformation_for_Regulatory_and_Identifi</code><br><code>cation_of_Medicinal_Products</code><code>  </code><br><code> </code><br><code>[4] Gurulingappa et al. (2020); Text mining for regulatory intelligence: taking an automated approach; Regulatory </code><br><code>Rapporteur; Vol. 17; No. 11; P. 25 </code><br><code>https://www.topra.org/TOPRA_Member/TOPRA/TOPRA_Member/REGRAP/Public/Regulatory_Rapporteur_Issue_Summar</code><br><code>y.aspx?DocumentKey=3C841163-C8FE-466C-9C7E-30724D1EE2D6</code><code>  </code><br><code> </code><br><code>[5] AI in Drug Discovery </code><br><code>https://www.emdgroup.com/en/research/science-space/envisioning-tomorrow/precision-medicine/generativeai.html</code><code>  </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663361_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2663361.pdf,2,2,2663361,attachments/2663361.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>[Status]</code><code> </code><br><code>ÔÇ∑</code><code> </code><code>Finally, EIT Health welcomes the assurances created by looking at the whole value chain, </code><br><code>and calls upon the Commission to find the right balance between support for innovation and </code><br><code>building public trust in clear regulation, enabling safe and secure exchange of data and the </code><br><code>development of new innovative AI. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665579_4,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665579.pdf,4,4,2665579,attachments/2665579.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>techUK response to the Commission‚Äôs proposed Artificial Intelligence Act </code><br><code>Under Article 14 (Human Oversight), we would challenge the reference to ‚Äúfull understanding‚Äù of </code><br><code>the capacities and limitations of AI systems, as this is simply a requirement that cannot be complied </code><br><code>with. On the other hand, we agree that humans should have a role in detecting issues and providing </code><br><code>feedback on high-risk AI models when anomalies occur. </code><br><code>Further guidance on Article 64 (2) would be helpful, particularly in relation to the definition of </code><br><code>‚Äúsource code‚Äù. This could be the code of the trained AI model, of the validated AI model, or the code </code><br><code>to build the AI model. The code may not always be retained so guidance on retention period would </code><br><code>be helpful. In some cases, the code may also be confidential or constitute commercially sensitive </code><br><code>information. It may be worth considering other options to satisfy the objectives of the provision, </code><br><code>including metrics to measure how certain fields influence the output of the model and therefore </code><br><code>determine the level of risk. Requesting access to source code also increases risk exposure for ML </code><br><code>processes from a cybersecurity perspective and could lead to supply chain and compliance risks for </code><br><code>companies. </code><br><code>Recital 16 states that the prohibition on unacceptable uses would not be stifling for research </code><br><code>however there is no other mention of research in the regulation when it comes to high-risk AI </code><br><code>systems for instance. AI research is crucial for innovation and its essential that the AIA does not </code><br><code>inadvertently disincentivize investment in AI research. techUK would therefore welcome a more </code><br><code>general statement on research ‚Äòas it relates to all AI systems‚Äô not being stifled by the regulation.  </code><br><code>Working towards global AI standards</code><code> </code><br><code>Regulating AI should be done in a way that prevents unnecessary barriers to trade and investments. </code><br><code>The EU should maintain an open dialogue with like-minded countries such as the US and UK in order </code><br><code>to ensure that a similar approach to AI is taken based on shared democratic values and that </code><br><code>European citizens are able to benefit from global innovations.</code><code> </code><br><code>Sandboxes</code><code> </code><br><code>techUK strongly supports the AI Act‚Äôs provisions for building voluntary regulatory sandboxes for the </code><br><code>development, testing and validation of innovative AI systems. However, we believe that the current </code><br><code>proposal is not ambitious enough and may lead to potential fragmentation in their implementation </code><br><code>and operation. Going forward, sandboxing should be made a cornerstone of the proposal to </code><br><code>encourage innovation. </code><br><code>Next steps </code><br><code>As the legislative process now moves over to the European Parliament, we call on legislators to work </code><br><code>together with industry and like-minded international partners, to find solutions to some of our </code><br><code>common AI challenges and strike a balance between regulation and innovation that ultimately </code><br><code>promotes trust in and drives forward the adoption of ethical and responsible AI. </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2660510_1,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660510.pdf,5,1,2660510,attachments/2660510.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>City of Stockholm </code><br><code>International Affairs Unit </code><br><code> </code><br><code>Ragnar √ñstbergs Plan 1 </code><br><code>SE-105 35 Stockholm </code><br><code>SWEDEN </code><br><code>Direct +46 8 5+46 8 29 311 </code><br><code>agata.uhlhorn@stockholm.se </code><br><code>start.stockholm </code><br><code> </code><br><code>Page 1 (5)</code><code>  </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>  </code><br><code>Feedback on the EU consultation on </code><code>the Artificial </code><br><code>Intelligence Act</code><code> </code><br><code>The City of Stockholm welcomes the regulation laying down </code><br><code>harmonised rules on artificial intelligence and amending certain </code><br><code>union legislative acts. However, certain concerns arise with the </code><br><code>current proposal for the Artificial Intelligence Act: </code><br><code> </code><br><code>1.</code><code> </code><code>There is a need to clearly specify which types of legal </code><br><code>requirements that are mandatory for the different authorities, </code><br><code>organisations and companies (etc) concerned.  </code><br><code>2.</code><code> </code><code>Who is to define whether or not certain users or systems fall </code><br><code>under the category of ‚Äúhigh risk‚Äù? </code><br><code>3.</code><code> </code><code>There is a need to conduct further risk analyses to sort out </code><br><code>the consequences for those affected by the legislation on </code><br><code>different levels, such as the public sector. </code><br><code>4.</code><code> </code><code>It should be investigated in further detail which issues </code><br><code>regarding AI that are suitable for regulation at EU regulation </code><br><code>level and which other regulation level.  </code><br><code>5.</code><code> </code><code>The AI Act should be developed in line with other current </code><br><code>legislation and legislative proposal in the field of data </code><br><code>sharing, data reliability and security. </code><br><code>6.</code><code> </code><code>How is ‚Äúcommon normative standards‚Äù defined? </code><br><code>7.</code><code> </code><code>Many citizens are sceptical of data collection and data </code><br><code>sharing and see it as an intrusion into their lives ‚Äì proactive </code><br><code>information campaigns on how AI works should be planned. </code><br><code>8.</code><code> </code><code>It is unclear whether innovation within the area of cloud </code><br><code>services will be encouraged. Will the AI Act encourage the </code><br><code>creation of European cloud services to test AI and AIcomplience?  </code><br><code> </code><br><code>1. Which types of legal requirements are mandatory for which </code><br><code>types of stakeholders? </code><br><code>In previous consultations, the City of Stockholm has stressed the </code><br><code>importance that in the development of regulations for AI there is a </code><br><code>need to clearly specify which types of legal requirements that are </code><br><code>mandatory for the different authorities, organisations and companies </code><br><code>(etc) concerned. In this proposal for regulation the requirements are </code><br><code>City of Stockholm</code><code> </code><br><code>International Affairs Unit </code><br><code>Feedback on EU consultation </code><br><code>16 June 2021 </code><br><code> </code><br><code> </code><br><code>Ragnar √ñstbergs Pl an 1 </code><br><code>105 35 Stockhol m </code><br><code>Telefon + 46 8 508 29 311 </code><br><code>agata.uhl horn@stockhol m.se </code><br><code>start.stockhol m</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662176_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662176.pdf,5,1,2662176,attachments/2662176.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Finans Danmark</code><code>  </code><code>|</code><code>  Amaliegade 7  </code><code>|</code><code>  1256 K√∏benhavn K  </code><code>|</code><code>  www.finansdanmark.dk</code><br><code> </code><br><code> </code><br><code> </code><br><code>Position paper </code><br><code> </code><br><code>13. juli 2021 </code><br><code>Dok. nr. FIDA-151247800-703844-v1 </code><br><code> </code><br><code> </code><br><code>The interplay between protection and </code><br><code>further use of artificial intelligence</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Position </code><br><code>Finance Denmark supports regulation of AI. It is important to provide a regulation </code><br><code>that protects the customer, through which trust in AI can be build. Much of what </code><br><code>the financial sector does, is fundamentally based on trust. Therefore, it is im-</code><br><code>portant that AI-solution now and in the future can be trusted through a sound </code><br><code>and appropriate regulatory framework on AI. </code><br><code>The proposal from the European Commission distinguishes between high and low </code><br><code>risk. Finance Denmark recommends that a risk-based approach with several lev-</code><br><code>els will be used instead.  </code><br><code>We support that the future supervision of the financial sectors use of artificial intel-</code><br><code>ligence is placed with the regulatory authorities that currently has the supervisory </code><br><code>oversight of the financial sector </code><br><code>Finance Denmark proposes that it will be possible to apply the same regulatory </code><br><code>risk-based approach for artificial intelligence as the approach used in connec-</code><br><code>tion to fintech.  </code><br><code> </code><br><code>Higher focus on the interplay between protecting the European citizens and the </code><br><code>further use of artificial intelligence </code><br><code> </code><br><code>The European Commission presented on the 21st of April 2021 its proposal for fu-</code><br><code>ture harmonized regulation on artificial intelligence ‚Äì </code><code>Proposal for a regulation of </code><br><code>the European parliament and of the council laying down harmonized rules on ar-</code><br><code>tificial intelligence (Artificial intelligence act) and amending certain union legisla-</code><br><code>tive acts.   </code><br><code> </code><br><code>Finance Denmark considers that it is important to have a clear and coherent Eu-</code><br><code>ropean regulation on artificial intelligence and we therefore welcome the Com-</code><br><code>mission‚Äôs proposal.  </code><br><code> </code><br><code>Finance Denmark assess that there can be a wide variety of opportunities in the </code><br><code>use of artificial intelligence. For example, artificial intelligence enables Europe to </code><br><code>increase the prosperity of its citizens and can accelerate/boost the growth of </code><br><code>businesses - and thereby include both economic and social benefits. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665438_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665438.pdf,2,1,2665438,attachments/2665438.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>  </code><br><code>1 of 2 </code><br><code>Merck KGaA contribution to the Public Consultation on the European Commission‚Äôs </code><br><code>proposed regulation of Artificial Intelligence (‚ÄúAI Act‚Äù)  </code><br><code>First of all, we would like to thank the EU Commission for the opportunity to participate and </code><br><code>discuss this important act and commend the effort to draft regulation on such a complex and </code><br><code>multi-faceted topic.  </code><br><code>AI has a huge potential to solve today and future challenges. We welcome Europe taking this </code><br><code>important step to develop regulatory certainty, which is essential to foster innovation and advance </code><br><code>Europe‚Äôs competitive position. In addition to managing the risks of AIs, the broader approach of </code><br><code>the EU should also create a positive AI innovation ecosystem and contribute to public acceptance. </code><br><code>It is an important signal if governments also adopt and employ AI systems, bringing real-world </code><br><code>experience and allowing the exchange of best practices. Furthermore, governments should fund </code><br><code>critical research and public-private partnerships, invest in workforce development and </code><br><code>infrastructure.  </code><br><code>Public acceptance requires trust in the technology, the producers, and the data. One can gain </code><br><code>trust through proven ethical behavior based on ethical standards and proven adherence to them </code><br><code>in a self-regulating approach. When Merck initiated AI projects, these ethical principles needed </code><br><code>to be defined first, and we launched a scientific project which resulted in a set of principles for </code><br><code>responsible use of data & algorithms/AI [1]. Based on our long experience in operationalizing </code><br><code>bioethical standards, we set an early benchmark by implementing this in our development </code><br><code>processes and have gained recognition by the public & private sectors.  </code><br><code>Taking a risk-based approach to AI is indispensable, and we welcome that the EU Commission </code><br><code>has embraced this concept. It is in line with our experiences of applying AI in areas subject to </code><br><code>compliance and regulation [2,3,4,5]. We hope that the concept of self-governance and </code><br><code>accountability will be further strengthened. The development of AI is a continuous process of </code><br><code>piloting, reassessing, and improving. Constant self-assessment can govern the development very </code><br><code>well, instead of a rigid ex-ante 3</code><code>rd</code><code>-party approval, which would come potentially in addition to </code><br><code>existing ex-ante-assessments, such as software as a medical device. Self-assessment will also </code><br><code>avoid additional competition on talents between developers and approval bodies, jeopardizing </code><br><code>innovation power. To operationalize this approach, the regulator must proactively accompany </code><br><code>standardization activities.  </code><br><code>Ultimately, providers with proven best practices, such as companies with independent advisory </code><br><code>panels, digital ethics codes, could have better ways to show compliance and get faster to market, </code><br><code>reducing administrative burden.  </code><br><code>We encourage the EU to consider alignment with key international partners. Regulatory </code><br><code>cooperation can avoid unnecessary barriers to collaboration and innovation, e.g., around crossborder data exchange. An even regulatory playing field will prevent companies and start-ups from </code><br><code>developing AI systems in more favorable markets and coming to Europe only in a second moment </code><br><code>after customers validation.  </code><br><code>Regulatory certainty is imperative for companies. While the draft AI Act sets the frame well, some </code><br><code>aspects remain open to interpretation. Requirements and definitions need to avoid generalization, </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665266_23,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665266.pdf,47,23,2665266,attachments/2665266.pdf#page=23,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>‚Äú Using technology often makes me feel </code><br><code>like I could be missing out on time </code><br><code>actually with my friends.‚Äù</code><code> ‚Äî Aged 15</code><br><code>23</code><br><code>4</code><code> </code><br><code> Slowing progress </code><code> </code><br><code>These barriers force a user to do or  </code><br><code>consume </code><code>‚Äòsomething‚Äô</code><code> before they can </code><br><code>access the information they are seeking. </code><br><code>They might take the form of sponsored  </code><br><code>ads above searches, videos that break  </code><br><code>up news articles or shopping sites that </code><br><code>prevent precision searches. Each barrier  </code><br><code>is small, but as the user swipes, removes </code><br><code>and negotiates the barriers, their time  </code><br><code>online is extended before getting to what </code><br><code>they initially sought. Anthony Wagner, </code><br><code>associate Professor of Psychology at </code><br><code>Stanford University, explained: </code><code>‚ÄúWhere </code><br><code>there are multiple sources of information... </code><br><code>[users] are not able to filter out what‚Äôs not </code><br><code>relevant to their current goal. That failure </code><br><code>to filter means they‚Äôre slowed down by </code><br><code>irrelevant information.‚Äù</code><br><code>89</code><code> </code><br><code>5</code><code> </code><br><code> Pace of play </code><code> </code><br><code>Users are more likely to stop if the pace </code><br><code>becomes predictable. Games partition </code><br><code>progress into levels and change the pace </code><br><code>and intensity of play, to offer the prospect of </code><br><code>resolution while obscuring the fact that the </code><br><code>game is designed to be played indefinitely, </code><br><code>or at least as long as possible. Varying pace </code><br><code>of play is not restricted to games. On social </code><br><code>media, users can be </code><code>‚Äòswiftly dragged into a </code><br><code>high speed retrospective of the last 24 hours </code><br><code>in the life of someone they may or may not </code><br><code>know‚Äô</code><br><code>90</code><code> before introducing a slower pace </code><br><code>as they scroll through endless feeds whilst </code><br><code>notifications offer pacy interjections. </code><br><code>6</code><code> </code><br><code> No save</code><code> </code><code> </code><br><code>Some games prevent users from saving </code><br><code>progress until they reach a predetermined </code><br><code>point. If they break away before this point, </code><br><code>all previous progress is lost. So, players  </code><br><code>play on. </code><br><code>Each of these design features creates  </code><br><code>a pull towards extended use.</code><br><code>The ubiquity of these strategies results  </code><br><code>in a very real sense of having ‚Äòlost time‚Äô  </code><br><code>doing you are not quite sure what.</code><br><code>3.4 Losing time</code><br><code>‚ÄúI have two kids now and I regret every minute </code><br><code>that I‚Äôm not paying attention to them because </code><br><code>my smartphone has sucked me in.‚Äù</code><br><code>88 </code><code> </code><br><code>Loren Brichter, designer of the  </code><br><code>pull-to-refresh mechanism</code><br><code>Routinely, the amount of time required  </code><br><code>or spent doing online tasks is concealed.  </code><br><code>The decision to continue watching, playing  </code><br><code>or scrolling is designed into the service.</code><br><code>Examples of this are ubiquitous in the digital </code><br><code>environment, but among them are:</code><br><code>1</code><code> </code><br><code> Auto play, auto suggestion  </code><br><code>and infinite feeds</code><code> </code><br><code>Each one automatically replaces the </code><br><code>next piece of content or action before </code><br><code>the previous one has finished, thereby </code><br><code>minimising or eliminating breaks during </code><br><code>which a user might decide to disengage. </code><br><code>2</code><code> </code><br><code> </code><code>Creating a bubble</code><code> </code><br><code>Music or sounds are introduced to </code><br><code>desensitise the user to their immediate  </code><br><code>real-life surroundings. These are combined </code><br><code>with sharp intrusive sounds that make the </code><br><code>player hyper-aware of the screen. Such </code><br><code>techniques are particularly used in gaming.</code><br><code>3</code><code> </code><br><code> Small demands</code><code> </code><br><code>These are invitations, such as </code><code>‚Äòclick here‚Äô</code><code>, </code><br><code>‚Äòwatch video‚Äô</code><code>, </code><code>‚Äòaccept invitation‚Äô</code><code>, </code><code>‚ÄòLike‚Äô</code><code>, </code><br><code>‚Äòagree‚Äô</code><code>, </code><code>‚Äòpost‚Äô</code><code> or </code><code>‚Äòread message‚Äô</code><code>. Each </code><br><code>demand seems small but will frictionlessly </code><br><code>lead to further demands for action.</code><br><code>Chapter Three </code><br><code>Strategies that keep users online</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665597_5,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665597.pdf,5,5,2665597,attachments/2665597.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Pour l‚Äôensemble des mod√®les d‚Äôapprentissage automatique, l‚Äôexplicabilit√© devra </code><br><code>s‚Äôappliquer sur tous les √©l√©ments du syst√®me d‚ÄôIA : jeu de donn√©es, algorithme </code><br><code>d‚Äôapprentissage, mod√®le, pr√©diction du mod√®le. </code><br><code>Il est √©vident que certains syst√®mes d‚ÄôIA bas√©s sur des r√©seaux de neurones </code><br><code>entra√Ænent une opacit√© par un effet bo√Æte noire. Il conviendra alors de s‚Äôappuyer sur </code><br><code>les travaux en cours pour trouver la meilleurs strat√©gie d‚Äôexplicabilit√© : explicabilit√© </code><br><code>par construction, explications a posteriori.</code><code> </code><br><code>En tout √©tat de cause, il existe des solutions pour expliquer les mod√®les propos√©s et </code><br><code>ne pas risquer d‚Äôexposer les salari√©s √† des d√©cisions arbitraires. </code><br><code> </code><br><code> </code><br><code>La CFE-CGC consid√®re que la s√©curit√© des donn√©es des salari√©s est fondamentale. </code><br><code>L‚Äôarticle 15 du r√®glement propos√© vient pr√©ciser les obligations des fournisseurs d‚ÄôIA </code><br><code>en mati√®re de robustesse et de cybers√©curit√©. </code><br><code>Cependant, aucune obligation de test d‚Äôintrusion n‚Äôest faite √† ces fournisseurs d‚ÄôIA et </code><br><code>on ne leur fournit aucun r√©f√©rentiel technique ou qualification de s√©curit√© en annexe. </code><br><code>Nous consid√©rons que le r√®glement ne permet √† ces fournisseurs d‚ÄôIA de respecter </code><br><code>les obligations √©nonc√©es dans l‚Äôarticle 15.  </code><br><code> </code><br><code> </code><br><code> </code><br><code>Pour terminer, il est annonc√© en propos liminaire que les droits fondamentaux des </code><br><code>salari√©s seront renforc√©s (article 31 de la charte des droits fondamentaux) par le </code><br><code>pr√©sent r√®glement. </code><br><code>En l‚Äô√©tat du r√®glement propos√© et √† la suite de l‚Äôensemble de nos remarques, la CFECGC s‚Äôinterroge sur le renforcement r√©el de ces droits pour les salari√©s. </code><br><code>Pour nous, les garde-fous n√©cessaires √† leur garantie tels que d√©finis par l‚Äôarticle 31 </code><br><code>ne sont pas suffisants. </code><br><code>Nous esp√©rons que les travaux port√©s par le Conseil de l‚ÄôEurope sur l‚ÄôIA viendront </code><br><code>compl√©ter le r√®glement pour une intelligence artificielle √©thique.  </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665503_7,other,../24212003_requirements_for_artificial_intelligence/attachments/2665503.pdf,11,7,2665503,attachments/2665503.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Seite 7 von 11 </code><br><code> </code><br><code>4. Einzelregelungen und Aspekte </code><br><code> </code><br><code>Ohne den Anspruch auf Vollst√§ndigkeit sei im Detail auf Folgendes hingewiesen: </code><br><code>- </code><br><code>Inhaltlich zu hinterfragen ist, warum Nutzer im Sinne der geplanten Verordnung </code><br><code>nur professionelle Nutzer sind (Art. 3 Abs. 4 KI-VO-E). Auch wenn das </code><br><code>wahrscheinlich praktisch nur in wenigen F√§llen von Bedeutung ist, fragt sich, was </code><br><code>f√ºr nichtprofessionelle Anbieter und Anwender (etwa bei Idealvereinen und </code><br><code>NGOS) hei√üt. Die DSGVO etwa kennt eine Ausnahme nur f√ºr nat√ºrliche </code><br><code>Personen zur Aus√ºbung ausschlie√ülich pers√∂nlicher oder famili√§rer T√§tigkeiten </code><br><code>(Art. 2 Abs. 2 lit. c) KI-VO-E). Der Unterschied √ºberzeugt nicht. </code><br><code>- </code><br><code>Weiter kritisch zu hinterfragen ist, dass die biometrischen Identifikationssysteme </code><br><code>weitgehend nur verboten sind, wenn sie in Realtime funktionieren (Art. 5 Nr. 1 lit. </code><br><code>d) KI-VO-E), sonst aber nur als Hochriskante KI-Systeme zu behandeln sind. Es </code><br><code>ist nicht zu erkennen, dass die Gefahren solcher Systeme so unterschiedlich </code><br><code>sind, dass sie diese unterschiedliche Behandlung rechtfertigen. </code><br><code>- </code><br><code>Auff√§llig ist, dass in Art. 9 KI-VO-E f√ºr Sicherheitsma√ünahmen lediglich der </code><br><code>anerkannte Stand der Technik</code><code> gefordert wird. Dieses ist gerade f√ºr Hochrisiko-</code><br><code>KI-Systeme √ºberraschend, zumal etwa das Produkthaftungsrecht und auch die </code><br><code>deutsche Rechtsprechung sogar die Einhaltung des Stands der Wissenschaft </code><br><code>und Technik fordern (vgl. ¬ß 1 Abs. 2 Nr. 5 ProdHaftG; </code><code>Hofmann</code><code>, CR 2020, 282, </code><br><code>284 f.). Hier sollte in Ansehung der Hoch-Risiko-Systeme einerseits und </code><br><code>Bu√ügelder sowie zur Innovationsf√∂rderung anderseits zumindest der </code><br><code>Kompromiss gesucht werden, auf den neuesten Stand der Technik abzustellen. </code><br><code>- </code><br><code>Nicht realistisch erscheint die Anforderung in Art. 10 Abs. 3 S. 1 KI-VO-E, nach </code><br><code>der Trainings-, Validierungs- und Testdatens√§tze relevant, repr√§sentativ, </code><br><code>fehlerfrei und vollst√§ndig sein m√ºssen. Gerade mit Blick auf die Vollst√§ndigkeit </code><br><code>und Fehlerfreiheit scheint dieses unm√∂glich. Erg√§nzt werden sollte ein </code><br><code>""bestm√∂glich"" oder ""im zumutbaren Rahmen"". Auch ist zu bedenken, dass </code><br><code>mitunter je nach Zweckbestimmung auch schlechte Daten gut sein k√∂nnen. </code><br><code>Au√üerdem lassen sich die Relevanz, Repr√§sentanz, Vollst√§ndigkeit und </code><br><code>Fehlerfreiheit der Daten nur vor dem Hintergrund des bestimmungsgem√§√üen </code><br><code>Verwendungszwecks definieren. Dieser Bezug in Art. 10 Abs. 3 S. 1 KI-VO-E </code><br><code>durch die Erg√§nzung ""‚Ä¶ </code><code>m√ºssen mit Blick auf die Zweckbestimmung des </code><br><code>Hochrisiko-KI-Systems relevant</code><code> ‚Ä¶""  klargestellt werden. Schlie√ülich fehlt zu Art. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665528_9,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665528.pdf,20,9,2665528,attachments/2665528.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>1 </code><br><code> </code><br><code>Public consultation on European Health Data Space </code><br><code>‚Äì EPF accompanying paper  </code><br><code>26 July 2021  </code><br><code>The European Patients‚Äô Forum (EPF) is an umbrella organisation of patients‚Äô organisations across </code><br><code>Europe and across disease-areas. EPF represents the interests of over 150 million patients with chronic </code><br><code>conditions across the EU who expect and rely on European cooperation to improve healthcare delivery </code><br><code>and quality for all. In concert with its 77 members, EPF ensures the patient perspective in European </code><br><code>key health debates, including digital health and health data. To achieve this goal, over the past years </code><br><code>EPF has been particularly active in these fields through both its policy work</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> and several projects</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>.</code><code> </code><br><code>This statement is an addition to the EPF‚Äôs response to the European Health Data Space (EHDS) Public </code><br><code>Consultation</code><code>, submitted through the EU Consultation portal. The response and this statement have </code><br><code>been developed in a consultative process with our members and our EPF Digital Health Working </code><br><code>Group. In this accompanying statement, we further elaborate on some of the key elements included </code><br><code>in our response and summarise our views on the EHDS. </code><br><code>NOTE ‚Äì the responses included in the Consultation and in this accompanying paper are based on the </code><br><code>current understanding of the European Health Data Space proposal development and on the </code><br><code>interpretation of the questions included in the questionnaire. On this point, several of the questions </code><br><code>have been identified quite broad and unclear, at least in some of their elements, in particular in terms </code><br><code>of prioritisation and identification of what should constitute a precondition for the EHDS design and </code><br><code>implementation. For instance, it is essential to note that </code><code>many of the proposed options, tools, </code><br><code>platforms, and policies mentioned in the questionnaire as ways to facilitate health data sharing, can </code><br><code>be considered viable choices only if patients are first ensured proper access and control over their </code><br><code>health data with a transparent and trustworthy framework</code><code>. Furthermore, EPF‚Äôs responses might not </code><br><code>entirely reflect individual organisations views or precisely capture national or disease-specific </code><br><code>challenges and suggestions. They should be therefore considered in parallel with the inputs shared by </code><br><code>our members, both patients‚Äô national coalitions and European disease-specific organisations </code><br><code>Introduction </code><br><code>Health is an area where Europe can undoubtedly benefit from the data revolution. Proper use of </code><br><code>health data can improve health systems‚Äô sustainability, increase the quality, safety and patientcentredness of healthcare, decrease costs and transform care into a more participatory process.</code><code>3</code><code> </code><br><code>Health data can support the work of regulatory bodies, facilitating the assessment of medical products </code><br><code>and demonstration of their safety and efficacy. Furthermore, the COVID-19 pandemic has </code><br><code>demonstrated how accurate and quickly accessible data is also fundamental in the management of </code><br><code>cross-border public health emergencies. Nevertheless, the road to fully exploit the potential benefits </code><br><code> </code><br><code>1</code><code> EPF policy and advocacy work related to digital health and data includes our </code><code>position paper on eHealth</code><code> </code><br><code>(2016), </code><code>GDPR guide for patients and patients‚Äô organisations</code><code> (2016), </code><code>Data and Artificial Intelligence EU Policy </code><br><code>Briefing for Patient Organisations</code><code> (2020) and a </code><code>brief summary of our recent EPF survey on Electronic </code><br><code>Healthcare Records</code><code> (2020). Furthermore, EPF responded to the EC Consultations on the </code><code>Data Strategy and AI </code><br><code>White Paper</code><code> (2020), </code><code>European Health Data Space roadmap and</code><code> </code><code>Data Governance Act</code><code> (2021).  </code><br><code>2</code><code> EPF recent projects related to digital health and data include: </code><code>Digital Health Europe, EHDEN ‚Äì The European </code><br><code>Health Data and Evidence Network</code><code>, and </code><code>Data Saves Lives</code><code>.  </code><br><code>3</code><code> Europe for patients Manifesto, </code><code>https://www.europeforpatients.eu/</code><code> </code>",FALSE_NEGATIVE
fitz_2663367_3,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663367.pdf,3,3,2663367,attachments/2663367.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>impact on our society should be avoided at all costs. If the regulation hinders innovation and deters </code><br><code>investments in the EU markets, the providers will offer their applications elsewhere.  </code><br><code> </code><br><code>The aim to build trust in AI is crucial and so is mitigating the potential negative impact of AI, while this </code><br><code>should not prevent the EU from reaping all the positive benefits that AI can bring to our society.  </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665537_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665537.pdf,3,2,2665537,attachments/2665537.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Need of new European regulations in line with technical progress</code><code> </code><br><code>6. The public interest of road safety has justified the European authorities concentrating their efforts, as a matter of priority, on the vehicle </code><br><code>connected to emergency aid. European Parliament and Council Regulation n¬∞ 2015/758 defines the legal framework for the deployment of </code><br><code>the on-board emergency call system (eCall)</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>. It states that the mandatory installation of this device must, as early as 2018, be without </code><br><code>prejudice to the </code><code>‚Äúright of all stakeholders, such as car manufacturers and independent operators, to offer additional emergency and / value </code><br><code>added, in parallel or on the basis of the embedded eCall system based on number 112</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>.‚Äù</code><code> Article 5 (7) of that regulation requires manufacturers </code><br><code>to make </code><code>‚Äúthe on-board eCall system based on number 112 accessible to all independent operators at a reasonable cost not exceeding a </code><br><code>nominal amount and without discrimination for repair and maintenance.‚Äù </code><br><code>7. In the event of failure to comply with these provisions, Article 11 of this regulation requires Member States to enforce </code><code>‚Äúeffective, </code><br><code>proportionate and dissuasive penalties‚Äù</code><code> which they must notify to the European Commission. Beyond sanctions better defined and controlled </code><br><code>by the European Commission, technical solutions to be put in place are evaluated and proposed in order to achieve the objective of opening </code><br><code>up the market for aftermarket services covered by the regulation in Article 5 (7). </code><br><code> </code><br><code>8. Thus, under Article 12, the European Parliament and the Council empower the European Commission to establish the specifications for </code><br><code>an interoperable, standardized, secure and open-access platform. The study of technical solutions for access to vehicle data, carried out by </code><br><code>the C-ITS expert group, revealed three types of platforms: two would be internal to the vehicle, while another would be external: an application </code><br><code>platform or an interface, which would be embedded in the vehicle; a data server platform. </code><br><code>9. These options were assessed in the framework of the Commission's work on access to data and on-board resources to be completed. </code><br><code>The European Commission Informed that the conclusions of this analysis serve as a basis for modernizing European provisions on access </code><br><code>to technical information for the maintenance and repair of connected vehicles. </code><br><code>10. National authorities and many specialists draw attention to the manufacturers‚Äô intellectual property rights, which oblige repairers to refrain </code><br><code>from any manipulation that would affect the initial configuration of the connected vehicle. The maintenance and repair of this vehicle, whose </code><br><code>sensors and actuators are governed by millions of lines of computer code thanks to the manufacturer's software, depends on the access to </code><br><code>specific data and AI information. </code><br><code>11. According to a study carried out in France by the National Institute of Industrial Property concerning the digital transformation of the </code><br><code>economy, ‚Äú</code><code>the search for a balance between the use of technology and the rules of competition can encourage the phenomenon of </code><br><code>standardization, the main objective of which is to harmonize technologies for the benefit of consumers</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code>.‚Äù</code><code> </code><br><code>12. The principle of non-discriminatory access embodied in the proposed ISO for the connected vehicle is reproduced in the new European </code><br><code>Regulation on the vehicle type approval. These new provisions, in particular those regarding recital 51, clearly prohibit the use of advanced </code><br><code>technologies to impede access by independent operators to data: </code><code>‚Äútechnological advances introducing new methods or techniques for the </code><br><code>diagnosis and repair of vehicles, such as remote access to vehicle information and software, should not weaken the objectives of this </code><br><code>regulation as regards access to repair and maintenance information for independent</code><code> </code><code>operators</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>.‚Äù </code><br><code>Answering the Consultation  </code><br><code>13. In view of difficulties arising from the use of advanced technologies to steer the consumer towards the manufacturer's network, being </code><br><code>the dominant player in the first six years of the vehicle, appropriate technical and legal solutions are urgent to be enforced.  </code><br><code>13.1. FNA representatives welcomed EU Commission study on </code><code>‚ÄòAccess to In-Vehicle Data and Resources‚Äô</code><code>  which indicates that the </code><br><code>‚Äúcentralisation of in-vehicle data as currently implemented by some market players might in itself not be sufficient to ensure fair and </code><br><code>undistorted competition between service providers.‚Äù</code><code> This centralisation system is the ‚Äú</code><code>extended-vehicle concept‚Äù</code><code>, a technological solution </code><br><code>that the EU study clearly shows that it is a way to exacerbate tensions on the aftermarket by strengthening unfair and distorted competition. </code><br><code>FNA urges to approve the right regulatory framework on access data and AI, because of the present negative economic impact on EU SMEs </code><br><code>who are being already cut out of the market, in particular independent repairers and third enterprises also involved in downstream activities. </code><br><code>13.2. The present new Regulation laying down harmonised rules on AI should correspond to the principles advocated by FNA for independent </code><br><code>repairers and already supported by the European Commission as follows: </code><br><code>-Unrestricted access to vehicle repair and maintenance data,  </code><br><code>-Effective competition in the market of services providing automotive data</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code>. </code><br><code>13.3. An effective European legal framework is, indeed, indispensable in order to achieve an ecosystem of trust and to avoid a fragmentation </code><br><code>of national rules and initiatives. The new Proposal of Regulation laying down harmonized rules on AI, in particular option 3 imposing mandatory </code><br><code>legal requirements for AI being used by car manufacturers is therefore welcomed by DBR and FNA representatives. </code><br><code>                                                           </code><br><code>2</code><code> Regulation (EU) No 2015/758 of the European Parliament and of the Council of 29 April 2015 on type-approval requirements for the deployment of the onboard eCall system 112 and amending Directive 2007/46 / EC, Official Journal of the European Union n¬∞ L 123/77 of 19 May 2015. </code><br><code>3</code><code> Paragraph 15 of the explanatory statement to Regulation (EU) No 2015/758 </code><br><code>4</code><code> INPI, </code><code>¬´ la propri√©t√© intellectuelle et la transformation num√©rique de l‚Äô√©conomie ¬ª,</code><code> contribution de M. Fr√©d√©ric BOURGUET et Mme Cristina BAYONA </code><br><code>PHILIPPINE, p.259, note 320 </code><code>https://www.inpi.fr/fr/services-et-prestations/etude-pi-et-economie-numerique </code><br><code>5</code><code> European Regulation of the European Parliament and of the Council on the type-approval and market surveillance of motor vehicles  </code><br><code>6</code><code> Draft Regulation of the European Parliament and of the Council on the type-approval and market surveillance of motor vehicles, op. cit. Paragraph 36. </code>",POSITIVE
fitz_2665562_5,other,../24212003_requirements_for_artificial_intelligence/attachments/2665562.pdf,10,5,2665562,attachments/2665562.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>includes standardisation on governance of AI systems, transparency, trustworthiness</code><br><code>and explainability.</code><br><code>A major strength of the European standardisation system has been its close linkage to</code><br><code>international standardisation including the possibility to adopt international standards</code><br><code>as European standards and the possibility of co-development. OFE would like to</code><br><code>encourage the European Commission to promote a close linkage with international</code><br><code>standardisation and the adoption and use of international standards whenever</code><br><code>possible.</code><br><code>Regarding the standardisation requests for the development of harmonised standards</code><br><code>OFE would like to recommend the following considerations:</code><br><code>1. Standardisation requests should be available early - ideally the first standardisation</code><br><code>requests should be issued before adoption of the AI Regulation so that work can start</code><br><code>early without undermining the legislative process.</code><br><code>2. Standardisation requests should be developed in close interaction with the ESOs</code><br><code>and the experts in the respective technical committees.</code><br><code>3. Avoid standardisation requests that are too prescriptive. The clear strength of the</code><br><code>NLF is that the technical realisation of how to meet legal requirements is developed by</code><br><code>experts provided by all stakeholders and agreed by consensus. This promotes that</code><br><code>European standards reflect the state-of-the-art. It also facilitates the adoption of</code><br><code>international standards.</code><br><code>4. Keep a close dialogue between the European Commission and the technical experts</code><br><code>throughout the entire development process. This is important to avoid</code><br><code>misunderstandings and prevent that standards might not meet the needs and</code><br><code>expectations as outlined in the respective standardisation requests.</code><br><code>5. Fast citation of harmonised standards in the Official Journal of the EU ÓÇÅOJEUÓÇÇ. This</code><br><code>is important to make the standards available for presumption of conformity.</code><br><code>Following these considerations above will also be of high importance for supporting</code><br><code>that the harmonised standards can be available in time and that the transition time of</code><br><code>24 months will be sufficient between the coming into force of the Regulation and the</code><br><code>moment it applies. As a number of harmonised standards will have to be available, and</code><br><code>given the scarcity of actual experts in the field of AI and available for doing the</code><br><code>standardisation work 24 months is not much and it will be very important that all</code><br><code>actors will work together very collaboratively and in close interaction and exchange.</code><br><code>OpenForum Europe AISBL</code><br><code>Avenue des Arts 56 4C</code><br><code>Brussels 1000</code><br><code>OFE in the EU transparency register: 2702114689</code><br><code>5</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665518_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665518.pdf,2,1,2665518,attachments/2665518.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Page </code><code>1</code><code> of </code><code>2</code><code> </code><br><code> </code><br><code>Feedback on proposal to regulate artificial intelligence  </code><br><code>By Norwegian labor union Negotia </code><br><code>Negotia is a labor union for private sector employees.‚ÄØ We organize members from business </code><br><code>areas such as service, sale, marketing, IT, administration, organization, laboratories, </code><br><code>logistics, accounting and finance. Negotia is a politically independent organization and </code><br><code>affiliated with the umbrella organization Norwegian Confederation of Vocational Unions </code><br><code>(YS). Approximately 21 500 workers employed in the private sector are organized in Negotia.  </code><br><code>Negotia would like to commend the European Commission for proposing legislation that </code><br><code>represents an important step towards protecting fundamental rights when it comes to the </code><br><code>rapidly evolving technology of artificial intelligence.   </code><br><code>It is our belief that new technology creates jobs and increased value for companies. As a trade </code><br><code>union, we are nevertheless concerned that the proposed legislation does not provide </code><br><code>employees with sufficient protection against the negative consequences of using artificial </code><br><code>intelligence in the workplace.  </code><br><code>We fully support categorizing the use of artificial intelligence in the workplace as high-risk, </code><br><code>based on the possible threat to workers' fundamental rights. The European Commission's </code><br><code>proposal mentions, in particular, the use of artificial intelligence in recruitment processes, in </code><br><code>decisions regarding promotions or termination of employment, in the distribution of work </code><br><code>tasks or the monitoring and evaluation of employees. Negotia agrees that all of these areas </code><br><code>have a major impact on the career opportunities and lives of individual workers. The use of </code><br><code>artificial intelligence in these areas can easily be perceived as intrusive. Flaws in the </code><br><code>technology used can, in worst case scenarios, lead to systematic discrimination. The </code><br><code>introduction of artificial intelligence in these areas must therefore be treated with great </code><br><code>caution. </code><br><code>Any introduction of artificial intelligence into an organization will affect the employees. It is </code><br><code>our belief that the Commission's proposed legislation on the use of artificial intelligence in the </code><br><code>workplace, does not provide adequate protection in the face of these risks. Negotia therefore </code><br><code>believes that all artificial intelligence used in the workplace should be defined as high-risk. </code><br><code>Furthermore, the proposed regulation does not take into consideration the unique power </code><br><code>dynamics in the workplace. The proposed system suggests that the users of artificial </code><br><code>intelligence systems are mostly consumers. In the workplace, however, adopting new </code><br><code>technology is a leadership decision. Even though it is the employees who risk the greatest </code><br><code>negative consequences when introducing such systems, they may not be involved in the </code><br><code>decision-making at all. If a new system is adopted, it is unlikely that any employee can choose </code><br><code>to opt-out. Negotia believes this should be clearly reflected in the regulation of artificial </code><br><code>intelligence in the workplace. </code><br><code>The proposed regulation also does not directly address the ethical implications of using </code><br><code>artificial intelligence in the workplace. For instance: If the employer's purpose in using </code><br><code>artificial intelligence is to monitor the employees, this may have a negative effect on them, </code><br><code>even if the technological solution performs as intended. The undesirable effects of this kind of </code><br><code>intrusive technology must be taken into account in the final regulation. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665597_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665597.pdf,5,3,2665597,attachments/2665597.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Par ailleurs, de fa√ßon plus courante, nous serons confront√©s √† des applications li√©es </code><br><code>au monde du travail (Annexe 3 point 4) et ayant des obligations sur la transparence </code><br><code>des algorithmes vis√©es par l‚Äôarticle 52, car utilisant des syst√®mes de reconnaissance </code><br><code>des √©motions. Nous trouvons aujourd‚Äôhui ce type d‚Äôapplication pour le recrutement, </code><br><code>par exemple</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>. L‚Äôoptimisation du processus de recrutement se fait par des syst√®mes </code><br><code>de reconnaissance des √©motions, afin d‚Äô√©valuer au mieux les candidats. Nous </code><br><code>pouvons imaginer que ce type de syst√®me va se p√©renniser pour, entre autres, tester </code><br><code>leur r√©sistance au stress. Ces applications font aussi une analyse de la voix du </code><br><code>candidat √† travers son rythme et son intensit√© (prosodie). La voix est consid√©r√©e </code><br><code>comme une donn√©e biom√©trique, car elle permet d‚Äôidendifier la personne, et peut </code><br><code>donc √™tre soumise aux obligations associ√©es (Annexe 3 point 1). </code><br><code> </code><br><code> </code><br><code>C‚Äôest une des limites de l‚Äôapproche par le risque propos√©e par la Commission </code><br><code>europ√©enne. Il aurait √©t√© plus pertinent de s‚Äôappuyer sur celle du Federal </code><br><code>Government‚Äôs Data Ethics Commission (‚ÄúDatenethikkommission‚Äù),</code><code> </code><code>fond√©e sur la </code><br><code>criticit√©, beaucoup plus pr√©cise</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>. En effet, une granularit√© plus fine des risques aurait </code><br><code>permis une simplification du mod√®le et donc, des obligations associ√©es. </code><br><code>Pour ce faire, il faudrait d√©finir des sous-niveaux par criticit√© dans les mod√®les √† haut </code><br><code>risque afin d‚Äôajouter des obligations claires inh√©rentes aux outils utilisant des </code><br><code>donn√©es biom√©triques, par exemple.  </code><br><code>Dans le cas des applications de recrutement avec utilisation de donn√©es </code><br><code>biom√©triques, on peut supposer que les fournisseurs d‚ÄôIA devront remplir toutes les </code><br><code>obligations li√©es √† l‚Äôarticle 16 et aux obligations de transparence de l‚Äôarticle 52 </code><br><code>(syst√®me de reconnaissance des √©motions). Les obligations li√©es aux syst√®mes d‚ÄôIA </code><br><code>utilisant la voix d√©pendront de la finalit√© recherch√©e et d√©finie par fournisseur d‚ÄôIA. </code><br><code>Que se passera-t-il pour ceux qui auront omis de d√©clarer l‚Äôune des obligations en </code><br><code>pr√©textant la bonne foi ? C‚Äôest une des limites du mod√®le auto-d√©claratif propos√© par </code><br><code>le r√®glement. </code><br><code> </code><br><code>Pour la CFE-CGE, l‚Äôauto√©valuation des syst√®mes √† haut risque par les fournisseurs </code><br><code>d‚ÄôIA n‚Äôest pas suffisante dans la r√©gulation propos√©e, m√™me si les obligations et </code><br><code>p√©nalit√©s associ√©es semblent assez contraignantes pour eux. </code><br><code>Nous pr√©conisons que des autorit√©s tierces veillent √† la conformit√© des syst√®mes </code><br><code>d‚ÄôIA avant la mise sur le march√©, et que ces conformit√©s soient syst√©matiquement </code><br><code>fournies aux repr√©sentants des salari√©s lors des informations ou consultations li√©es √† </code><br><code>une introduction de syst√®me d‚ÄôIA dans le monde professionnel. </code><br><code>Ces autorit√©s devraient aussi avoir la possibilit√© d‚Äôauditer ces syst√®mes d‚ÄôIA afin de </code><br><code>v√©rifier la conformit√© des produits tout au long de leur cycle de vie. </code><br><code>Ceci est d√©j√† rendu possible par la demande d‚Äôune autorit√© nationale comp√©tente de </code><br><code>l‚Äôarticle 16 (point j). </code><br><code>Il faut donc rendre obligatoires ces demandes pour les applications √† haut risque </code><br><code>concernant le monde professionnel.  </code><br><code>Pour toutes les applications d√©j√† sur le march√© dans le domaine des RH et du </code><br><code>monde du travail en g√©n√©ral, celles-ci devront passer par un processus d‚Äô√©valuation </code><br><code>strict vu les d√©rives que nous constatons d√©j√† en France et en Europe. </code><br><code> </code><br><code>1</code><code> </code><code>https://www.nouvelobs.com/economie/20191029.OBS20418/recale-d-un-job-apres-un-entretien-video-vousn-avez-peut-etre-pas-plu-a-l-ia.html</code><code> </code><br><code>2</code><code> </code><code>https://www.bmjv.de/DE/Themen/FokusThemen/Datenethikkommission/Datenethikkommission_EN_node.h</code><br><code>tml;jsessionid=AC45E5C2DA3EA6D4BEBC9FE46A1C36E5.2_cid297</code><code> </code>",POSITIVE
fitz_2665578_5,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665578.pdf,9,5,2665578,attachments/2665578.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Comments on the proposed  </code><br><code>Artificial Intelligence Act </code><br><code>by Women in AI Austria </code><br><code> </code><br><code> </code><br><code> </code><br><code>Women in AI Austria </code><br><code>Transparency Register ID:  </code><br><code>815698241750-24 </code><br><code>Reschgasse 2/5 </code><br><code>AT-1220 Vienna </code><br><code>carina@womeninai.co</code><code> </code><br><code>5</code><code> of </code><code>9</code><code> </code><br><code> </code><br><code>the Charter of Fundamental Rights, especially regarding the dignity and freedoms of persons. Law </code><br><code>enforcement and other public authorities operate on the basis of a clear mandate to protect the </code><br><code>public interest; private companies are under no such obligation and should not be able to exert </code><br><code>control over persons living in the European Union to a greater extent than public authorities. The </code><br><code>restrictions accorded to the use of these technologies should not only apply to law enforcement, but </code><br><code>to all actors alike.  </code><br><code>Art. 7 Amendments to Annex III </code><br><code>Art. 7 empowers the Commission to add new applications to Annex III which fall into one of the </code><br><code>previously established high-risk areas and have equal or higher risk than those applications already </code><br><code>listed by way of a delegated act. We support the flexibility this provision offers and encourage a </code><br><code>regular evaluation of the market for AI systems and the effects of AI systems in certain use cases. </code><br><code>However, we believe Art. 7 may not be sufficient to adapt the AI Act to new and emerging AI use </code><br><code>cases and offer long-term protection of fundamental rights due to its limited scope. Since only the </code><br><code>applications may be updated, but not the use case areas, we believe the definition of high-risk AI </code><br><code>systems may not be future-proof and thus not offer long-term safety for people affected by AI </code><br><code>systems in a negative way. We therefore support expanding this Article to allow for the amendment </code><br><code>of new high-risk areas by the Commission via delegated acts.  </code><br><code>Art. 49 CE marking of conformity </code><br><code>In the current version of the AI Act, providers of high-risk AI systems specified in Annex III may </code><br><code>conduct self-assessments to ascertain conformity (Art. 43/2 - in case of biometric identification </code><br><code>systems, only those applying harmonised standards or common specifications as detailed in Art. </code><br><code>43/1). In effect, this means that providers of high-risk AI systems as defined in Annex III must apply </code><br><code>the CE marking called for in Art. 49, yet for users of these systems, there is no possibility to verify </code><br><code>whether the provider has acted in due diligence. In our view, there is a high risk that the CE marking </code><br><code>may be applied inappropriately and the penalties may not be a strong enough deterrent from noncompliance. We would prefer an obligation for providers of high-risk AI systems which have </code><br><code>conducted a self-assessment to nonetheless register with a notified body, which accepts the </code><br><code>required documentation and issues a certificate of conformity. This will allow users to verify at </code><br><code>minimum that the documentation has been presented to a notified body and is accessible to the </code><br><code>market surveillance authority in case of any issues.  </code><br><code>Art. 52 Transparency obligations for certain AI systems </code><br><code>In its paragraphs, Article 52 enumerates three use cases of AI systems which shall be subjected to </code><br><code>disclosure obligations. The first use concerns automated systems interacting with natural persons </code><br><code>(bots for short), the second concerns emotion recognition or biometric categorisation systems and </code><br><code>the third prescribes disclosure of the synthetic nature of AI-generated content (or deep fakes). We </code><br><code>believe that these provisions may be insufficient in mitigating potential harm caused by the use of </code><br><code>these systems. Regarding the interaction with bots in Art. 52(1), affected parties should have the </code><br><code>right to interact with the user in a different format, e.g. in the case of a chatbot, affected parties </code><br><code>could instead contact the user via a service hotline or an e-mail address. The lack of such alternatives </code><br><code>is more severe in Art. 52(2), as it explicitly legitimises the use of emotion recognition systems (which </code><br><code>are not scientifically sound). Since these AI systems are not based on valid science, their output </code><br><code>cannot be considered reliable, creating greater harm for affected parties who ‚Äì for whichever reason </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665625_18,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665625.pdf,28,18,2665625,attachments/2665625.pdf#page=18,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>b) Extending the ban on certain AI Systems to private actors as well as public</code><br><code>authorities (Article 5,1(c),(d),(e))</code><br><code>We believe that some AI systems, which are currently approved by the AI Act under certain</code><br><code>circumstances, should be banned in their entirety. Specifically, these systems are:</code><br><code>‚óè</code><br><code>social scoring;</code><br><code>‚óè</code><br><code>‚Äòreal-time‚Äô remote biometric identification systems;</code><br><code>‚óè</code><br><code>systems that categorise individuals from biometrics into clusters according to certain</code><br><code>identity criteria; and</code><br><code>‚óè</code><br><code>systems that infer emotions of a natural person, except for special situations.</code><br><code>Please find our detailed rationale for each of the proposed bans below.</code><br><code>(i) Social scoring (Article 5,1(c))</code><br><code>The current draft of Article 5,1(c) of the AI Act prohibits the use of AI systems by</code><code> public</code><br><code>authorities</code><code> or on their behalf for the evaluation or classification of the</code><code> trustworthiness of natural</code><br><code>persons</code><code> over a certain period of time based on their social behaviour or known or predicted</code><br><code>personal or personality characteristics. It does so where this ‚Äúsocial score‚Äù could lead to</code><br><code>detrimental or unfavourable treatment of either individual natural persons or groups in social</code><br><code>contexts which are unrelated to the contexts in which the data was originally generated or</code><br><code>collected; or to detrimental or unfavourable treatment of certain natural persons or groups that is</code><br><code>unjustified or disproportionate</code><code> to their social behaviour or its gravity.</code><br><code>It is our belief that these restrictions should be adopted by the AI Act in order to</code><code> ban all such AI</code><br><code>systems used by private actors as well.</code><br><code>The</code><code> EDPB-EDPS Joint Opinion</code><code> is very clear that the use of AI for social scoring can lead to</code><br><code>discrimination, stating ‚Äú</code><code>Private companies, notably social media and cloud service providers,</code><br><code>can process vast amounts of personal data and conduct social scoring. Consequently, the</code><br><code>Proposal should prohibit any type of social scoring</code><code>.‚Äù</code><br><code>AlgorithmWatch</code><code> also expressed its concern that ‚Äú</code><code>the prohibition of AI systems used for social</code><br><code>scoring purposes is also limited to those deployed by public authorities. Again, private actors</code><br><code>are kept out of the line of fire</code><code>‚Äù.</code><br><code>Furthermore, the</code><code> Demystifying the Draft EU Artificial Intelligence Act</code><code> article also has interesting</code><br><code>views on social scoring. Initially, the authors point out that the AI Act does not define</code><br><code>trustworthiness, and relying on a 2000 paper on Political Trust and Trustworthiness, go on to</code><br><code>define trustworthiness as</code><code> ‚Äúa combination of attributes that indicate that an entity will not betray</code><br><code>another due to bad faith such as misaligned incentives, lack of care, disregard for</code><br><code>promise-keeping (commitment) or through ineptitude at a task (competence)</code><code>‚Äù.</code><br><code>18</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665406_9,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665406.pdf,10,9,2665406,attachments/2665406.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>9 </code><br><code> </code><br><code>sufficiently consider that the wide-scale use of such systems can enable </code><code>indiscriminate mass </code><br><code>surveillance, which is inherently incompatible with fundamental rights, creates chilling effects, and </code><br><code>undermines fundamental principles of democratic societies.  </code><br><code>I</code><code> </code><br><code>First, the prohibition of the use of these systems should not be triggered by any specific type of </code><br><code>technology but depend on the impacts they have: </code><code>Whenever biometric recognition systems </code><br><code>enable indiscriminate mass surveillance, arbitrarily-targeted or discriminatorily-targeted </code><br><code>surveillance, the use of these systems is incompatible with fundamental rights and should be </code><br><code>banned. </code><code>In its current version, Article 5 does not live up to that demand.  </code><br><code>II</code><code> </code><code>Second, the prohibition of real-time remote biometric identification systems only applies to systems </code><br><code>used for law enforcement purposes in publicly accessible spaces, thus neither to systems used by </code><br><code>other public authorities</code><code> nor to those used by </code><code>private actors that act on behalf of public </code><br><code>authorities</code><code> or in the framework of Public-Private-Partnerships. Evidently, the major risks to </code><br><code>fundamental rights associated with such systems are not limited to law enforcement purposes‚Äîa </code><br><code>fact that the proposal does not sufficiently reflect.  </code><br><code>III</code><code> </code><code>Third, in the same vein, the risks are not limited to real-time biometric identification systems: If, for </code><br><code>example, ‚Äú</code><code>post‚Äù biometric identification</code><code> </code><code>systems</code><code> are subsequently applied to video footage of an </code><br><code>event, this may still enable mass surveillance.  </code><br><code>IV</code><code> </code><code>Third, a </code><code>range of exceptions</code><code> to the prohibition creates a number of loopholes that authorities could </code><br><code>try to exploit. For example, the use of real-time biometric identification can be allowed in relation </code><br><code>to a wide range of criminal offenses (Article 5(3)), or for the ‚Äúprevention of a specific, substantial and </code><br><code>imminent threat to the life or physical safety of natural persons or of a terrorist attack‚Äù (Article 5(2)), </code><br><code>the interpretation of which leaves wide discretionary power to the authorities. While judicial </code><br><code>authorization is necessary for such exceptional uses, it can be postponed in cases of urgency.  </code><br><code>/</code><code> </code><br><code>As the General Data Protection Regulation (GDPR) and the Law Enforcement Directive (LED) have </code><br><code>been interpreted controversially with respect to biometric identification and, thereby, do not </code><br><code>reliably protect against such mass surveillance techniques, </code><code>the AI Act would provide an </code><br><code>opportunity to comprehensively ban all uses of biometric recognition systems in public </code><br><code>space that lead to mass surveillance and that are, therefore, inherently in conflict with </code><br><code>fundamental rights. We urge the Council and the Parliament to take that opportunity and </code><br><code>improve the effectiveness of the ban</code><code>.  </code><br><code>V</code><code> </code><code>The </code><code>other prohibitions</code><code> listed in Article 5 exhibit similar ambiguities. The ban on AI systems used </code><br><code>for social scoring purposes is limited to those deployed by public authorities (Art. 5(1)(c)). Private </code><br><code>actors (that do not act on behalf of public authorities) are spared without any meaningful </code><br><code>justification. The condition of causing ‚Äúphysical or psychological harm‚Äù necessary for the bans on </code><br><code>manipulative systems (Art. 5(1)(a) and 5(1)(b)) raises the threshold enormously so as to </code><br><code>unacceptably narrow the scope of this provision. Furthermore, Article 5 lacks a ban on AI systems </code><br><code>used for predictive policing that clearly violate fundamental rights, which are only declared highrisk.  </code><br><code>/</code><code> </code><br><code>We recommend that the prohibitions listed in Article 5 are better defined so as to reconcile </code><br><code>them with the ultimate objective‚Äîthe protection of fundamental rights. </code><br><code>8 Guarantee Participation and Enhance Capacity to Protect Workers‚Äô Autonomy </code><br><code>We welcome the Commission‚Äôs decision to include </code><code>AI systems relating to employment, workers </code><br><code>management, and access to self-employment</code><code> in the catalog of high-risk AI systems. Complex </code><br><code>computational systems are increasingly being used to monitor, score, manage, promote, and even fire </code><br><code>employees. These systems have the potential to profoundly influence, alter, and redirect the lives of </code><br><code>people at work and, therefore, impact their life chances in general. In order to protect workers‚Äô rights‚Äî</code><br><code>and, first and foremost, their autonomy‚Äîthere need to be mandatory provisions for transparency and </code><br><code>participation. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665474_3,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665474.pdf,5,3,2665474,attachments/2665474.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>  </code><br><code> </code><br><code>- 3 - </code><br><code>Feedback ‚Äì AI Act </code><br><code> </code><br><code>SPECTARIS e. V.  </code><br><code>Berlin, Germany | 6</code><code>th</code><code> August 2021 </code><br><code> </code><br><code>need to be provided. </code><br><code> </code><br><code>ÔÅÆ</code><code> </code><code>SPECTARIS also considers it necessary to further align key requirements and definitions with </code><br><code>MDR/IVDR.</code><code> This includes aligned requirements on Quality Management Systems (QMS), cybersecurity, and </code><br><code>EUDAMED for vigilance and post-market surveillance, as well as alignment of requirements for Notified Bodies </code><br><code>(e.g. submissions of technical documentation, conformity assessments, CE marking). Only comprehensive </code><br><code>alignment of the two frameworks will provide legal certainty for manufacturers and thus prevent duplicated and </code><br><code>unnecessary administrative procedures, such as doubled conformity assessments or two technical </code><br><code>documentations. This will also help to avoid further capacity issues that Notified Bodies already have to deal </code><br><code>with today. Furthermore, definitions need to be matched with MDR/IVDR terms, since current terms (e.g. </code><br><code>‚Äúprovider‚Äù vs. ‚Äúmanufacturer‚Äù) clearly diverge.</code><code> </code><br><code>ÔÅÆ</code><code> </code><code>Consistent implementation of the AIA across all EU member states is particularly important ‚Äì harmonisation </code><br><code>should be pursued in order to avoid differing national efforts (as in the case of the GDPR) and a fragmented </code><br><code>Single Market. Harmonisation is not only crucial to maintain a high level of AI safety throughout all of Europe, </code><br><code>but also for providing suitable grounds for innovation: SPECTARIS welcomes the idea of regulatory sandboxes. </code><br><code>However, relevant sectoral authorities (e.g., the AI Board and/or MDCG) must leverage the outcomes from </code><br><code>national regulatory sandboxes at the EU level, so that AI systems created and tested in such regulatory </code><br><code>sandboxes could be easily moved to and placed on the market in other member states. </code><br><code>ÔÅÆ</code><code> </code><code>Additionally, the AIA‚Äôs broad definition of an ‚ÄúAI system‚Äù (Article 3, point 1 and Annex I) that this legislation </code><br><code>presents, only complicates the process of distuinguishing between basic medical device software and AI-based </code><br><code>medical device software. For example, in Annex I (b) and (c) ‚Äúknowledge bases‚Äù as well as software including </code><br><code>‚Äústatistical approaches‚Äù or ‚Äúsearch and optimization methods‚Äù are deemed as artificial intelligence systems. This </code><br><code>could have the effect of including medical devices into the scope of the Regulation merely if they embed basic </code><br><code>software functionality that enables users to search for information from a dataset, or rank the results of a query </code><br><code>in terms of their relevance. Hence, most medical device software would be considered high-risk AI systems, </code><br><code>even if devices only include basic software components. In effect, this broad definition of AI systems could lead </code><br><code>to an unnecessarily wide range of medical devices falling under this regulation by falling under the category of </code><br><code>high-risk devices, despite only using software components that are usually not characterised as actual AI </code><br><code>elements. </code><code>SPECTARIS thus recommends to remove statistical approaches, simple databases as well as </code><br><code>search functions from Annex I, or to alternatively offer a more comprehensive and clear-cut depiction </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662463_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662463.pdf,2,2,2662463,attachments/2662463.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>performance optimisation, whilst the ethical challenges may be similar to AI, they should not be unduly </code><br><code>subject to regulation which is targeting a different area.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Duplicative legislative regimes.</code><code> The GDPR already provides individuals with strong rights to the use of </code><br><code>personal data and places obligations of transparency, accountability and ensuring appropriate </code><br><code>security. The Proposal for an AI Regulation should not overlay additional obligations which lead to </code><br><code>notification fatigue, consent fatigue or other obligations which would confuse or compete with existing </code><br><code>legislative requirements.  </code><br><code> </code><br><code>  </code><br><code> </code><br><code> </code><br><code>For additional information, please contact:  </code><br><code> </code><br><code>Nina Elzer </code><br><code>Senior Public Affairs Manager </code><br><code>EACA ‚Äì European Association of Communications Agencies </code><br><code>nina.elzer@eaca.eu</code><code>.  </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665431_3,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665431.pdf,5,3,2665431,attachments/2665431.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>We connect Polish Business and Science with the EU</code><code> </code><br><code> </code><br><code>Page 3 of 4</code><code> </code><br><code> </code><br><code>EU Transparency Register Number: 548212735276-89</code><code> </code><br><code> </code><br><code> </code><br><code>3/ The requirement to confirm the compatibility of algorithms in each case </code><br><code>(conformity assessment) </code><br><code>The proposed requirement to confirm the compliance of the algorithm in case of  </code><br><code>every substantial update of the algorithm does not bear in mind that the algorithms require, </code><br><code>since their fundamental assumptions, regular and systematic updating, ""learning"" and </code><br><code>‚Äútuning‚Äù on the basis of real time data. As a result, an indented and targeted purpose that an </code><br><code>algorithm could be ‚Äòfrozen‚Äô, certified and implemented may never be reached. On the </code><br><code>contrary, the algorithms are constantly evolving. As a result, issuing consent only for a </code><br><code>specific version of the algorithm, without consideration of its future modifications, will result </code><br><code>in the use of an outdated SI algorithm (developed on the basis of historical, not real time </code><br><code>data). This may result in duplicating the algorithmic bias as the algorithm would be trained </code><br><code>on a limited data pool. With this in mind, we acknowledge that reporting standard </code><br><code>deviations would be more effective solution than issuing approvals for release to use. </code><br><code> </code><br><code>4/ Penalties for non-compliance with the regulation </code><br><code>In our opinion, the fines for the non-compliance  of a high-risk Artificial Intelligence system </code><br><code>with requirements under the proposed regulation are too high. Businesses or public </code><br><code>authorities that develop or use AI applications that constitute a high risk for the safety  </code><br><code>or fundamental rights of citizens would have to comply with specific requirements and </code><br><code>obligations. Compliance with these requirements would imply costs amounting  </code><br><code>to approximately EUR 6000 to EUR 7000 for the supply of an average high-risk AI system  </code><br><code>of </code><br><code>around </code><br><code>EUR </code><br><code>170000 </code><br><code>by </code><br><code>2025. </code><br><code>For </code><br><code>AI </code><br><code>users, </code><br><code>there </code><br><code>would </code><br><code>also </code><br><code>be the annual cost for the time spent on ensuring human oversight where this  </code><br><code>is appropriate, depending on the use case. Those have been estimated at approximately  </code><br><code>EUR 5000 to EUR 8000 per year. Verification costs could amount to another EUR 3000  </code><br><code>to EUR 7500 for suppliers of high-risk AI. Businesses or public authorities that develop or use </code><br><code>any AI applications not classified as high risk would only have minimal obligations of </code><br><code>information. However, they could choose to join others and together adopt a code of </code><br><code>conduct to follow suitable requirements and to ensure that their AI systems  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665484_10,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665484.pdf,15,10,2665484,attachments/2665484.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>10 </code><br><code> </code><br><code>Despite persistent reports of mass inaccuracy and intrinsic bias, in both real-world and </code><br><code>theoretical applications, use of emotion recognition and biometric categorization systems has </code><br><code>exploded across multiple sectors, many of them extremely high-impact for data subjects. For </code><br><code>example, companies may employ emotion recognition systems to generate ‚Äúemployability‚Äù </code><br><code>scores for job applicants, analyze the purported impact of advertisements and the emotional </code><br><code>status of customers, and attempt to detect shoplifters.</code><code style=""font-weight: 1000; background-color: #FF0000;"">53</code><code> Some firms have also suggested that </code><br><code>these programs be used by law enforcement, claiming that the AI systems may detect signs of </code><br><code>deception, anger, stress, and anxiety in detained individuals.</code><code style=""font-weight: 1000; background-color: #FF0000;"">54</code><code> These systems all rely on </code><br><code>algorithms based on early research that proposed the existence of universal emotions and a </code><br><code>strong correlation between emotion and facial expression.</code><code style=""font-weight: 1000; background-color: #FF0000;"">55</code><code> However, a 2019 meta-analysis of </code><br><code>the relevant scientific literature revealed that there is actually no reliable evidence that an </code><br><code>individual‚Äôs emotional state can be inferred from their facial movements.</code><code style=""font-weight: 1000; background-color: #FF0000;"">56</code><code> Emotion recognition </code><br><code>technology is unable to ‚Äúconfidently infer happiness from a smile, anger from a scowl, or </code><br><code>sadness from a frown‚Äù because it glosses over cultural and social contexts.</code><code style=""font-weight: 1000; background-color: #FF0000;"">57</code><code>  </code><br><code>Algorithms often fail to capture the complexity of human emotion when used in the real </code><br><code>world.</code><code style=""font-weight: 1000; background-color: #FF0000;"">58</code><code> For instance, data shows that people only scowl approximately 30% of the time when </code><br><code>they‚Äôre angry‚Äîtherefore, if an algorithm views a scowl as a necessary component of anger, it </code><br><code>will be wrong about the subject‚Äôs emotional state about 70% of the time.</code><code style=""font-weight: 1000; background-color: #FF0000;"">59</code><code> Similarly, since </code><br><code>women are often socialized to smile in the workplace in order to avoid negative repercussions </code><br><code>and appear more pleasant, a smile is not a reliable indicator of actual happiness or agreement.</code><code style=""font-weight: 1000; background-color: #FF0000;"">60</code><code> </code><br><code>In addition, emotion recognition systems do not consider other factors such as an individual‚Äôs </code><br><code>body movement, personality, and tone of voice in their perception of emotion, and cannot even </code><br><code>distinguish between an intentional wink or an involuntary blink.</code><code style=""font-weight: 1000; background-color: #FF0000;"">61</code><code> Many software companies </code><br><code> </code><br><code>53</code><code> James Vincent, </code><code>Discover the Stupidity of AI Emotion Recognition with This Little Browser Game</code><code>, The Verge </code><br><code>(Apr. 6, 2021), https://www.theverge.com/2021/4/6/22369698/ai-emotion-recognition-unscientific-emojifyweb-browser-game; </code><code>see also </code><code>Kate Crawford, </code><code>Artificial Intelligence is Misreading Human Emotion</code><code>, The </code><br><code>Atlantic (Apr. 27, 2021), https://www.theatlantic.com/technology/archive/2021/04/artificial-intelligencemisreading-human-emotion/618696/.  </code><br><code>54</code><code> Charlotte Gifford, </code><code>The Problem with Emotion-Detection Technology</code><code>, The New Economy (June 15, 2020), </code><br><code>https://www.theneweconomy.com/technology/the-problem-with-emotion-detection-technology.  </code><br><code>55</code><code> </code><code>Id.</code><code>; Crawford, </code><code>supra </code><code>note 53.  </code><br><code>56</code><code> Lisa Feldman Barret et al., </code><code>Emotional Expressions Reconsidered: Challenges to Inferring Emotion from </code><br><code>Human Facial Movements</code><code>, 20 Ass‚Äôn for Psych. Sci., 1, 46 (2019), available at </code><br><code>https://journals.sagepub.com/doi/pdf/10.1177/1529100619832930. </code><br><code>57</code><code> </code><code>Id.</code><code>; </code><code>see also </code><code>Krys, Kuba et al., </code><code>Be Careful Where You Smile: Culture Shapes Judgments of Intelligence and </code><br><code>Honesty of Smiling Individuals</code><code>, Journal of Nonverbal Behavior Vol. 40, 101-116 (2016), available at </code><br><code>https://doi:10.1007/s10919-015-0226-4; Gifford, </code><code>supra </code><code>note 54. </code><br><code>58</code><code> </code><code>See </code><code>Abeba Birhane, </code><code>The Impossibility of Automating Ambiguity, </code><code>Art. Life Vol. 27(1), 44-61. </code><br><code>59</code><code> James Vincent, </code><code>AI ‚ÄúEmotion Recognition‚Äù Can‚Äôt Be Trusted</code><code>, The Verge (July 25, 2019), </code><br><code>https://www.theverge.com/2019/7/25/8929793/emotion-recognition-analysis-ai-machine-learning-facialexpression-review.  </code><br><code>60</code><code> Cheryl Teh, ‚Äú</code><code>Every Smile You Fake‚Äù ‚Äì an AI Emotion-Recognition System Can Assess How ‚ÄúHappy‚Äù </code><br><code>China‚Äôs Workers are in the Office</code><code>, Insider (June 25, 2021), https://www.insider.com/ai-emotion-recognitionsystem-tracks-how-happy-chinas-workers-are-2021-6.  </code><br><code>61</code><code> Douglas Heaven, </code><code>Why Faces Don‚Äôt Always Tell the Truth About Feelings</code><code>, Nature (Feb. 26, 2020), </code><br><code>https://www.nature.com/articles/d41586-020-00507-5; Vincent, </code><code>supra </code><code>note 59. </code>",POSITIVE
fitz_2635987_4,other,../24212003_requirements_for_artificial_intelligence/attachments/2635987.pdf,5,4,2635987,attachments/2635987.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Common specifications (Article 41) provide no adequate alternative to harmonized </code><br><code>standards </code><br><code>The proposed Article 41 on common specifications exceeds the limits layed out in EU Regulation 1025/2012 on European standardization that establishes a partnership agreement between the Commission and the ESOs.  </code><br><code>Following the principle of proportionality, Europe should continue to rely on voluntary and consensus-based standards that ensure compliance of products with Union legislation and provide </code><br><code>a clear separation between legislation, standardization and conformity assessment, as well as </code><br><code>include all stakeholders. </code><br><code>Harmonized European standards (hEN) according to their definition in Regulation 1025/2012</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> </code><br><code>offer significant added value over common specifications resulting from implementing acts of </code><br><code>the Commission: </code><br><code>‚Ä¢</code><code> They respect European values through </code><br><code>o</code><code> transparency in the process and the standardization work programme; </code><br><code>o</code><code> openness and inclusiveness of all relevant and interested stakeholders representing the widest possible technical expertise as well as consumers‚Äô interests </code><br><code>and offering a right balance of participation among these stakeholders; </code><br><code>o</code><code> taking into account environmental, health and occupational safety aspects; </code><br><code>o</code><code> the consensus-based development of standards with a bottom-up approach. </code><br><code>‚Ä¢</code><code> They offer technological leadership potential through the ESOs strong links to international standardisation at ISO and IEC.  </code><br><code>‚Ä¢</code><code> They respect the World Trade Organization‚Äôs (WTO) Agreement on Technical Barriers to Trade (TBT Agreement). </code><br><code>‚Ä¢</code><code> They are trusted by manufactures and consumers alike, market relevant and fit-forpurpose because the requirements of potential users of the standards are taken into </code><br><code>account in the drafting process. </code><br><code>‚Ä¢</code><code> They ensure the inclusion of small and medium-sized enterprises (SMEs) and of societal stakeholders in the standardization process. </code><br><code>Circumventing harmonised standards through common specifications would lead to a loss of </code><br><code>these benefits and significantly weaken the participation opportunities of SMEs, consumers </code><br><code>and civil society in the development of responsible framework conditions for the use of AI. </code><br><code>Article 3 paragraph 2 of the NLF Decision 768/2008/EC</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> supports the special status of hEN: </code><br><code>‚ÄúWhere Community harmonisation legislation sets out essential requirements, it shall provide for recourse to be had to harmonised standards, adopted in accordance with Directive 98/34/EC, which shall express those requirements in technical terms and which </code><br><code>shall, alone or in conjunction with other harmonised standards, provide for the presumption of conformity with those requirements, while maintaining the possibility of setting the </code><br><code>level of protection by other means.‚Äù </code><br><code>Recommendation: Article 41 must be deleted </code><code>to avoid confusion in the marketplace and </code><br><code>make sure that the EU follows its own legislation, i.e. Regulation 1025/2012 on European </code><br><code>standardization, and the NLF. </code><code> </code><br><code> </code><br><code> </code><br><code>                                                      </code><br><code>4</code><code> According to Regulation 1025/2012 Article 2 paragraph 1c a ‚Äûharmonized standard‚Äú is a ‚Äû a European standard </code><br><code>adopted on the basis of a request made by the Commission for the application of Union harmonisation legislation‚Äù.  </code><br><code>5</code><code> </code><code>Decision on a common framework for the marketing of products</code><code>.</code><code> </code>",POSITIVE
fitz_2665624_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665624.pdf,11,2,2665624,attachments/2665624.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>johner-institut.de  </code><br><code> </code><br><code> </code><code>Johner Institut GmbH </code><br><code>Reichenaustra√üe 1 </code><br><code>78467 Konstanz </code><br><code>T </code><code>+49 7531 94500-20 </code><br><code>info@johner-institut.de </code><br><code>deduction engines, </code><br><code>(symbolic) reasoning and </code><br><code>expert systems </code><br><code>‚Ä¢</code><code> </code><br><code>Statistical approaches, </code><br><code>Bayesian estimation, search </code><br><code>and optimization methods </code><br><code>The AI act requires </code><br><code>cybersecurity, risk </code><br><code>management, post-marketsurveillance, a vigilance system, </code><br><code>technical documentation, a </code><br><code>QM system, etc. And the </code><br><code>regulation addresses explicitly </code><br><code>also medical and in-vitro </code><br><code>devices.</code><code> </code><br><code> </code><br><code>This results in duplication of </code><br><code>requirements. MDR and IVDR </code><br><code>already demand </code><br><code>cybersecurity, risk </code><br><code>management, post-marketsurveillance, a vigilance </code><br><code>system, technical </code><br><code>documentation, a QM system, </code><br><code>etc. Manufacturers will have </code><br><code>to prove compliance with two </code><br><code>regulations.</code><code> </code><br><code>In addition, it should not be </code><br><code>expected of manufacturers to </code><br><code>check for any discrepancies, </code><br><code>redundancies as well as </code><br><code>tightening within the two </code><br><code>regulations.</code><code> </code><br><code>The regulation should either </code><br><code>clarify that there are no </code><br><code>additional requirements to </code><br><code>MDR / IVDR related to risk </code><br><code>management, cybersecurity </code><br><code>usability engineering, vigilance </code><br><code>and post-market surveillance </code><br><code>or it should explicitly list these </code><br><code>additional requirements.</code><code> </code><br><code> </code><br><code>Terminology like ‚Äúserious </code><br><code>incident‚Äù should be </code><br><code>synchronized with other </code><br><code>regulations. The AI act could </code><br><code>for example describe in an </code><br><code>annex, which specific demands </code><br><code>need to be fulfilled in addition </code><br><code>to the MDR/IVDR.</code><code> </code><br><code>The regulation applies </code><br><code>regardless of what the AI is </code><br><code>used for in the medical device.</code><code> </code><br><code>Even an AI with which a lowwear operation of an engine </code><br><code>is to be realized, would fall </code><br><code>within the scope of the AI </code><br><code>regulation. As a consequence, </code><br><code>manufacturers will ponder if </code><br><code>they will use the AI. This </code><br><code>could have a negative impact </code><br><code>on innovation but also on </code><br><code>security and performance. As </code><br><code>manufacturers usually </code><br><code>implement AI to improve the </code><br><code>security, performance and/or </code><br><code>efficiency of their products. </code><br><code>Software as a medical device </code><br><code>(class I* and higher) should not </code><br><code>be considered a high-risk </code><br><code>product per se. Rather, the AI </code><br><code>act should follow its own </code><br><code>reasoning and base this </code><br><code>decision on the risk, specifically </code><br><code>based on the AI and not based </code><br><code>on the medical device which </code><br><code>contains the AI. This decision </code><br><code>should also be based on the </code><br><code>actual risk and not only on the </code><br><code>severity of potential harm.</code><code> </code><br><code>The EU has already made this </code><br><code>mistake with the MDR rule 11 </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665474_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665474.pdf,5,2,2665474,attachments/2665474.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>  </code><br><code> </code><br><code>- 2 - </code><br><code>Feedback ‚Äì AI Act </code><br><code> </code><br><code>SPECTARIS e. V.  </code><br><code>Berlin, Germany | 6</code><code>th</code><code> August 2021 </code><br><code> </code><br><code>1. Introduction: </code><br><code>On April 21</code><code>st</code><code> 2021, the European Commission published a proposal for a Regulation of the European Parliament and </code><br><code>of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act; AIA), as part of a new </code><br><code>comprehensive European framework that aims at regulating products and services with AI elements. The ever-growing </code><br><code>use and development of AI technologies requires regulation in order to further research and innovation, while also </code><br><code>maintaining a high level of safety for EU citizens - SPECTARIS agrees with these objectives and welcomes this </code><br><code>proposed AI Act. </code><br><code>Nevertheless, the planned legislation includes additional regulatory requirements for medical devices by rigidly </code><br><code>classifying most medical devices as high-risk AI devices - regardless of their respectively used AI elements and risk </code><br><code>class as defined in MDR (Regulation (EU) 2017/745 on medical devices) and IVDR (Regulation (EU) 2017/746 on in </code><br><code>vitro diagnostic medical devices). Since applicable medical technology regulations have already set extensive AI </code><br><code>requirements for medical devices and software, this also points to a more general concern: The result of two parallel </code><br><code>regulatory frameworks regarding AI components in medical devices could lead to regulatory overlapping, discrepancies </code><br><code>and additional legal uncertainty for medical technology manufacturers. </code><br><code>Hence, SPECTARIS recommends further alignment of the proposed AI Act with MDR & IVDR as well as additional </code><br><code>conceptual clarifications and changes. Detailed feedback on key issues ‚Äì including MDR/IVDR alignment, definitions, </code><br><code>harmonisation, datasets, human oversight and transitional period - can be found in the following sections. </code><br><code>2. In detail: </code><br><code>ÔÅÆ</code><code> </code><code>Medical devices are already classified by their risk level based on MDR and IVDR ‚Äì ranging from class I (low </code><br><code>risk) to class III (high risk) devices. Medium and high</code><code>-</code><code> </code><code>risk devices (starting from class IIa) have to undergo strict </code><br><code>conformity assessments carried out by Notified Bodies. However, the introduced classification system in the AIA </code><br><code>draft document seems to automatically categorize all medical devices of medium and high risk with AI elements </code><br><code>as high</code><code>-</code><code> </code><code>risk devices under the AIA, regardless of their initial MDR/IVDR risk class and the specific AI </code><br><code>component that is included. In its current form, this AIA classification itself is unclear ‚Äì it not only lacks examples </code><br><code>of how to differentiate between low- and high-risk AI systems, but also clearly differs from and undermines the </code><br><code>well-established MDR/IVDR classification system. In order to prevent legal uncertainity and misunderstandings </code><br><code>for manufacturers, or - at worst - even direct contradictions or duplications,</code><code> SPECTARIS suggests fully </code><br><code>aligning the AI Act with the already existing MDR and IVDR classification system. </code><code>Also, to offer </code><br><code>manufacturers a better understanding of the distinction between low- and high-risk devices, more examples </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662904_5,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662904.pdf,7,5,2662904,attachments/2662904.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>exigences √† leur √©gard qui varient en fonction du niveau de risque pr√©sent√© par les </code><br><code>applications d‚ÄôIA envisag√©es. S‚Äôagissant des garanties accord√©es aux personnes en relation </code><br><code>avec un syst√®me d‚ÄôIA ‚Äì un utilisateur ou une personne vis√©e par un algorithme, le projet de </code><br><code>r√®glement renvoie √† d‚Äôautres textes de l‚ÄôUE, en particulier le RGPD. En d√©finitive, la CNCDH </code><br><code>regrette la faible prise en compte des droits fondamentaux au sein de ce projet de </code><br><code>r√©glementation, et souhaite formuler quelques observations √† ce sujet. </code><br><code>La l√©gitimation de dispositifs attentatoires aux droits fondamentaux  </code><br><code>Le projet de r√®glement prohibe un certain nombre d‚Äôapplication de l‚ÄôIA, jug√©es </code><br><code>contraires aux valeurs de l‚ÄôUE, notamment aux droits fondamentaux (art. 5).  </code><br><code>La CNCDH s‚Äôinterroge toutefois sur la port√©e limit√©e de certaines interdictions, ainsi </code><br><code>que sur l‚Äôabsence de prohibition √† l‚Äô√©gard de plusieurs types d‚Äôutilisation de l‚ÄôIA. </code><br><code>Tout d‚Äôabord, bien que le projet de r√®glement pose une interdiction de principe √† </code><br><code>l‚Äôutilisation de syst√®mes d'identification biom√©trique √† distance ""en temps r√©el"" dans des </code><br><code>espaces accessibles au public, √† des fins de pr√©servation de l‚Äôordre public, il admet un </code><br><code>certain nombre d‚Äôexceptions. La troisi√®me en particulier (iii) est particuli√®rement </code><br><code>pr√©occupante : en autorisant l‚Äôutilisation de la reconnaissance faciale, notamment, √† des fins </code><br><code>de d√©tection, de localisation, d'identification ou de poursuite d'un auteur ou d'un suspect, </code><br><code>pour plus d‚Äôune trentaine d‚Äôinfractions p√©nales</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>, le projet de r√®glement remet en cause </code><br><code>l‚Äôinterdiction de principe affirm√©e de prime abord.  </code><br><code>Cette large utilisation de technologies d‚Äôidentification biom√©trique √† distance </code><br><code>consacr√©e par la Commission europ√©enne s‚Äô√©carte des pr√©conisations formul√©es par </code><br><code>certaines instances internationales. Dans ses lignes directrices sur la reconnaissance </code><br><code>faciale, le Comit√© consultatif de la convention pour la protection des personnes √† l‚Äô√©gard du </code><br><code>traitement automatis√© des donn√©es √† caract√®re personnel (Conseil de l‚ÄôEurope) estime que, </code><br><code>dans les espaces publics, ¬´ </code><code>le recours aux technologies de reconnaissance faciale √† la vol√©e </code><br><code>devrait √™tre soumis √† un d√©bat d√©mocratique comprenant la possibilit√© d‚Äôun moratoire en </code><br><code>attendant une analyse compl√®te du fait de leur nature intrusive pour la vie priv√©e et la dignit√© </code><br><code>des personnes, ajout√© √† un risque d‚Äôimpact pr√©judiciable sur d‚Äôautres droits de l‚ÄôHomme et </code><br><code>libert√©s fondamentales</code><code> ¬ª</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code>.  </code><br><code>Plus r√©cemment, dans un avis conjoint relatif au projet de r√®glement, le Contr√¥leur et </code><br><code>le Comit√© europ√©en de la protection des donn√©es, ont appel√© la Commission √† s‚Äôen tenir √† </code><br><code>une interdiction g√©n√©rale de la reconnaissance faciale</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code>. </code><br><code>En raison de l‚Äôimpact sur les droits et libert√©s fondamentaux d‚Äôune technologie encore </code><br><code>tr√®s incertaine, qui remet tout particuli√®rement en cause l‚Äôanonymat garant de leur exercice </code><br><code>effectif dans l‚Äôespace public, la CNCDH souscrit pleinement √† ces recommandations.  </code><br><code>Le projet de r√®glement ne prohibe pas, par ailleurs, l‚Äôutilisation de l‚ÄôIA aux fins de </code><br><code>d√©tection des √©motions des personnes. Le projet de r√®glement mentionne ce type d‚Äôusage </code><br><code>                                                           </code><br><code>5</code><code> Le R√®glement renvoie √† l‚Äôarticle 2 (2) de la D√©cision-cadre du Conseil du 13 juin 2002 relative au </code><br><code>mandat d'arr√™t europ√©en et aux proc√©dures de remise entre √âtats membres. </code><br><code>6</code><code> Lignes directrices sur la reconnaissance faciale, 28 janvier 2021. </code><br><code>7</code><code> Avis conjoint 5/2021 sur la proposition de r√®glement √©tablissant des r√®gles harmonis√©es sur </code><br><code>l‚Äôintelligence artificielle, 18 juin 2021. </code>",POSITIVE
fitz_2663328_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2663328.pdf,1,1,2663328,attachments/2663328.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>T</code><code>ransparency Register number:  </code><br><code> </code><br><code> </code><br><code> </code><br><code>60974102057-03 </code><br><code>29/07/2021 </code><br><code>Contribution to the Artificial Intelligence Act:</code><code> </code><br><code>It should first be noted that the implementation of software techniques commonly referred to as ""artificial </code><br><code>intelligence"" is already making it possible to market autonomous machines used in various industrial sectors.  </code><br><code>These techniques make it possible, among other things, to improve the reliability of components, anticipate </code><br><code>maintenance operations, increase the lifespan of machines, optimise energy consumption, or to adapt </code><br><code>production to customer demand. </code><br><code>The deployment of these techniques is therefore a tremendous opportunity for European industry and </code><br><code>contributes greatly to the EU's competitiveness. It is imperative to foster innovation in this sector. </code><br><code> </code><br><code>Here are, </code><code>inter alia</code><code>, several points to be taken into account regarding this legislative proposal: </code><br><code>‚Ä¢</code><code> </code><br><code>France Industrie wishes to draw attention to the fact that </code><code>there is no agreed definition of artificial </code><br><code>intelligence</code><code>. Indeed, there is </code><code>no objective criterion to distinguish an ""artificial intelligence"" from a </code><br><code>traditional algorithm</code><code>. However, the definition proposed in the proposed Regulation (Art 3.1) is very broad </code><br><code>and includes software techniques already widely used in industry.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Furthermore, it is important to remember that the designer of a machine must, under the general principles </code><br><code>and the principles of safety integration (paragraph 1.1.2) of Annex I of the Machinery Directive, carry out a </code><br><code>risk assessment and determine which essential requirements apply. </code><code>Artificial intelligence is a technical </code><br><code>means among others, allowing to improve the functioning of a machine and it does not intrinsically create </code><br><code>a new dangerous phenomenon</code><code>. France Industrie therefore considers that AI systems used in already </code><br><code>regulated products and used in the workplace should be excluded from the Regulation. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Moreover, </code><code>some provisions of Chapters 2 and 3 of Title III are</code><code>, according to France Industrie, </code><br><code>disproportionate to the objective of the proposed Regulation</code><code>, in particular Articles 8 to 15. Apart from the </code><br><code>fact that these provisions are already taken into account in the various regulations listed in Annex II, section </code><br><code>A, the fact is that in these regulations </code><code>the designer has the possibility of implementing solutions that are </code><br><code>adapted and proportionate to the general health and safety objective</code><code>. However, the provisions of </code><br><code>Chapters 2 and 3 are presented as firm and absolute: for example, Article 14 provides for human supervision </code><br><code>(presence of an operator) to be systematic in the presence of autonomous machines, where the risk analysis </code><br><code>would allow the designer to determine whether or not such human supervision is required, as well as the </code><br><code>operational conditions for its possible implementation.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>France Industrie is also concerned </code><code>about the evolution of the legal framework of AI as well as its riskbased approach</code><code>. Indeed, the taking into account of feedbacks does not seem to have been considered in </code><br><code>the proposed Regulation. As regards the adaptation of the risk analysis, the uncertainties lie in the fact that </code><br><code>generally, the risk is measured at the level of the system, and not of the component itself, which again </code><br><code>seems to be ignored by the proposed Regulation. Furthermore, there is a risk of creating two distinct </code><br><code>qualification paths making the whole process rather complex. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Compliance</code><code> for companies is also an important point of attention, as the necessary tools for compliance </code><br><code>are lacking: the Regulation is binding, but there is a </code><code>risk of not knowing how to implement it effectively</code><code> for </code><br><code>companies. For example, explicability and transparency are very difficult to implement and demonstrate. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Finally, </code><code>access to source codes is a major point of concern</code><code>. Indeed, France Industrie sees both a risk to </code><br><code>sovereignty and competitiveness in the possibility for the authorities to have access to the source code and </code><br><code>to share it with the European Artificial Intelligence Board. </code><code>Vigilance will be required</code><code> in particular regarding </code><br><code>governance and its openness to non-European players, even when presented as independent experts.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662182_4,other,../24212003_requirements_for_artificial_intelligence/attachments/2662182.pdf,5,4,2662182,attachments/2662182.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>4 </code><br><code> </code><br><code>obtenciones puedan suponer un menoscabo al reconocimiento y a los derechos de los </code><br><code>creadores humanos. </code><br><code> </code><br><code>Queremos se√±alar que en el punto 14 de la Resoluci√≥n del Parlamento Europeo sobre </code><br><code>los derechos de propiedad intelectual para el desarrollo de las tecnolog√≠as relativas a la </code><br><code>inteligencia artificial (2020/2015(INI)) se indica que ‚Äú</code><code>la ¬´autonomizaci√≥n¬ª del proceso de </code><br><code>generaci√≥n de contenidos art√≠sticos puede plantear cuestiones relacionadas con la </code><br><code>titularidad de los DPI sobre esos contenidos; considera, en este sentido, que </code><code>no ser√≠a </code><br><code>adecuado tratar de dotar a las tecnolog√≠as de IA de personalidad jur√≠dica y pone </code><br><code>de relieve el impacto negativo de esta posibilidad en los incentivos para los </code><br><code>creadores humanos</code><code>‚Äù. </code><br><code> </code><br><code>No podemos estar m√°s de acuerdo con esta afirmaci√≥n. No se debe dotar de </code><br><code>personalidad jur√≠dica a los sistemas de IA y, por ello, no pueden ser consideradas obras </code><br><code>protegibles por derechos de autor las creaciones de sistemas aut√≥nomos de IA.  </code><br><code> </code><br><code>S√≥lo las creaciones fruto del intelecto humano deben ser consideradas obras originales </code><br><code>protegibles por derechos de autor, tal y como se desprende de los tratados </code><br><code>internacionales (art. 2, pfo. 5¬∫ del Convenio de Berna en el que se establece que la </code><br><code>protecci√≥n de las obras literarias o art√≠sticas han de constituir una ‚Äú</code><code>creaci√≥n intelectual‚Äù</code><code>) </code><br><code>y derecho de la Uni√≥n (considerando noveno de la Directiva 2001/29/CE y art√≠culo 6 y </code><br><code>considerando 16 de la Directiva 2006/116/CE). Ello, adem√°s, ha sido recalcado por la </code><br><code>jurisprudencia del TJUE, entre otras, en la sentencia del asunto C-5/08 (</code><code>Infopaq</code><code>), que </code><br><code>claramente establece que el derecho de autor s√≥lo aplica a las obras que constituyen </code><br><code>creaciones intelectuales atribuibles a este (apdo. 37). </code><br><code> </code><br><code>Como decimos, atribuir derechos de autor al contenido creado de forma aut√≥noma por </code><br><code>sistemas artificiales podr√≠a llegar a tener un impacto negativo en los creadores. No </code><br><code>obstante, s√≠ consideramos que deben gozar de alg√∫n tipo de protecci√≥n para no </code><br><code>desincentivar la innovaci√≥n en este terreno. Una opci√≥n podr√≠a ser la atribuci√≥n de </code><br><code>derechos vecinos o conexos a favor de la persona f√≠sica o jur√≠dica que edite o divulgue </code><br><code>de forma l√≠cita este contenido. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665586_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665586.pdf,3,1,2665586,attachments/2665586.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>1 </code><br><code> </code><br><code>Philips feedback to EC Proposal for AI Act </code><br><code>Philips welcomes the opportunity to provide feedback on the proposed AI Act. </code><br><code>In healthcare, AI is already a reality. It can help to address healthcare‚Äôs most pressing challenges and </code><br><code>enable people to live healthier lives. The opportunities for AI across the health continuum, from </code><br><code>prevention, through diagnosis and treatment, to home care, lie in its potential to help make sense of data, </code><br><code>turning these data into actionable insights for better health and better care. AI has also proven useful in </code><br><code>the fight against the COVID-19 pandemic. It adapts to patient-specific contexts and can be embedded into </code><br><code>workflows of healthcare providers or people‚Äôs daily environment. At Philips, we believe AI can augment </code><br><code>healthcare providers to deliver high-quality care to patients and increase operational efficiency. </code><br><code>While a robust regulatory framework providing certainty to all stakeholders involved is crucial as new </code><br><code>technologies are being developed and brought to the market, medical device sector has been highly </code><br><code>regulated for a long time already to make sure that safe and performant devices are placed on the EU </code><br><code>market. The Medical Devices Regulation (EU MDR), combined with the GDPR, already offers detailed and </code><br><code>extensive requirements covering various aspects of the proposed AI Act</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>.  </code><br><code>The following elements should be considered in the upcoming legislative process: </code><br><code>-</code><code> </code><br><code>Scope and definitions</code><code>: The proposed AI Act defines high-risk AI system so widely that almost all </code><br><code>medical device software may fall in its scope and be considered a high-risk system. Several other </code><br><code>proposed definitions and requirements are not aligned (e.g. ‚Äòprovider‚Äô, ‚Äòuser‚Äô, ‚Äòimporter‚Äô, ‚Äòputting </code><br><code>into service‚Äô), missing (e.g. definition of ‚Äòrisk‚Äô which is the core concept for conformity assessment) </code><br><code>or conflicting with safety and performance requirements of the EU MDR/IVDR. Different </code><br><code>definitions and conflicting requirements will lead to an incoherent framework, possibly requiring </code><br><code>two sets of technical documentation (instead of one), inefficient information flows with parallel </code><br><code>incident reporting channels and overlapping technical standards. As medical device software has </code><br><code>specific needs, contradictory technical requirements resulting from different definitions or </code><br><code>conflicting requirements between the AI Act and EU MDR must be avoided to ensure legal </code><br><code>certainty and consistency. </code><code>AI-based medical devices should be regulated by the applicable </code><br><code>sectorial legislation, accompanied by detailed guidance developed by the Medical Device </code><br><code>Coordination Group to support new requirements, and address any remaining gaps.</code><code> </code><br><code> </code><br><code>-</code><code> </code><br><code>Requirements for high-risk AI systems</code><code>: Although most of the proposed requirements seem </code><br><code>appropriate to make sure that only safe and performant AI systems are placed on the market, </code><br><code>some of them should be aligned with the state-of-the-art software development practices without </code><br><code>creating barriers for innovative medical software. For instance: </code><br><code> </code><br><code>o</code><code> </code><code>Data and data governance (Article 10):</code><code> The proposed requirement for error-free and </code><br><code>complete data for training, validation and testing is impracticable, especially when testing </code><br><code>in real-world conditions. Datasets are often inaccurate to a certain extent. Accuracy is </code><br><code>about the measurement results being as close as possible to the true value while precision </code><br><code> </code><br><code>1</code><code> For a detailed analysis of AI under EU MDR and examples please consult the paper developed by COCIR : </code><br><code>https://www.cocir.org/media-centre/publications/article/cocir-analysis-on-ai-in-medical-device-legislation-may2021.html</code><code>  </code>",POSITIVE
fitz_2662473_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2662473.pdf,2,2,2662473,attachments/2662473.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Stellungnahme</code><code> </code><br><code>2/2 </code><br><code>Es ist festzustellen, dass der Entwurf eine sehr weite Definition des Begriffs der KI </code><br><code>zugrunde legt. Davon d√ºrften auch etliche in der Vergangenheit entwickelte und </code><br><code>angewandte Algorithmen erfasst sein. Zwar w√§re es f√ºr eine zielf√ºhrende Defini-</code><br><code>tion zu eng, allein auf das Vorliegen ‚Äûautonomer Verhaltensweisen‚Äú abzustellen. </code><br><code>Dies ist die h√∂chste Stufe der Automatisierung, die von den heutigen technischen </code><br><code>Systemen noch nicht erreicht wird. Auf der anderen Seite k√∂nnten die in Anhang I </code><br><code>‚ÄûTechniken und Konzepte der K√ºnstlichen Intelligenz gem√§√ü Artikel 3 Absatz 1‚Äú </code><br><code>unter den Punkten b) und c) genannten Konzepte zu weit gehen. Von einer KI </code><br><code>sollte daher nur dann die Rede sein, wenn mit der Hilfe von Algorithmen, die mit </code><br><code>einer oder mehreren der in Anhang I aufgef√ºhrten Techniken entwickelt wurden, </code><br><code>genuin bisher dem Menschen vorbehaltene Vorhersagen, Empfehlungen oder </code><br><code>Entscheidungen erfolgen.   </code><br><code> </code><br><code>Vor diesem Hintergrund ist es nicht auszuschlie√üen, dass einzelne von den Sozi-</code><br><code>alversicherungstr√§gern eingesetzte Programme/Anwendungen schon heute die </code><br><code>Merkmale von KI erf√ºllen. Dennoch wird im Bereich der eigenen Anwendungen </code><br><code>der Sozialversicherungstr√§ger derzeit kein Einsatz von KI ‚Äûmit hohem Risiko‚Äú ge-</code><br><code>sehen.  </code><br><code> </code><br><code>Dies k√∂nnte sich in Zukunft allerdings √§ndern, falls z. B. maschinelle Lernverfah-</code><br><code>ren f√ºr die Unterst√ºtzung personen- und fallbasierter Entscheidungen zum Einsatz </code><br><code>kommen sollten. In diesem Fall ist die Schaffung von Transparenz, Nachvollzieh-</code><br><code>barkeit, Nichtdiskriminierung und menschlicher Letztverantwortung beim Einsatz </code><br><code>maschineller Unterst√ºtzungsverfahren w√ºnschenswert. Vor diesem Hintergrund </code><br><code>sind √úberwachungs- und Kontrollmechanismen sinnvoll.   </code><br><code> </code><br><code>F√ºr den Einsatz von KI-Anwendungen, die kein hohes Risiko darstellen, k√∂nnen </code><br><code>die Sozialversicherungstr√§ger deren Vertrauensw√ºrdigkeit dadurch st√§rken, dass </code><br><code>sie eigene freiwillige Verhaltenscodizes aufstellen oder sich den Verhaltenscodi-</code><br><code>zes repr√§sentativer Verb√§nde anschlie√üen. </code><br><code> </code><br><code>Im √úbrigen wird auf die Stellungnahme der Deutschen Sozialversicherung zur √∂f-</code><br><code>fentlichen Konsultation der EU-Kommission zum Wei√übuch ‚ÄúZur K√ºnstlichen Intel-</code><br><code>ligenz - ein europ√§isches Konzept f√ºr Exzellenz und Vertrauen‚Äú verwiesen.</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> </code><br><code> </code><br><code>  </code><br><code> </code><br><code> </code><br><code>1</code><code> https://dsv-europa.de/lib/2020-06-11-DSV-Position-Weissbuch-Endfassung.pdf   </code>",POSITIVE
fitz_2665296_5,other,../24212003_requirements_for_artificial_intelligence/attachments/2665296.pdf,9,5,2665296,attachments/2665296.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Data, AI and Robotics (DAIRO) aisbl </code><br><code>Avenue des Arts, 56 </code><code>‚ö´</code><code> 1000 Brussels </code><code>‚ö´</code><code> Belgium </code><br><code>E-Mail: </code><code>info@core.bdva.eu </code><code>‚ö´</code><code> Web: </code><code>www.bdva.eu</code><code>          </code><br><code>Finally, an </code><code>explicit mention of a need for trustworthiness metrics in place</code><code> (e.g. predictive accuracy, </code><br><code>robustness, fairness, interpretability, computational complexity) of machine learning models shall be </code><br><code>added.  </code><br><code> </code><br><code>4. </code><code>Risk-assessment and risk classification</code><code> </code><br><code>BDVA/DAIRO welcomes the risk-based approach that aims to look at the effects of AI rather than trying </code><br><code>to formulate bottom-up technical requirements. Balancing innovation, human rights and strategic </code><br><code>autonomy is a delicate exercise and the current proposal is a big step in the right direction. The proposal </code><br><code>of a Regulation on AI is in line with our concerns about the need for risk-assessment and risk </code><br><code>classifications. The association welcomes the idea of having an Annex (i.e., Annex III to the Regulation), </code><br><code>for listing the risks and apply a set of criteria and risk assessment methodology. It would be very helpful </code><br><code>to include a </code><code>definition</code><code> of the respective levels and applications and the respective risk levels. The AI </code><br><code>risk classification and assessment criteria and metrics could be aligned explicitly with international </code><br><code>standardization</code><code> results. The </code><code>standard risk metrics</code><code> could be used to make Article 9 (Risk </code><br><code>Management system) more specific, especially for article 9.2 (risk assessment process). </code><br><code>Some  issues should be considered: on the one hand, a more </code><code>detailed classification of risk</code><code> could be </code><br><code>necessary for industry to perform self-assessment of risk associated with their products. Risk and levels </code><br><code>cannot be easily generalized over many application of AI. Moreover, risk granularity is coarse (High </code><br><code>Risk, Low Risk etc). On the other hand, </code><code>risk management</code><code> need </code><code>standard metrics</code><code>, as in finance and </code><br><code>other regulated fields. Additionally, the issue of </code><code>bias</code><code> shall be looked at closely. BDVA/DAIRO underlines </code><br><code>that there is still no established practice in research to deal with bias in general, and that a framework </code><br><code>would be welcome. Finally, the current text does not specify the means that the European Commission </code><br><code>intends to deploy to ensure the compliance of high-risk AI developed outside the EU. </code><br><code>In general, regulation would need to </code><code>constantly move with research</code><code> and innovation efforts, in </code><br><code>particular, for the definition of risk-assessment and risk classification patterns and the Regulation would </code><br><code>need to be based on the </code><code>continuously updated standards based on  ongoing research. </code><br><code> </code><br><code>5. </code><code>Balance innovation-regulation and elements supporting innovation</code><code> </code><br><code>BDVA/DAIRO is very much aligned with the idea that the regulation should not hinder </code><br><code>innovation, but rather facilitate investment and innovation</code><code>. For this reason, it supports the specific </code><br><code>objective of the proposed regulatory framework on AI to </code><code>ensure legal certainty to facilitate investment </code><br><code>and innovation in AI </code><code>(page 3 of the Proposal). However, the regulation might create difficulties to </code><br><code>establish balance innovation-regulation on a constant basis. At the moment it might be an administrative </code><br><code>burden especially to universities, research centers and SME. On the other hand, BDVA/DAIRO agrees </code><br><code>that a Regulation can drive innovation. Thus, regulatory actions and innovation shouldn't view these as </code><br><code>antithetical to each other. </code><br><code>The Regulation should come along with strong investments in implementation, definition of European </code><br><code>frameworks to support adoption, skills development programmes and support to SMEs. </code><br><code>BDVA/DAIRO welcomes the proposal of</code><code> Regulatory sandboxes as to establish protected </code><br><code>environments to experiment innovation. </code><code>This is a major advance in European law that will bring the </code><br><code>EU in line with the standards of its international competitors. A </code><code>close monitoring</code><code> to assess whether </code><br><code>such regulatory sandboxes and the way those are implemented in practice, actually facilitate innovation </code><br><code>is needed and strongly recommended. </code><code> </code><code>Moreover, </code><code>the conditions for access to and use of </code><br><code>sandboxes need to be clarified</code><code>. BDVA/DAIRO recommends that for startups the scheme should be </code><br><code>strengthened with a precise definition of eligible actors, the duration of the scheme and, above all, the </code><br><code>list of obligations from which the scheme should exempt them. The Commission should not delegate </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665289_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665289.pdf,4,2,2665289,attachments/2665289.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>standardisation. ETSI understands that the first step in application of conformance rules is to check </code><br><code>if a software (AI) solution falls into the category of High-Risk, and only afterwards to check whether </code><br><code>the technology is one that is described in the broad types listed in Annex 1. Therefore the clarity of </code><br><code>classification of ""high-risk AI systems"" (Annex III) is very important. </code><br><code>ETSI invites the EC to engage in further discussions related to the definitions of AI and AI systems in </code><br><code>order to clearly differentiate i) those AI functions and systems that are considered high-risk and fall </code><br><code>under this regulation; ii) those AI functions and systems requiring recommendations to the industry </code><br><code>to be applied;  and iii) those AI functions and systems requiring no regulation.  </code><br><code>Regulation of High-Risk Systems</code><code> </code><br><code>The allocation of any particular AI application to a risk level (fitting the risk model of the Commission) </code><br><code>is a first approximation, and it may need reviewing on a case by case basis as experience with </code><br><code>deployed AI solutions shows that the AI solution use case is higher or lower risk than expected with </code><br><code>regard to human values/lives. Such reviews must be performed transparently and with some </code><br><code>transition period for changes.  It would greatly aid efficient development of standards and detection </code><br><code>of ""standardisation gaps"" if the EC would establish a collaborative effort to create examples of use </code><br><code>cases that are definitively ""high-risk"". </code><br><code>No ex-ante Conformance requirements on ""early adopter"" AI Solutions</code><code> </code><br><code>ETSI welcomes the assurance of the Commission that solutions entering the market prior to the AI </code><br><code>Regulation coming into force will not be subject to ex-ante regulation and conformance testing. At </code><br><code>the same time, it is understood that when substantial updates of such solutions are made, in the </code><br><code>normal business of adaptive solutions, then the updated solution must be assessed to see whether </code><br><code>it is subject to constraints under the AI Regulation. In such case, the standards and testing and </code><br><code>conformance rules will apply that have in the meantime been established. With its role as an ESO </code><br><code>and its open and inclusive standardisation processes ETSI will have a key role in assisting market </code><br><code>participants as well as regulators here by providing all information about standards under </code><br><code>development in the usual highly transparent manner so that all stakeholders can follow and </code><br><code>participate in the development processes and prepare for adoption of the respective standards.  </code><br><code>Monitoring of some AI Solutions in the market</code><code> </code><br><code>Article 64 ""Access to data and documentation"" specifies that the market surveillance authorities shall </code><br><code>be granted full access to the training, validation and testing datasets used by the provider, including </code><br><code>through application programming interfaces (‚ÄòAPI‚Äô). It is understood by ETSI from remarks made by </code><br><code>the EC that this is not intended as a routine event, but for exceptional cases, and that the ""API"" need </code><br><code>be openly defined but not the same in all cases, i.e. the API does not need to be a single interface for </code><br><code>all the various AI solution on the market.  </code><br><code>ETSI also recommends that documenting the processes utilized to ensure training data quality could </code><br><code>be used by market surveillance authorities instead of accessing training data which would then need </code><br><code>to be stored for an indefinite period of time. For example, collaborative federated solutions for </code><br><code>obtaining and integrating training data across a number of distinct data holders demand a more </code><br><code>flexible approach for quality assurance than accessing the training data itself.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665469_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665469.pdf,3,1,2665469,attachments/2665469.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>V1.0 5. august 2021 KMD  </code><br><code>Side 1 af 3 </code><br><code> </code><br><code> </code><br><code>August 5, 2021 </code><br><code> </code><br><code>Feedback to the proposal for an AI Act by the </code><br><code>European Commission </code><br><code> </code><br><code> </code><br><code>1 KMD‚Äôs position </code><br><code> </code><br><code>1.1</code><code> </code><br><code>General remarks to the proposed regulation </code><br><code>KMD welcomes the proposal for an AI Act by the European Commission. </code><br><code> </code><br><code>KMD finds that regulation of AI will strengthen trust in the solutions and providers of the </code><br><code>technology, and as such it will benefit the tech sector, authorities, and society at large. As AI </code><br><code>has achieved a certain level of maturity and is being implemented in a wide array of software </code><br><code>solutions, the time is ripe for establishing common standards and regulation of the use of AI </code><br><code>across the EU. Clear and common requirements for AI solutions in EU makes the competitive </code><br><code>situation equal across the Union. </code><br><code> </code><br><code>KMD also applauds the idea of creating an EU alternative to the implementation of AI in third </code><br><code>countries which to various extents ignores the need to secure fairness, democratic control, </code><br><code>transparency, and measures to combat bias and discrimination. As part of the proposed AI Act </code><br><code>and the attention following from the proposal, KMD expects a more qualified and informed </code><br><code>public debate about AI. </code><br><code> </code><br><code>KMD, however, finds that there is room for improving the proposal as per our remarks below.  </code><br><code> </code><br><code>1.2</code><code> </code><br><code>KMD‚Äôs concerns and suggestions </code><br><code>KMD develops IT solutions for every aspect of the changing digital needs of modern societies </code><br><code>and organizations. AI regulation will affect a great part of the solutions that KMD ‚Äì as a leading </code><br><code>supplier of govtech and business-critical solutions ‚Äì has already placed and plan to place on </code><br><code>the market.  </code><br><code> </code><br><code>As societies grow and evolve, digital solutions play an increasingly vital role in fostering </code><br><code>efficiency, growth, and welfare. For half a century, KMD‚Äôs contributions to Danish society has </code><br><code>helped shape one of the most modern and progressive public sectors in the world. The high </code><br><code>degree of trust that Danes have in digital and data driven solutions today stems from decades </code><br><code>of developing solutions that are reliable, secure, transparent, unbiased, and explainable. KMD </code><br><code>aims to continue building on this foundation in the future, and AI is a central component of this </code><br><code>vision. </code><br><code> </code><br><code>An overall concern from KMD is finding the right balance of the regulation, so that innovation </code><br><code>and development will not be impaired, and the requirements for testing, documentation etc. </code><br><code>will not stand in the way of using and inventing AI solutions that benefit society and citizens.  </code><br><code> </code><br><code>1.3</code><code> </code><br><code>AI is weakly defined </code><br><code>The object of the regulation is AI, but without a more specific definition, even widespread </code><br><code>statistics software and simple search engines (cf. Annex 1) may fall under the term </code><code>AI</code><code>. This </code><br><code>could potentially mean that systems using simple keyword searches would be classified as </code><br><code>‚Äúhigh risk‚Äù, and that seems to be beside the target. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665519_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665519.pdf,3,1,2665519,attachments/2665519.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Rue Guimard 15 (4th Floor) - 1040 Brussels - Belgium </code><br><code>Tel. : +32 (0)2 732 72 02 - Fax : +32 (0)2 732 73 44 - E-mail : secretariat@cpme.eu - Web : www.cpme.eu </code><br><code>Company registration number : 0462509658 - Transparency register number : 9276943405-41 </code><br><code> </code><br><code> </code><br><code>CPME Feedback on Commission Proposal for a  </code><br><code>Regulation on Artificial Intelligence </code><br><code> </code><br><code> </code><br><code>The Standing Committee of European Doctors (CPME) represents national medical associations across </code><br><code>Europe. We are committed to contributing the medical profession‚Äôs point of view to EU and European </code><br><code>policy-making through pro-active cooperation on a wide range of health and healthcare related issues. </code><br><code> </code><br><code>General Comments  </code><br><code>CPME commends the European Commission‚Äôs for developing a ground-breaking legislation laying </code><br><code>down harmonised rules on artificial intelligence (Artificial Intelligence Act) (‚Äòthe Proposal‚Äô).</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> </code><br><code> </code><br><code>European doctors welcome the overall risk-based approach for artificial intelligence (AI), the creation </code><br><code>of the </code><code>European Artificial Board</code><code>, the development of the </code><code>EU database for high-risk AI</code><code> and the proposed </code><br><code>risk management system</code><code>.  </code><br><code> </code><br><code>European doctors also welcome the proposed definition of the AI system and the possibility to update </code><br><code>the list of AI techniques and approaches in accordance with market and technological developments. </code><br><code>In healthcare, the proposed definition will ensure that where an AI system is a safety component of a </code><br><code>medical device, or is by itself a medical device (software), subject to third-party conformity assessment </code><br><code>under the medical devices framework, then the AI system is considered of high-risk for the purpose of </code><br><code>the AI proposal. This high-risk classification is needed to guarantee patient utmost safety. To minimise </code><br><code>additional burdens to providers, CPME further appreciates that the AI systems requirements set out in </code><br><code>the AI proposal will be examined as part of the existing third-party conformity assessment procedures </code><br><code>under the relevant medical device framework, ensuring alignment between both legal regimes.  </code><br><code> </code><br><code>Detailed Comments </code><br><code> </code><br><code>1.</code><code> </code><code>Stand-alone high-risk AI listed in Annex III </code><br><code>CPME notes that the Annex III list of high-risk AI systems referred to in Article 6(2) should still include </code><br><code>the use of AI i) for determining insurance premium, ii) for assessing medical treatments and iii) for </code><br><code>health research. CPME advises that this list should be regularly updated in accordance with market and </code><br><code>technological developments. </code><br><code> </code><br><code>2.</code><code> </code><code>Data and data governance - Article 10(6) </code><br><code>CPME recommends that it should be identified as an </code><code>appropriate data governance and management</code><code> </code><br><code>practice the need to consult regularly, or conduct audits, by an AI external auditor. These audits by </code><br><code>external auditors should be harmonized and standardized internationally or as minimum within the </code><br><code>EU.  The EU AI Board could be considered to serve as audit standardization and harmonization </code><br><code> </code><br><code>1</code><code> </code><code>COM(2021) 206 final</code><code>. </code>",POSITIVE
fitz_2662463_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662463.pdf,2,1,2662463,attachments/2662463.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>EACA Comment on the European Commission‚Äôs Proposal for a</code><code> </code><br><code>Regulation laying down harmonised rules on artificial intelligence</code><code>  </code><br><code>COM (2021) 206 final </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>14/07/2021 </code><br><code> </code><br><code>The European Association of Communications Agencies (EACA) represents more than 2,500 communications </code><br><code>agencies and agency associations from nearly 30 European countries that directly employ more than 120,000 </code><br><code>people. EACA members include advertising, media, digital, branding and PR agencies.  </code><br><code> </code><br><code>On 21 May 2021, the European Commission presented a proposal for a Regulation of the European Parliament </code><br><code>and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and </code><br><code>amending certain legislative acts of the EU (‚ÄúProposal for an AI Regulation‚Äù). </code><br><code> </code><br><code>Communication agencies are mostly using AI in the context of engagement and communication; namely </code><br><code>chatbots or virtual assistants, programmes to optimise advertising campaigns, detecting and addressing ad </code><br><code>fraud or bot detection, and to generate better performance insights. It is also used but to a lesser extent, for </code><br><code>automated content creation, language translation and audience discovery.  </code><br><code> </code><br><code>We support the objectives of the Proposal for an AI Regulation, namely to proportionately address the risks </code><br><code>associated with certain uses of AI and to ensure legal certainty to facilitate investment and innovation in the </code><br><code>sector. However, we believe that some of the concepts used in the Proposal for an AI Regulation remain rather </code><br><code>vague and would merit further clarification in order for providers to better understand what is expected from </code><br><code>them and to ensure legal certainty.  </code><br><code> </code><br><code>We understand that certain requirements of Proposal for an AI Regulation apply to providers of high-risk AI </code><br><code>systems and that they will need to subject their AI systems to conformity assessments that demonstrate that the </code><br><code>system complies with certain requirements. We believe that these requirements need to be better defined.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>High quality of data sets feeding the system</code><code>: We believe that it is in the nature of AI to learn from the </code><br><code>data that is being fed to it and to improve through iterative loops. It is not always possible to feed the </code><br><code>system with high quality data, e.g. if the AI system is meant to recognise patterns from huge amounts of </code><br><code>data.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>High level of robustness and accuracy</code><code>: These terms require a definition to be actionable. Will there be a </code><br><code>threshold defined for these? It should also be noted that accuracy is not confined to a single measure </code><br><code>but can be distinct across different sub-groups in the population, which is the major critique against </code><br><code>machine visioning algorithms.   </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Need to consider implications of different errors</code><code>: The question is how to balance between false positives </code><br><code>and false negatives. In crime prediction, high false negative means that AI failed to identify people </code><br><code>who could commit crimes but high false positive means that AI will infringe human rights. An effective AI </code><br><code>programme may need to balance the trade-off between false positive and false negative rather than </code><br><code>prioritise one over another.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Need to better define and distinguish AI from non-regulated techniques of machine learning and </code><br><code>predictive modelling:</code><code> For example, techniques such as the creation of lookalike audiences and </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665416_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665416.pdf,9,2,2665416,attachments/2665416.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>2</code><br><code>world‚Äôs largest AI developers, NEC is likely to be significantly impacted by the Draft </code><br><code>Regulation. </code><br><code>2.4</code><br><code>As a final preliminary remark, it should be noted that, whilst NEC develops a range of AI </code><br><code>technologies, it is generally the customer / user who determines the domain or context </code><br><code>in which the AI system is deployed. The customer / user may also tweak or make final </code><br><code>configurations to the AI system to determine, ultimately, how it will operate when it is </code><br><code>deployed (for example, the customer might train the AI system on specialised data sets). </code><br><code>This point is relevant to some of NEC‚Äôs feedback comments below. </code><br><code>2.5</code><br><code>In broad terms, whilst NEC appreciates that the Draft Regulation builds upon the EU‚Äôs </code><br><code>New Legislative Framework regime (relying on similar concepts and terms used in that </code><br><code>regime), NEC is concerned that this regime is not, in all circumstances, appropriate for </code><br><code>AI systems and that a more flexible legal regime is required. </code><br><code>3.</code><br><code>General comments on the Draft Regulation </code><br><code>NEC supports the purpose behind the Draft Regulation</code><br><code>3.1</code><br><code>NEC fully supports the Commission‚Äôs purpose and intention behind the Draft Regulation. </code><br><code>In particular, NEC endorses the Commission‚Äôs comment in paragraph 1.1 of the </code><br><code>Explanatory Memorandum to the Draft Regulation (echoed in Recital (5)) that ‚Äú</code><code>Rules for </code><br><code>AI available in the Union market or otherwise affecting people in the Union should‚Ä¶ be </code><br><code>human centric, so that people can trust that the technology is used in a way that is safe </code><br><code>and compliant with the law, including the respect of fundamental rights</code><code>‚Äù.</code><br><code>3.2</code><br><code>NEC shares the same vision in its role as a leading developer of AI technologies. As </code><br><code>noted above, NEC‚Äôs work in this area (including its AI research and development work) </code><br><code>seeks to complement human capabilities and improve social welfare. NEC firmly </code><br><code>believes in the protection of human rights in the context of AI technologies; a philosophy </code><br><code>which is enshrined in the</code><code> NEC Group AI and Human Rights Principles</code><code>.</code><br><code>3.3</code><br><code>NEC therefore welcomes the Commission‚Äôs proposal for a legal framework governing </code><br><code>the development and use of AI, particularly so as to ensure a high level of protection for </code><br><code>the public. In its comments below, NEC hopes to provide useful feedback on various </code><br><code>aspects of the Draft Regulation which, in NEC‚Äôs view, could be improved so as to meet </code><br><code>the Commission‚Äôs aims in proposing this Draft Regulation. </code><br><code>Striking the right balance between protection, innovation and competition</code><br><code>3.4</code><br><code>NEC notes that, in proposing the Draft Regulation, the Commission (rightly) seeks to </code><br><code>‚Äú</code><code>foster the development, use and uptake of artificial intelligence in the internal market</code><code>‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><br><code>and to propose a legal framework ‚Äú</code><code>that is limited to the minimum necessary requirements </code><br><code>to address the risks and problems linked to AI, without unduly constraining or hindering </code><br><code>3</code><code> Recital (5).</code>",POSITIVE
fitz_2665524_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2665524.pdf,3,1,2665524,attachments/2665524.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>  </code><br><code> R√©glementation sur l‚ÄôIntelligence Artificielle et Justice </code><code> </code><br><code> </code><code>  </code><br><code>Avec l‚Äôappui scientifique de Monsieur Bruno Deffains </code><code>  </code><br><code>Professeur √† l‚ÄôUniversit√© Paris II </code><code>Panth√©on Assas</code><code> </code><br><code> </code><code>  </code><br><code> </code><code> </code><code> </code><code>  </code><br><code>Note destin√©e √† la Commission Europ√©enne </code><code>  </code><br><code> </code><code>  </code><br><code> </code><code>    </code><br><code>Avec la proposition de r√®glement sur l‚Äôintelligence artificielle (IA) pr√©sent√©e le 21 avril 2021, </code><br><code>la Commission europ√©enne r√©affirme l‚Äôimportance strat√©gique de l‚ÄôIA pour l'Europe et la </code><br><code>n√©cessit√© d‚Äôencadrer son usage dans les diff√©rents secteurs d‚Äôapplication.  Nous tenons √† </code><br><code>saluer cette initiative unique dans le monde, qui place l‚Äôintelligence artificielle au centre des </code><br><code>r√©flexions juridiques, politiques et soci√©tales, desquelles elle est d√©sormais indissociable. En </code><br><code>investissant le terrain r√®glementaire, l‚ÄôUnion Europ√©enne entend se positionner en chef de </code><br><code>file, comme elle l‚Äôa d√©j√† fait avec le RGPD et c‚Äôest une excellente nouvelle.   </code><br><code> </code><br><code>Comme avec le RGPD, l‚Äôobjectif est d‚Äôinstaurer un climat de confiance tout en permettant </code><br><code>l‚Äôinnovation. L‚Äôapproche par les risques est √©quilibr√©e (I) pour permettre un juste </code><br><code>d√©ploiement des syst√®mes d‚ÄôIA en Europe. N√©anmoins, en raison de ses enjeux sp√©cifiques et </code><br><code>d√©licats, nous appelons √† traiter le secteur de la Justice comme un secteur hautement sensible </code><br><code>pour nos concitoyens (II). Nous regrettons que le texte soit trop restrictif et ne comprenne </code><br><code>pas suffisamment les enjeux de la justice pr√©dictive avec les garde-fous n√©cessaires au bon </code><br><code>respect des droits fondamentaux des individus. </code><br><code> </code><br><code>1.</code><code> </code><code>L‚Äôapproche par les risques : une approche √©quilibr√©e  </code><br><code> </code><br><code>L‚Äôapproche de la Commission europ√©enne se fonde ainsi avant tout sur une identification et un </code><br><code>encadrement des risques selon les domaines d‚Äôapplications (publics ou priv√©s) concern√©s. La </code><br><code>proposition de la Commission europ√©enne classifie les syst√®mes d‚ÄôIA en quatre groupes : a) </code><br><code>ceux cr√©ant un risque inacceptable (art.5), b) ceux √† haut risque (art.6 et s.) et c) ceux avec un </code><br><code>risque faible (art. 52), devant r√©pondre √† des exigences de transparence (comme les chatbots, o√π </code><br><code>l‚Äôon devra savoir que l‚Äôon se trouve face √† une ¬´ IA ¬ª, ou les deepfakes) ou d) minimum (p.13). </code><br><code>Les risques sont ainsi consid√©r√©s comme inacceptables dans le champ de la s√©curit√©, des moyens </code><br><code>de subsistance et des droits des personnes. Les risques sont per√ßus comme √©lev√©s dans les </code><br><code>domaines touchant les infrastructures critiques (√©nergie, transports‚Ä¶), l‚Äô√©ducation, la formation </code><br><code>professionnelle, l‚Äôemploi, les ressources humaines, les publics essentiels √† l‚Äôimage du maintien </code><br><code>de l‚Äôordre, de la justice ou des processus de d√©cision d√©mocratiques. Ainsi, par exemple, les </code>",NO_FOOTNOTES_ON_PAGE
fitz_2635975_2,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2635975.pdf,9,2,2635975,attachments/2635975.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>2 </code><br><code>In principle, statistical methods could involve techniques as simple as a linear fit (e.g. a linear </code><br><code>regression). If the definition of AI intends to include any statistical approach, then this would </code><br><code>effectively be a regulation on the use of any form of statistics, or algorithmic decision making, not </code><br><code>just on what is today technically referred to as Artificial Intelligence. If this definition does not </code><br><code>intend to include simpler statistical approaches, then, the boundary of what counts as an AI </code><br><code>system would need to be further clarified.</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> However, the Commission is showing here the political </code><br><code>will to be technically neutral and not to adopt legislation that is too dependent on technology which </code><br><code>could be quickly outdated. </code><br><code> </code><br><code>It should be added that the AI system thus defined will only come within the scope of the regulation </code><br><code>if it has the effect of generating results, such as content, predictions, recommendations or even </code><br><code>decisions influencing environments with which they interact. </code><br><code> </code><br><code>The second definition of the proposal focuses on when AI must be regulated. Here the focus is </code><br><code>on high-risk systems. These are applications of artificial intelligence with the potential to have a </code><br><code>large societal impact. These applications include the use of AI for biometrics, AI systems for </code><br><code>critical infrastructure (e.g., transportation), or systems involved in people‚Äôs access to organization </code><br><code>and benefits. The latter includes multiple forms of scoring, such as university admission systems, </code><br><code>credit scoring, human resource applications, educational and vocational training systems, </code><br><code>assignment of public benefits, and administration of justice, among others.  </code><br><code> </code><br><code>Beyond the definition of AI and that of high-risk systems, the scope of the regulation is limited by </code><br><code>other considerations. For example, military applications are explicitly excluded, which is justified </code><br><code>by the fact that the military domain is not within the competence of the European Union. </code><br><code>Governments could therefore adopt regulations to allow, for example, the use of AI technology </code><br><code>(eg facial recognition) for national defense purposes. </code><br><code> </code><br><code>The regulations nevertheless prohibit certain practices, such as rating citizens. It also prohibits </code><br><code>the use of ""real-time"" remote biometric identification systems in spaces accessible to the public </code><br><code> </code><br><code>inference and deductive engines, (symbolic) reasoning and expert systems; (c) Statistical approaches, </code><br><code>Bayesian estimation, search and optimization methods.‚Äù </code><br><code>4</code><code> A more comprehensive, albeit verbose definition, was provided in a 2019 document by the High Level </code><br><code>Expert Group on Artificial Intelligence. </code><code>https://digital-strategy.ec.europa.eu/en/library/definition-artificialintelligence-main-capabilities-and-scientific-disciplines</code><code> and by a JRC </code><br><code>https://publications.jrc.ec.europa.eu/repository/handle/JRC118163</code><code>  </code>",POSITIVE
fitz_2665634_13,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665634.pdf,13,13,2665634,attachments/2665634.pdf#page=13,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>AI Index: POL 30/4567/2021 </code><br><code> </code><br><code> 13/13 </code><br><code> </code><br><code>Securing justice and redress both for individuals directly affected and in order to </code><br><code>protect the rights of society as a whole are vital elements of human rights.</code><code style=""font-weight: 1000; background-color: #FF0000;"">29</code><code> The </code><br><code>UN Special Rapporteur on Freedom of Expression has highlighted how AI systems </code><br><code>often interfere with the right to remedy.</code><code style=""font-weight: 1000; background-color: #FF0000;"">30</code><code> There is an inherent challenge around </code><br><code>informing affected individuals, as ‚Äúindividuals are not aware of the scope, extent </code><br><code>or even existence of algorithmic systems that are affecting their rights‚Äù. This </code><br><code>opacity is exacerbated because algorithms are constantly adapting and changing, </code><br><code>such that even the designers of the system may not or poorly be able to explain </code><br><code>how they reached their outcomes.</code><code style=""font-weight: 1000; background-color: #FF0000;"">31</code><code> The onus is on governments to make </code><br><code>algorithmic systems visible, allow outputs or impacts to be queried and appealed, </code><br><code>and create accessible and practical routes for remedy and redress when human </code><br><code>rights are negatively impacted.</code><code style=""font-weight: 1000; background-color: #FF0000;"">32</code><code> </code><br><code>Amnesty International recommends including in the AIA a complaints and redress </code><br><code>mechanism for individuals that have suffered human rights harm. The mechanism </code><br><code>must 1) facilitate equal and effective access to justice; 2) adequate, effective, </code><br><code>and prompt reparation for harm suffered and 3) access to relevant information </code><br><code>concerning violations and reparation mechanisms.</code><code style=""font-weight: 1000; background-color: #FF0000;"">33</code><code> </code><br><code> </code><br><code>Convention for the Protection of Human Rights and Freedoms; Article 47 of the Charter of Fundamental Rights of the </code><br><code>European Union. </code><br><code>29</code><code> Right to remedy is also discussed in Amnesty International, Injustice Incorporated: Corporate Abuses and the </code><br><code>Human Right to Remedy, 2014. </code><br><code>30</code><code> David Kaye, 2018, para 40. </code><br><code>31</code><code> AI Now Institute, Annual Report 2017, p. 30, </code><code>https://ainowinstitute.org/AI_Now_2017_Report.pdf</code><code>  </code><br><code>32</code><code> Amnesty International Submission to the European Commission‚Äôs Consultation on Artificial Intelligence. </code><br><code>33</code><code> Principal 11 of the UN Basic Principles and Guidelines on the Right to a Remedy and Reparation for Victims of </code><br><code>Gross Violations of International Human Rights Law and Serious Violations of International Humanitarian Law. </code>",POSITIVE
fitz_2665491_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665491.pdf,9,1,2665491,attachments/2665491.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>KU Leuven Centre for IT and IP Law‚Äôs</code><br><code>Comments</code><br><code>to</code><br><code>the</code><br><code>proposed Artificial</code><br><code>Intelligence Act</code><br><code>Authors: Koen Vranckaert, Lydia Belkadi, No√©mie Krack, Emine √ñzge Yildirim, Jan Czarnocki,</code><br><code>Katherine Quezada-Tavarez, Jenny Bergholm.</code><br><code>Introduction</code><br><code>First, we offer our congratulations to the Commission for this great piece of legislation,</code><br><code>consisting of a solid combination of general principles and very concrete answers to the many</code><br><code>questions raised in recent years regarding AI. It is a very rich document, offering the</code><br><code>long-awaited standards and clarity about AI applications and uses, efforts and results that are</code><br><code>welcomed enormously. Nevertheless, there are some reservations and areas of improvement</code><br><code>that need to be highlighted, as listed below.</code><br><code>On the scope of ‚Äòartificial intelligence system‚Äô under the proposed AI Act (Article 3(1)) Koen Vranckaert</code><br><code>We note that the Commission has departed from prior, open conceptions of ‚Äòartificial</code><br><code>intelligence‚Äô, as had been proposed by its Communication for AI, as well as the reports of the AI</code><br><code>High-Level Expert Group. Rather, the Commission has opted for a ‚Äòclosed list‚Äô of techniques that</code><br><code>constitute ‚Äòartificial intelligence‚Äô for the purpose of regulation. The reference to techniques match</code><br><code>the requirement of legal certainty and ease of enforceability. However, this approach may also</code><br><code>invite AI developers to attempt ‚Äòdesigning around‚Äô the proposed AI Act (insofar as that is</code><br><code>possible), developing new techniques to not have to comply with the proposed AI Act. We</code><br><code>understand that the Commission grants itself the power to add new AI techniques to the closed</code><br><code>list as listed under Article 4. However, we believe that the option should be considered to keep</code><br><code>the list under Annex I, but as an exemplative rather than an exhaustive listing of techniques</code><br><code>covered by the notion of ‚Äòartificial intelligence‚Äô, in order to ensure resilience of the proposed AI</code><br><code>Act‚Äôs provisions in the face of rapid technological innovation.</code><br><code>1</code>",NO_FOOTNOTES_ON_PAGE
fitz_2663375_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663375.pdf,2,2,2663375,attachments/2663375.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>training purposes, for instance by fostering the set-up of a large EU test database, with the </code><br><code>adequate control / encryption systems, that could be used by the respective EU players to train </code><br><code>their AI algorithms </code><br><code> </code><br><code>AI definitions creating an uneven playing field  </code><br><code> </code><br><code>While the definition of AI is necessarily wide in the existing draft, trying to define an emerging </code><br><code>technology is challenging. Any definition could be rendered obsolete as new techniques emerge </code><br><code>and are implemented.  </code><br><code> </code><br><code>This would create a significant distortion of competition between developers of AI applications </code><br><code>as per definition of the draft, and those developers leveraging other, more modern techniques, </code><br><code>not directly addressed by the regulation for the same, potentially high-risk applications. By </code><br><code>specifying what AI is and what it is not, the proposals threaten the EU goal of a wide-reaching, </code><br><code>technology agnostic regulation. </code><br><code> </code><br><code> </code><br><code>Building an advanced ecosystem  </code><br><code> </code><br><code>SIA supports the provision of legal, trusted identity for all and the development of inclusive </code><br><code>digital services. AI-enabled biometrics is already a key and growing part of this environment ‚Äì </code><br><code>providing considerable benefits to citizens, commercial organisations (such as airlines and </code><br><code>transport providers) and to law enforcement. </code><br><code> </code><br><code>To facilitate greater leadership in this space, SIA strongly believes that Europe must seize the </code><br><code>opportunity to structure an advanced ecosystem for biometric performance assessment, akin to </code><br><code>the National Institute for Standards and Technology, under the US Department of Commerce. </code><br><code>Building this capability requires considerable scientific and technical expertise which EU can </code><br><code>fulfill through its vibrant academic and not-for-profit ecosystem.  </code><br><code> </code><br><code> </code><br><code>As an expert and globally recognised not-for-profit organisation, SIA is ready to work with the </code><br><code>EU ‚Äì offering the combined support of its members in evolving proposals to ensure a fit-forpurpose and future proofed approach that will indeed drive growth of this important sector </code><br><code>across the EU. </code><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665582_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665582.pdf,3,2,2665582,attachments/2665582.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>2 </code><br><code> </code><br><code> </code><br><code>This project has received funding from the European Union‚Äôs Horizon 2020 Research and Innovation Programme Under Grant Agreement no. 786641 </code><br><code> </code><br><code> </code><br><code>SHERPA (Shaping the ethical dimensions of smart information systems (SIS) ‚Äì a European </code><br><code>perspective) is a project that focuses on ethical and human rights aspects of smart information </code><br><code>systems (artificial intelligence and big data analytics).  </code><br><code> </code><br><code>Regulatory governance systems are a key part of the SHERPA Final Recommendations,</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> which </code><br><code>include a call for creating an EU regulatory framework</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code> and establishing an EU Agency for AI.</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> </code><br><code>SHERPA believes a robust, mandatory legal framework at the EU level is needed to ensure that </code><br><code>ethical issues and human rights concerns related to AI are adequately addressed. There are many </code><br><code>specific elements of the SHERPA recommendations that appear in the proposed text of the </code><br><code>Regulation, like red lines for some AI applications, mandatory requirements for high-risk AI </code><br><code>systems, and the creation of a centralised body. However, the draft text does not do enough to </code><br><code>protect fundamental rights, often lacks conceptual clarity, and leaves many questions </code><br><code>unanswered </code><br><code> </code><br><code>For more information, please visit: </code><code>https://www.project-sherpa.eu</code><code> </code><br><code> </code><br><code>Risk-based approach</code><code> </code><br><code>The proposed Regulation adopts a four-tiered risk-based approach, where AI systems are subject </code><br><code>to different rules depending on the level of risk. While one purpose of the regulatory framework </code><br><code>is to guarantee safety and fundamental rights of EU citizens, the risk-based approach adopted by </code><br><code>the Commission may not be sufficient.  There is no reference to the EU Agency for Fundamental </code><br><code>Rights, nor are there provisions on complaint and redress mechanisms available to those whose </code><br><code>rights are violated by AI systems. Furthermore, the proposed regulation has a somewhat binary </code><br><code>approach, failing to adequately take into account impacts across the spectrum of risk. Most </code><br><code>mandatory requirements apply only to high-risk systems; by comparison, low-risk AI systems are </code><br><code>only subject to transparency requirements and minimal-risk AI systems have no requirements. </code><br><code>Definition of AI</code><code> </code><br><code>SHERPA recommended that AI be clearly defined in each use context with regard to relevant </code><br><code>issues. While it is a challenge to precisely define AI, definitions used in the proposed Regulation </code><br><code>are often overly broad and too open to interpretation. Additionally, despite attempts to be </code><br><code>‚Äútechnology neutral and as future proof as possible‚Äù, the proposed definition of AI is linked to </code><br><code>‚Äòsoftware‚Äô, leaving out potential future developments of AI. The proposal takes into account the </code><br><code>difficulty of defining AI by moving the definition into an appendix which is subject to review and </code><br><code>revision. While this is reasonable in light of the problematic nature of the term AI, it does add to </code><br><code>uncertainty about the future scope of the Regulation.  </code><br><code>Red lines</code><code> </code><br><code>Under the proposal‚Äôs risk-based approach, AI systems that pose the highest level of risk to </code><br><code>fundamental rights and safety are prohibited. The short list of banned AI systems ‚Äì only four </code><br><code>categories ‚Äì includes social scoring and AI that subliminally manipulates human behaviour in a </code><br><code>harmful way. While SHERPA welcomes the explicit inclusion of red lines in the regulatory </code><br><code>framework, the short list is incomplete and has many loopholes.  For example, use of remote, </code><br><code> </code><br><code>1</code><code> https://www.project-sherpa.eu/recommendations/ </code><br><code>2</code><code> https://www.project-sherpa.eu/regulatory-framework/ </code><br><code>3</code><code> https://www.project-sherpa.eu/european-agency-for-ai/</code><code> </code>",POSITIVE
fitz_2665648_6,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665648.pdf,8,6,2665648,attachments/2665648.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>3. the manipulative consequences of reinforcement learning (RL) algorithms in social media,</code><br><code>which learn to send sequences of content items that modify a person‚Äôs psychological state and</code><br><code>tendencies so that they become more reliable consumers of certain types of (often extreme)</code><br><code>content.</code><br><code>In the future, policymakers may consider prohibiting the use of reinforcement learning in</code><br><code>public-facing social media algorithms that interact with natural persons sequentially by recommending</code><br><code>or communicating content in order to maximize an internal metric related to clickthrough or</code><br><code>engagement, when health, safety, and well-being are not suÔøΩciently taken into account.</code><br><code>2. Section 3.5 of the Explanatory Memorandum: Include the protection of</code><br><code>mental integrity in the Explanatory Memorandum</code><br><code>Article 3 of the European Union Charter of Fundamental Rights, which deals with the right to</code><br><code>protection of</code><code> mental integrity</code><code>, among other topics, should be mentioned prominently in Section 3.5</code><br><code>of the Explanatory Memorandum</code><code>. We strongly recommend Article 3 of the European Union</code><br><code>Charter of Fundamental Rights and its mention of mental integrity in the Explanatory</code><br><code>Memorandum.</code><br><code>3. Article 53 (AI regulatory sandboxes): Recognize the limit of sandboxes</code><br><code>Unless there are real humans inside the sandbox, it is hard to see how this could reveal (say)</code><br><code>potential psychological harms.</code><code> We suggest that Article 13 recognizes the limits of sandboxes for</code><br><code>unknown risks and takes a prudent approach in dealing with psychological harms.</code><br><code>III.</code><br><code>Clarify key components of the regulation</code><br><code>1. Article 3 (1) DeÔøΩnition of AI: Delete the mention of ‚Äúa given set of</code><br><code>human-deÔøΩned objectives‚Äù</code><br><code>In the deÔøΩnition of AI, the mention of ‚Äúfor a given set of human-deÔøΩned objectives‚Äù seems to</code><br><code>require the objectives to be explicitly present in the AI system or for the system to accept objectives as</code><br><code>input. That leaves out systems with implicit objectives. For example, in a self-driving car, the objective</code><br><code>to avoid collisions with pedestrians is not explicitly present anywhere in the system, although the</code><br><code>human-supplied destination is explicit. In the ‚Äúnew model‚Äù for AI proposed in</code><code> Human Compatible</code><code> by</code><br><code>Stuart Russell</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>, AI systems will operate with explicit uncertainty about what the human objectives are.</code><br><code>3</code><code> Russell, S., 2019.</code><code> Human Compatible</code><code>. 1st ed. Penguin Books.</code><br><code>6</code>",POSITIVE
fitz_2665329_5,other,../24212003_requirements_for_artificial_intelligence/attachments/2665329.pdf,10,5,2665329,attachments/2665329.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>     </code><code> </code><br><code>Document title </code><br><code>: Feedback on the AI Act  </code><br><code>Prepared by </code><br><code>           : NEN Medical Device / AI Expert Group</code><code>  </code><br><code>Feedback on the AI Act - Dutch Medical Device / AI expert group </code><br><code>     Page </code><code>5</code><code> of </code><code>10</code><code> </code><br><code>Annex 1 ‚Äì Analysis of AI Act / Medical Device Regulation </code><br><code> </code><br><code>1)</code><code> </code><code>Topic: Risk management  </code><br><code> </code><br><code>Clause AI Act</code><code> </code><br><code>Clause MDR</code><code> </code><br><code>Analysis</code><code> </code><br><code>Article 9: </code><br><code>Paragraphs 1, 2 </code><br><code>(subsections) </code><br><code>(a), (b), (c) , (d), </code><br><code>3, 4, 7, 8</code><code> </code><br><code>Annex I: </code><br><code>Section 3 (a), </code><br><code>(b), (c), (d), (e), </code><br><code>(f), 4 (a), (b), (c), </code><br><code>5(a), 5(b)</code><code> </code><br><code>All referenced requirements are covered under the existing Annex I of the </code><br><code>MDR.</code><code> </code><br><code>Article 9: </code><br><code>Paragraphs </code><br><code>2 </code><br><code>(main), 5, 6, 7 </code><br><code> </code><br><code>Article 14: </code><br><code>Full article</code><code> </code><br><code> </code><br><code>N/A</code><code> </code><br><code>AIA requires considering: </code><br><code>-</code><code> </code><br><code>the AI Lifecycle </code><br><code>-</code><code> </code><br><code>testing to identify the most appropriate risk management measures </code><br><code>-</code><code> </code><br><code>testing against preliminary defined metrics and probabilistic thresholds </code><br><code>-</code><code> </code><br><code>Human oversight </code><br><code> </code><br><code>AI Lifecycle</code><code> </code><br><code>There is currently no definition of the AI lifecycle. If such lifecycle will be defined under the AI Act, and potentially </code><br><code>in standards under the AI Act, such as the ISO 5338 (under development), there is a large potential for misalignment with standards under the MDR such as IEC 62304:2006 (Software Lifecycle Processes).  </code><br><code> </code><br><code>Testing  </code><br><code>The AI Act requires manufacturers to identify the most appropriate risk management measures, and to test the AIsystem against preliminary defined metrics and probabilistic thresholds. These testing frameworks are not defined </code><br><code>in the MDR and subsequent standards (IEC 62304:2006, IEC 82304-1:2016, ISO 14917:2019). Under MDR risks are </code><br><code>required to be reduced as far as possible, this is more rigorous than ‚Äòmost appropriate risk management methods‚Äô </code><br><code>as defined in the AI Act. Software medical devices are tested throughout their lifecycle per the processes defined </code><br><code>in IEC 62304:2006), including rigorous clinical validation per MDR requirements (not existing in the AI Act). Additional testing frameworks under the AI Act would not benefit the already existing testing framework for medical </code><br><code>devices.  </code><br><code> </code><br><code>In addition, current BSI/AAMI 34971 (under development) will address risk management requirements for medical </code><br><code>devices and will address specific requirements for risk management for AI based medical devices. Other standards </code><br><code>under development for risk management include ISO/IEC 23894 (at SC 42), for AI systems, do not address medical </code><br><code>devices, and excludes risk management processes for safety and security, and leans heavily on ISO 31000:2018 </code><br><code>which does not align in terms of definitions of risk with ISO 14971:2021 for medical devices. </code><br><code> </code><br><code>Human Oversight </code><br><code>The existing Post Market Surveillance systems, and reporting of the PSUR (periodic safety update reports), as demanded by the MDR 2017/745, require manufacturers of any type of medical device to ensure oversight over </code><br><code>medical devices and continuous re-evaluation of the benefit - risk ratio of medical devices. As suggested by other </code><br><code>regulatory authorities, such as FDA, the development of Good Machine Learning Practices offer opportunities for </code><br><code>specific medical device requirements in post market surveillance systems. The ability to shut down medical devices </code><br><code>should (already) be demanded by risk management systems, where the malfunction of a medical device could </code><br><code>directly impact patient safety. Similarly, labeling of medical devices are required to explain the intended use and </code><br><code>user of the system, and details regarding the involvement of the user (e.g. how humans should interact with the </code><br><code>AI based medical device).  </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665210_10,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665210.pdf,11,10,2665210,attachments/2665210.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>ABI 08/06/2021 </code><br><code> </code><br><code> 10  </code><br><code>information to notified bodies and national competent authorities in reply to </code><br><code>a request. </code><br><code>As in the Regulation 679/2016/UE, the European Commission continues to </code><br><code>provide for the instrument of administrative sanctions, defining a wide range </code><br><code>of penalties, limited to the maximum but not the minimum, which combines </code><br><code>several violations, depending on the severity. </code><br><code>However, the non-punctual reference to the precise obligations makes it very </code><br><code>difficult to focus specifically a particular precept. In other words, in article 71 </code><br><code>there are continuous references to entire articles, or even, as in the case of </code><br><code>paragraph 4, ""</code><code>to the requirements or obligations laid down in the regulation</code><code>"" </code><br><code>instead of referring to a specific duty or prohibition set out in the regulation. </code><br><code>Furthermore, it is not clear whether the sanctions will be directly applicable </code><br><code>or whether member states will be able to modulate the scope of the </code><br><code>administrative sanctions. </code><br><code>Finally, we wish to express our concerns that sanctions are potentially very </code><br><code>heavy. We believe it is necessary to reflect the impact of such sanction on </code><br><code>the business viability of any business entity or business initiative. The risk is </code><br><code>that overwhelming penalties may bring to negative and undesired impacts. </code><br><code> </code><br><code>CONCLUSION </code><br><code>In conclusion, precisely because ABI considers Artificial Intelligence a catalyst </code><br><code>for energy and investment, demonstrated by the growing trend of </code><br><code>investments in the field of AI, also thanks to the significant investments that </code><br><code>the banking sector has made over the years, ABI recommends that these </code><br><code>investments should not be compromised by the introduction of new rules that </code><br><code>could erode the past with further investments.  </code><br><code>At the same time, ABI highlights the importance to act strongly to ensure the </code><br><code>same conditions among entities which operate in the area of credit score/ </code><br><code>worthiness (level playing field).  </code><br><code>According to ABI, the value that AI systems could bring towards a radical </code><br><code>transformation of the way we live and consume should be highlighted, </code><br><code>spreading awareness that allows people to overcome beliefs dictated by </code><br><code>prejudice and polarization, accepting different points of view. </code><br><code>Therefore, a risk-based regulation should also consider the positive impact </code><br><code>that AI solutions will be able to generate (e.g., the reduction in operational </code><br><code>risk and internal fraud risk that are typically conveyed by AI-enabled smart </code><br><code>automation solutions) and not only investigate the negative impacts. </code><br><code> </code><br><code>In addition, ABI points out that there is a problem of overlap with existing </code><br><code>regulations. For example, the area of credit is over-regulated (Consumer </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665323_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665323.pdf,3,2,2665323,attachments/2665323.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Page </code><code>2</code><code> / </code><code>3</code><code> </code><br><code>Laboratoire national de m√©trologie et d‚Äôessais </code><br><code>developers of AI solutions, it is important that the requirements of the AI Act do not duplicate these </code><br><code>existing regulations. It is therefore necessary to focus this regulation on the intrinsic vulnerabilities of </code><br><code>AI, particularly in terms of performance, robustness and explainability.  </code><br><code> </code><br><code>Articles 8-15 (Title III, chap. 2) ‚Äì Requirements for high-risk AI systems </code><br><code>With regard to the requirements set out in Articles 8 to 15, the LNE has noted the following: </code><br><code>ÔÇ∑</code><code> </code><br><code>Some of the data requirements (Article 10) need to be clarified, such as the notion of </code><br><code>""relevance"" of processing operations (Article 10(2c)), the ""appropriateness"" of the statistical </code><br><code>properties of training, validation and test datasets (Article 10(3)), etc. This is also true for </code><br><code>other requirements: AI systems are expected to have an ""appropriate level of accuracy"" (cf. </code><br><code>Article 15(1)), without it being made clear how this level is to be determined. </code><br><code>ÔÇ∑</code><code> </code><br><code>Certain steps in the development process of an AI system, such as the annotation (see Article </code><br><code>10) of training and test data for machine learning algorithms, could be subject to more </code><br><code>specific requirements to ensure that they are carried out properly. For example, it could be </code><br><code>required that the quality of annotations be assessed by means of inter- and intra-annotator </code><br><code>qualification.  </code><br><code>ÔÇ∑</code><code> </code><br><code>The elements to be presented in the ‚Äúinstructions for use‚Äù could also include a description of </code><br><code>the infrastructure (hardware, operating system, software), the types of deployment (public </code><br><code>or private cloud, on-premise etc.) supported by the AI functionality and the dependency on </code><br><code>underlying AI technologies, as well as the interfaces required for the use of the AI system, the </code><br><code>main factors influencing the performance of the AI system (weather conditions, etc.), the </code><br><code>contraindications and non-conditions associated with the use of the AI.  </code><br><code>ÔÇ∑</code><code> </code><br><code>The requirements on post-market monitoring of the AI system (see Article 61) could be </code><br><code>extended to the need to ensure ""maintenance in operational condition"", which could be </code><br><code>achieved through a dedicated set of requirements.  </code><br><code>ÔÇ∑</code><code> </code><br><code>The question arises as to whether the version to be indicated in the technical documentation </code><br><code>(see Annex IV(1a)) should be updated whenever the parameters of an AI system with </code><br><code>continuous learning change. </code><br><code>ÔÇ∑</code><code> </code><br><code>The requirement for the automatic recording of events of the high-risk AI system (see Article </code><br><code>12(1)) could be extended to include a replay capability, for example for accident situations.  </code><br><code>ÔÇ∑</code><code> </code><br><code>Certain characteristics of AI systems, such as their robustness and resilience, which are </code><br><code>required by the draft AI Act (see Article 15), need to be defined, for example by clarifying </code><br><code>how to measure them. </code><br><code> </code><br><code>Article 43.2  </code><br><code>Article 43(2) of the draft Regulation provides that for the high-risk AI systems referred to in Annex III, </code><br><code>points 2 to 8, providers shall follow the conformity assessment procedure based on internal control </code><br><code>as referred to in Annex VI, which does not provide for the involvement of a notified body. As a result </code><br><code>of this provision, the assessments of a large majority of high-risk AI systems will not involve an </code><br><code>independent third party body. In order to increase confidence in the compliance of AI systems and to </code><br><code>enhance the robustness of the assessment mechanisms used, we recommend greater use of the </code><br><code>conformity assessment procedure involving a notified body, described in Annex VII. </code><br><code>The use of Notified Bodies is common to many regulations within the scope of the new legislative </code><br><code>framework. It has proven to be effective at a limited cost and adapted to the structure of the </code><br><code>assessed companies.  </code><br><code>It should be noted that by considerably limiting the scope of intervention of notified bodies, the draft </code><br><code>regulation hampers the emergence of independent conformity assessment bodies, which are </code><br><code>indispensable actors in controlling the development of high-risk AI systems. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665504_7,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665504.pdf,11,7,2665504,attachments/2665504.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>materially distort a person‚Äôs behaviour in a manner that causes or is likely to cause that person </code><br><code>or another person physical or psychological harm (Article 5, (1)(a)). Again, regardless of the set </code><br><code>of techniques used, from the field of AI or not,  why would a system, a service or a product that </code><br><code>causes harm (in short) not already fall under laws against physical damage or psychological </code><br><code>harasment, for example? What is the need for an AI-specific regulation on that matter? This </code><br><code>article might be cancelled.</code><br><code>The same goes for (1)(c) on the prohibition of the placing on the market, putting into service or </code><br><code>use of AI systems by public authorities or on their behalf for the evaluation or classification of </code><br><code>the trustworthiness of natural persons over a certain period of time based on their social </code><br><code>behaviour or known or predicted personal or personality characteristics. Regardless of the set </code><br><code>of techniques used, from the field of AI or not,  why would a system, a service or a product  that </code><br><code>clearly discriminate between people not already fall under laws against discrimination, for </code><br><code>example? What is the need for an AI-specific regulation on that matter? This article might be </code><br><code>cancelled.</code><br><code>Same holds for (1)(d).</code><br><code>II.3 Title III on High-risk AI systems.</code><br><code>The requirements for high-risk AI systems (Chapter 2 of Title III), are sometimes vague. This  </code><br><code>limitation is actually inherent to the fact that the project of regulation regulates a set of </code><br><code>methods, not applications. Thus, being specific then on applications classification is not </code><br><code>feasible, because the whole text is decorrelated of any application.</code><br><code>The Standardisation and Technical Documentation that will come with the regulation will enter </code><br><code>into conflict with standardization specified by the many regulations on applications that are </code><br><code>already in place according to the more ‚Äúvertical‚Äù mindset which prevails. And on top of </code><br><code>incompatibility, the time and money required to satisfy yet another regulation with respect to </code><br><code>standardization and documentation will bear upon competitivity of EU companies (excepted </code><br><code>consultants in regulatory matters, obviously). We have difficulties imagining complete fields of </code><br><code>products/services that would not already be covered by a regulation already stating </code><br><code>standardization and technical documentation prescriptions. So it will be a double burden, </code><br><code>administratively, time-wise and financialy.</code><br><code>II.4 Title IV on transparency of certain AI systems.</code><br><code>This title covers the obligation of providers of AI systems that interact with natural persons, to </code><br><code>ensure that these systems are designed and developed in such a way that natural persons are </code><br><code>informed transparently about the fact that they are interacting with an AI system.  Given the </code><br><code>scope of the AI definition, is it possible to imagine a single piece of software on the market that </code><br><code>will not fall under the scope of the regulation? Imagine the day of any office-worker, sitting in </code><br><code>front of a computer all day-long, having to acknowledge the fact that he/she is using a piece of </code><br><code>Copyright @T.Helleputte ‚Äì 2021</code><br><code>Page 7/11</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665640_9,other,../24212003_requirements_for_artificial_intelligence/attachments/2665640.pdf,18,9,2665640,attachments/2665640.pdf#page=9,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>9 </code><br><code> </code><br><code>Unfortunately, the provisions of the proposed Regulation are largely silent on manipulative </code><br><code>influence, although the proposal purports to be animated by concerns about manipulation. </code><br><code>Recital 15 notes that AI ‚Äúcan also be misused and provide novel and powerful tools for </code><br><code>manipulative, exploitative and social control practices. Such practices are particularly </code><br><code>harmful and should be prohibited because they contradict Union values of respect for human </code><br><code>dignity, freedom, equality, democracy and the rule of law and Union fundamental rights‚Äù. </code><br><code>Nevertheless, the Regulation only robustly regulates two specific and exceptional forms of </code><br><code>manipulative influence‚Äîthose that employ subliminal stimuli, and those that exploit the </code><br><code>vulnerabilities of especially vulnerable groups.  </code><br><code>Subliminal Stimuli </code><br><code>Article 5.1 (a) of the Regulation prohibits AI systems deploying ‚Äúsubliminal techniques </code><br><code>beyond a person‚Äôs consciousness in order to materially distort a person‚Äôs behaviour in a </code><br><code>manner that causes or is likely to cause that person or another person physical or </code><br><code>psychological harm‚Äù. Subliminal techniques are usually understood to be interventions that </code><br><code>recipients cannot perceive, for example because they are presented so quickly that they do </code><br><code>not rise to conscious awareness. The Regulation does not define the term, but Recital 16 </code><br><code>suggests that it uses it in the same way. Research shows that such stimuli can have very </code><br><code>limited effects. It is right to ban them. Nonetheless, subliminal techniques were the fear of </code><br><code>the mid 20</code><code>th</code><code> century and it is neither clear that they have ever been put to alarming use, nor </code><br><code>that there are AI-related applications drawing on them. This prohibition, we suggest, is </code><br><code>largely symbolic.   </code><br><code>Moreover, one may wonder why the Regulation only prohibits subliminal stimuli that lead </code><br><code>physical or psychological harm. As both terms remain undefined, they are to be understood </code><br><code>as they usually are in law, as physical or psychological injuries, setbacks to health or </code><br><code>biological or social functioning. Yet nonconsensual subliminal interventions that significantly </code><br><code>alter thought or behaviour are plausibly ethically wrong‚Äîbecause they bypass rational </code><br><code>control‚Äîeven if they inflict no harm. As ethically acceptable uses of nonconsensual </code><br><code>subliminal interventions are hard to conceive, they should be banned in-principle, with only </code><br><code>tightly defined exceptions (perhaps, for example, allowing uses in specific forms of scientific </code><br><code>research). (This motivates to Amendment 1, below.)  </code><br><code>Vulnerable Groups </code><br><code>Article 5.1 (b) of the Regulation prohibits AI systems exploiting ‚Äúany of the vulnerabilities of </code><br><code>a specific group of persons due to their age, physical or mental disability, in order to </code><br><code>materially distort the behaviour of a person pertaining to that group in a manner that causes </code><br><code>or is likely to cause that person or another person physical or psychological harm‚Äù. Again, the </code><br><code>restriction to physical and psychological harm does not seem warranted. Vulnerable people </code><br><code>should also be protected against significant influences that bypass, weaken or undermine </code><br><code>rational control, regardless of whether those influences can be expected to cause harm (see </code><br><code>suggested Amendment 2, below).  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665528_16,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665528.pdf,20,16,2665528,attachments/2665528.pdf#page=16,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>8 </code><br><code> </code><br><code>We need to keep patient choice and control in primary consideration, </code><code>as some cannot access these </code><br><code>services and even those who can, may not wish to use these products. While digitalisation is extremely </code><br><code>important, it should be seen as supplementary/complementary to existing models of healthcare and </code><br><code>services. </code><br><code>‚Ä¢</code><code> </code><code>Access and sharing health data nationally and across borders through digital </code><br><code>health services and devices </code><br><code>Accessing and sharing health data through digital health services and devices must go hand in hand </code><br><code>with ensuring and safeguarding proper consent coming from the patients</code><code>. They must be in full </code><br><code>control of what kind of data they want to share/transmit. Indeed, patients are generally willing to </code><br><code>provide access to their data provided that proper and clear consent is granted and that they have </code><br><code>control over how and what kind of the data is accessed and for what purpose.  In EPF‚Äôs view, actions </code><br><code>to improve how patients control their data, for instance, granting enhanced possibilities to transmit it </code><br><code>from their m-health/tele-health tools into both EHRs and an EU health data exchange infrastructure, </code><br><code>are important elements for the development of the EHDS framework.  </code><br><code>Once the consent is clearly granted, and the actual use of data is respectful of such consent, data </code><br><code>can be considered as a fundamental tool to improve collaboration between HCPs and patients for </code><br><code>the delivery of better care. </code><br><code>Furthermore, the relationship between healthcare professionals and patients over health data </code><br><code>through digital health services and devices should be integrated in the European Health Data Space as </code><br><code>a collaborative interaction to ensure information to patients about the opportunities offered by digital </code><br><code>health; exploitation of existing opportunities provided by digital health to improve care and selfmanagement; facilitating control of their data and digital health use. </code><br><code>‚Ä¢</code><code> </code><code>Minimise risks related to tele-health and improve the relationship between </code><br><code>patients and healthcare professionals  </code><br><code>While the correct application of tele-health solutions can improve the relationship between patients </code><br><code>and healthcare professionals, and access to care, there are some essential elements to be taken into </code><br><code>consideration: </code><br><code>‚Ä¢</code><code> </code><br><code>Tele-health should, in normal conditions, not be seen as a replacement for traditional care </code><br><code>but rather as an additional tool; </code><br><code>‚Ä¢</code><code> </code><br><code>Increased trust issues from the patients' point of view; </code><br><code>‚Ä¢</code><code> </code><br><code>The correct use of tele-health needs adequate skills and access to digital health solutions, </code><br><code>both for healthcare professionals and patients;  </code><br><code>‚Ä¢</code><code> </code><br><code>Additional stress for both patients and doctors, from difficulties in accessing and using digital </code><br><code>solutions to the de-personalisation of care, and adopting additional tools in already </code><br><code>overcrowded schedules; </code><br><code>‚Ä¢</code><code> </code><br><code>Potential risks of misdiagnosis, errors and miscommunication exacerbated by the use of telehealth solutions; </code><br><code>‚Ä¢</code><code> </code><br><code>Tele-health also requires proper access to digital tools. The digital divide currently existing </code><br><code>within and across EU-countries should be therefore taken into consideration. </code><br><code>‚Ä¢</code><code> </code><br><code>Patients with hearing, vision or physical impairment, dementia and other conditions are </code><br><code>potentially prevented from using technologies related to tele-health. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665481_19,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665481.pdf,20,19,2665481,attachments/2665481.pdf#page=19,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>19 </code><br><code>products and services, will undermine innovative small companies from entering the market, and </code><br><code>reduce competition in the market.   </code><br><code>But the burdens associated with this proposal need not be overwhelming if the </code><br><code>Commission chooses to cap the period of time for which such logs and records must be </code><br><code>maintained.  A sensible limitation of time is three years, which will provide regulators sufficient </code><br><code>time to look back to review historical records, but is less burdensome to AI products or systems </code><br><code>providers who will only have to retain such information for a limited period of time.  </code><br><code>2.</code><code> </code><code>Reporting  </code><br><code>The Proposed Rules would also mandate certain incident reporting obligations.  But the </code><br><code>current proposal is vague and ambiguous.  Absent clarification from the Commission (and </code><br><code>significant narrowing of the reporting obligations), there is a real risk that the rule will lead to </code><br><code>significant uncertainty in the market and potential over-reporting of incidents that do not present </code><br><code>material harms and which may simply burden Commission and EU staff responsible for </code><br><code>reviewing and responding to such reports. </code><br><code>As currently drafted, the rule requires providers of high-risk AI systems placed on the EU </code><br><code>market to report ‚Äúany serious incident‚Äù or ‚Äúany malfunctioning‚Äù of these systems that constitutes </code><br><code>a breach of obligations under EU law intended to protect fundamental rights.  Framed this way, </code><br><code>providers will be forced to first asses what constitutes a ‚Äúserious‚Äù incident or any malfunctioning </code><br><code>of the system, and second whether these events have led to a breach of obligations under EU law </code><br><code>to protect fundamental rights.  Thus, providers will be forced to make subjective decisions about </code><br><code>the level of severity of an incident, whether a system has malfunctioned (in any way) and </code><br><code>whether laws protecting fundamental rights have been breached.  All of these criteria are broad, </code><br><code>open-ended triggers that will vary significantly based upon the facts of any particular </code><br><code>circumstance.  Further, they will undoubtedly lead to significant range of self-reported events, </code><br><code>some serious and worth the Commission/EU staff‚Äôs time, and many others not. </code><br><code>Further, the reporting proposal fails to account for the fact that there may be multiple </code><br><code>algorithms that are used in a model or AI system, and the proposed rule does not acknowledge </code><br><code>this fact or explain whether these reports are due on an algorithm, model or system basis.  The </code><br><code>Commission should clarify this point.  Similarly, the Commission should clarify that conformity </code><br><code>assessments are not required on a per model basis, but are required only of an entire AI product </code><br><code>or system. </code><br><code>V.</code><code> </code><br><code>CONCLUSION </code><br><code>CTA and its members have a significant interest in ensuring that European consumers </code><br><code>benefit from AI-powered products and services.  The Commission should proceed carefully to </code><br><code>ensure that its policies promote continued development and deployment of AI products or </code><br><code>systems that enhance the lives, safety and interests of European consumers.  CTA stands ready to </code><br><code>continue its central roles in the development of consensus-based standards that advance these </code><br><code>goals and promotion of policies supporting continued dynamic growth and innovation </code><br><code>throughout the consumer technology industry. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662182_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662182.pdf,5,3,2662182,attachments/2662182.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>3 </code><br><code> </code><br><code>parezca real ha sido elaborado por medios autom√°ticos, en nuestra opini√≥n, tambi√©n se </code><br><code>debe exigir la divulgaci√≥n de qu√© contenido concreto ha sido empleado para la </code><br><code>elaboraci√≥n de ese nuevo contenido. Ello es indispensable para garantizar que los </code><br><code>creadores y otros titulares de derechos sean remunerados por la utilizaci√≥n de sus obras </code><br><code>o prestaciones.  </code><br><code> </code><br><code>Por otro lado, se exige la regulaci√≥n de un sistema de responsabilidad para aquellos </code><br><code>casos en los que sistemas aut√≥nomos generen contenido que vulnere DPI. Si el </code><br><code>resultado generado por un sistema aut√≥nomo transforma o reproduce contenido ajeno </code><br><code>¬øante qui√©n deben reclamar los titulares de derechos afectados?. La falta de respuesta </code><br><code>legal a esta cuesti√≥n no puede dejar desamparados a los creadores. </code><br><code> </code><br><code>En este sentido, en la Resoluci√≥n del Parlamento Europeo, de 20 de octubre de 2020, </code><br><code>con recomendaciones destinadas a la Comisi√≥n sobre un r√©gimen de responsabilidad </code><br><code>civil en materia de inteligencia artificial (2020/2014(INL)) se considera que es necesario </code><br><code>realizar adaptaciones espec√≠ficas y coordinadas de los reg√≠menes de responsabilidad </code><br><code>civil para evitar situaciones en las que personas que sufran un da√±o o un menoscabo a </code><br><code>su patrimonio por el empleo de sistemas de IA acaben sin indemnizaci√≥n. </code><br><code> </code><br><code>SEGUNDO: CONTENIDO CREADO POR SISTEMAS DE INTELIGENCIA ARTIFICIAL  </code><br><code> </code><br><code>Buena parte de la doctrina se ha volcado en el planteamiento del reto que supone la </code><br><code>protecci√≥n de los resultados o contenido obtenido mediante sistemas de IA. Mientras </code><br><code>que el empleo de sistemas de IA por parte de un creador como una mera herramienta </code><br><code>en la creaci√≥n no debe suponer ning√∫n reto ni variaci√≥n a la legislaci√≥n existente, s√≠ que </code><br><code>plantea dudas la protecci√≥n de los resultados de aquellos sistemas que puedan llegar a </code><br><code>crear de forma aut√≥noma.  </code><br><code> </code><br><code>En primer lugar, queremos destacar que la posible protecci√≥n de los contenidos creados </code><br><code>por sistemas aut√≥nomos no debe suponer un menoscabo a los intereses o derechos de </code><br><code>los creadores humanos. Las creaciones obtenidas por m√°quinas no deben llegar a </code><br><code>competir ni a sustituir a las creaciones humanas. Es altamente probable que las </code><br><code>creaciones obtenidas de forma aut√≥noma por sistemas artificiales lleguen a ser </code><br><code>monopolio de unas pocas empresas tecnol√≥gicas, por lo que se ha de evitar que estas </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665441_8,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665441.pdf,8,8,2665441,attachments/2665441.pdf#page=8,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>SEMI Europe </code><code>| Rue de la Science 14, 1040 | EU Transparency Register: 402302029423-14                                                     </code><br><code>Tel.: +32 (0) 2 609 53 18 | www.semi.org/eu </code><br><code> </code><br><code>Page | 8 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Conclusion </code><br><code>The Commission‚Äôs proposal for a regulation on AI indicates Europe‚Äôs willingness to move forward to </code><br><code>capture the huge potential ahead in the rapidly emerging AI technologies. SEMI Europe would welcome </code><br><code>the opportunity to discuss the above-mentioned recommendations with the European Commission, the </code><br><code>Parliament and the Council. The microelectronics sector is a key enabler and remains ready to work </code><br><code>together with all interested stakeholders to develop new avenues of growth to reinforce Europe‚Äôs AI </code><br><code>leadership. </code><br><code>About SEMI Europe </code><br><code>SEMI Europe is the European arm of SEMI, the industry association connecting more than 2,400 </code><br><code>semiconductor and electronics manufacturing companies worldwide, including nearly 300 European </code><br><code>headquartered businesses. SEMI members are responsible for the innovations in materials, design, </code><br><code>equipment, software, devices and services that enable smarter, faster, more powerful and more </code><br><code>affordable electronic products. Since 1970, SEMI has built connections that have helped its members </code><br><code>prosper, create new markets and address common industry challenges together. </code><br><code>Association Contact </code><br><code>Marek Kysela, Senior Coordinator Advocacy, SEMI Europe | mkysela@semi.org </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662603_5,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2662603.pdf,9,5,2662603,attachments/2662603.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>An Assessment of the AI Regulation Proposed by the European Commission</code><br><code>5</code><br><code>desverband, 2021). Bitkom, the federation of German digital companies, is also of</code><br><code>the opinion that even a more general horizontal regulation of ‚Äúalgorithmic systems""</code><br><code>is not practicable (Bitkom, 2020).</code><br><code>2.3 Lack of Delimitation from Existing Regulations</code><br><code>The application areas prohibited in</code><code> Article 5</code><code> of the proposed regulation are very</code><br><code>broadly defined and lead to uncertainties for all stakeholders due to the correspondingly wide scope for interpretation</code><code>5</code><code>. Instead, the definition of specifically prohibited</code><br><code>use cases would be much more effective. It should also be examined whether these</code><br><code>would have to be explicitly prohibited at all or whether this is already the case today</code><br><code>under other laws, such as criminal codes or the General Data Protection Regulation</code><br><code>(GDPR) (European Commission, 2016). New prohibitions should also address the</code><br><code>relevant use cases in general, and without any reference to AI, as they could (in the</code><br><code>future) possibly be implemented without the explicit use of AI methods.</code><br><code>For the same reasons, the definition of concrete use cases without explicit reference to AI would therefore also be helpful in</code><code> Article 6 (Classification rules for highrisk AI systems)</code><code> for a more precise definition of ‚Äúhigh-risk applications"" (Heikkil√§,</code><br><code>2021). It is also unclear from the proposed regulation what the European Commission specifically means by ‚Äúsafe"" applications. Furthermore, it is not defined how</code><br><code>‚Äúsafety"" could be established in individual use cases at all, especially since many</code><br><code>machine learning methods have statistical uncertainty embedded in their definition.</code><br><code>In conjunction with existing regulations, the proposed regulation also poses the</code><br><code>risk of contradictory and twofold requirements, for example in automated lending</code><br><code>(Herwartz, 2021).</code><br><code>2.4 Unfulfillable Requirements for ‚ÄúHigh-Risk Applications""</code><br><code>In addition to the very broad definition of ‚Äúhigh-risk applications""</code><code>6</code><code> in</code><code> Article 6</code><code>,</code><br><code>the proposed regulation provides for corresponding documentation requirements in</code><br><code>Article 11 (Technical documentation)</code><code>, registration requirements in</code><code> Article 60 (EU</code><br><code>database for stand-alone high-risk AI systems)</code><code> and reporting requirements in</code><code> Article</code><br><code>62 (Reporting of serious incidents and of malfunctioning)</code><code>. These requirements for</code><br><code>the development or use of AI in safety-critical application areas are comparable to</code><br><code>the operation of nuclear power plants or the development of aircraft. They therefore</code><br><code>will likely inhibit innovation and are thus disproportionate.</code><br><code>5</code><code> For example, even operating a search engine on a potentially unrepresentative database could be</code><br><code>prohibited by</code><code> Article 5(a)</code><code> due to the potential impact of the search results.</code><br><code>6</code><code> I would like to thank in particular Tobias Manthey of EvoTegra GmbH for our extensive discussions</code><br><code>on the topic of ‚Äúhigh-risk applications"".</code>",FALSE_NEGATIVE
fitz_2663310_4,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663310.pdf,13,4,2663310,attachments/2663310.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>CAIDP Statement </code><br><code> </code><br><code>EU AI Regulation </code><br><code>28 July 2021 </code><br><code> </code><br><code>European Commission </code><br><code>4 </code><br><code>Democratic Value</code><code>s (CAIDP 2020). This ban should not only be limited to facial recognition </code><br><code>systems, but also new /emerging forms of biometric recognition analyses such as gait, voice, </code><br><code>etc. </code><br><code> </code><br><code> </code><br><code>If biometric identification systems are to be used by law enforcement for specific </code><br><code>investigation purposes ‚Äòafter‚Äô due process is followed and ‚Äòfor the specific location and target </code><br><code>person(s)‚Äô, there should also be a time limit of how long these records are retained. The </code><br><code>regulation should clearly state that the records cannot be retained infinitely. </code><code>There should also </code><br><code>be independent oversight of the deployment, management, and termination of these AI-based </code><br><code>surveillance techniques. </code><br><code> </code><br><code>Evaluation or classification of the trustworthiness of natural persons over a certain period </code><br><code>based on their social behavior or known or predicted personal or personality characteristics, </code><br><code>with the social score  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><code>The wording of the prohibited use case does not cover the use of this system by </code><br><code>private companies for their own commercial purposes, or as a service provided to </code><br><code>other private companies (for example risk scoring of individuals for online behavior, </code><br><code>employability scoring based on online behavior) or other risk scoring systems used </code><br><code>by public entities, such as for criminal sentencing.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><code>Paragraph 17 states that ‚ÄòAI systems </code><code>evaluate or classify the trustworthiness of </code><br><code>natural persons based on their social behavior</code><code> in multiple contexts or known or </code><br><code>predicted personal or personality characteristics. The social score obtained from </code><br><code>such AI systems may lead to the detrimental or unfavorable treatment of natural </code><br><code>persons or whole groups thereof in social contexts, which are </code><code>unrelated to the </code><br><code>context in which the data was originally generated or collected or to a detrimental </code><br><code>treatment that is disproportionate or unjustified to the gravity of their social </code><br><code>behavior</code><code>. Such AI systems </code><code>should be therefore prohibited</code><code>.‚Äù However, despite the </code><br><code>above acknowledgement, Proposal also allows credit scoring systems and scoring of </code><br><code>personality (in recruitment) by classifying them as ‚Äòhigh-risk‚Äô systems.  The </code><br><code>explanations at the beginning of the Proposal conflict with the articles of the </code><br><code>Proposal in this sense. It is a fact that majority of these systems incorporate </code><br><code>behavioral and social data in their models from a wide range of data sources usually </code><br><code>unrelated to the context of their eventual use.   </code><br><code> </code><br><code>CAIDP recommends a ban on any score-based profiling of individuals by private </code><br><code>companies that is not fully compliant with all legal obligations prior to deployment and for </code><br><code>which an algorithmic impact assessment has not been conducted ex ante.</code><code> </code><br><code> </code><br><code>HIGH RISK USE CASES</code><code> </code><br><code>Biometric identification and categorisation of natural persons: </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663263_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663263.pdf,2,1,2663263,attachments/2663263.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>R√ºckmeldung zum Verordnungsvorschlag √ºber ein europ√§isches Konzept f√ºr </code><br><code>k√ºnstliche Intelligenz </code><br><code> </code><br><code>Wir begr√º√üen den risikobasierten Ansatz der Kommission, der im Verordnungsvorschlag </code><br><code>√ºber k√ºnstliche Intelligenz skizziert wird, und teilen das Bestreben, KI sicher, rechtm√§√üig und </code><br><code>im Einklang mit den EU-Grundrechten zu gestalten.  </code><br><code> </code><br><code>Grunds√§tzliche Anforderungen </code><br><code>Die Pr√ºfung des Verordnungsvorschlags durch Parlament und Rat muss darauf abzielen: </code><br><code>1. ein angemessenes Verh√§ltnis zwischen </code><code>Risikopr√§vention</code><code> </code><code>und neuen Belastungen </code><code>zu </code><br><code>schaffen, welches weder Innovation erstickt, noch die Einf√ºhrung von KI verlangsamt </code><br><code>oder gar aufh√§lt, </code><br><code>2. die </code><code>angemessene Durchf√ºhrbarkeit</code><code> f√ºr Herstellende und Nutzende sicherzustellen, </code><br><code>3. eine </code><code>ausreichende Flexibilit√§t</code><code> zur Anpassung an neue Erkenntnisse sowie an </code><br><code>unterschiedliche Organisationsstrukturen innerhalb der KI-Wertsch√∂pfungsketten </code><br><code>sicherzustellen, </code><br><code>4. die </code><code>Koh√§renz</code><code> </code><code>mit</code><code> den </code><code>bestehenden Rechtsvorschriften</code><code> zu gew√§hrleisten und  </code><br><code>5. einen </code><code>flexiblen Marktzugangsrahmen</code><code> zu f√∂rdern. </code><br><code> </code><br><code>Daf√ºr sollten folgende Punkte beachtet werden: </code><br><code> </code><br><code>Anwendungsbereich klug anpassen </code><br><code>- </code><br><code>Anpassungen des </code><code>Anwendungsbereichs</code><code> sind entscheidend, um sicherzustellen, dass </code><br><code>die neuen Regeln </code><code>effektiv, verh√§ltnism√§√üig und rechtlich eindeutig</code><code> auf hochriskante </code><br><code>KI-Systeme anwendbar sind und zu den gew√ºnschten politischen Ergebnissen f√ºhren.  </code><br><code>- </code><br><code>Die aktuelle </code><code>Definition von KI </code><code>(Art. 3(1)), die Kriterien zur Bestimmung </code><code>verbotener </code><br><code>Praktiken</code><code> (Art. 5) und die Klassifizierung von KI-Systemen als </code><code>Hochrisiko-Systeme</code><code> </code><br><code>(Art. 6) m√ºssen daf√ºr besser gekl√§rt und eingegrenzt werden, um sich auf die Bereiche </code><br><code>zu konzentrieren, in denen die h√∂chsten und weitreichendsten Risiken erwartbar sind.  </code><br><code>- </code><br><code>So wie sie derzeit formuliert ist, w√ºrde zum Beispiel die Definition die </code><code>meisten </code><br><code>modernen Softwares </code><code>umfassen</code><code>, die rein statistische und wissensbasierte Ans√§tze </code><br><code>f√ºr die herk√∂mmliche Datenanalyse verwenden,</code><code> </code><code>die nur geringe Auswirkungen auf </code><br><code>die Einzelnen haben.</code><code> </code><br><code>- </code><br><code>Stattdessen sollte die Hochrisiko-Definition die Komponente der menschlichen Aufsicht </code><br><code>ber√ºcksichtigen und damit KI-Systeme, die lediglich Empfehlungen geben, nicht zu den </code><br><code>Hochrisiko-Systemen z√§hlen und sich auf intelligente KI-Systeme konzentrieren, die </code><br><code>tats√§chliche Entscheidungen treffen k√∂nnen.  </code><br><code> </code><br><code>Einstufung von KI-Systemen mit hohem Risiko unbedingt nachbessern </code><br><code>- </code><br><code>Wir stimmen zu, dass einige eigenst√§ndige KI-Einsatzf√§lle in den in Anhang III </code><br><code>aufgelisteten Bereichen spezifischen Anforderungen unterworfen werden m√ºssen, aber </code><br><code>die</code><code> Definition</code><code> dieser Bereiche ist zu breit.  </code><br><code>- </code><br><code>Die Annahme, dass bei KI-Systemen ein hohes Risiko besteht, wenn sie im Bereich </code><br><code>Schul- oder Berufsbildung,</code><code> </code><code>Besch√§ftigung, Personalmanagement und </code><br><code>Selbstst√§ndigkeit </code><code>(Anhang III, Nr. 3 und 4, Art. 6, Abs. 2)</code><code> </code><code>eingesetzt wird, w√ºrde zu </code><br><code>Rechtsunsicherheit</code><code> und unverh√§ltnism√§√üigem b√ºrokratischen Aufwand f√ºr </code><br><code>Unternehmen f√ºhren, die versuchen festzustellen, ob sie davon betroffen sind. </code><br><code>- </code><br><code>Ob KI als hohes Risiko betrachtet werden sollte, h√§ngt vom spezifischen Kontext und von </code><br><code>Situationen, in denen das KI-System die endg√ºltigen Entscheidungen trifft, ab ‚Äì ein </code><br><code>pauschaler Ansatz ist nicht angemessen</code><code>. </code><br><code>- </code><br><code>Die Einstufung w√ºrde die Verbreitung innovativer KI-Anwendungen behindern, </code><br><code>insbesondere von KI-L√∂sungen, die eine st√§rkere Nutzung im </code><code>Besch√§ftigungs- und </code><br><code>Bildungskontext</code><code> unterst√ºtzen. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663395_5,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663395.pdf,7,5,2663395,attachments/2663395.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>www.efpia.eu</code><code>         </code><code> 5</code><code> </code><br><code> </code><br><code>2.</code><code> </code><code>AI literacy and competence building is an enabler</code><code> </code><br><code>‚Ä¢</code><code> </code><code>EFPIA views the </code><code>skills agenda</code><code> as being critical when it comes to AI in healthcare. </code><code>EFPIA </code><br><code>supports partnerships between the public and private sectors, bringing together leadership </code><br><code>and commitment from organisations to ensure coordination of research and innovation in </code><br><code>AI.</code><code> EFPIA members recognise through their accelerator and innovation hub initiatives that </code><br><code>SMEs and academia are key components of innovation in AI, and that SMEs require both </code><br><code>access to finance and support in the adoption of AI. Therefore, coordination of research </code><br><code>centres of excellence and leadership from a lighthouse centre of research, innovation and </code><br><code>expertise is critical to establishing European leadership in AI. </code><br><code>‚Ä¢</code><code> </code><code>It is important that healthcare professionals have the right skills to understand and utilise AI </code><br><code>solutions and to have ability to exercise oversight in line with their mandate. There should </code><br><code>be an educational focus on use of AI-based solutions for example in healthcare workforce‚Äôs </code><br><code>curricula and complementary courses as part of continuous professional development. </code><br><code>Similarly, a focus on AI literacy for the broader society and skills by patient communities </code><br><code>would also be warranted. </code><code>EFPIA calls for investments e.g. as part of EU4Health 2021-2027 </code><br><code>programme, in reskilling and lifelong learning opportunities which are required to </code><br><code>overcome social and cultural challenges affecting stakeholders in healthcare. </code><code> </code><br><code> </code><br><code>3.</code><code> </code><code>Access to high-quality data is critical to AI deployment  </code><br><code>‚Ä¢</code><code> </code><code>In order to assure high quality AI solutions and to safeguard the EU‚Äôs competitiveness in the </code><br><code>international AI marketplace, the easy yet compliant </code><code>access to a significant amount of highquality, representative data will be indispensable. </code><code>This would further help to reduce bias, </code><br><code>discrimination and ensure highest levels of safety and robustness of AI solutions in </code><br><code>healthcare. Patients and citizens should be empowered through feedback mechanisms to </code><br><code>help address bias and other potential concerns. EFPIA supports initiatives which can provide </code><br><code>for increased and appropriate access to high-quality health data, e.g. through the proposed </code><br><code>EU Health Data Space. In particular, we encourage the Commission to </code><code>identify obstacles that </code><br><code>would prevent an adequate use and deployment of AI in Europe through optimal data </code><br><code>sharing between stakeholders in a data-agile economy and to explore appropriate </code><br><code>standards, tools or frameworks to incentivize data sharing</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> such as for example Findata.</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> </code><code> </code><br><code> </code><br><code>4.</code><code> </code><code>Data Governance is core   </code><br><code>‚Ä¢</code><code> </code><code>GDPR is aimed at standardising and strengthening the protection of personal data, including </code><br><code>the rights of individuals to be better informed about how their data are used. It also sets out </code><br><code>clear responsibilities and obligations on healthcare professionals and companies using such </code><br><code>data, with stringent penalties for infringements.  </code><br><code>‚Ä¢</code><code> </code><code>GDPR requirements and concepts, local and institutional regulations, as well as ethical </code><br><code>oversight</code><code>, </code><code>may require further clarification if they are to support innovation in AI. </code><br><code>Furthermore, it will be important to address the fragmented application of the GDPR and </code><br><code>other legislation bearing on scientific research across the EU. It has to be ensured that data </code><br><code>are managed and analysed within a secure and ethical governance framework.</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code> </code><code>Human </code><br><code>involvement in AI-augmented </code><code>decision making ensures that appropriate oversight is </code><br><code>embedded into business processes.  </code><code> </code><br><code>                                                </code><br><code>5</code><code> </code><code>https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1593073685620&uri=CELEX%3A52020DC0066</code><code>,  p. 13 </code><br><code>6</code><code> https://www.findata.fi/en/about-us/what-is-findata/ </code><br><code>7</code><code> https://www.hma.eu/fileadmin/dateien/HMA_joint/00-_About_HMA/03-Working_Groups/Big_Data/Final__Priority_Recommendations_of_the_HMA-EMA_joint_Big_Data_Task_Force.pdf </code>",POSITIVE
fitz_2662543_2,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2662543.pdf,2,2,2662543,attachments/2662543.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>The Guild of European Research-Intensive Universities ‚Ä¢ Rue du Tr√¥ne 98 ‚Ä¢ B-1050 Brussels </code><br><code>Phone +32 (0)2 2740 500 ‚Ä¢ office@the-guild.eu ‚Ä¢ www.the-guild.eu </code><br><code> </code><br><code>2 </code><br><code>asks that the AI Act does not add to the ethical requirements of EU grants and does not increase the </code><br><code>burdens on researchers through a blanket obligation to demonstrate that the AI systems to be </code><br><code>developed, deployed and/or used, in the proposed research projects, do not infringe the AI Act.</code><code> The </code><br><code>European Commission may consider instead requiring an ethical approval only for the research </code><br><code>proposals that involve the development, deployment and/or use of specific risky AI systems.  </code><br><code>The Guild anticipates that the AI Act may create legal uncertainties, especially if there is no </code><br><code>harmonization in its national transpositions and interpretations across the European Union‚Äôs Member </code><br><code>States. A similar situation with the General Data Protection Regulation</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> has already detrimental effects </code><br><code>on health research.</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> </code><code>The Guild calls not to reproduce the same mistake and recommends giving the </code><br><code>European Artificial Intelligence Board the mandate ‚Äì with the support of an high-level expert group </code><br><code>‚Äì to ensure the harmonized implementation of the AI Act in the Member States</code><code>. The AI Act may </code><br><code>create uncertainties also by introducing concepts such as ‚Äòtrustworthy AI systems‚Äô in a technological </code><br><code>field evolving at a fast pace. Even though the European Commission clearly define the process e.g. for </code><br><code>the ex-ante conformity assessment of high-risk AI systems, AI developers may be still uncertain on how </code><br><code>to concretely ensure that their systems are trustworthy and comply with all requirements listed in the </code><br><code>AI Act. The Guild contends that universities may offer them solutions. </code><code>The European Commission </code><br><code>should support research projects aimed at elucidating ‚Äì especially from a technological perspective </code><br><code>‚Äì the concepts introduced in the AI Act (e.g. trustworthy AI, robust AI etc.) and finding how AI </code><br><code>developers can concretely comply with the Act</code><code>.  </code><br><code> </code><br><code>For further information on The Guild‚Äôs position, please contact Julien Chicot (julien.chicot@theguid.eu). </code><br><code> </code><br><code>4</code><code> Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the </code><br><code>protection of natural per-sons with regard to the processing of personal data and on the free </code><br><code>movement of such data. </code><br><code>5</code><code> European Commission (2021) </code><code>Assessment of the EU Member States‚Äô rules on health data in the light </code><br><code>of GDPR</code><code>. Luxembourg: Publications Office of the European Union. DOI: 10.2818/546193 </code>",POSITIVE
fitz_2661384_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2661384.pdf,3,3,2661384,attachments/2661384.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>         3 </code><br><code> Evidence base and data collection </code><code> </code><br><code>In recent years, the Commission has undertaken significant legislative reviews of sectorial legislation that make </code><br><code>use of harmonised European standards for technical harmonisation requirements. It has also adopted new </code><br><code>legislative initiatives that include legal provisions on harmonised European standards. In this context, large-scale </code><br><code>stakeholder consultations were undertaken and analysed. This initiative will take stock of the evidence gathered </code><br><code>in these exercises, including the impact assessments of the </code><code>Medical Devices Regulations</code><code>, the </code><code>evaluation</code><code> and </code><br><code>impact assessment</code><code> of the Machinery Safety Directive, the impact assessment of the </code><code>Artificial Intelligence</code><code> </code><br><code>Regulation, the impact assessment of the </code><code>Batteries Regulation</code><code>; as well as ongoing reviews, like the evaluation of </code><br><code>the </code><code>Construction Products Regulation</code><code> and stakeholder feedback on the </code><code>General Product Safety Directive</code><code>.  </code><br><code>Further data sources will include reports, workshops and meetings with academic and industry experts with the </code><br><code>help of the EU-funded International Digital Cooperation project on ICT standardisation. </code><br><code>Any measures or proposals that may result from this initiative will comply with the Better Regulation principles, </code><br><code>as appropriate. </code><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665626_3,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665626.pdf,7,3,2665626,attachments/2665626.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Recommendations on classification of AI systems as high-risk </code><br><code>As AI capabilities and deployments evolve, new areas of risk will inevitably emerge, and ways of </code><br><code>regulating AI may need to change. It is crucial that the proposed regulation can respond to these </code><br><code>risks as they arise, in order to avoid becoming overly narrow or outdated. </code><br><code> </code><br><code>To its credit, the regulatory proposal already includes a range of mechanisms aimed at ensuring </code><br><code>adaptability, including the general risk-focused orientation, broad definition of AI, and the ability to </code><br><code>revise and expand the list of both AI techniques (in Annex I) and the list of high-risk AI systems (in </code><br><code>Annex III). However, we believe that more can and should be done to ensure the regulation can </code><br><code>adapt as the risk profile of AI changes. </code><br><code> </code><br><code>We suggest that the Commission: </code><br><code>Broaden the provision to add new high-risk systems </code><br><code>We recommend broadening the provision to add new high-risk systems and risk areas. Specifically, </code><br><code>we suggest modifying Article 7 to empower the Commission to add high-risk AI systems to the list </code><br><code>in Annex III:</code><code> </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>under areas outside of those listed in points 1 to 8 of Annex III, or </code><br><code>‚Ä¢</code><code> </code><br><code>if the AI systems pose a substantial risk of societal harm. </code><br><code> </code><br><code>Currently, the regulation only allows the addition of new high-risk AI </code><code>systems </code><code>if they both fall under </code><br><code>any of the eight listed areas and are deemed to pose at least as great a risk (to health and safety or </code><br><code>adverse impact on fundamental rights) as systems already in Annex III. We see two limitations </code><br><code>imposed by these conditions. Firstly, while the eight domains listed are broad, we do not believe </code><br><code>they exhaust the range of domains within which AI systems may come to have significant impacts on </code><br><code>citizens‚Äô lives. </code><code>AI systems‚Äô use in various other domains could raise significant additional risks that </code><br><code>are not well captured by these eight risk areas. </code><code>For example, AI-based personal digital assistants </code><br><code>could be used to give individuals important financial, legal, or medical advice with significant </code><br><code>consequences for health and safety, and do not appear to be covered by the current risk categories. </code><br><code>Moreover, the general purpose nature of AI technology and its rapid rate of progress makes it </code><br><code>difficult to anticipate impacts in advance. In light of this uncertainty, it makes sense to have </code><br><code>provision for adding areas. Therefore, we suggest empowering the Commission to add high-risk AI </code><br><code>systems under areas outside of those listed in points 1 to 8 of Annex III. This can easily be done by </code><br><code>removing point 1(a) from Article 7. </code><br><code> </code><br><code>Secondly, while many risks from AI technology can be thought of as the potential for harms to </code><code>an </code><br><code>individual‚Äôs </code><code>health and safety or of adverse impact on their fundamental rights, AI may also cause </code><br><code>significant harm, on a </code><code>societal</code><code> level. For example, digital personal assistants could be used to </code><br><code>promote certain products, services, or even ideologies well above others, with the potential to </code><br><code>contribute to substantial and potentially harmful shifts in our markets, democracies, and </code><br><code>information ecosystems. However, the impacts on individual health, safety, or fundamental rights </code><br><code>may be negligible or difficult to discern. We believe the proposed regulation should include provision </code><br><code>to identify and regulate systems which could pose this kind of harm, and therefore suggest </code><br><code>empowering the Commission to add high risk AI systems to Annex III that pose a substantial risk of </code><br><code>societal harm. One concrete way to do this could be to broaden the interpretation of 7.2.d (‚Äúthe </code><br><code>potential extent of such harm or such adverse impact, in particular in terms of its intensity and its </code><br><code>ability to affect a plurality of persons</code><code>‚Äù) </code><code>to include societal as well as individual harms. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665462_31,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665462.pdf,31,31,2665462,attachments/2665462.pdf#page=31,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,<code>Access Now‚Äôs submission to the European Commission‚Äôs adoption</code><br><code>consultation on the AI Act</code><br><code>31</code>,NO_FOOTNOTES_ON_PAGE
fitz_2665167_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665167.pdf,6,2,2665167,attachments/2665167.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Recomenda√ß√£o da APDSI para a consulta p√∫blica da Comiss√£o Europeia em mat√©ria de Intelig√™ncia </code><br><code>Artificial</code><code> </code><br><code>APDSI ‚Äì Associa√ß√£o para a Promo√ß√£o e Desenvolvimento da Sociedade da Informa√ß√£o </code><br><code> 2 </code><br><code>constru√≠do de forma personalizada para o distribuidor pelo programador) o </code><br><code>distribuidor pode ser o mesmo que o prestador. Mas, noutros casos, tal n√£o </code><br><code>acontecer√° se sistemas IA de prop√≥sito geral forem utilizados. </code><br><code>‚Ä¢</code><code> </code><code>Os distribuidores devem suportar a responsabilidade prim√°ria de observ√¢ncia, </code><br><code>conformidade, avalia√ß√£o e monitoriza√ß√£o post-market, pois s√≥ eles podem verificar </code><br><code>as aplica√ß√µes finais para as quais os seus sistemas est√£o a ser usados e outra </code><br><code>informa√ß√£o adicional que tenha sido introduzida da forma√ß√£o do seu sistema. Ao </code><br><code>contr√°rio, seria igual a responsabilizar os fabricantes de tijolo por assegurar a </code><br><code>integridade estrutural de uma torre, ao inv√©s de os arquitetos, engenheiros e </code><br><code>construtores que desenharam e constru√≠ram a mesma. </code><br><code>o</code><code> </code><code>Para ser claro, o √≥nus deve ser colocado aos distribuidores em todas as </code><br><code>circunst√¢ncias, independentemente da marca ou da maneira precisa em </code><br><code>que o sistema de IA foi obtido. Caso se esteja a utilizar IA de prop√≥sito </code><br><code>geral, tirado da prateleira, numa opera√ß√£o de alto risco ou caso o sistema </code><br><code>tenha sido modificado, s√≥ a organiza√ß√£o que utiliza o sistema de IA √© que </code><br><code>ter√° conhecimento sobre como estar√° a ser utilizado o sistema. </code><br><code> </code><br><code>2.</code><code> </code><code>Rever a linguagem usada em standards invi√°veis: √â importante manter requisitos </code><br><code>realistas, em concord√¢ncia com as boas pr√°ticas e pr√°ticas vi√°veis da ind√∫stria. </code><br><code>Enquanto concordamos com a dire√ß√£o dos requisitos para sistemas de IA de risco </code><br><code>elevado, consideramos que alguma linguagem utilizada merece aten√ß√£o acrescida de </code><br><code>forma a evitar a cria√ß√£o de standards que s√£o de facto imposs√≠veis para qualquer </code><br><code>fornecedor alcan√ßar. Em particular: </code><br><code>‚Ä¢</code><code> </code><code>O artigo 10 (3) indica que ‚ÄúOs conjuntos de dados de treino, valida√ß√£o e teste </code><br><code>devem ser pertinentes, representativos, isentos de erros e completos.‚Äù No </code><br><code>entanto, √© imposs√≠vel garantir este n√≠vel de perfei√ß√£o e consequentemente </code><br><code>imposs√≠vel de alcan√ßar este requisito, pois algumas t√©cnicas de privacidade </code><br><code>introduzem deliberadamente erros (em forma de ru√≠do) nos conjuntos de dados. </code><br><code>Adicionalmente, √© imposs√≠vel a complei√ß√£o total de conjuntos de dados, uma vez </code><br><code>que a natureza destes conjuntos materializa-se numa amostra da realidade e </code><br><code>porventura n√£o inclui todos os dados dispon√≠veis. Sugerimos uma frase mais </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665614_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665614.pdf,1,1,2665614,attachments/2665614.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>      </code><br><code> </code><br><code> </code><br><code> </code><br><code>28 June 2021 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Inquiries:</code><code> </code><br><code>Jussi M√§kinen, Head of Digital Regulation, jussi.makinen@techind.fi, +358 40 900 3066 </code><br><code>Alexander T√∂rnroth, Head of AI Accelerator, alexander.tornroth@techind.fi, +358 40 187 7353 </code><br><code> </code><br><code>AI Act ‚Äì Focus on the Process and Predictability  </code><br><code> </code><br><code> </code><br><code>The Commission has come along with an ambitious proposal to regulate AI. Getting the balance </code><br><code>right in ensuring that obligations drive policy outcomes, while allowing AI innovators sufficient </code><br><code>flexibility in meeting those obligations, is going to be critical. Especially the ethical requirements </code><br><code>will drive forward trust and sustainable use of AI solutions.</code><code> </code><br><code> </code><br><code>AI one of the key technologies for reforming the European industry as AI-driven analysis and </code><br><code>optimization tools can easily be implemented to any process of which data is available. AI plays a </code><br><code>major role in making energy-consuming processes green. Use cases for AI can be numerous and it is </code><br><code>essential to concentrate on regulation on truly horizontal issues, that are good software development </code><br><code>practices. AI is software but the proposed definition covers not only AI, but basically all software.  </code><br><code> </code><br><code>The proposal sets horizontal AI regime on top of harmonised NLF EU law. This structure brings along </code><br><code>risk to cohesion of EU requirements. At least, this is an issue that needs close coordination from the </code><br><code>Commission and close cooperation with the industries. Scope is set so that societally important use </code><br><code>cases (Annex III) and AI-driven safety components and stand-alone AI products meant in the NLF </code><br><code>framework are regarded being high-risk use cases. </code><br><code> </code><br><code>General requirements of the Chapter II are quite well oriented with the process of development of AI </code><br><code>systems. However, the requirements on data ‚Äì complete and free form errors - veer away from </code><br><code>realism. As a rule, requirements should follow good AI development practice, e.g. MLOps. Set of </code><br><code>general and role-based requirements are not essentially well-suited for in-house or specifically </code><br><code>developed AI systems. As the Act is so detailed, it will place a heavy administrative burden, that is </code><br><code>most heavily felt on SME companies.   </code><br><code> </code><br><code>As a new field of technology, there are no existing standards or technical requirements for many usecases of AI. Commission cannot fix this by setting common specifications on its own, without any </code><br><code>interaction with the industries. Here, the proposed regulatory sandboxes should be put into use.  </code><br><code> </code><br><code>Commission has extensive powers to adopt delegated acts to adjust the definition of AI, use cases and </code><br><code>set common specifications for AI. These major powers not only to fine-tune but to essentially adjust </code><br><code>major elements of the Act bring along concern of legal predictability.  </code><br><code> </code><br><code>Our suggestions  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><br><code>Put more emphasis on due process and lineate requirements with good industry practices, such </code><br><code>as MLOps. </code><br><code>ÔÇ∑</code><code> </code><br><code>Remove excessive, casuistic, and highly detailed requirements. </code><br><code>ÔÇ∑</code><code> </code><br><code>Concentrate on ethical issues and take good care of cohesion of regulatory requirements.  </code><br><code>ÔÇ∑</code><code> </code><br><code>Put regulatory sandboxes into proper use when developing common specifications for AI. </code><br><code>ÔÇ∑</code><code> </code><br><code>Introduce proper limits for Commission powers to adopt delegated acts.  </code><br><code>ÔÇ∑</code><code> </code><br><code>Ensure there is a predictable lot for low-risk AI needed to optimise promises in many areas of </code><br><code>society by limiting the scope, definition and introducing strict criteria for adjustment of highrisk use cases.</code><code> </code><br><code>ÔÇ∑</code><code> </code><br><code>To fuel innovation, build clarity on how privacy-preserving technologies can be used to facilitate </code><br><code>processing of personal data. Ensure full lineation with the GDPR to have high-quality learning </code><br><code>data for AI systems. </code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663276_50,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2663276.pdf,82,50,2663276,attachments/2663276.pdf#page=50,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>50 </code><br><code> </code><br><code>Stability </code><br><code>A monitoring tool has been implemented as early as the initial deployment of the ML model, so as to </code><br><code>detect any operational anomaly or model drift. That tool periodically checks several indicators </code><br><code>characterizing the model, the input data, the output score distribution, etc.  </code><br><code>Technical teams indicated during the workshop that it was still too early to determine whether drifts </code><br><code>of the ML model were more or less frequent than the need to reconfigure the enterprise software. </code><br><code>Updating the ML model would nevertheless be simpler than updating the parameters of the business </code><br><code>rule engine for several reasons: it is a simple retraining phase without addition of new features, it is </code><br><code>also fully automatable, and the entirety of model parameters are adjusted without any manual </code><br><code>intervention. Besides, the ML model update ‚Äì from retraining to deployment to production ‚Äì would </code><br><code>not take longer than 2 to 3 days, which is significantly less than a reconfiguration of the enterprise </code><br><code>software. </code><br><code>8.2. </code><br><code>Topic 2: Internal models in banking and insurance </code><br><code>The second topic for the exploratory works conducted by the ACPR pertained to internal risk and </code><br><code>capital requirements models. In fact, candidates on this topic suggested to study use cases in a slightly </code><br><code>different domain. </code><br><code>As a consequence, this topic pivoted toward risk credit modelling, considering both granted to </code><br><code>individuals and to businesses. It consisted of two distinct workshops:</code><code> </code><br><code>-</code><code> </code><br><code>A workshop focusing on credit granting models: those models usually compute a credit score. </code><br><code>The participant to this workshop is a banking group. </code><br><code>-</code><code> </code><br><code>Another workshop relative to so-called behavioural credit models: those models aim to </code><br><code>estimate a probability of default on a given time horizon for a current credit. The participant </code><br><code>to this workshop is a large consulting firm which provides to banking organizations an ML </code><br><code>model construction platform.  </code><br><code>8.2.1. </code><br><code>Regulatory context </code><br><code>Both workshops shared the following initial observations: </code><br><code>ÔÇ∑</code><code> </code><br><code>Classical internal models are generally relatively easy to audit but perform poorly. More </code><br><code>advanced or more complex models should provide a performance improvement, albeit at the </code><br><code>cost of explainability. </code><br><code>ÔÇ∑</code><code> </code><br><code>Regulatory requirements are identified as hindrances to the implementation of innovative </code><br><code>algorithms, especially those based on ML: such requirements pertain to stability of the </code><br><code>resulting models, to their auditability, but also to the transparency and explainability of the </code><br><code>algorithms. </code><br><code>ÔÇ∑</code><code> </code><br><code>Additional challenges related to personal data protection, along with limitations inherent to </code><br><code>the data (in terms of access or completeness, for example), make it challenging to analyse </code><br><code>correlations among multiple variables characterizing customers and their behaviour. </code><br><code>8.3. </code><br><code>Workshop on credit scoring </code><br><code>8.3.1. </code><br><code>Purpose of the exercise </code><br><code>The banking group in question has implemented methodological guidelines for credit scoring models. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665536_14,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665536.pdf,19,14,2665536,attachments/2665536.pdf#page=14,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>14 </code><br><code>manner to demonstrate compliance with regulatory requirements, particularly where digital </code><br><code>services and new technologies are concerned. As a first mover and leader in the AI governance </code><br><code>space, EU should adopt a global approach to standardisation that will enable other </code><br><code>jurisdictions to follow suit in a manner that does not detract from innovation or lead to </code><br><code>unnecessary divergences. In addition, global standards help enable interoperability, establish </code><br><code>a common understanding and set a level playing field for AI based products and services, </code><br><code>which are key to enable not only the Single European market but also to reduce barriers for </code><br><code>international trade. </code><br><code> </code><br><code>Article 41 grants the Commission powers to adopt common specifications via implementing </code><br><code>acts in cases where relevant harmonised European standards do not exist or are found to be </code><br><code>insufficient for the protection of fundamental rights. Following this path despite the existence </code><br><code>of suitable standards from standards bodies other than the ESOs or ISO/IEC carries a risk for </code><br><code>potential divergence from international standards and can adversely impact the ability of </code><br><code>European companies to compete in global markets and also reduce consumer benefit. We </code><br><code>urge lawmakers to avoid in all instances the development of any bespoke (and therefore </code><br><code>region-specific) technical specifications, and instead where necessary rely exclusively on </code><br><code>international standards. Adopting technical specifications outside of an open and consensusbased model can result in frameworks that are not future and technology-proof. Rather, the </code><br><code>EU should rely on global standards developed in organisations such as ISO/IEC JTC1 SC 42 </code><br><code>which champion an inclusive, open and diverse approach to standards creation and are built </code><br><code>on a consensus basis by technical subject matter experts, thus providing above outlined </code><br><code>benefits to all stakeholders in the ecosystem. </code><br><code> </code><br><code>Third-Party Conformity Assessment </code><br><code>Conformity assessment for Artificial Intelligence technologies is a nascent field for which </code><br><code>there is neither a commonly understood practice nor the established conformity assessment </code><br><code>infrastructure necessary to carry out the requisite assessments contemplated by the </code><br><code>proposed Act. For this reason, there are significant practical and logistical concerns regarding </code><br><code>precisely how Notified Bodies, once identified, accredited, and designated, would carry out </code><br><code>the task of assessing the conformity of certain high-risk AI systems with the broad </code><br><code>requirements outlined in the Act.  Given that tools and processes for assessing compliance in </code><br><code>this field are still emerging, it is unclear how existing facilities would have to be transformed </code><br><code>to perform these tasks in a timely way and with the needed skill and expertise, and what type </code><br><code>of guidance would be needed to ensure appropriate capacity of the testing bodies. Logistical </code><br><code>problems, including the lack of sufficient designated Notified Bodies, may also lead to backlog </code><br><code>for testing bodies, which could significantly slow down the adoption of certain AI technologies </code><br><code>in the EU market.  </code><br><code> </code><br><code>Decision 768/2008 and the corresponding provisions contained in the AI Act require that any </code><br><code>third-party conformity assessment be carried out by a Notified Body established under EU law </code><br><code>(i.e., located in the territory of an EU member state). While this requirement derives from the </code><br><code>New Legislative Framework, in an area in which reliance on conformity assessment yields a </code><br><code>number of technical and practical questions, we would strongly encourage lawmakers to inbuild greater flexibility as concerns the acceptance of international test results, including as a </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665642_31,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665642.pdf,31,31,2665642,attachments/2665642.pdf#page=31,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Regulators might define regulatory ceilings (eg produce guidance for contexts where price discrimination</code><br><code>is acceptable), moratoria (no use of ADM in certain sectors until safe ADM has been developed) or even ban</code><br><code>ADM in specific contexts (eg facial recognition in the</code><br><code>supply of commercial health care services). Sector-specific legislation might be necessary to protect fundamental rights, for instance where ADM reduces equal</code><br><code>access to utilities that are important to the public at</code><br><code>large, such as credit, education, or health insurance.</code><br><code>Such legislation might be accompanied by sunset</code><br><code>clauses, meaning that a law automatically ceases to exist</code><br><code>unless the legislator explicitly decides to evaluate and review the law. As stated earlier, there is scope for regulatory sandboxes, and for cooperation between oversight</code><br><code>bodies to share and exchange experiences. Regulators</code><br><code>might establish trusted organizations to offer opportunities for scientific research and to keep track of developments and the effectiveness of DPIA, including</code><br><code>fundamental rights impact assessments.</code><br><code>Concluding observations</code><br><code>Fundamental rights, an important body of law that fortifies individual freedom in society, are a relatively new</code><br><code>set of rules for private controllers. This article works</code><br><code>towards a practical approach for private controllers to</code><br><code>conduct a DPIA as required by the GDPR, and to engage them in a productive discussion about the implementation of technical and organizational measures to</code><br><code>mitigate fundamental rights impacts at individual and</code><br><code>societal levels by ADM at early design and development stages. It seeks to interrogate controllers by way</code><br><code>of four benchmarks about potential fundamental</code><br><code>rights risks due to the use of ADM systems, whether</code><br><code>their design is appropriate, and how they can correct</code><br><code>inappropriate functioning of their ADM system. The</code><br><code>approach will help controllers to achieve better compliance with fundamental rights, while it also offers</code><br><code>them opportunities to explain and account for their</code><br><code>use of ADM. Meanwhile, regulators should assist private controllers with appropriate regulatory guidance,</code><br><code>and, where necessary, with regulatory measures as to</code><br><code>how fundamental rights are engrained in DPIA (eg by</code><br><code>developing practical questionnaires). Interdisciplinary</code><br><code>scientific research may help regulators forward in this</code><br><code>space. Though an initial, practical concept, this approach to a fundamental rights impact assessment presages a way forward to ensure that private controller</code><br><code>ADM systems of the future are better lined up with</code><br><code>fundamental rights, including personal freedoms and</code><br><code>human dignity.</code><br><code>doi:10.1093/idpl/ipz028</code><br><code>Advance Access Publication 6 March 2020</code><br><code>106</code><br><code>ARTICLE</code><br><code>International Data Privacy Law,</code><code> 2020, Vol. 10, No. 1</code><br><code>Downloaded from https://academic.oup.com/idpl/article/10/1/76/5788543 by University Library, University of Amsterdam user on 19 June 2021</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665503_11,other,../24212003_requirements_for_artificial_intelligence/attachments/2665503.pdf,11,11,2665503,attachments/2665503.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Seite 11 von 11 </code><br><code> </code><br><code> </code><br><code>Unbenommen dessen ist es richtig, davon auszugehen, dass die Regelungen die </code><br><code>zivilrechtliche Haftung zumindest der Anbieter mit pr√§gen sollten; andernfalls m√ºssten </code><br><code>sie sich auf zwei Haftungsma√üst√§be einstellen: einerseits auf einen Ma√üstab des </code><br><code>√∂ffentlichen Sicherheitsrechts und anderseits auf einen zivilrechtlichen </code><br><code>Haftungsma√üstab. Dies d√ºrfte unzumutbar sein. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2636017_6,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2636017.pdf,6,6,2636017,attachments/2636017.pdf#page=6,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>6 </code><br><code> </code><br><code>only individual rights and access to data. The regulation must be respecting labour rights </code><br><code>and data rights at national level and should be an addition, not a substitution, of cogovernance models respecting social relations and dialogue. </code><br><code>-</code><code> </code><br><code>Regulatory sandboxes should not be allowed for AI systems that are implemented at the </code><br><code>workplace. </code><br><code>-</code><code> </code><br><code>We need an effective appeal procedure and remedies enabling individuals to address </code><br><code>harmful or incorrect decisions made by AI. </code><br><code> </code><br><code>c)</code><code> </code><code>Skills and training/employability of workers  </code><br><code>-</code><code> </code><br><code>Training and upskilling regarding AI based systems is essential to provide employees with </code><br><code>the necessary skills when AI systems are implemented and to ensure that employees have </code><br><code>a basic understanding of how an AI system works or decides. Besides digital literacy, </code><br><code>employees also need to understand how an AI system could possibly impact on their </code><br><code>working conditions, health and safety and other relevant areas.  </code><br><code>-</code><code> </code><br><code>Collective bargaining is the best way to identifying the training needs in the specific </code><br><code>company context. </code><br><code>-</code><code> </code><br><code>To raise awareness about the ethical aspects of AI and to promote ethical and </code><br><code>trustworthy AI, it is important to integrate ethics in training for engineers and to promote </code><br><code>a more interdisciplinary approach for R & D. There should be a special training for those </code><br><code>operating AI systems or to counteract if necessary. </code><br><code>-</code><code> </code><br><code>We need diversity of skills and competences, not reduced to STEM related upskilling. </code><br><code>With an increase of the use of AI systems, typical ‚Äúhuman‚Äù skills like empathy or creativity </code><br><code>become even more important and should be promoted. Investment in training should not </code><br><code>be exclusive but address the career paths of workers across job categories. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662226_1,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2662226.pdf,8,1,2662226,attachments/2662226.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>s</code><br><code>ANEC comments on the European Commission </code><br><code>proposal for an Artificial Intelligence Act </code><br><code> </code><br><code>(Regulation laying down harmonised rules on artificial intelligence </code><br><code>and amending certain Union legislative acts) </code><br><code>COM(2021) 206 final, 2021/0106 (COD) </code><br><code>POSITION PAPER </code><br><code>European Association for the Co-ordination of </code><br><code>Consumers Representation in Standardisation aisbl </code><br><code>Rue d‚ÄôArlon 80 - B-1040 Brussels, Belgium </code><br><code>T: +32-2-7432470 / anec@anec.eu</code><code> / </code><code>www.anec.eu </code><br><code>Ref: ANEC-DIGITAL-2021-G-071  </code><br><code>ANEC is supported financially by </code><br><code>the European Union & EFTA </code><br><code> </code><br><code>July 2021  </code><br><code>Raising standards for consumers </code><br><code>Contact:</code><code> Chiara Giovannini </code><br><code>Chiara.Giovannini@anec.eu </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663405_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2663405.pdf,10,3,2663405,attachments/2663405.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>3/10</code><br><code>Position Paper on the EU AI Draft Regulation  | IDEMIA </code><code> </code><br><code>Date | #ref                          </code><br><code> </code><br><code> </code><br><code>Agenda </code><br><code>Introduction ....................................................................................................................................................... 4 </code><br><code>General remarks ................................................................................................................................................ 4 </code><br><code> </code><br><code>Technology neutrality shall be a mantra ............................................................................................... 4 </code><br><code> </code><br><code>Risk based approach needs to be improved ......................................................................................... 5 </code><br><code>Detailed comments ........................................................................................................................................... 5 </code><br><code> </code><br><code>Real-time biometric identification, a high risk AI system (article 5) ..................................................... 5 </code><br><code> </code><br><code>Requirements for high-risk AI Systems.................................................................................................. 6 </code><br><code> </code><br><code>Ex ante</code><code> versus ex post conformity assessment ..................................................................................... 6 </code><br><code> </code><br><code>Data Governance (article 10) ................................................................................................................. 7 </code><br><code> </code><br><code>Technical documentation (article 11) and transparency (article 13 and 52); access to data and </code><br><code>documentation (article 64) ............................................................................................................................ 7 </code><br><code> </code><br><code>Certification and test approach ............................................................................................................. 8 </code><br><code>CONCLUSION ..................................................................................................................................................... 8 </code><br><code>RECOMMENDATIONS ...................................................................................................................................... 10 </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665548_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665548.pdf,4,1,2665548,attachments/2665548.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Hogan Lovells International LLP is a limited liability partnership registered in England and Wales with registered number OC323639 and is authorised and regulated by the </code><br><code>Solicitors Regulation Authority of England and Wales (SRA ID 449616).  Registered office and principal place of business: Atlantic House, Holborn Viaduct, London </code><br><code>EC1A 2FG. </code><br><code>""Hogan Lovells"" is an international legal practice that includes Hogan Lovells International LLP and Hogan Lovells US LLP, with offices in:  Alicante  Amsterdam  Baltimore  </code><br><code>Beijing  Birmingham  Boston  Brussels  Colorado Springs  Denver  Dubai  Dusseldorf  Frankfurt  Hamburg  Hanoi  Ho Chi Minh City  Hong Kong  Houston  Johannesburg  </code><br><code>London  Los Angeles  Luxembourg  Madrid  Mexico City  Miami  Milan  Minneapolis  Monterrey  Moscow  Munich  New York  Northern Virginia  Paris  Perth  Philadelphia  </code><br><code>Rome  San Francisco  S√£o Paulo  Shanghai  Silicon Valley  Singapore  Sydney  Tokyo  Warsaw  Washington, D.C.   Associated Offices:  Budapest  Jakarta  Riyadh  </code><br><code>Shanghai FTZ  Ulaanbaatar  Zagreb.   Business Services Centers:  Johannesburg  Louisville.   Legal Services Center: Berlin. </code><br><code>The word ""partner"" is used to describe a partner or member of Hogan Lovells International LLP, Hogan Lovells US LLP or any of their affiliated entities or any employee or </code><br><code>consultant with equivalent standing.  Certain individuals, who are designated as partners, but who are not members of Hogan Lovells International LLP, do not hold </code><br><code>qualifications equivalent to members.  For more information about Hogan Lovells, the partners and their qualifications, see www.hoganlovells.com. </code><br><code> </code><br><code>Hogan Lovells International LLP </code><br><code>Atlantic House </code><br><code>Holborn Viaduct </code><br><code>London EC1A 2FG </code><br><code>T  +44 20 7296 2000 </code><br><code>F  +44 20 7296 2001 </code><br><code>www.hoganlovells.com </code><br><code> </code><br><code> </code><br><code> </code><br><code>6 August 2021 </code><br><code> </code><br><code> </code><br><code>By email </code><br><code> </code><br><code> </code><br><code>Dear Sir/Madam, </code><br><code>Response to the European Commission‚Äôs proposal for a regulation on harmonised rules for </code><br><code>artificial intelligence </code><br><code>We are responding to the European Commission‚Äôs proposal for the establishment of harmonised </code><br><code>rules governing artificial intelligence (the </code><code>Draft Regulation</code><code>).  </code><br><code> </code><br><code>The Draft Regulation is an ambitious and comprehensive framework that provides a useful starting </code><br><code>point for an important discussion that needs to occur between policymakers and industry on the </code><br><code>appropriate standards that should apply to the development and use of artificial intelligence. We </code><br><code>welcome the efforts to create a pragmatic, risk-based approach that has been taken by the </code><br><code>Commission in a number of areas, which acknowledges that not all AI systems should be subject </code><br><code>to additional regulation. </code><br><code> </code><br><code>Set out below are our comments on a number of principal areas in which we consider the Draft </code><br><code>Regulation could be further clarified in order to achieve its purposes in a more effective manner. </code><br><code> </code><br><code>Definition of ‚ÄòAI system‚Äô </code><br><code> </code><br><code>The proposed definition of what constitutes an ‚ÄòAI system‚Äô appears to be unnecessarily broad.  </code><br><code> </code><br><code>Many of the stated concerns associated with AI are related to cases where the technology is </code><br><code>designed to behave autonomously, including for example where a system does not perform as </code><br><code>expected, produces discriminatory decisions or generates inaccurate outputs, which may go </code><br><code>undetected due to a lack of interpretability. </code><br><code> </code><br><code>However, the proposed definition does not currently contemplate situations in which AI systems </code><br><code>are being at least partially operated and overseen by humans in a live environment, meaning they </code><br><code>are only semi-autonomous. Where this arises, and there is a degree of human involvement, then </code><br><code>many of the risks outlined above can be suitably mitigated. We therefore suggest that the </code><br><code>Commission consider narrowing the definition of an AI system so that it takes into account the </code><br><code>degree of autonomy that the AI system exercises, including for example when determining if it falls </code><br><code>within the scope of the Draft Regulation and should be considered to be ‚Äòhigh-risk‚Äô. </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665616_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665616.pdf,3,2,2665616,attachments/2665616.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Need of new European regulations in line with technical progress</code><code> </code><br><code>6. The public interest of road safety has justified the European authorities concentrating their efforts, as a matter of priority, on the vehicle </code><br><code>connected to emergency aid. European Parliament and Council Regulation n¬∞ 2015/758 defines the legal framework for the deployment of </code><br><code>the on-board emergency call system (eCall)</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>. It states that the mandatory installation of this device must, as early as 2018, be without </code><br><code>prejudice to the </code><code>‚Äúright of all stakeholders, such as car manufacturers and independent operators, to offer additional emergency and / value </code><br><code>added, in parallel or on the basis of the embedded eCall system based on number 112</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>.‚Äù</code><code> Article 5 (7) of that regulation requires manufacturers </code><br><code>to make </code><code>‚Äúthe on-board eCall system based on number 112 accessible to all independent operators at a reasonable cost not exceeding a </code><br><code>nominal amount and without discrimination for repair and maintenance.‚Äù </code><br><code>7. In the event of failure to comply with these provisions, Article 11 of this regulation requires Member States to enforce </code><code>‚Äúeffective, </code><br><code>proportionate and dissuasive penalties‚Äù</code><code> which they must notify to the European Commission. Beyond sanctions better defined and controlled </code><br><code>by the European Commission, technical solutions to be put in place are evaluated and proposed in order to achieve the objective of opening </code><br><code>up the market for aftermarket services covered by the regulation in Article 5 (7). </code><br><code> </code><br><code>8. Thus, under Article 12, the European Parliament and the Council empower the European Commission to establish the specifications for </code><br><code>an interoperable, standardized, secure and open-access platform. The study of technical solutions for access to vehicle data, carried out by </code><br><code>the C-ITS expert group, revealed three types of platforms: two would be internal to the vehicle, while another would be external: an application </code><br><code>platform or an interface, which would be embedded in the vehicle; a data server platform. </code><br><code>9. These options were assessed in the framework of the Commission's work on access to data and on-board resources to be completed. </code><br><code>The European Commission Informed that the conclusions of this analysis serve as a basis for modernizing European provisions on access </code><br><code>to technical information for the maintenance and repair of connected vehicles. </code><br><code>10. National authorities and many specialists draw attention to the manufacturers‚Äô intellectual property rights, which oblige repairers to refrain </code><br><code>from any manipulation that would affect the initial configuration of the connected vehicle. The maintenance and repair of this vehicle, whose </code><br><code>sensors and actuators are governed by millions of lines of computer code thanks to the manufacturer's software, depends on the access to </code><br><code>specific data and AI information. </code><br><code>11. According to a study carried out in France by the National Institute of Industrial Property concerning the digital transformation of the </code><br><code>economy, ‚Äú</code><code>the search for a balance between the use of technology and the rules of competition can encourage the phenomenon of </code><br><code>standardization, the main objective of which is to harmonize technologies for the benefit of consumers</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code>.‚Äù</code><code> </code><br><code>12. The principle of non-discriminatory access embodied in the proposed ISO for the connected vehicle is reproduced in the new European </code><br><code>Regulation on the vehicle type approval. These new provisions, in particular those regarding recital 51, clearly prohibit the use of advanced </code><br><code>technologies to impede access by independent operators to data: </code><code>‚Äútechnological advances introducing new methods or techniques for the </code><br><code>diagnosis and repair of vehicles, such as remote access to vehicle information and software, should not weaken the objectives of this </code><br><code>regulation as regards access to repair and maintenance information for independent</code><code> </code><code>operators</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>.‚Äù </code><br><code>Answering the Consultation  </code><br><code>13. In view of difficulties arising from the use of advanced technologies to steer the consumer towards the manufacturer's network, being </code><br><code>the dominant player in the first six years of the vehicle, appropriate technical and legal solutions are urgent to be enforced.  </code><br><code>13.1. FNA representatives welcomed EU Commission study on </code><code>‚ÄòAccess to In-Vehicle Data and Resources‚Äô</code><code>  which indicates that the </code><br><code>‚Äúcentralisation of in-vehicle data as currently implemented by some market players might in itself not be sufficient to ensure fair and </code><br><code>undistorted competition between service providers.‚Äù</code><code> This centralisation system is the ‚Äú</code><code>extended-vehicle concept‚Äù</code><code>, a technological solution </code><br><code>that the EU study clearly shows that it is a way to exacerbate tensions on the aftermarket by strengthening unfair and distorted competition. </code><br><code>FNA urges to approve the right regulatory framework on access data and AI, because of the present negative economic impact on EU SMEs </code><br><code>who are being already cut out of the market, in particular independent repairers and third enterprises also involved in downstream activities. </code><br><code>13.2. The present new Regulation laying down harmonised rules on AI should correspond to the principles advocated by FNA for independent </code><br><code>repairers and already supported by the European Commission as follows: </code><br><code>-Unrestricted access to vehicle repair and maintenance data,  </code><br><code>-Effective competition in the market of services providing automotive data</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code>. </code><br><code>13.3. An effective European legal framework is, indeed, indispensable in order to achieve an ecosystem of trust and to avoid a fragmentation </code><br><code>of national rules and initiatives. The new Proposal of Regulation laying down harmonized rules on AI, in particular option 3 imposing mandatory </code><br><code>legal requirements for AI being used by car manufacturers is therefore welcomed by DBR and FNA representatives. </code><br><code> </code><br><code>2</code><code> Regulation (EU) No 2015/758 of the European Parliament and of the Council of 29 April 2015 on type-approval requirements for the deployment of the onboard eCall system 112 and amending Directive 2007/46 / EC, Official Journal of the European Union n¬∞ L 123/77 of 19 May 2015. </code><br><code>3</code><code> Paragraph 15 of the explanatory statement to Regulation (EU) No 2015/758 </code><br><code>4</code><code> INPI, </code><code>¬´ la propri√©t√© intellectuelle et la transformation num√©rique de l‚Äô√©conomie ¬ª,</code><code> contribution de M. Fr√©d√©ric BOURGUET et Mme Cristina BAYONA </code><br><code>PHILIPPINE, p.259, note 320 </code><code>https://www.inpi.fr/fr/services-et-prestations/etude-pi-et-economie-numerique </code><br><code>5</code><code> European Regulation of the European Parliament and of the Council on the type-approval and market surveillance of motor vehicles  </code><br><code>6</code><code> Draft Regulation of the European Parliament and of the Council on the type-approval and market surveillance of motor vehicles, op. cit. Paragraph 36. </code>",POSITIVE
fitz_2665494_2,company,../24212003_requirements_for_artificial_intelligence/attachments/2665494.pdf,2,2,2665494,attachments/2665494.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>2 </code><br><code> </code><br><code>supply of water, gas, heating and electricity are high-risk. This causes uncertainty in determining a </code><br><code>safety component (e.g. what exactly constitutes road traffic management and operation?). Annex III </code><br><code>would gain clarity by amending it to limit its scope to AI systems which are developed with a clearly </code><br><code>safety-related intended use. Further, only those AI systems developed with a safety-related intended </code><br><code>use to operate road infrastructure components should be in scope, therefore excluding those which </code><br><code>are integrated in vehicles; otherwise these would conflict with Regulation (EU) 2018/858 and Article </code><br><code>2.2. </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665480_51,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665480.pdf,64,51,2665480,attachments/2665480.pdf#page=51,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>   </code><br><code>46 </code><br><code>complaint and inform the complainant of the progress and the outcome of the investigation </code><br><code>within a reasonable period, in particular if further investigation or coordination with another </code><br><code>supervisory authority is necessary.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">77</code><code> Article 77 of the GDPR further provides for a ‚Äòright to </code><br><code>lodge a complaint with a supervisory authority‚Äô for data subjects. </code><br><code>While the Proposal does not preclude national competent authorities from establishing a </code><br><code>complaints mechanism on their own initiative, the absence of harmonisation of such initiatives </code><br><code>renders it likely that individuals in different EU Member States face different levels of </code><br><code>protection. The inclusion of a provision which mandates a complaints mechanism, similar to </code><br><code>the GDPR‚Äôs, would be beneficial to the Proposal‚Äôs aim of ensuring the protection of </code><br><code>fundamental rights. Moreover, it would contribute to the rule of law across the Union. Firstly, </code><br><code>it would provide individuals with a clear procedure to follow in case they suspect that an AI </code><br><code>system they are subjected to does not meet the requirements of Title III or operates in </code><br><code>contravention of Title II of the Proposal. Secondly, it would help national competent authorities </code><br><code>to fulfil their tasks, since these complaints can lead to more effective monitoring and evaluation </code><br><code>of problematic AI practices.  </code><br><code>4.2.4</code><code> </code><code>The Proposal‚Äôs enforcement mechanism is inadequate  </code><br><code>In addition to the absence in the Proposal of the individual affected by AI systems, the </code><br><code>Proposal‚Äôs enforcement architecture copes with several practical problems. For the proposed </code><br><code>Regulation to be Legally Trustworthy, it must conform to rule of law standards, and its </code><br><code>enforcement must therefore be congruent with the promulgated norms. This may be </code><br><code>undermined by the fact that the current enforcement structure is relatively complex and heavily </code><br><code>relies on the competencies of national authorities, which may be uneven and under-resourced </code><br><code>(a). Furthermore, the role of notified bodies and the scope of the procedural rights conferred </code><br><code>on the Proposal‚Äôs legal subjects could be further clarified (b). </code><br><code>a)</code><code> </code><code>The enforcement structure hinges too much on national competencies </code><br><code>The enforcement of the Proposal risks being undermined by the practical aspects of Member </code><br><code>State competencies. Article 59 of the Proposal enables Member States to designate or establish </code><br><code>national competent authorities ‚Äúfor the purpose of ensuring the application and implementation </code><br><code>of this regulation.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">78</code><code> Unless otherwise provided for by the Member State in question, these </code><br><code>authorities will be required to act as </code><code>both </code><code>‚Äònotifying authority‚Äô and ‚Äòmarket surveillance </code><br><code>authority‚Äô as part of a combined ‚Äònational supervisory authority.‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">79</code><code> Notified bodies are </code><br><code>responsible for verifying the conformity of high-risk systems, and market surveillance </code><br><code>authorities are responsible for the evaluation of high-risk AI systems ‚Äúin respect of its </code><br><code>compliance with all the requirements and obligations‚Äù of the Proposal.</code><code style=""font-weight: 1000; background-color: #FF0000;"">80</code><code> If a given system is </code><br><code>not compliant, the market surveillance authority shall take ‚Äúall appropriate provisional </code><br><code>measures to prohibit or restrict the AI system's being made available on its national market, to </code><br><code>withdraw the product from that market or to recall it.‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">81</code><code> There are three practical concerns </code><br><code>about this setup. </code><br><code> </code><br><code>77</code><code>  See European Parliament and Council, ‚ÄúRegulation (EU) 2016/679 of the European Parliament and of the </code><br><code>Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data </code><br><code>and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection </code><br><code>Regulation), OJ L 119, 4.5.2016‚Äù, 2016, 1‚Äì88 (‚ÄúGDPR‚Äù), Article 57. </code><br><code>78</code><code>  European Commission, ‚ÄúThe Proposal,‚Äù Article 59(1). </code><br><code>79</code><code>  European Commission, ‚ÄúThe Proposal,‚Äù Article 59(2). </code><br><code>80</code><code>  European Commission, ‚ÄúThe Proposal,‚Äù Article 65(2) </code><br><code>81</code><code>  European Commission, ‚ÄúThe Proposal,‚Äù Article 65(5). </code>",POSITIVE
fitz_2665440_4,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665440.pdf,8,4,2665440,attachments/2665440.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>  </code><br><code> </code><br><code> </code><br><code>www.homodigitalis.gr</code><code>, ŒëœáŒΩŒπŒ¨Œ¥œâŒΩ 17-19, 118 54,  ŒëŒ∏ŒÆŒΩŒ±-Œ°ŒøœÖœÜ, ŒëœÑœÑŒπŒ∫ŒÆ, </code><code>info@homodigitalis.gr</code><code> </code><br><code> </code><br><code>œÖœÄŒ¨œÅœáŒøœÖŒΩ œÄŒµœÅŒπœÄœÑœéœÉŒµŒπœÇ Œ∫Œ±œÑŒ¨ œÑŒπœÇ ŒøœÄŒøŒØŒµœÇ Œ∑ ""œÉŒµ œçœÉœÑŒµœÅŒø œáœÅœåŒΩŒø"" œáœÅŒÆœÉŒ∑ Œ≤ŒπŒøŒºŒµœÑœÅŒπŒ∫ŒÆœÇ </code><br><code>œÑŒ±œÖœÑŒøœÄŒøŒØŒ∑œÉŒ∑œÇ ŒºœÄŒøœÅŒµŒØ ŒΩŒ± ŒøŒ¥Œ∑Œ≥ŒÆœÉŒµŒπ œÉœÑŒ∑ŒΩ œÄŒ±œÅŒ±Œ≤ŒØŒ±œÉŒ∑ œÑœâŒΩ Œ∏ŒµŒºŒµŒªŒπœâŒ¥œéŒΩ Œ¥ŒπŒ∫Œ±ŒπœéŒºŒ¨œÑœâŒΩ œÑœâŒΩ </code><br><code>Œ±œÑœåŒºœâŒΩ, ŒµŒΩœé ŒµŒØŒΩŒ±Œπ Œ¥œÖœÉœáŒµœÅŒÆœÇ Œ∑ ŒµŒΩŒΩŒøŒπŒøŒªŒøŒ≥ŒπŒ∫ŒÆ œÄœÅŒøœÉŒ≠Œ≥Œ≥ŒπœÉŒ∑ œÑŒ∑œÇ ""œÉœáŒµŒ¥œåŒΩ œÑŒ±œÖœÑœåœáœÅŒøŒΩŒ∑œÇ </code><br><code>Œ∂œâŒΩœÑŒ±ŒΩŒÆœÇ ŒºŒµœÑŒ¨Œ¥ŒøœÉŒ∑œÇ"", œÄŒøœÖ œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒµŒØ Œ∑ œÄœÅœåœÑŒ±œÉŒ∑ (Œ±ŒπœÑ. œÉŒ∫Œ≠œàŒ∑ 8). ŒëœÜ' ŒµœÑŒ≠œÅŒøœÖ, Œ∑ </code><br><code>Œ±œÄŒ±Œ≥œåœÅŒµœÖœÉŒ∑ œÄŒµœÅŒπŒøœÅŒØŒ∂ŒµœÑŒ±Œπ œÉŒµ Œ¥Œ∑ŒºŒøœÉŒØœâœÇ œÄœÅŒøœÉŒ≤Œ¨œÉŒπŒºŒ± ŒºŒ≠œÅŒ∑, œÑŒ± ŒøœÄŒøŒØŒ± œÉœçŒºœÜœâŒΩŒ± ŒºŒµ œÑŒøŒΩ </code><br><code>ŒøœÅŒπœÉŒºœå œÄŒøœÖ Œ¥ŒØŒΩŒµœÑŒ±Œπ œÉœÑŒ∑ŒΩ œÄœÅœåœÑŒ±œÉŒ∑ [Œ¨œÅŒ∏œÅŒø 3(39) Œ∫Œ±Œπ Œ±ŒπœÑ. œÉŒ∫Œ≠œàŒ∑ 9], Œ¥ŒµŒΩ Œ∫Œ±ŒªœçœÄœÑŒøœÖŒΩ œÑŒøœÖœÇ </code><br><code>ŒµœÄŒπŒ≥œÅŒ±ŒºŒºŒπŒ∫ŒøœçœÇ œáœéœÅŒøœÖœÇ, œåœÄœâœÇ Œ≥ŒπŒ± œÄŒ±œÅŒ¨Œ¥ŒµŒπŒ≥ŒºŒ± chatrooms. ŒúŒ≠ŒΩŒµŒπ Œ±œÉŒ±œÜŒ≠œÇ ŒµŒ¨ŒΩ ŒºœÄŒøœÅŒøœçŒΩ </code><br><code>ŒµŒΩœÑŒ≠ŒªŒµŒπ ŒΩŒ± œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒ∑Œ∏ŒµŒØ Œ∑ ŒµŒæ Œ±œÄŒøœÉœÑŒ¨œÉŒµœâœÇ Œ≤ŒπŒøŒºŒµœÑœÅŒπŒ∫ŒÆ Œ±ŒΩŒ±Œ≥ŒΩœéœÅŒπœÉŒ∑ œÉŒµ Œ±œÖœÑŒ≠œÇ œÑŒπœÇ </code><br><code>œÄŒµœÅŒπœÄœÑœéœÉŒµŒπœÇ. </code><br><code> </code><br><code>Œ§Œ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ± œÑŒµœáŒΩŒ∑œÑŒÆœÇ ŒΩŒøŒ∑ŒºŒøœÉœçŒΩŒ∑œÇ œÄŒøœÖ œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒøœçŒΩœÑŒ±Œπ Œ≥ŒπŒ± œÑŒø (Œ¥) ŒºœÄŒøœÅŒøœçŒΩ Œ∫Œ±œÑ‚Äô </code><br><code>ŒµŒæŒ±ŒØœÅŒµœÉŒ∑ ŒΩŒ± œáœÅŒ∑œÉŒπŒºŒøœÄŒøŒπŒ∑Œ∏ŒøœçŒΩ œÖœÄœå œÉœÖŒ≥Œ∫ŒµŒ∫œÅŒπŒºŒ≠ŒΩŒµœÇ œÉœÖŒΩŒ∏ŒÆŒ∫ŒµœÇ œåœÄœâœÇ Œ∑ Œ±ŒΩŒ±Œ∂ŒÆœÑŒ∑œÉŒ∑ </code><br><code>œÉœÖŒ≥Œ∫ŒµŒ∫œÅŒπŒºŒ≠ŒΩœâŒΩ œÄŒπŒ∏Œ±ŒΩœéŒΩ Œ∏œÖŒºŒ¨œÑœâŒΩ ŒµŒΩœåœÇ ŒµŒ≥Œ∫ŒªŒÆŒºŒ±œÑŒøœÇ, Œ∑ œÄœÅœåŒªŒ∑œàŒ∑ ŒµœÄŒπŒ∫ŒµŒØŒºŒµŒΩœâŒΩ Œ±œÄŒµŒπŒªœéŒΩ </code><br><code>Œ∫Œ±Œπ Œ∑ ŒµœçœÅŒµœÉŒ∑ Œ±œÑœåŒºœâŒΩ œÄŒøœÖ Œ∫Œ±œÑŒ∑Œ≥ŒøœÅŒøœçŒΩœÑŒ±Œπ Œ≥ŒπŒ± ŒµŒ≥Œ∫ŒªŒÆŒºŒ±œÑŒ± œÄŒøœÖ œÑŒπŒºœâœÅŒøœçŒΩœÑŒ±Œπ ŒºŒµ œÄŒøŒπŒΩŒÆ </code><br><code>œÜœÖŒªŒ¨Œ∫ŒπœÉŒ∑œÇ ŒÆ ŒµŒΩœÑŒøŒªŒÆ Œ∫œÅŒ¨œÑŒ∑œÉŒ∑œÇ Œ≥ŒπŒ± ŒºŒ≠Œ≥ŒπœÉœÑŒø œáœÅŒøŒΩŒπŒ∫œå Œ¥ŒπŒ¨œÉœÑŒ∑ŒºŒ± Œ≠œâœÇ œÑŒøœÖŒªŒ¨œáŒπœÉœÑŒøŒΩ œÑœÅŒØŒ± </code><br><code>œáœÅœåŒΩŒπŒ±.  </code><br><code> </code><br><code>Œ†œÅŒøŒ≤ŒªŒ∑ŒºŒ±œÑŒπŒ∫ŒÆ œÜŒ±ŒØŒΩŒµœÑŒ±Œπ Œ∑ ŒµœÄŒπŒªŒøŒ≥ŒÆ œÑŒ∑œÇ œÄœÅœåœÑŒ±œÉŒ∑œÇ ŒΩŒ± ŒµœÄŒπœÑœÅŒ≠œàŒµŒπ œÑŒ∑ŒΩ œáœÅŒÆœÉŒ∑ œÑœâŒΩ </code><br><code>œÉœÖŒ≥Œ∫ŒµŒ∫œÅŒπŒºŒ≠ŒΩœâŒΩ œÉœÖœÉœÑŒ∑ŒºŒ¨œÑœâŒΩ Œ§Œù œáœâœÅŒØœÇ ŒªŒÆœàŒ∑ œÄœÅŒøŒ∑Œ≥ŒøœçŒºŒµŒΩŒ∑œÇ Œ¨Œ¥ŒµŒπŒ±œÇ, œÉŒµ œÄŒµœÅŒπœÄœÑœéœÉŒµŒπœÇ </code><br><code>œÄŒøœÖ Œ∫Œ¨œÑŒπ œÑŒ≠œÑŒøŒπŒø ŒµŒØŒΩŒ±Œπ œÄœÅŒ±Œ∫œÑŒπŒ∫Œ¨ Œ∫Œ±Œπ Œ±ŒΩœÑŒπŒ∫ŒµŒπŒºŒµŒΩŒπŒ∫Œ¨ Œ±Œ¥œçŒΩŒ±œÑŒø (Œ±ŒπœÑ. œÉŒ∫Œ≠œàŒ∑ 20-22). </code><br><code>ŒîœÖœÉœÑœÖœáœéœÇ, œÖœÄŒ¨œÅœáŒµŒπ œÄŒµŒØœÅŒ± Œ±œÄœå Œ±ŒΩœÑŒØœÉœÑŒøŒπœáŒµœÇ ŒµŒæŒ±ŒπœÅŒ≠œÉŒµŒπœÇ Œ¨ŒªŒªœâŒΩ ŒΩŒøŒºŒøŒ∏ŒµœÑŒ∑ŒºŒ¨œÑœâŒΩ, œåœÄœâœÇ ŒøŒπ </code><br><code>Œ¥ŒπŒ±œÑŒ¨ŒæŒµŒπœÇ œÄŒøœÖ Œ±œÜŒøœÅŒøœçŒΩ œÑŒ∑ŒΩ Œ¨œÅœÉŒ∑ œÑŒøœÖ Œ±œÄŒøœÅœÅŒÆœÑŒøœÖ, œåœÄŒøœÖ ŒøŒπ Œ±œÅœáŒ≠œÇ ŒµœÄŒπŒ≤ŒøŒªŒÆœÇ œÑŒøœÖ ŒΩœåŒºŒøœÖ </code><br><code>ŒµŒæŒ±ŒΩœÑŒªŒøœçŒΩ œÑŒ± œåœÅŒπŒ± œÑœâŒΩ ŒµŒæŒ±ŒπœÅŒ≠œÉŒµœâŒΩ Œ∫Œ±Œπ Œ∫Œ±Œ∏ŒπœÉœÑŒøœçŒΩ œÑŒ∑ŒΩ Œ¥ŒπŒ±Œ¥ŒπŒ∫Œ±œÉŒØŒ± ŒªŒÆœàŒ∑œÇ Œ¨Œ¥ŒµŒπŒ±œÇ </code><br><code>Œ±ŒΩŒµœÜŒ¨œÅŒºŒøœÉœÑŒ∑. Œ£œÖŒΩŒµœÄœéœÇ, œÑŒø ŒµŒΩŒ¥ŒµœáœåŒºŒµŒΩŒø ŒµŒæŒ±ŒπœÅŒµœÑŒπŒ∫ŒÆœÇ œáœÅŒÆœÉŒ∑œÇ ŒµŒΩœåœÇ œÑŒ≠œÑŒøŒπŒøœÖ œÉœÖœÉœÑŒÆŒºŒ±œÑŒøœÇ </code><br><code>Œ∏Œ± Œ≠œÄœÅŒµœÄŒµ ŒΩŒ± œÄŒµœÅŒπŒøœÅŒπœÉœÑŒµŒØ œÉœÑŒπœÇ œÄŒµœÅŒπœÄœÑœéœÉŒµŒπœÇ ŒªŒÆœàŒ∑œÇ œÄœÅŒøŒ∑Œ≥ŒøœçŒºŒµŒΩŒ∑œÇ Œ±Œ¥ŒµŒØŒ±œÇ. Œ¶œÖœÉŒπŒ∫Œ¨, </code><br><code>œÄŒ±œÅŒ≠œáŒµœÑŒ±Œπ Œ∑ Œ¥œÖŒΩŒ±œÑœåœÑŒ∑œÑŒ± œÉœÑŒ± Œ∫œÅŒ¨œÑŒ∑ ŒºŒ≠ŒªŒ∑ ŒΩŒ± ŒºŒ∑ŒΩ ŒµœÄŒπœÑœÅŒ≠œàŒøœÖŒΩ Œ∫Œ±ŒºŒØŒ± ŒµŒæŒ±ŒØœÅŒµœÉŒ∑ œÉœÑŒ∑ œáœÅŒÆœÉŒ∑ </code><br><code>œÉœÖœÉœÑŒ∑ŒºŒ¨œÑœâŒΩ Œ§Œù œÑŒ≠œÑŒøŒπŒ±œÇ ŒµœÄŒπŒ∫ŒπŒΩŒ¥œÖŒΩœåœÑŒ∑œÑŒ±œÇ, Œ±ŒªŒªŒ¨ Œ∫Œ±Œπ Œ∑ Œ¥œÖŒΩŒ±œÑœåœÑŒ∑œÑŒ± œÄŒµœÅŒπŒøœÅŒπœÉŒºŒøœç œÑœâŒΩ </code><br><code>ŒµœÄŒπœÑœÅŒµœÄœåŒºŒµŒΩœâŒΩ œÉŒ∫ŒøœÄœéŒΩ œáœÅŒÆœÉŒ∑œÇ. </code><br><code> </code><br><code>Œó Œ∫œÅŒπœÑŒπŒ∫ŒÆ Œ≥ŒπŒ± œÑŒπœÇ œÄœÅŒøŒ∫ŒªŒÆœÉŒµŒπœÇ œÄŒøœÖ Œ±ŒΩŒ±Œ∫œçœÄœÑŒøœÖŒΩ Œ±œÄœå œÑŒø Œ¨œÅŒ∏œÅŒø 5Œ¥ Œ≠œáŒøœÖŒΩ Œ±œÄŒøœÑŒµŒªŒ≠œÉŒµŒπ </code><br><code>Œ±ŒΩœÑŒπŒ∫ŒµŒØŒºŒµŒΩŒø ŒµŒΩŒ¥ŒµŒªŒµœáŒøœçœÇ Œ±ŒΩŒ¨ŒªœÖœÉŒ∑œÇ Œ∫ŒøœÅœÖœÜŒ±ŒØœâŒΩ Œ±Œ∫Œ±Œ¥Œ∑ŒºŒ±œäŒ∫œéŒΩ ŒµœÄŒØ Œ∂Œ∑œÑŒ∑ŒºŒ¨œÑœâŒΩ </code><br><code>œÄœÅŒøœÉœÑŒ±œÉŒØŒ±œÇ œÄœÅŒøœÉœâœÄŒπŒ∫œéŒΩ Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩœâŒΩ Œ∫Œ±Œπ ŒΩŒ≠œâŒΩ œÑŒµœáŒΩŒøŒªŒøŒ≥ŒπœéŒΩ,</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> ŒµŒΩœé Œ∑ ŒïœÖœÅœâœÄŒ±œäŒ∫ŒÆ </code><br><code>Œ†œÅœâœÑŒøŒ≤ŒøœÖŒªŒØŒ± Œ†ŒøŒªŒπœÑœéŒΩ </code><code>ReclaimYourFace</code><code>, Œ∑ ŒøœÄŒøŒØŒ± œÖœÄŒøœÉœÑŒ∑œÅŒØŒ∂ŒµœÑŒ±Œπ Œ±œÄœå œÄŒªŒÆŒ∏ŒøœÇ </code><br><code>ŒøœÅŒ≥Œ±ŒΩœéœÉŒµœâŒΩ œÑŒ∑œÇ Œ∫ŒøŒπŒΩœâŒΩŒØŒ±œÇ œÑœâŒΩ œÄŒøŒªŒπœÑœéŒΩ, œáŒπŒªŒπŒ¨Œ¥ŒµœÇ ŒµœÖœÅœâœÄŒ±ŒØŒøœÖœÇ œÄŒøŒªŒØœÑŒµœÇ, Œ∫Œ±Œπ ŒºŒµŒ≥Œ¨ŒªŒø </code><br><code>Œ±œÅŒπŒ∏Œºœå œÄŒøŒªŒπœÑŒπŒ∫œéŒΩ œÜŒøœÅŒ≠œâŒΩ, Œ≠œáŒµŒπ Œ±ŒΩŒ±Œ¥ŒµŒØŒæŒµŒπ œÑŒøœÖœÇ œÉŒ∑ŒºŒ±ŒΩœÑŒπŒ∫ŒøœçœÇ Œ∫ŒπŒΩŒ¥œçŒΩŒøœÖœÇ œÄŒøœÖ Œ±œÄŒøœÅœÅŒ≠ŒøœÖŒΩ </code><br><code>Œ±œÄœå œÑŒ∑ ŒºŒ±Œ∂ŒπŒ∫ŒÆ Œ≤ŒπŒøŒºŒµœÑœÅŒπŒ∫ŒÆ œÄŒ±œÅŒ±Œ∫ŒøŒªŒøœçŒ∏Œ∑œÉŒ∑ œÉŒµ Œ¥Œ∑ŒºœåœÉŒπŒøœÖœÇ œáœéœÅŒøœÖœÇ Œ≥ŒπŒ± œÑŒ± ŒîŒπŒ∫Œ±ŒπœéŒºŒ±œÑŒ± </code><br><code>œÑŒøœÖ ŒëŒΩŒ∏œÅœéœÄŒøœÖ, œÑŒ∑ ŒîŒ∑ŒºŒøŒ∫œÅŒ±œÑŒØŒ± Œ∫Œ±Œπ œÑŒø ŒöœÅŒ¨œÑŒøœÇ ŒîŒπŒ∫Œ±ŒØŒøœÖ. ŒúŒµ œÑŒ∑ŒΩ œÄŒ±œÅŒøœçœÉŒ± œÖœÄŒøŒ≤ŒøŒªŒÆ Œ∏Œ± </code><br><code>Œ∏Œ≠ŒªŒ±ŒºŒµ ŒµœÄŒØœÉŒ∑ŒºŒ± ŒΩŒ± œÖœÄŒøœÉœÑŒ∑œÅŒØŒæŒøœÖŒºŒµ œÑŒπœÇ Œ∏Œ≠œÉŒµŒπœÇ œÑŒøœÖ Œ¥ŒπŒ∫œÑœçŒøœÖ </code><code>European Digital Rights</code><code>, </code><br><code>œÉœÑŒø ŒøœÄŒøŒØŒø Œ±œÄŒøœÑŒµŒªŒøœçŒºŒµ ŒºŒ≠ŒªŒøœÇ, ŒµœÄŒØ œÑŒøœÖ Œ¨œÅŒ∏œÅŒøœÖ 5, œåœÄœâœÇ Œ±œÖœÑŒ≠œÇ Œ≠œáŒøœÖŒΩ Œ±ŒΩŒ±œÅœÑŒ∑Œ∏ŒµŒØ œÉœÑŒ∑ŒΩ </code><br><code>ŒµœÄŒØœÉŒ∑ŒºŒ∑ œÉŒµŒªŒØŒ¥Œ± </code><code>œÑŒ∑œÇ Œ±ŒΩŒøŒπœáœÑŒÆœÇ Œ¥ŒπŒ±Œ≤ŒøœçŒªŒµœÖœÉŒ∑</code><code> </code><code>œÑŒ∑œÇ ŒïœÖœÅœâœÄŒ±œäŒ∫ŒÆœÇ ŒïœÄŒπœÑœÅŒøœÄŒÆœÇ. </code><code> </code><br><code>ŒïŒΩœåœÑŒ∑œÑŒ± 3: Œ†Œ±œÅŒ±œÑŒ∑œÅŒÆœÉŒµŒπœÇ ŒµœÄŒØ œÑœâŒΩ ŒÜœÅŒ∏œÅœâŒΩ 6 Œ≠œâœÇ 25 </code><br><code> </code><br><code>Œü Œ∫Œ±ŒΩŒøŒΩŒπœÉŒºœåœÇ ŒøœÅŒØŒ∂ŒµŒπ œÉœÖŒ≥Œ∫ŒµŒ∫œÅŒπŒºŒ≠ŒΩŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ± œÑŒµœáŒΩŒ∑œÑŒÆœÇ ŒΩŒøŒ∑ŒºŒøœÉœçŒΩŒ∑œÇ œâœÇ œÉœÖœÉœÑŒÆŒºŒ±œÑŒ± </code><br><code>œÖœàŒ∑ŒªŒøœç Œ∫ŒπŒΩŒ¥œçŒΩŒøœÖ, œåœÑŒ±ŒΩ ŒµŒΩŒ≠œáŒøœÖŒΩ Œ∫ŒπŒΩŒ¥œçŒΩŒøœÖœÇ œÄŒ±œÅŒ±Œ≤ŒØŒ±œÉŒ∑œÇ Œ∏ŒµŒºŒµŒªŒπœâŒ¥œéŒΩ Œ¥ŒπŒ∫Œ±ŒπœâŒºŒ¨œÑœâŒΩ.  </code><br><code>ŒúŒµœÑŒ±Œæœç Œ¨ŒªŒªœâŒΩ œÉœÖŒºœÄŒµœÅŒπŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒ±Œπ œÉœÖœÉœÑŒÆŒºŒ±œÑŒ± Œ≤ŒπŒøŒºŒµœÑœÅŒπŒ∫ŒÆœÇ Œ±ŒΩŒ±Œ≥ŒΩœéœÅŒπœÉŒ∑œÇ, Œ≥ŒπŒ± </code><br><code>Œ±œÉœÜŒ¨ŒªŒµŒπŒ± œÖœÄŒøŒ¥ŒøŒºœéŒΩ, Œ≥ŒπŒ± ŒµœÅŒ≥Œ±œÉŒπŒ±Œ∫ŒøœçœÇ ŒÆ Œ∫Œ±Œπ ŒµŒ∫œÄŒ±ŒπŒ¥ŒµœÖœÑŒπŒ∫ŒøœçœÇ œÉŒ∫ŒøœÄŒøœçœÇ, Œ≥ŒπŒ± Œ±ŒæŒπŒøŒªœåŒ≥Œ∑œÉŒ∑ </code><br><code> </code><br><code>3</code><code> ŒíŒªŒ≠œÄŒµœÑŒµ Michael Veale, Frederik Zuiderveen Borgesius, </code><code>Demystifying the Draft EU Artificial Intelligence Act</code><code>, 2021. </code><code> </code>",POSITIVE
fitz_2665463_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665463.pdf,8,1,2665463,attachments/2665463.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>DIHK | Deutscher Industrie- und Handelskammertag e.V. </code><br><code>Address for visitors: Breite Strasse 29 | 10178 Berlin-Mitte | Postal address: DIHK | 11052 Berlin </code><br><code>Tel. +49 30-20308-0 | Fax +49 30-20308-1000 | Internet: </code><code>www.dihk.de</code><code> </code><br><code>- 1 - </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Berlin, August 5, 2021 </code><br><code> </code><br><code> </code><br><code>Association of German Chambers of Commerce and Industry </code><br><code>(Deutscher Industrie- und Handelskammertag, DIHK)</code><code> </code><br><code> </code><br><code>On the EU Commission‚Äôs proposal for a European Law on Artificial Intelligence</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> </code><code> </code><br><code>This statement is based on the economic policy and European policy positions of the Association of </code><br><code>German Chambers of Commerce and Industry (DIHK) and has been drawn up by a cross-industry </code><br><code>working group on ‚ÄúAI Regulation‚Äù. The working group is composed of members of various DIHK Expert Committees, as well as desk officers from the Chambers of Commerce and Industry (CCIs). </code><br><code>The comments received from CCIs and committee members up to the time of the submission of the </code><br><code>statement have been taken into consideration. </code><code>If the DIHK receives further relevant comments that </code><br><code>have not yet been taken into account in this statement, the DIHK will supplement this statement accordingly.</code><code> </code><br><code>A. The most important Aspects in Brief</code><code> </code><br><code>Artificial intelligence (AI) is considered to be one of the key technologies of digitalisation and a driver </code><br><code>of economic growth. In order to ensure that the course is set for the successful development and </code><br><code>application of AI, the DIHK advocates the improvement of the AI framework conditions at both state </code><br><code>and federal level, as well as at EU level. For SMEs in particular, it is important that security and confidence are strengthened with respect to the use of AI technologies. A European legal framework </code><br><code>can make an important contribution to this. The key here is to find the right balance between safe AI </code><br><code>systems and innovation-friendly framework conditions. The legal provisions must not impose unnecessary barriers to the further development of AI, but instead should have the effect of promoting innovation. The key demands: the creation of legal certainty through a definition of the term ""AI system"" that is not only differentiated, but also as clear as possible. Here, the actual specific risk posed </code><br><code>by these systems should be taken into consideration for the risk qualification. Obligations are to be </code><br><code>designed in a pragmatic manner so that the bureaucratic burden for companies is kept as small as </code><br><code> </code><br><code>1</code><code> Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts‚Äù, COM(2021) 206 final, of 21 </code><br><code>April 2021, see: eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-958501aa75ed71a1.0019.02/DOC_1&format=PDF). </code>",POSITIVE
fitz_2665210_4,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665210.pdf,11,4,2665210,attachments/2665210.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>ABI 08/06/2021 </code><br><code> </code><br><code> 4  </code><br><code>- </code><br><code>Adaptivity: the ability of an IT system to improve performance </code><br><code>by learning from experience </code><br><code>- </code><br><code>Autonomy: the ability of an IT system to perform tasks in </code><br><code>complex environments without constant guidance by a human </code><br><code>user </code><br><code>The identification of Adaptivity and Autonomy, as the discriminant </code><br><code>characteristics that an AI System, derives from the work done by the </code><br><code>University of Helsinki and published the online course ‚ÄòElements of AI‚Äô </code><br><code>(</code><code>https://www.elementsofai.com/eu2019fi</code><code>). This relevant initiative was </code><br><code>endorsed by the EU during the Finnish EU Presidency in December </code><br><code>2019. </code><br><code>o</code><code> </code><code>As reported in Recital 6 and Article 3, AI Systems are defined by the </code><br><code>ability, for a given set of human-defined objectives, to </code><code>generate</code><code> </code><br><code>outputs such as content, predictions, recommendations, or decisions </code><br><code>which influence the environment with which the system interacts. At </code><br><code>this regard it is not so clear the meaning of the term ‚Äú</code><code>generate</code><code>‚Äù.</code><code> </code><code>To </code><br><code>better focus the perimeter of AI Systems, it might be reasonable to </code><br><code>refer to data driven systems (systems that ‚Äì given by human </code><br><code>objectives, contexts, data and constraints ‚Äì create the algorithm or the </code><br><code>model autonomously) excluding from the scope any system based on </code><br><code>algorithm or model driven systems (systems that basically execute </code><br><code>tasks defined and programmed by a human). The reference to data </code><br><code>driven systems could be read as the explicit form of the term </code><br><code>‚Äú</code><code>generate</code><code>‚Äù.</code><code> </code><br><code>o</code><code> </code><br><code>We consider important to mark the difference between decisionmaking AI Systems (that autonomously execute business decisions) </code><br><code>and decision-support AI System (that support a human based decision </code><br><code>making). We may state that the introduction of decision-support AI </code><br><code>System is less likely to generate significant increase in the overall risk </code><br><code>of a business process, also considering that autonomous systems are </code><br><code>also subject to Article 22 of GDPR, which already identifies strong </code><br><code>requirements.    </code><code> </code><br><code>o</code><code> </code><code>Although the EC acknowledges the need to update the list of AI </code><br><code>techniques, as set out in Article 4, we believe the definition risks, given </code><br><code>the rapid evolution of the context, to be too inflexible and static, raising </code><br><code>doubts about the actual perimeter and the future evolution. However, </code><br><code>ABI suggests specifying that the update of AI techniques has no </code><br><code>retroactive effect on existing AI systems or, alternatively, providing a </code><br><code>reasonable period of time to adopt the new provisions.  </code><br><code>o</code><code> </code><code>As for the list of AI techniques, it would be appropriate to provide for </code><br><code>the possibility for the Commission to adopt delegated acts to update </code><br><code>the European Union harmonized legislation listed in Annex II. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663486_4,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2663486.pdf,8,4,2663486,attachments/2663486.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>4 </code><br><code>AI tools storing a vast amount of data cause inevitable risks </code><code>on data protection, privacy</code><code> and </code><br><code>intellectual property rights</code><code> of teachers and academics and other education personnel. </code><br><code>ETUCE highlights that ensuring data protection and privacy of  teachers and students should </code><br><code>be a priority of the AI Regulation and calls on the EU Commission and the Member States </code><br><code>to develop appropriate</code><code> data-retention policies</code><code> applicable to Artificial Intelligence in </code><br><code>education,  in the respect of national competencies in education.    </code><br><code> </code><br><code>Equality and inclusion in the design and use of AI in education:  </code><br><code>As enshrined in the EU Pillar of Social Rights and the EU Charter of Fundamental Rights, nondiscrimination in education is a fundamental principle of our society. In this regard, the EU </code><br><code>Commission‚Äôs proposal states that the AI regulation ‚Äú</code><code>will minimise the risks of erroneous or </code><br><code>biased AI-assisted decision on education and training</code><code>‚Äù. In this context, ETUCE recognizes </code><br><code>that the use of Artificial Intelligence has the potential to advance the quality of life and </code><br><code>inclusion of teachers and students in education. Nonetheless, the persistent </code><code>lack of </code><br><code>diversity </code><code>and underrepresentation of women, ethnic minorities, Black People and </code><br><code>disadvantaged groups in the population of professionals responsible for designing, testing </code><br><code>and training the algorithms and data of AI tools translate in the presence of biases in AI </code><br><code>tools, leading to a </code><code>detrimental impact on inclusion and equality in education</code><code>. Therefore, </code><br><code>ETUCE calls on the European Commission and Member States to provide adequate public </code><br><code>investment to </code><code>encourage more diversity in the STEAM sector</code><code> and ensure that </code><code>AI tools are </code><br><code>designed and used with the full representation of the wide society.</code><code> </code><br><code>Besides, </code><code>research</code><code> shows that </code><code>cyber-violence, cyber-bullying and cyber-harassment</code><code> have </code><br><code>increased with the development of digitalisation in education. ETUCE underlines that it is </code><br><code>important to further explore how Artificial Intelligence systems can act as supporting tools </code><br><code>to detect and counter cyber-violence, cyber-bullying and cyber-harassment.  </code><br><code> </code><br><code> </code><br><code>*The European Trade Union Committee for Education (ETUCE) represents 127 Education </code><br><code>Trade Unions and 11 million teachers in 51 countries of Europe. ETUCE is a Social Partner </code><br><code>in education at the EU level and a European Trade Union Federation within ETUC, the </code><br><code>European Trade Union Confederation. ETUCE is the European Region of Education </code><br><code>International, the global federation of education trade unions. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665578_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665578.pdf,9,2,2665578,attachments/2665578.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Comments on the proposed  </code><br><code>Artificial Intelligence Act </code><br><code>by Women in AI Austria </code><br><code> </code><br><code> </code><br><code> </code><br><code>Women in AI Austria </code><br><code>Transparency Register ID:  </code><br><code>815698241750-24 </code><br><code>Reschgasse 2/5 </code><br><code>AT-1220 Vienna </code><br><code>carina@womeninai.co</code><code> </code><br><code>2</code><code> of </code><code>9</code><code> </code><br><code> </code><br><code>Art. 3 point (35) ‚Äòbiometric categorisation system‚Äô means an AI system for the purpose </code><br><code>of assigning natural persons to specific categories, such as sex, age, hair colour, eye </code><br><code>colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their </code><br><code>biometric data;  </code><br><code>The proposed definition supposes some relation between biometric data and ethnic origin and </code><br><code>sexual or political orientation. We strongly refute the claim that a person's ethnicity, sexual or </code><br><code>political orientation may be detected using biometric data, since these are socially constructed </code><br><code>categories and are not connected to measurable attributes of a person's physical presence in this </code><br><code>world.  </code><br><code>Art. 3 point (39): ‚Äòpublicly accessible space‚Äô means any physical place accessible to the </code><br><code>public, regardless of whether certain conditions for access may apply </code><br><code>In the course of the extraordinary circumstances of the past nine months, we have all experienced a </code><br><code>shift from physical to virtual spaces. An AI system may be employed irrespective of the type of space </code><br><code>it engages with because it draws on input data, which can be produced both in physical and in virtual </code><br><code>spaces. Given the increasing use of digital space in different contexts ‚Äì professional, educational, </code><br><code>leisurely ‚Äì and the indifference of AI systems towards different types of spaces, we would </code><br><code>recommend to remove the reference to ‚Äúphysical place‚Äù, instead defining publicly accessible space </code><br><code>as ‚Äúany place accessible to the public, regardless of whether certain conditions for access may </code><br><code>apply‚Äù.  </code><br><code>Art. 3 point (44): ‚Äòserious incident‚Äô means any incident that directly or indirectly leads, </code><br><code>might have led or might lead to any of the following:  </code><br><code>(a) the death of a person or serious damage to a person‚Äôs health, to property or the </code><br><code>environment, </code><br><code>(b) a serious and irreversible disruption of the management and operation of critical </code><br><code>infrastructure. </code><br><code>We strongly support the inclusion of damage to environment in the definition of ‚Äòserious incident‚Äô, in </code><br><code>particular since there seem to be very promising applications for AI systems in environmental </code><br><code>protection. However, as the definition in Art. 3 para. (44a) stands, the irreversible loss of access or </code><br><code>opportunity caused by a misclassification of a person would never have to be reported, despite </code><br><code>breaching fundamental rights. The wording of the requirement in Art. 62 para. (1) may be </code><br><code>understood as a requirement to report malfunctions constituting a breach of fundamental rights as </code><br><code>well as serious incidents: therefore, an explicit reference to fundamental rights in this definition </code><br><code>would provide more clarity.  </code><br><code>Art 3 (new definition for AI subjects including also companies) </code><br><code>The AI Act sets forth new obligations for operators of AI systems in order to minimise the risk of </code><br><code>harm ‚Äì but one definition conspicuously absent is that of the person(s) who are to be protected. We </code><br><code>urge the Commission to include a definition of AI subject in the AI Act, as has been done in the GDPR </code><br><code>for data subject. We further believe that the term ""AI subject"" should be broad enough to include </code><br><code>legal persons such as companies as this would be consistent with the broad definition of user and </code><br><code>both natural and legal persons can be affected by AI systems. In our view, the AI Act is a better place </code><br><code>to introduce such a concept, rather than in other legislation focusing on liability, because it focuses </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665515_5,company,../24212003_requirements_for_artificial_intelligence/attachments/2665515.pdf,5,5,2665515,attachments/2665515.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>It is well understood that the definition is only relevant in combination with high-risk methodologies as defined </code><br><code>in Annex II.  It may be worth pointing that out explicitly.  </code><br><code>There are two issues. First, the exemplary character of this definition is not to be underestimated. As the AI Act </code><br><code>will serve as a basis for many other regulatory frameworks, it is likely that this definition will be adopted by those.  </code><br><code>Second, the proposed definition risks to capture in the regulation also traditional software systems that process </code><br><code>data and take decisions. These systems are already rigorously tested and covered by current legislation.   </code><br><code>Finally, there is no definition of ‚Äúdata‚Äù.  This means that an every-day interpretation of the word needs to be used.  </code><br><code>ANNEX</code><code> </code><code>IV:</code><code> </code><code>TECHNICAL</code><code> </code><code>DOCUMENTATION</code><code> </code><code> </code><br><code>REFERRED TO IN </code><code>A</code><code>RTICLE </code><code>11(1)</code><code> </code><code> </code><br><code>The technical documentation referred to in Article 11(1) shall contain at least the following information, as </code><br><code>applicable to the relevant AI system:  </code><br><code>1.</code><code> </code><code>A general description of the AI system including:  </code><br><code>(a)</code><code> </code><code>its intended purpose, the person/s developing the system the date and the version of </code><br><code>the system</code><code>;  </code><br><code>It is problematic to determine ‚Äúthe person/s developing the system‚Äù, esp. in large organisations. How do we handle </code><br><code>copy/pasted code, for instance, or other 3rd party code where the individuals cannot be named?  Naming of a </code><br><code>responsible department should suffice. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665430_5,other,../24212003_requirements_for_artificial_intelligence/attachments/2665430.pdf,8,5,2665430,attachments/2665430.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>input to output.‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">32</code><code> At the same time, other scholars define interpretability in the same way: ‚Äòwhen </code><br><code>one can clearly trace the path that your input data takes when it goes through the AI model.‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">33</code><code> The </code><br><code>specified definitions might require opening the black-boxes of the most sophisticated algorithms </code><br><code>which is not always possible. That is why it is crucial to understand whether the specified or similar </code><br><code>definitions (that might lead to the obligation to open black boxes) are intended by the legislator to </code><br><code>be applicable to one of the concepts and if so, to which one. </code><code>This would define whether the </code><br><code>opaquest AI models are eligible for application in high-risk AI systems because if opening the </code><br><code>black box is needed for interpretability, providers of such AI systems would not be able to </code><br><code>comply with this obligation.</code><code>   </code><br><code> </code><br><code>How interpretability is distinguished (or not) from the explainability requirement of automated </code><br><code>decision-making established in the GDPR</code><code style=""font-weight: 1000; background-color: #FF0000;"">34</code><code> is another relevant question. The issues with alignment </code><br><code>of the AI Act with the GDPR has been already mentioned by scholars and policymakers,</code><code style=""font-weight: 1000; background-color: #FF0000;"">35</code><code> and </code><br><code>correlation between interpretability-transparency requirement in the AI Act and </code><br><code>explainability</code><code style=""font-weight: 1000; background-color: #FF0000;"">36</code><code> in the GDPR</code><code> is the example of such non-alignment.    </code><br><code> </code><br><code>Clarification of the transparency-interpretability requirement is needed not only for defining </code><br><code>models that are eligible to be used in high-risk AI systems and for correlating it with explainability, </code><br><code>but also for </code><code>guiding their providers in compliance</code><code>. It is much appreciated that the AI Act gives </code><br><code>providers discretion in determining their obligations: ‚Äò</code><code>an appropriate type and degree of </code><br><code>transparency shall be ensured, with a view to achieving compliance with the relevant obligations </code><br><code>of the user and of the provider set out in Chapter 3 of the Title III</code><code>.‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">37</code><code> This way the legislator ensures </code><br><code>the risk-based approach established in the AI Act and takes into consideration the technical </code><br><code>limitations of AI interpretability that can be an issue for AI providers.  </code><br><code> </code><br><code>The reference to other obligations of AI providers and users is important for clarifying the </code><br><code>interpretability requirement. Obligations of AI users refer to the use of AI systems in accordance </code><br><code>with instructions and quality monitoring. The possibility of quality monitoring by users can be </code><br><code>deemed as good guidance for interpretability - a user shall be able to interpret AI‚Äôs system outcome </code><br><code>to such degree that is necessary to see the deviations from expected/normal outcome. Another </code><br><code>clarification relates to the human-oversight requirement ‚Äì it is needed, inter alia, to enable users to </code><br><code>interpret the outcome of AI system. Thus, human oversight measures can be deemed as part of </code><br><code>interpretability measures.  </code><br><code> </code><br><code>While these correlations with other requirements are important to direct AI providers in their </code><br><code>understanding of the interpretability requirement, more is needed. Considering the variety of </code><br><code>interpretability definitions by scholars from different areas, it can be understood very differently. </code><br><code>At least the direction of what shall be deemed as an interpretable outcome of AI system or </code><br><code>what is the scale of AI interpretability would be of much help to AI providers.</code><code> Otherwise, they </code><br><code>are left alone with the most challenging element of AI systems where the highest extent of the </code><br><code>compliance with the obligation is not always possible (opening black-box) and the lesser extents </code><br><code> </code><br><code>32</code><code> Kiseleva (n 20) referring to E. Onose, ‚ÄòExplainability and Auditability in ML: Definitions, Techniques, and Tools‚Äô (Neptune </code><br><code>Blog, July 2021) <</code><code> </code><code>https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools</code><code>> accessed August 02, 2021.  </code><br><code>33</code><code> </code><code>Kiseleva (n 20) referring to B. Dickson, ‚ÄòAI models need to be ‚Äòinterpretable‚Äô rather than just ‚Äòexplainable‚Äô (Neural Today, </code><br><code>August 2020) <</code><code> </code><code>https://thenextweb.com/news/ai-models-need-to-be-interpretable-rather-than-just-explainable</code><code>> accessed August </code><br><code>02, 2021. </code><code> </code><br><code>34</code><code> </code><code>GDPR, recital 71.</code><code> </code><br><code>35</code><code> J. Bergholm, ‚ÄòThe GDPR and the Artificial Intelligence Regulation ‚Äì it takes two to tango?‚Äô (CiTiP Blog, July 2021) < </code><br><code>https://www.law.kuleuven.be/citip/blog/the-gdpr-and-the-artificial-intelligence-regulation-it-takes-two-to-tango/</code><code>> </code><br><code>accessed </code><br><code>August 02, 2021 and EDPB-EDPS Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and of the </code><br><code>Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) as of June 18, 2021 < </code><br><code>https://edpb.europa.eu/system/files/2021-06/edpb-edps_joint_opinion_ai_regulation_en.pdf</code><code>> accessed August 02, 2021.  </code><br><code>36 </code><code>Together with the provision of the meaningful information, recital 71 of the GDPR.</code><code> </code><br><code>37</code><code> EC Proposal for the AI Act, art. 13 (1). </code><code> </code>",POSITIVE
fitz_2665452_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2665452.pdf,4,4,2665452,attachments/2665452.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><code>Press release </code><br><code>Date </code><code> </code><code>12 October 2020 </code><br><code>Subject </code><br><code> </code><code>BMW Group code of ethics for artificial intelligence. </code><br><code>Page </code><br><code> </code><code>4 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Corporate Communications </code><br><code>www.bmwgroup.com</code><code> </code><code> </code><br><code>Facebook: </code><code>http://www.facebook.com/BMWGroup</code><code>  </code><br><code>Twitter: </code><code>http://twitter.com/BMWGroup</code><code>  </code><br><code>YouTube: </code><code>http://www.youtube.com/BMWGroupView</code><code>  </code><br><code>Instagram: </code><code>https://www.instagram.com/bmwgroup</code><code>  </code><br><code>LinkedIn: </code><code>https://www.linkedin.com/company/bmw-group/</code><code> </code><br><code> </code>,NO_FOOTNOTES_ON_PAGE
fitz_2665406_5,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665406.pdf,10,5,2665406,attachments/2665406.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>5 </code><br><code> </code><br><code>classify as low-risk or no-risk) turn out to have major negative impacts on individuals and society. </code><br><code>We call on the Council and the Parliament to reconsider the specific approach of risk </code><br><code>categories of the Commission‚Äôs proposal in light of the above reflections.</code><code> </code><br><code>IV</code><code> </code><code>Moreover, the Commission‚Äôs </code><code>harmonization efforts may have self-defeating effects</code><code>, undermining </code><br><code>the initial intention of subjecting AI systems to greater‚Äîrather than less‚Äîscrutiny. On the one </code><br><code>hand, the introduction of the high-risk category as the core of its proposal may unacceptably </code><br><code>preclude Member States from introducing stricter or additional requirements in domestic law for </code><br><code>systems beyond this category.</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> On the other hand, as soon as a high-risk system has the CE </code><br><code>marking</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code> affixed, Member States are no longer allowed to ‚Äúcreate unjustified obstacles‚Äù on their use </code><br><code>(para. 67), a requirement that urgently calls for clarification.</code><code style=""font-weight: 1000; background-color: #FF0000;"">8</code><code>  </code><br><code>/</code><code> </code><br><code>The Council and the Parliament must make sure that the Act‚Äôs harmonization efforts do not </code><br><code>result in ADM systems being scrutinized less thoroughly. Member States must have the right </code><br><code>to subject such systems to additional requirements in order to mitigate their potentially </code><br><code>harmful impact.  </code><br><code>3 Make Sure Risk-Assessment is More than a Fig Leaf  </code><br><code>A further concern refers to the requirements to which operators (providers and users) are subjected. </code><br><code>In general, AlgorithmWatch has long advocated for the instrumental role of transparency in furthering </code><br><code>a responsible use of AI that benefits individuals and society. Against this background, we</code><code> </code><code>applaud the </code><br><code>Commission for recognizing the instrumental value of </code><code>transparency, </code><code>and we support the approach of </code><br><code>subjecting operators of AI systems to a variety of requirements in terms of transparency and riskassessment, which creates an incentive to promote compliance-by-design approaches. Yet, it must be </code><br><code>ensured that these requirements do not end up as mere fig leaves</code><code>.  </code><br><code>I</code><code> </code><br><code>Article 43 stipulates that only AI systems intended to be used for remote biometric identification </code><br><code>are subject to third-party conformity assessment by notified bodies, while </code><code>for all other AI systems </code><br><code>classified as high-risk, a self-assessment by the provider will suffice</code><code>, including systems used in </code><br><code>sensitive areas such as predictive policing, migration control, or recruitment. Most requirements </code><br><code>apply to the provider of a system (as opposed to its user, that is the one deploying it, who has fewer </code><br><code>obligations). In most cases, providers of AI systems are </code><code>corporate actors</code><code>. In our view, it is </code><br><code>unacceptable to leave such an important assessment solely to (mostly) corporate actors who have </code><br><code>a great self-interest in the deployment of these systems. Systems that are likely to have </code><br><code>consequential effects on individuals and society‚Äîor that threaten to come with special risks‚Äî</code><br><code>should be subject to adequate third-party oversight.  </code><br><code>II</code><code> </code><code>According to the proposal, the conformity assessment operators will have to conduct is the </code><br><code>assessment of whether they are either in conformity with the essential requirements of the Act or </code><br><code>with the standards developed on their basis. In the words of the Act, </code><code>systems in conformity with </code><br><code>harmonized standards will be presumed to be in conformity with the requirements the Act sets </code><br><code>for high-risk systems</code><code> (Art. 40). This raises a variety of questions: First, this presumption of </code><br><code>conformity also seems to apply to biometric identification systems, the only use case for the </code><br><code>assessment of which a third-party notified body would actually be foreseen. Thus, there is some </code><br><code>uncertainty in the current proposal as to the role‚Äîand effective influence‚Äîof notified bodies, even </code><br><code>where they are foreseen. Second, while the introduction of technical standards can be a valuable </code><br><code>means toward regulating rapidly developing fields, standardization procedures tend to be opaque, </code><br><code>prone to industry lobbying, and hardly accessible to all relevant stakeholders‚Äîespecially not to civil </code><br><code>                                                 </code><br><code>6</code><code> Cf. Veale, Michael / Zuiderveen Borgesius, Frederik, ¬´Demystifying the Draft EU Artificial Intelligence Act¬ª, in Computer Law </code><br><code>Review International 22(4), 2021 (forthcoming), preprint available at </code><code>https://osf.io/preprints/socarxiv/38p5f</code><code>, pp. 20-23. </code><br><code>7 </code><code>The Conformit√® Europ√´enne (CE) mark is the EU‚Äôs mandatory conformity marking which regulates the goods sold within the </code><br><code>European Economic Area. </code><br><code>8</code><code> Cf. DGB, Initial Assessment of Issues Relevant to Labour Policy on the Draft of the EU Commission on a European AI </code><br><code>Regulation, 21 April 2021, </code><code>https://www.dgb.de/-/08J</code><code>, p. 5. </code>",POSITIVE
fitz_2662543_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2662543.pdf,2,1,2662543,attachments/2662543.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Aarhus University ‚Ä¢ Babe»ô-Bolyai University ‚Ä¢ University of Bern ‚Ä¢ University of Bologna ‚Ä¢ Ghent University ‚Ä¢ University of Glasgow </code><br><code>University of G√∂ttingen ‚Ä¢ University of Groningen ‚Ä¢ Jagiellonian University ‚Ä¢ King‚Äôs College London ‚Ä¢ University of Ljubljana </code><br><code>University of Louvain ‚Ä¢ University of Oslo ‚Ä¢ Universit√© de Paris ‚Ä¢ Pompeu Fabra University ‚Ä¢ Radboud University  </code><br><code>University of Tartu ‚Ä¢ University of T√ºbingen ‚Ä¢ Uppsala University ‚Ä¢ University of Vienna ‚Ä¢ University of Warwick </code><br><code>Proposals for the Artificial Intelligence Act </code><br><code>In April 2021, the European Commission published its proposal for an Artificial Intelligence (AI) Act.</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> Its </code><br><code>ambition is to ensure that AI systems comply with the fundamental rights and the values of the </code><br><code>European Union, while facilitating the development of a single market for ‚Äúlawful, safe and trustworthy </code><br><code>AI applications‚Äù. The proposal is accompanied by a revised Coordinated Plan on Artificial Intelligence </code><br><code>which calls for an alignment and further coordination of national and EU-level policies and investments </code><br><code>‚Äúto create EU global leadership on trustworthy AI‚Äù.</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>  </code><br><code>The Guild fully endorses the ambition of the proposal for an AI Act to ensure that AI systems are </code><br><code>trustworthy and do not threat the fundamental rights and values of the EU. The present document </code><br><code>nevertheless highlights some areas of concerns and formulates recommendations on how to address </code><br><code>them. </code><br><code>The European Commission, advised by a high-level expert group, decided on a risk-based approach. AI </code><br><code>systems, whose use could create unacceptable risk, will be banned, while high-risk AI systems will need </code><br><code>to go through ex-ante conformity assessment and comply with few other obligations. The Guild is </code><br><code>concerned about two caveats in this approach. It requires a definition of AI systems that reflects the </code><br><code>fast technological developments while being operationalizable by legal practitioners. An annex to the </code><br><code>AI Act includes the definition of techniques and approaches that allow the development of AI systems. </code><br><code>This will allow for easy update. However, </code><code>The Guild recommends that the European Commission </code><br><code>establishes a high-level expert group, composed of academic researchers among others. Its tasks will </code><br><code>include advising on whether any technological progress requires a revision to the annex or to the </code><br><code>body text of the AI Act</code><code>. The second caveat is that the approach in the AI Act may be interpreted as an </code><br><code>attempt to regulate AI systems instead of practices. The Guild agrees that the practices presented as </code><br><code>causing unacceptable risks should be banned for a better protection of fundamental rights and the EU </code><br><code>values. However, </code><code>the European Commission may consider, as a more effective and/or </code><br><code>complementary approach, setting up regulatory frameworks that explicitly ban those jeopardizing </code><br><code>practices, such as social scoring</code><code>. </code><br><code>The Guild acknowledges that the AI Act aims to regulate AI systems put onto the market (either as </code><br><code>standalone systems or embedded in products or services) depending on the degree of risks their uses </code><br><code>could create. Doing so, the proposal of the European Commission does not have the ambition to create </code><br><code>obligations for research on AI and AI systems. However, The Guild foresees that the AI Act could define </code><br><code>new standards for responsible and ethical research and have therefore an indirect impact on research. </code><br><code>It is especially concerned about future additional burdens on researchers applying for EU funding. All </code><br><code>proposals for an EU grant (including Horizon Europe) that involve the development, deployment and/or </code><br><code>use of AI must already provide minimum information on the potential ethic risks and risk mitigation </code><br><code>measures. If risks are foreseen, the applicants need to conduct an ethics self-assessment.</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> </code><code>The Guild </code><br><code> </code><br><code>1</code><code> Proposal for a regulation laying down harmonised rules on Artificial Intelligence (Artificial Intelligence </code><br><code>Act), COM(2021) 206 final.  </code><br><code>2</code><code> Coordinated Plan on Artificial Intelligence 2021 Review, COM(2021) 205 final Annex. </code><br><code>3</code><code> European Commission (2021) </code><code>EU Grants: How to complete your ethics self-assessment</code><code>. Version 1.0. </code>",POSITIVE
fitz_2665521_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665521.pdf,5,1,2665521,attachments/2665521.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>    </code><br><code> </code><br><code>  </code><br><code>datainnovation.org </code><br><code>Feedback on the Artificial Intelligence Act </code><code> </code><br><code>The Center for Data Innovation (Transparency Register #: 367682319221-26) is pleased to respond </code><br><code>to the European Commission‚Äôs public consultation on the Artificial Intelligence Act (AIA).</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code> We agree </code><br><code>with the Commission‚Äôs position that artificial intelligence (AI) technologies should be subject to a </code><br><code>well-designed regulatory framework. This framework should encourage the responsible adoption and </code><br><code>use of the technology to benefit society, provide guardrails that address potential harms, and foster </code><br><code>growth and innovation in the European digital economy. AI represents a new frontier of digital </code><br><code>technology whose impact on the economy and society will be transformative in the next decades. It is </code><br><code>of central importance that the EU creates conditions where citizens can take advantage of the </code><br><code>incredible range of opportunities AI represents for new sources of growth, productivity, and scientific </code><br><code>progress that will make Europe richer, healthier, and safer.  </code><br><code>OVERVIEW OF THE CENTER FOR DATA INNOVATION‚ÄôS POSITION ON THE </code><br><code>ARTIFICIAL INTELLIGENCE ACT </code><br><code>The AIA is too broad in its attempt to regulate an entire stack of technologies and applications at </code><br><code>such an early stage in the development of AI. The added cost for the development and deployment of </code><br><code>AI imposed by the many regulatory obligations in the Act will impose an expensive burden on the </code><br><code>European digital ecosystem. In particular, the AIA, along with other regulatory barriers to market </code><br><code>entry and growth, will make it difficult for European digital entrepreneurs to set up new businesses, </code><br><code>grow them, and in the process create jobs, technological progress, and wealth. In 2000, the Lisbon </code><br><code>Strategy set out to make Europe ‚Äúthe most competitive and dynamic knowledge-based economy in </code><br><code>the world.‚Äù The EU has not achieved this goal, and the proposed top-down regulatory framework for </code><br><code>AI development and use will once again create difficult conditions for new businesses to enter a </code><br><code>revolutionary digital market, resulting in more missed opportunities for growth in Europe.  </code><br><code>The AIA sets up an overarching regulatory framework for the widest possible range of AI algorithms. </code><br><code>This approach is a flawed way to regulate a general-purpose technology like AI, which is more akin to </code><br><code>the wheel, electricity, or the internal combustion engine, rather than specific technologies like </code><br><code>automotive vehicles, airplanes, or the Haber process for ammonia production. AI involves a </code><br><code>constantly evolving set of tools, methodologies, algorithmic frameworks‚Äîin other words, it is not an </code><br><code>explicit device or engineering process, but rather an information-technological approach that will end </code><br><code> </code><br><code>1</code><code>  ‚ÄúArtificial intelligence ‚Äì ethical and legal requirements,‚Äù European Commission, n.d., </code><br><code>https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethicaland-legal-requirements_en</code><code>. </code><br><code> </code>",POSITIVE
fitz_2665170_7,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665170.pdf,12,7,2665170,attachments/2665170.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Hub France IA ‚Äì Groupe de Travail Banques et Auditabilit√© </code><br><code> </code><br><code>p 7/ 12  </code><br><code>¬´ 5.2.3. En cas de modification substantielle des syst√®mes d‚ÄôIA, ceux-ci devront faire l‚Äôobjet de </code><br><code>r√©√©valuations ex ante de la conformit√© (notamment lorsque les modifications vont au-del√† de ce qui est </code><br><code>pr√©d√©termin√© par le fournisseur dans sa documentation technique et v√©rifi√© lors de l‚Äô√©valuation ex ante </code><br><code>de la conformit√©) ¬ª </code><br><code>Pour que les r√©apprentissages r√©guliers soient possibles, la documentation technique initiale doit donc adopter </code><br><code>une description ¬´ large ¬ª des modifications possibles : comment ces limites seront-elles appr√©ci√©es par </code><br><code>l‚Äôorganisme certificateur ? Si la modification est consid√©r√©e comme substantielle, quels seront les d√©lais pour la </code><br><code>mise en conformit√© ? Si le r√©apprentissage n√©cessite une mise en conformit√© syst√©matique il est √† craindre une </code><br><code>perte de performance des syst√®mes d‚ÄôIA int√©grant du r√©apprentissage qui ne seront alors plus mis √† jour aussi </code><br><code>r√©guli√®rement. Ces pertes de performance se traduiront par des co√ªts en augmentation, qui, in fine, seront </code><br><code>support√©s par les utilisateurs finaux. </code><br><code>Une autre approche classique en IA, pour les syst√®mes d‚Äôapprentissage, consiste √† d√©velopper le syst√®me dont </code><br><code>on √©value les performances techniques, puis, quand celles-ci sont satisfaisantes, l‚Äô√©valuation ¬´ sur le terrain ¬ª </code><br><code>pour valider que ces performances techniques se traduisent bien en performances m√©tier. Apr√®s it√©rations, on </code><br><code>d√©cide alors de mettre sur le march√©. Comment peut-on √©valuer sur le march√© la solution pour arriver √† la </code><br><code>solution finale ? Est-ce que ce type d‚Äôapproche entre dans le champ de la r√©glementation si elle est utilis√©e pour </code><br><code>la mise en ≈ìuvre d‚Äôun syst√®me √† haut risque ? ou bien ces √©valuations sont-elles consid√©r√©es comme des tests </code><br><code>au sens de l‚Äôarticle 9 : Les tests des syst√®mes d‚ÄôIA √† haut risque sont effectu√©s, selon les besoins, √† tout moment </code><br><code>pendant le processus de d√©veloppement et, en tout √©tat de cause, avant la mise sur le march√© ou la mise en </code><br><code>service. Les tests sont effectu√©s sur la base de m√©triques et de seuils probabilistes pr√©alablement d√©finis, qui </code><br><code>sont adapt√©s √† la destination du syst√®me d‚ÄôIA √† haut risque. </code><br><code>De m√™me, comment certifier des approches de type ¬´ A/B testing ¬ª ? Comment les certifier ex-ante alors que la </code><br><code>solution finale n‚Äôest pas encore totalement d√©finie ? Est-ce que ce type d‚Äôapproche entre dans le champ de la </code><br><code>r√©glementation si elle est utilis√©e pour la mise en ≈ìuvre d‚Äôun syst√®me √† haut risque ? </code><br><code>Recommandation 6 </code><code>: pr√©ciser le processus √† suivre en cas de r√©apprentissage</code><code> </code><br><code>ALGORITHMES PRE-ENTRAINES PAR TIERS </code><br><code>De plus en plus, les composants IA sont d√©velopp√©s en utilisant des ¬´ solutions de d√©veloppement ¬ª comme une </code><br><code>plateforme de d√©veloppement de solutions IA (e.g. Watson d‚ÄôIBM, Palantir, DataRobot, Dataiku ‚Ä¶), des </code><br><code>biblioth√®ques d‚Äôalgorithmes open source (e.g. PyTorch, scikitlearn ‚Ä¶) ou des mod√®les pr√©-entra√Æn√©s disponibles </code><br><code>en open source (e.g.  yolov3, FaceNet, BERT et d√©riv√©s ‚Ä¶). Si une entreprise d√©veloppe un syst√®me IA √† haut </code><br><code>risque en utilisant de telles solutions de d√©veloppement, on doit consid√©rer (Article 28) qu‚Äôil est le fournisseur </code><br><code>du syst√®me IA √† haut risque et que, comme il a apport√© une ¬´ modification substantielle ¬ª √† la solution de </code><br><code>d√©veloppement (c) ci-dessous) le fournisseur n‚Äôest plus assujetti aux contraintes de conformit√© : </code><br><code>Tout distributeur, importateur, utilisateur ou autre tiers est consid√©r√© comme un fournisseur aux fins du </code><br><code>pr√©sent r√®glement et est soumis aux obligations incombant au fournisseur au titre de l'article 16 dans </code><br><code>toutes les circonstances suivantes: </code><br><code>(a) </code><br><code>il met sur le march√© ou met en service un syst√®me d‚ÄôIA √† haut risque sous son propre nom ou sa </code><br><code>propre marque; </code><br><code>(b) </code><br><code>il modifie la destination d'un syst√®me d‚ÄôIA √† haut risque d√©j√† mis sur le march√© ou mis en service; </code><br><code>(c) </code><br><code>il apporte une modification substantielle au syst√®me d‚ÄôIA √† haut risque. </code><br><code>2. </code><br><code>Lorsque les circonstances vis√©es au paragraphe 1, point b) ou c), se produisent, le fournisseur </code><br><code>qui a initialement mis sur le march√© ou mis en service le syst√®me d‚ÄôIA √† haut risque n‚Äôest plus consid√©r√© </code><br><code>comme un fournisseur aux fins du pr√©sent r√®glement. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665527_4,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665527.pdf,5,4,2665527,attachments/2665527.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>We enthusiastically embrace the concept of establishing an EU database for stand-alone</code><br><code>high-risk AI systems (‚Äú</code><code>EU Database</code><code>‚Äù) (art. 60). We have been supporting this idea</code><br><code>since it appeared in WHITE PAPER On Artificial Intelligence - A European approach to</code><br><code>excellence and trust, which was subject to public consultation. We recommended this</code><br><code>solution in 2019 in the Foundation‚Äôs report: ‚ÄúalGOVrithm. The State of Play‚Äù on the use</code><br><code>of algorithms and systems based on Automatic Decision Making in state-citizen relations.</code><br><code>The Foundation proposed then the creation of an appropriate governmental body or</code><br><code>equipping the existing one with additional competences in the area of supervision of the</code><br><code>development, implementation and use of automatic decision-making.</code><br><code>Pursuant to art. 60 of the European Commission's Proposal: ‚ÄúThe Commission shall, in</code><br><code>collaboration with the Member States, set up and maintain a EU database containing</code><br><code>information referred to in paragraph 2 concerning high-risk AI systems referred to in</code><br><code>Article 6(2) which are registered in accordance with Article 51.‚Äù</code><br><code>The EU Database is to include only those AI systems that qualify as high-risk AI systems,</code><br><code>which means that some of the systems will operate outside the EU Database. We</code><br><code>appreciate that Annex III listing high-risk AI systems is quite broad in scope, but does</code><br><code>not cover all systems that may be used by the public sector and that may have a</code><br><code>negative impact on fundamental rights. This applies, for example, to systems that</code><br><code>allocate judges or officials to specific court cases.</code><br><code>We</code><br><code>would</code><br><code>also</code><br><code>like</code><br><code>to</code><br><code>point</code><br><code>out</code><br><code>that</code><br><code>the</code><br><code>EU</code><br><code>Database</code><br><code>will</code><br><code>be</code><br><code>established</code><br><code>and</code><br><code>administered by the European Commission, while information about the systems will be</code><br><code>posted there by their suppliers, not by competent officials. The Foundation sees the risk</code><br><code>in the mechanism that allows suppliers to self-assess, especially in high-risk systems. At</code><br><code>the same time, we strongly support the idea that the information contained in the EU</code><br><code>Database is publicly available.</code><br><code>In 2019, we recommended creating a publicly available database (at the</code><br><code>national level) containing all AI-based systems used by state institutions. We</code><br><code>are reiterating this recommendation because we believe that any use of the AI</code><br><code>system in the public sector may have an impact on the situation of citizens and</code><br><code>as such may generate a high risk for their rights and obligations.</code><code> A database</code><br><code>should contain at least: basic information about the AI system along with an indication of</code><br><code>the public sector entity using the system and the contact information of this entity. Such</code><br><code>a database would allow interested citizens to obtain information that a given public</code><br><code>institution uses AI-based solutions in the performance of its tasks, and in the next stage</code><br><code>3</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665518_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665518.pdf,2,2,2665518,attachments/2665518.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>Page </code><code>2</code><code> of </code><code>2</code><code> </code><br><code> </code><br><code>The proposed regulation does not address the role of shop stewards in ensuring that worker's </code><br><code>rights are safeguarded when artificial intelligence is introduced in the workplace. The best </code><br><code>solutions are found through local dialogue between management and worker's representatives. </code><br><code>To ensure co-determination from employees, Negotia believes that the social dialogue should </code><br><code>have a clearly defined role in the regulation of artificial intelligence in the workplace. We also </code><br><code>believe that the conformity assessment of artificial intelligence solutions that are intended to </code><br><code>be used in the workplace should be performed by an independent third party. </code><br><code>In conclusion, Negotia believes the European Commission's proposal to regulate artificial </code><br><code>intelligence, is a great step in the right direction, but that it has clear weaknesses when it </code><br><code>comes to regulating technology in the workplace. Further measures should be put in place to </code><br><code>ensure workers' rights, in the face of technological developments. </code><br><code>We are grateful for the opportunity to provide feedback on the proposal,and look forward to </code><br><code>following the legislative process going forward.</code><code> </code><br><code> </code><br><code> </code><br><code>Monica A. Paulsen </code><br><code>President of Negotia </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665638_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665638.pdf,4,3,2665638,attachments/2665638.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Page 3/4</code><code> </code><br><code>Tour & Taxis Building - Avenue du Port 86c - Box 302 - B-1000 Brussels - T‚Äâ+‚Äâ32 2 203 38 03 - info@wecglobal.org - </code><code>www.weceurope.org</code><code> </code><br><code> </code><br><code>-</code><code> </code><br><code>The creation of job descriptions and vacancies to improve their accessibility to more jobseekers;  </code><br><code>-</code><code> </code><br><code>Applications that check on hard and objective criteria (such as availability, legal age, work permits, willingness to </code><br><code>commute etc.) </code><br><code>-</code><code> </code><br><code>Systems that support the scheduling of job interviews </code><br><code>-</code><code> </code><br><code>Systems that do not generate outputs such as content, predictions, recommendations, or decisions within </code><br><code>recruitment, selection, promotion, or termination outcomes without human involvement or oversight. </code><code> </code><br><code>-</code><code> </code><br><code>Applications used to parse unstructured CV's into structured data.</code><code> </code><br><code> </code><br><code> </code><br><code>High risk </code><br><code>-</code><code> </code><br><code>Systems that process biometric data </code><br><code>-</code><code> </code><br><code>Systems that include autonomous decision-making on (receiving) job opportunities, promotions, interviews or </code><br><code>otherwise beneficial opportunities based on sensitive personal data </code><br><code>-</code><code> </code><br><code>AI systems that monitor performance and behaviour that include sensitive personal data  </code><br><code> </code><br><code>‚ñ™</code><code> </code><br><code>WEC-Europe emphasizes that AI is a tool that can both </code><code>minimise</code><code> and amplify conscious and unconscious </code><br><code>human biases in recruitment and employment.  </code><br><code>In this respect, WEC-Europe finds that the current administrative requirements are disproportionately balanced </code><br><code>towards the latter, thus preventing the easy application of AI that minimizes human biases. Further administrative </code><br><code>intervention, requirements and audits will tip the balance further in the wrong direction for labour market </code><br><code>inclusiveness. In this respect we strongly emphasize to maintain the self-assessment for high-risk AI applications in </code><br><code>recruitment and employment. </code><br><code>‚ñ™</code><code> </code><br><code>WEC-Europe welcomes the high-risk approach of the proposed AI Act </code><br><code>WEC-Europe concurs that specific AI applications in recruitment hold risks for labour market inclusivity and </code><br><code>participation and recognizes that human oversight, security, transparency, record-keeping, and data-governance are </code><br><code>key elements when private </code><code>and public</code><code> employment services apply them.  </code><br><code>‚ñ™</code><code> </code><br><code>WEC-Europe welcomes the diversified approach to oversight, integrating existing public mechanisms, </code><br><code>third-party certification, and self-regulatory mechanisms such as self-assessment and codes of conduct.  </code><br><code>The application and impact of AI differs from sector to sector. As such, social partners, governmental auditors, and </code><br><code>policy makers in these respective sectors need to be fully involved in the specific sectoral applications. This prevents </code><br><code>implementation that is insufficiently coordinated with sectoral public and private practitioners, to the detriment of </code><br><code>sectoral functioning, existing oversight, or social dialogue.  </code><br><code>‚ñ™</code><code> </code><br><code>The legislative process for a European AI Act is not the right policy platform or tool for regulating </code><br><code>employment relations. In this context, WEC-Europe notes that Annex III pt. 4 significantly bypasses the </code><br><code>legal realities on differences between (the creation, assessment, and termination of) a commercial (B2B </code><br><code>or B2C) contract and an employment contract. </code><br><code>To prevent significant legal inconsistencies and uncertainty, we advise to separate the two types of contracts </code><br><code>(employment contracts and business contracts with self-employed service providers) and assess risks and regulatory </code><br><code>intervention separately. In this respect WEC-Europe strongly emphasizes that the application (and enforcement) of an </code><br><code>appropriate and clear worker classification regime is the most important gateway to employee protections, including </code><br><code>against automated decision-making on their employment contract (including those protections provided in the GDPR). </code><br><code>Worker status classification would especially be relevant to those gigs in location-based platforms providing delivery or </code><br><code>personal mobility services.  </code><br><code>‚ñ™</code><code> </code><br><code>WEC-Europe welcomes the technical opportunities for AI developers to identify, minimise and fight </code><br><code>conscious and unconscious human (labour market) biases and discrimination.</code><code>  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663395_1,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663395.pdf,7,1,2663395,attachments/2663395.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>www.efpia.eu</code><code>         </code><code> 1</code><code> </code><br><code> </code><br><code>Summary</code><code> </code><br><code>Artificial Intelligence (AI) powered solutions have the potential to enhance the healthcare ecosystem. </code><br><code>EFPIA‚Äôs vision is </code><code>to maximise the potential of using AI to develop novel therapies and approaches to </code><br><code>identify, treat and care for patients more efficiently, while preserving patient safety and privacy. </code><br><code>EFPIA inputs into ongoing policy debates suggesting recommendations on AI to ensure that they foster </code><br><code>innovation while guaranteeing high healthcare standards and patient safety. This position paper </code><br><code>contains key areas for consideration by legislators and regulators as they are developing a legislative </code><br><code>framework on AI. </code><code> </code><br><code> </code><br><code>EFPIA fully embraces the benefits which AI powered solutions could bring to the healthcare ecosystem </code><br><code>and its potential to positively impact the lives of patients and support healthcare professionals in </code><br><code>delivering care. </code><code>EFPIA supports </code><code>adaptation of existing frameworks for the acceptability in decision </code><br><code>making and adoption of AI technologies</code><code> to provide a path through which AI can be developed, </code><br><code>adopted and used in healthcare systems. The innovative biopharmaceutical industry recognises the </code><br><code>current challenges faced by European health systems and aims to work with partners in achieving </code><br><code>sustainable value-based and outcome-focused healthcare by leveraging opportunities provided by the </code><br><code>increasing use of AI technologies.   </code><br><code> </code><br><code>EFPIA recommends the following aspects to be taken on board in future AI policy development: </code><br><code>‚Ä¢ </code><br><code>Rules on AI should be adequate, appropriate, clear and consistent,</code><code> </code><code>fostering a harmonised </code><br><code>approach across the EU. </code><br><code>‚Ä¢ </code><br><code>AI literacy and competence building is an enabler. </code><br><code>‚Ä¢ </code><br><code>Access to high-quality data is critical to AI deployment. </code><br><code>‚Ä¢ </code><br><code>Data Governance is fundamental. </code><br><code>‚Ä¢ </code><br><code>Transparency should be defined in a way that it allows for sufficient interpretability or </code><br><code>explainability of the AI and underlying data sets. </code><br><code>‚Ä¢ </code><br><code>Intellectual Property protection should be effective and predictable. </code><br><code>‚Ä¢ </code><br><code>Leadership and coordination are needed. </code><br><code> </code><br><code>The EU has an opportunity to leverage its capabilities in healthcare by </code><code>accelerating the development </code><br><code>of an AI ecosystem through inclusivity, capacity and trust. </code><code>EFPIA wishes to underline the importance </code><br><code>of this agenda to the pharmaceutical industry and its commitment to contribute to its uptake and </code><br><code>successful adoption.</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>EFPIA Position Paper on Artificial Intelligence </code><br><code> </code><br><code> </code><br><code>Author:</code><code> </code><code>Digital Health WG</code><code>  </code><code>Date:</code><code> </code><code>December 2020 </code><code> </code><code>Version:</code><code> </code><code>FINAL </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665507_3,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665507.pdf,4,3,2665507,attachments/2665507.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>3 </code><br><code> </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>For each AI system, each actor involved in the supply chain would prepare and provide a </code><br><code>standardized, encrypted and immutable ‚Äúassurance file‚Äù and share it with the next ‚Äúnode‚Äù </code><br><code>in the production network.  </code><br><code> </code><br><code>q</code><code> Filling this assurance file requires preforming the legal and technical tests relevant for </code><br><code>compliance with regulations and enabling trust. </code><br><code>  </code><br><code>q</code><code> The template file would be ‚Äúrole-specific‚Äù; e.g. the identified dataset provider would fill </code><br><code>a document on conformance tests relevant to data, the algorithm provider would fill a </code><br><code>document on conformance tests relevant to algorithms, and so on. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>The assurance files capture answers to a series of relevant questions and also provide </code><br><code>technical evidence of compliance with regulatory requirements (including requirements </code><br><code>and tests as set out in harmonized standards). </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>The files ‚Äútravel‚Äù through the value chain as the AI system is developed and are ultimately </code><br><code>compiled at the customer-facing node.  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>The framework would form a step-by-step fulfillment of the various requirements imposed </code><br><code>by regulators and ensure transparency along the value chain. Consideration should be </code><br><code>given as to how the data in these files could be easily machine readable to facilitate future </code><br><code>automation. </code><br><code> </code><br><code>The framework must be adaptable over time, and help industrial procurement systems to </code><br><code>converge on common requirements. The framework would be mutually beneficial in the following </code><br><code>sense: </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>As a supplier, this helps ensure that the responsibility for compliance is fulfilled regardless </code><br><code>of what the customers uses the AI system for. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>As a customer, this ensures you can trust the suppliers and the rest of the supply chain </code><br><code>and have contractually fulfilled your due diligence. </code><br><code>  </code><br><code>These files would be standardised to ensure modularity of the value chain: knowing the </code><br><code>framework‚Äôs protocol enables you to coordinate with all others in the industry, rather than having </code><br><code>to tailor documentation to each customer‚Äôs specific protocal or approach.  </code><br><code> </code><br><code>Our recommendation  </code><br><code> </code><br><code>We propose that industry develops a framework along the lines set out above designed to facilitate </code><br><code>industry-wide compliance with the AI regulations.  </code><br><code> </code><br><code>In the context of high-risk AI systems, the framework would </code><br><code> </code><br><code>‚Ä¢</code><code> </code><br><code>Help collect relevant documentation throughout the value chain for compliance with </code><br><code>regulatory obligations such as risk management, data and data governance, technical </code><br><code>documentation, record-keeping, information provision, quality management systems, </code><br><code>conformity assessment, registration, etc. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665517_10,company,../24212003_requirements_for_artificial_intelligence/attachments/2665517.pdf,11,10,2665517,attachments/2665517.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>10 </code><br><code> </code><br><code>Hence, Article 61 should be significantly reduced, since it implies only additional burden for </code><br><code>each provider. Not all systems can be monitored, especially if it is a product with an integrated </code><br><code>AI system (e.g. autonomous cars). The provider would have to monitor each of these AI </code><br><code>systems, since all AI systems are generating data independently. This is not feasible for the </code><br><code>reasons listed above. </code><br><code> </code><br><code>General Remarks </code><br><code>Extensive Powers by the Commission to adopt Delegated Acts (Title XI) </code><br><code>The proposed AI Regulation grants the EU-Commission extensive powers to change the </code><br><code>definitions, scope, requirements and procedures in different areas through delegated acts. </code><br><code>Such substantial modifications of the law should only be possible with consultation of </code><br><code>stakeholders and relevant institutions. This refers to the extensive delegation of power in </code><br><code>Article 4 (AI Definition in Annex I); Article 7 (High-Risk Areas in Annex III); Article 11 (Technical </code><br><code>Documentation); Article 43 (Conformity assessment); Article 48 (Declaration of Conformity). </code><br><code>  </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665345_16,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665345.pdf,17,16,2665345,attachments/2665345.pdf#page=16,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>ÔÇß</code><code> </code><br><code>√úberpr√ºfbarkeit von KI-Entscheidungen herstellen </code><br><code> </code><br><code>KI √§hnelt oft einer Blackbox. Nicht einmal die Hersteller selbst k√∂nnen immer das (zuk√ºnftige) Verhalten </code><br><code>von selbstlernenden Systemen vorhersagen. Eine h√∂chstm√∂gliche √úberpr√ºfbarkeit muss aber stets gegeben sein. Der Mensch sollte immer die Kontrolle behalten. </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>Ausnahmen hintanhalten </code><br><code> </code><br><code>Viele (begr√º√üenswerte) Ziele und Grunds√§tze werden in der VO durch zahlreiche Ausnahmen und Einschr√§nkungen stark durchl√∂chert. In der Praxis bleibt oft vom Bekenntnis zu einem umfassenden Schutz </code><br><code>von Grundrechten, Datenschutz, ArbeitnehmerInnen-Schutz, dem Schutz der Privatsph√§re und √§hnlichem nicht mehr viel √ºbrig. Grunds√§tzlich verbotene Praktiken wirken stark aufgeweicht, notwendige </code><br><code>Teilbereiche werden nicht abgedeckt.  </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>effektiver Rechtsschutz  </code><br><code> </code><br><code>Rechtsschutzm√∂glichkeiten f√ºr einzelne aber auch durch kollektive Interessensvertretungen (zB im </code><br><code>Wege von Verbandsklagen) m√ºssen umfassend zug√§nglich sein. Auch auf eine ausreichende Ressourcenausstattung der Vollzugs- bzw Kontrollbeh√∂rden (inklusive Arbeitsinspektorate) ist dabei zu achten. </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>Aus- und Weiterbildung </code><br><code> </code><br><code>KI am Arbeitsplatz bedarf einer Verpflichtung zu vorbeugenden Ma√ünahmen wie Aus- und Weiterbildung. Das sch√ºtzt ArbeitnehmerInnen und versetzt sie in die Lage, die Rolle von Daten und KI, sowie </code><br><code>ihren Einfluss auf Arbeitsorganisation zu verstehen. Neue Technologien enthalten Unsicherheiten und </code><br><code>unbekannte Risiken, darum m√ºssen Pr√§vention und das Vorsorgeprinzip Teil des regulatorischen Rahmenwerks sein. </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>Ungew√ºnschte Anwendungen verbieten </code><br><code> </code><br><code>KI erm√∂glich durch den permanenten Strom an Daten bisher ungeahnte M√∂glichkeiten. (Beil√§ufig) anfallende Daten k√∂nnen mittels Algorithmen benutzt werden um Betroffene zu √ºberwachen, zu kontrollieren, zu bewerten und sie zu identifizieren (auch nachtr√§glich aus anonymisierten Daten). Sowohl im </code><br><code>Arbeitszusammenhang, als auch bei KonsumentInnen bedarf es deshalb strenger Regelungen und Mitbestimmungsm√∂glichkeiten. </code><code>Bestimmte Anwendungen sollten dabei g√§nzlich verboten sein.  </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>ArbeitnehmerInnenschutz und Inklusion </code><br><code> </code><br><code>Der Sicherheit und Gesundheit am Arbeitsplatz sollte ein wichtiger Stellenwert einger√§umt werden. </code><br><code>Schutz vor k√∂rperlichen und psychischen Risiken ist f√ºr ArbeitnehmerInnen wichtig. Gerade im Hinblick </code><br><code>auf Arbeitsunf√§lle und arbeitsbedingten Erkrankungen bedarf es ausreichender Kontrollm√∂glichkeiten </code><br><code>und der unabh√§ngigen Zertifizierung von Systemen. Wie bei der CE-Kennzeichnung zu sehen ist, f√ºhrt </code><br><code>eine Selbstkontrolle zu l√ºckenhafter Sicherheit, deren M√§ngel zu Arbeitsunf√§llen und Berufskrankheiten </code><br><code>f√ºhren k√∂nnen. Deren Folgen werden externalisiert und die Kosten den Sozialversicherungen sowie </code><br><code>zum Gro√üteil den betroffenen ArbeitnehmerInnen selber aufgeb√ºrdet. Unternehmen haben zudem ein </code><br><code>Interesse daran, ein ‚ÄûMinimum Viable Product‚Äú auf den Markt zu bringen, um Entwicklungskosten gering </code><br><code>zu halten. Demgegen√ºber braucht ArbeitnehmerInnenschutz aber h√∂chstm√∂glich sichere Maschinen </code><br><code>oder Technologien. In diesem Spannungsfeld ist die EU gefragt, die ArbeitnehmerInnen zu sch√ºtzen. </code><br><code>Ebenso fehlt auch eine st√§rkere Ber√ºcksichtigung von Menschen mit Behinderungen (zB H√∂r- und Sehdefizite) in der Betrachtung. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665480_41,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665480.pdf,64,41,2665480,attachments/2665480.pdf#page=41,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>   </code><br><code>36 </code><br><code>ensure robust human oversight, which consists of </code><code>at least: </code><code>training for decision-makers, </code><br><code>logging requirements, and clear processes for </code><code>ex post</code><code> review and redress.  </code><br><code>Finally, a clarification is needed regarding Article 14(5), which imposes an additional oversight </code><br><code>requirement when biometric identification systems are used. In such case, the system‚Äôs user </code><br><code>cannot take any action or decision based on the identification resulting from the system, unless </code><br><code>the result was verified and confirmed by at least two natural persons. While such strengthened </code><br><code>oversight sounds laudable, two points can be raised. First, for this provision to be meaningful, </code><br><code>the confirmation given by the ‚Äòtwo natural persons‚Äô should be based on a separate assessment </code><br><code>(with one ‚Äòon the ground,‚Äô for instance, to sight the individual in question) rather than being </code><br><code>reduced to two people looking at the same computer screen. Second, reliance on human </code><br><code>oversight as a safeguard should only be used as a last resort once the use of such intrusive </code><br><code>systems has been proven to be necessary and proportionate in a democratic society, and not as </code><br><code>a legitimation of the use of technologies that should in fact not be used in light of their rightsviolating nature. Human oversight is not a panacea for the problems that certain AI systems </code><br><code>might introduce, and should hence not be used as an excuse for their deployment where there </code><br><code>is no basis to do so.  </code><br><code>The danger of over-reliance on the outputs of an AI system is best evidenced through the </code><br><code>Viog√©n </code><code>system, used in Spain to predict and prevent the risk of domestic violence against </code><br><code>women. Fourteen out of the fifteen women who were killed in a domestic violence incident in </code><br><code>2014 had previously reported their aggressor, yet had been classified by Viog√©n as being at </code><br><code>low or non-specific risk.</code><code style=""font-weight: 1000; background-color: #FF0000;"">63</code><code> This shows that depending on the context of the system, even </code><br><code>decisions resulting in ‚Äòlow risk‚Äô classification can produce significant dangers, where the </code><br><code>decision-maker does not have the requisite technical knowledge to robustly understand how </code><br><code>the system works and to consider its limitations.  </code><br><code>In light of the above, we therefore suggest the Commission to strengthen the protection </code><br><code>afforded by the mandatory requirements for high-risk AI systems, and to clarify the open </code><br><code>questions they raise.    </code><br><code>The previous sections considered the ways in which the Proposal could be amended to come </code><br><code>closer to attaining the first element of Legally Trustworthy AI ‚Äì the appropriate allocation of </code><br><code>responsibilities for harms caused by AI, particularly regarding fundamental rights. We </code><br><code>addressed the Proposal‚Äôs conception of fundamental rights, its scope, the content of the </code><br><code>prohibitions, the regulatory framework around biometrics, and high-risk systems. The </code><br><code>following sections address the second pillar of Legally Trustworthy AI: an effective </code><br><code>enforcement framework which promotes the rule of law. </code><br><code>4.2</code><code> </code><code>The Proposal does not ensure an effective framework for the enforcement of legal </code><br><code>rights and responsibilities (rule of law) </code><br><code>As explained in Chapter 3, one of the functions of law is to allocate and distribute responsibility </code><br><code>for harms and wrongs in society. The previous sections explained that the Proposal does not </code><br><code>allocate responsibilities in ways which adequately protect against fundamental rights </code><br><code>infringements. Additionally, Chapter 3 argued that the distinctive character of </code><code>legal</code><code> (as </code><br><code>opposed to </code><code>ethical</code><code>) rules lies in an effective and legitimate framework through which legal </code><br><code>rights and duties are enforced. The following sections therefore comment on the enforcement </code><br><code>framework proposed in the proposed Regulation. </code><br><code> </code><br><code>63</code><code>  AlgorithmWatch, </code><br><code>Automating </code><br><code>Society </code><br><code>Report </code><br><code>2020, </code><br><code>AlgorithmWatch</code><code>, </code><br><code>2020, </code><br><code>https://automatingsociety.algorithmwatch.org/, 227. </code>",POSITIVE
fitz_2665235_5,company,../24212003_requirements_for_artificial_intelligence/attachments/2665235.pdf,5,5,2665235,attachments/2665235.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>exception creates legally a grey area as it is not clear from whose standpoint the obvious context or </code><br><code>circumstance will be determined.  </code><br><code>Enforcement</code><code> </code><br><code>‚ñ™</code><code> </code><br><code>It is necessary to have a clear delegation of duties between different national authorities so that AI </code><br><code>providers are not responsible for managing the overlap or gaps between various authorities. The lack </code><br><code>of expertise on the evaluation of algorithms and models will also need to be addressed with a </code><br><code>harmonized European approach to ensure an efficient and uniform enforcement of rules and </code><br><code>conducting audits.  </code><br><code>‚ñ™</code><code> </code><br><code>Based on Article 43 paragraph 4, high-risk AI systems should undergo a new conformity assessment </code><br><code>every time they are subject to substantial modifications. The definition of substantial modification in </code><br><code>the proposal entails changes with regards to the intended purpose or changes affecting the compliance </code><br><code>of the AI system with the mandatory requirements (Title III, Chapter 2 of the proposal). We would </code><br><code>recommend providing a more specific definition that includes specific details whether such modification </code><br><code>will entail retraining, changing the weights of the model or updating the model in general. These details </code><br><code>again raise issues around the definition of AI in the proposal that does not define the component parts </code><br><code>of AI systems.  Repeated conformity assessments will bring forward excessive administrative burden </code><br><code>and costs for providers, specifically for smaller companies that might not have the necessary resources </code><br><code>to cover such repeated assessments. It will also have the unintended consequence of disincentivizing </code><br><code>product improvements or continuous development cycles (agile processes to make many small </code><br><code>improvements over time) that are currently a key approach in which AI products are typically </code><br><code>developed. Furthermore, this proposal does not sufficiently account for self-learning, transfer learning </code><br><code>or reinforcement learning processes when used in providing continuous improvement or feedback to </code><br><code>algorithms based on performance. Such continuous and collective learning approaches are expected </code><br><code>to be an integral element of future AI systems, which goes beyond traditional cycle-based software </code><br><code>development practices and therefore needs to be well-considered in this proposal. Eventually, the </code><br><code>extensive testing and audits will deter companies from developing AI products that could result in the </code><br><code>decline of AI adoption and could reduce overall competitiveness of Europe. On the other hand, the </code><br><code>volume of requests to repeat conformity assessments will need to be managed by the respective </code><br><code>notified bodies that will impact their operation and resources significantly.  </code><br><code>‚ñ™</code><code> </code><br><code>Expert Group</code><code>: include explicitly in the proposal to ensure a direct role for industry, clarify its mandate, </code><br><code>composition, responsibilities. </code><br><code>‚ñ™</code><code> </code><br><code>Article 83 paragraph 2 determines the application of the regulation for high-risk AI systems already </code><br><code>placed on the EU market or put into service. Such existing AI systems will have to comply with the </code><br><code>regulation in case they are subject to significant changes in their design or their intended purpose from </code><br><code>the application date of the regulation. Firstly, the Article does not define the application of the regulation </code><br><code>to non-high-risk AI systems subject to transparency requirements that already have been placed on </code><br><code>the EU market or put into service. Secondly, the legislation does not provide a legal definition for </code><br><code>significant changes in design that will cause legal uncertainties for companies. We believe the </code><br><code>application of the regulation to existing high-risk AI systems in the EU market should be clear and </code><br><code>simple going forward to ensure legal certainty for providers and users in order to carry out the </code><br><code>necessary compliance work. </code><br><code> </code><br><code>Copyright/Trademark </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665292_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665292.pdf,4,2,2665292,attachments/2665292.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>www.bitkom.org </code><br><code>Bitkom principles for the Artificial Intelligence (AI) Act </code><br><code>Seite 2|4 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>trustworthiness in AI practical and operationalizable for economic operators, in particular </code><br><code>through harmonized standards.</code><code> </code><br><code>Central for the success of the proposed European legal framework are: clarity and simple </code><br><code>implementation with regard to requirements</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>, obligations</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code>, conformity assessment, plac-</code><br><code>ing on the market and continuous monitoring during the life cycle.  It is important to note </code><br><code>that clarity and ease of implementation include much more than questions of the wording </code><br><code>and legal interpretation of the future AI Act. In particular, it is also about which institu-</code><br><code>tional framework the EU and the Member States will design for conformity assessment, </code><br><code>market access and market surveillance and the concrete practical operationalisation of the </code><br><code>requirements should they remain as layed out by the Commission. The goal and guiding </code><br><code>principle of the digital EU‚ÄôS Digital Single Market (DSM) must be at the centre of this EU-</code><br><code>wide design. Lessons and negative experiences from data protection should be analysed </code><br><code>carefully and should be taken into account in this context.  Overall, we see the risk that the </code><br><code>sum and overlap of requirements and obligations, including in parts vaguely defined high </code><br><code>risk applications, and the associated legal uncertainty in operationalisation, creates a com-</code><br><code>plexity and compliance burden that inhibits the development of AI systems in the high risk </code><br><code>area in the EU. We also see risks of overlap of requirements and obligations with other leg-</code><br><code>islations, i.e. Medical Device Regulation and the newly proposed draft for a Machinery Reg-</code><br><code>ulation. </code><br><code>In addition to the general comments outlined above, the following three clusters are cen-</code><br><code>tral in our view. </code><br><code>I Definition of AI & scope of high-risk.   </code><br><code>The definition of AI in the proposal is extremely broad</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> and thus a very large number of </code><br><code>software applications would be covered by the regulatory framework. Therefore, we rec-</code><br><code>ommend the deletion of the terms</code><code>: ‚Äúinference and deductive engines‚Äù and ‚Äústatistical ap-</code><br><code>proaches‚Äù</code><code> from Annex I. Moreover, in many cases it is unclear and open to interpretation </code><br><code>whether specific applications in certain application scenarios are high-risk systems or not. </code><br><code>Also, a systematic risk-assessment and risk differentiation of high-risk AI systems, has to </code><br><code> </code><br><code>1</code><code> Articles 8-15 are defining a variety of requirements regarding a.o. risk management, data governance , technical documentation, transparency, robustness, accuracy, human oversight etc. </code><br><code>2</code><code> Articles 16-29 are defining a variety of obligatins such as setting up a quality management system, </code><br><code>information duties and the duty to undergo a conformity assessment procedure </code><br><code>3</code><code> Especially the ‚Äútechniques and approaches‚Äù: ‚Äúinference and deductive engines‚Äù and ‚Äústatistical approaches‚Äù (Annex I) </code>",POSITIVE
fitz_2665640_17,other,../24212003_requirements_for_artificial_intelligence/attachments/2665640.pdf,18,17,2665640,attachments/2665640.pdf#page=17,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>17 </code><br><code> </code><br><code>This tentative formulation will no doubt need refinements, but it captures some key concerns. </code><br><code>For instance, an AI system drawing on behavioral or emotional data to ensure vigilance in </code><br><code>the operation of a car seeks‚Äîand if well designed is likely‚Äîto preserve rational control over </code><br><code>one‚Äôs driving behaviour; whereas a system drawing on the same information to keep people </code><br><code>engaged on a social media platform that they, on reflection, would like to spend less time on, </code><br><code>may reduce rational control. An AI system that identifies and exploits flaws and biases in </code><br><code>reasoning, for example, to increase the desire to buy a product, is likely to bypass the capacity </code><br><code>for rational control, whereas an AI system that instructs a Socratic bot to present arguments </code><br><code>and evidence in a balanced way, plausibly, does not.  </code><br><code>An advantage of classifying significant manipulative influences as ‚Äòhigh risk‚Äô, rather than </code><br><code>simply prohibiting outright, is that this allows for a nuanced regulatory approach that is also </code><br><code>responsive to the interests and moral and legal rights of those who place AI systems on the </code><br><code>market or make use of them in their services or products. Some such uses might, for instance, </code><br><code>be protected by the right to freedom of expression. Given the diversity of potential </code><br><code>manipulative AI influences, balancing the rights and interests of influencers with those of </code><br><code>influencee‚Äôs will, initially at least, need to be done on a case-by-case basis.   </code><br><code>An amendment along these lines is necessary for the Regulation to observe the demands of </code><br><code>fundamental rights, but also to realize the objective of developing trustworthy AI-systems </code><br><code>which are taken up by citizens. While it may lead to impediments in the dynamic </code><br><code>development of AI systems, it will primarily affect applications which in any case lie in an </code><br><code>ethical and legal grey area. Their elimination is in the medium-term interest not only of the </code><br><code>EU and its citizens, but also of industries and software developers wishing to create </code><br><code>trustworthy AI in competitive markets. </code><br><code> </code><br><code>Suggested Amendments </code><br><code> </code><br><code>Amendment 1 </code><br><code>Art. 5. 1 (a) should be replaced with:  </code><br><code>‚ÄúAI system that deploys subliminal techniques beyond a person‚Äôs consciousness in order to </code><br><code>influence her thoughts or opinions, or materially distort a person‚Äôs behaviour or decisions.‚Äù  </code><br><code> </code><br><code>Amendment 2 </code><br><code>Art. 5. 1 (b) should be replaced with:  </code><br><code>‚ÄúAI system that exploits any of the vulnerabilities of a specific group of persons due to their </code><br><code>age, physical or mental disability, in order to influence thoughts, opinions or weaken the </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665205_3,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665205.pdf,4,3,2665205,attachments/2665205.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>3 </code><br><code>preserving the professional and pedagogical autonomy and academic freedom of teachers </code><br><code>and academics. </code><br><code> </code><br><code>Transparency and AI literacy and CPD of teachers on AI: </code><br><code>ETUCE welcomes that the proposal of AI Regulation requires that </code><code>users of AI tools</code><code> (who </code><br><code>include students, teachers, academic and education staff for the education sector) must be </code><br><code>adequately informed</code><code> about the intended purpose, level of accuracy, residual risks of AI </code><br><code>tools. Nevertheless, ETUCE highlights that providing </code><code>information is not sufficient</code><code> to ensure </code><br><code>the transparency of the AI tools when users miss the adequate digital skills and data and AI</code><code> </code><br><code>literacy</code><code> to interpret it. Therefore, it is of utmost importance to improve the importance of </code><br><code>digital skills, AI literacy and data literacy</code><code> in educational curricula and raise awareness on</code><code> </code><br><code>the</code><code> risks</code><code> related to the use of AI tools in education. It is also essential to ensure that </code><br><code>infrastructures</code><code> of education institutions are adequately equipped for digital education as </code><br><code>well as to provide </code><code>equal access to digital technologies and ICT tools</code><code> to all teachers and </code><br><code>students, with particular attention to the most disadvantaged groups. To these purposes, </code><br><code>sustainable public investment should be provided by national governments and the </code><br><code>European Commission should provide financial support through European funding such as </code><br><code>Horizon Europe, Digital Europe and in the framework of National Recovery and Resilience </code><br><code>Facility. </code><br><code>While the AI Regulation blandly mention to the possibility of providing users with </code><code>training </code><br><code>on Artificial Intelligence</code><code>, ETUCE emphasises that it is crucial that sustainable public funding </code><br><code>are provided at national and European level to ensure that teachers, trainers, academics </code><br><code>and other education personnel receive </code><code>up-to-date and free of charge continuous training </code><br><code>and professional development on the use of AI tools</code><code> in accordance with their professional </code><br><code>needs.  </code><br><code> </code><br><code>EdTech expansion and issues of intellectual property rights, data privacy of teachers</code><code>:  </code><code> </code><br><code>ETUCE points out that the development of the use of Artificial Intelligence in education has </code><br><code>been accompanied by </code><code>the expansion of Ed-tech companies</code><code> that are progressively </code><br><code>increasing their influence in the education sector, especially under the pressure of </code><br><code>emergency online teaching and learning during the COVID-19 pandemic. ETUCE reminds </code><br><code>that education is a human right and public good whose value needs to be protected. ETUCE </code><br><code>calls for further public responsibility from national governments that should not limit their </code><br><code>scope to regulating the EdTech sector and should develop and implement public platforms </code><br><code>for online teaching and learning to protect the public value of education. In addition, public </code><br><code>platforms should be implemented in full respect of professional autonomy of teachers and </code><br><code>education personnel as well as academic freedom and autonomy of education institutions, </code><br><code>without creating pressure on teachers and education personnel regarding the education </code><br><code>material and pedagogical methods they use. It is also essential to </code><code>protect the accountability </code><br><code>and transparency</code><code> in the governance of public education systems from the influence of </code><br><code>private and commercial interests and actors.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665494_1,company,../24212003_requirements_for_artificial_intelligence/attachments/2665494.pdf,2,1,2665494,attachments/2665494.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>1 </code><br><code> </code><br><code> </code><br><code>AI EU ACT: Main issues identified by Intel and recommended corrections </code><br><code>Intel appreciates that the proposed Regulation takes a risk-based approach to Artificial Intelligence </code><br><code>(AI).  However, the complexity of the AI value chain and the fact that some key definitions leave </code><br><code>room for interpretation may result in uncertainty in determining which entities are considered </code><br><code>providers of AI systems and which AI systems are high risk. To address these concerns, we offer the </code><br><code>following comments to clarify which entities and products are in scope of the draft Regulation. </code><br><code>AI system</code><code> ‚ÄìWe recognize the difficulty of unambiguously defining AI and understand that the </code><br><code>proposed definition of an AI System in the draft Regulation is drawn from the OECD. As this </code><br><code>definition is overly broad, we support the approach to further qualify it by specifying the techniques </code><br><code>used to develop AI systems.   However, while the techniques listed in Annex I (a) ‚Äú</code><code>machine learning‚Äù</code><code> </code><br><code>and (b) ‚Äú</code><code>knowledge based‚Äù</code><code>  are intrinsically identified with AI, the ones listed in (c) ‚Äú</code><code>statistical </code><br><code>approaches, Bayesian estimation, search and optimization methods‚Äù </code><code>are techniques that are either </code><br><code>not unequivocally related to AI or just support other AI techniques. The designation ‚Äús</code><code>tatistical </code><br><code>approaches</code><code>‚Äù lacks precision, ‚Äú</code><code>Bayesian estimation‚Äù</code><code> is frequently used outside machine learning, and </code><br><code>‚Äú</code><code>search and optimization methods‚Äù</code><code> have been used for decades to perform various types of search, </code><br><code>training, and optimization (e.g. statistical estimation was used in 2G cellular systems). Not every </code><br><code>search algorithm, optimization problem, or statistical calculation is an AI problem. For this reason, </code><br><code>we recommend removing point (c) of Annex I, thereby focusing only on AI approaches and </code><br><code>techniques listed in points (a) and (b) of Annex I. </code><br><code>AI value chain</code><code> ‚Äì Recital 60 recognises the complexity of the AI value chain, made of ‚Äúrelevant third </code><br><code>parties, notably the ones involved in the sale and the supply of software, software tools  and </code><br><code>components, pre-trained models and data, or providers of network services‚Äù. The broad definition of </code><br><code>‚ÄúAI system‚Äù described above and subsequently of a ‚Äúprovider‚Äù poses serious challenges in </code><br><code>determining which AI systems and ‚Äúproviders‚Äù are in scope of the proposed Regulation. Therefore, </code><br><code>we suggest clarifying the proposed Regulation to differentiate roles in the AI value chain, such that </code><br><code>entities developing toolkits, software libraries, etc. are not considered ‚Äúproviders‚Äù by stating that</code><code> </code><br><code>‚Äúthese relevant third parties, inter alia,  providers of general purpose AI systems intended to be </code><br><code>incorporated or refined into a final product, are not considered providers of AI systems in the </code><br><code>proposed Regulation‚Äù</code><code>.</code><code> </code><br><code>Safety component</code><code> ‚Äì the draft Regulation utilises the concept of ‚Äúsafety component‚Äù in the </code><br><code>determination of the level of risk of an AI system. The proposed definition of what constitutes a </code><br><code>‚Äúsafety component‚Äù is open to interpretation and remains a source of uncertainty for the </code><br><code>qualification of high-risk AI systems. To reduce this ambiguity, we believe it is important that </code><br><code>references to ‚Äúsafety component‚Äù leverage EU harmonised legislation to align with any relevant </code><br><code>essential requirements. In other words, when assessing an AI system for the purposes of paragraph </code><br><code>1 of Article 6, a safety component is assessed based on the relevant EU harmonisation legislation </code><br><code>listed in Annex II. </code><br><code>Management and operation of critical infrastructure</code><code> ‚Äì the draft Regulation states that AI systems </code><br><code>intended to be used as safety components in the management and operation of road traffic and the </code>",NO_FOOTNOTES_ON_PAGE
fitz_2660610_3,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2660610.pdf,8,3,2660610,attachments/2660610.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>it is, it allows for the Regulation to focus on catching all AI possible (without</code><br><code>hampering innovation), and leave it up for the Annex I to define further. While</code><br><code>not all outputs for the AI may be relevant for anyone else but the AI or its</code><br><code>user/provider, this does not mean they should not potentially be able to be</code><br><code>regulated long term.</code><br><code>3.1.2</code><br><code>Preamble 15.</code><br><code>This preamble stands out</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> </code><code>for pointing out that AI can be used in novel ways to</code><br><code>be</code><code> tools for manipulative, exploitative and social control practices</code><code>. But pointing</code><br><code>out that AI can do this, without preventing existing tools and human means</code><br><code>to manipulate, exploit and socially control populations, is contradictive and</code><br><code>therefore goes against all existing legislation. What is meant here, is that if the</code><br><code>AI Regulation wants to prevent these practices when it comes to AI, the EU</code><br><code>should prevent these practices with all types of technologies at every instance</code><br><code>(including Member States that make use of these practices themselves). This is</code><br><code>especially important for the New Legislative Framework, as this novel approach</code><br><code>to preventing these means should be extended to all product legislation in the</code><br><code>EU.</code><br><code>There are therefore two ways to improve this preamble, the first being to</code><br><code>remove it entirely, or change preamble 15 into:</code><code> Like any technology or means</code><br><code>to manipulate, exploit and socially control humans, AI must be prevented from</code><br><code>being used for such goals as well, regardless of whether it is misused or not</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code>.</code><br><code>Such practices are particularly harmful and should be prohibited because they</code><br><code>contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to</code><br><code>non-discrimination, data protection and privacy and the rights of the child.</code><br><code>3.1.3</code><br><code>Preamble 47.</code><br><code>Any AI, regardless of high-risk or not, should not be incomprehensible to natural</code><br><code>persons.</code><br><code>This does not mean that no AI should be useable by anyone but</code><br><code>experts, but assuming the opposite seems to go against the spirit of the Act</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>.</code><br><code>The following suggested change therefore is:</code><code> To address the opacity that may</code><br><code>make AI systems incomprehensible to or too complex for natural persons, a</code><br><code>certain degree of transparency should be required for all AI systems, with higher</code><br><code>amounts of transparency needed for high-risk AI systems. Users should be able</code><br><code>to interpret the system output and use it appropriately. High-risk AI systems</code><br><code>should therefore be accompanied by documentation and instructions of use and</code><br><code>include concise and clear information, including in relation to possible risks to</code><br><code>fundamental rights and discrimination, where appropriate.</code><br><code>3</code><code>Also for the out of context use of ‚Äùthat‚Äù in the Act.</code><br><code>4</code><code>This way, the preamble does not sound like these practices are prevented elsewhere, as it</code><br><code>does now.</code><br><code>5</code><code>This is written knowing the intention is to regulate non-high-risk as little as possible.</code>",POSITIVE
fitz_2665600_4,eu_citizen,../24212003_requirements_for_artificial_intelligence/attachments/2665600.pdf,6,4,2665600,attachments/2665600.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Page </code><code>3</code><code> of </code><code>5</code><code> </code><br><code> </code><br><code>4. Principle-based obligations, non-discrimination and fairness </code><br><code>The Draft AI Regulation introduces</code><code style=""font-weight: 1000; background-color: #FF0000;"">8</code><code> explicit obligations around transparency, accuracy, robustness, </code><br><code>cybersecurity, data quality and governance. They are welcome ‚Äì in my view ‚Äì given the fundamental </code><br><code>concerns around those aspects of AI operations.  </code><br><code>Yet, the Draft somehow elegantly misses out on explicit obligations for non-discrimination and fairness. The </code><br><code>Draft introduces requirements for data quality/governance, which is much needed given that training, testing </code><br><code>and input data predetermine to a certain extent predictions and inferences that AI systems come up with. </code><br><code>However, those requirements are not sufficient to allay possible discriminatory or unfair outcomes from the </code><br><code>use of AI.</code><code style=""font-weight: 1000; background-color: #FF0000;"">9</code><code> The requirements for data quality essentially cater only for the quality of the input into the AI </code><br><code>system. They do not alone warrant bias-free conceptual design, technical development, practical </code><br><code>deployment and use of the system.</code><code style=""font-weight: 1000; background-color: #FF0000;"">10</code><code>     </code><br><code>Explicit obligations must be introduced in the final text of the Artificial Intelligence Act to the effect that AI </code><br><code>systems must be designed, developed, deployed and used in a way that ensures non-discrimination and </code><br><code>fair treatment of the individuals or categories of individuals concerned by the use of those AI systems.     </code><br><code> </code><br><code>5. Compliance requirements </code><br><code>The Draft AI Regulation introduces</code><code style=""font-weight: 1000; background-color: #FF0000;"">11</code><code> a complex web of compliance requirements that hinge on risk </code><br><code>assessment, quality management and data management. They would require internal controls and </code><br><code>dedicated resources by AI developers/providers in order to comply.  </code><br><code>Whether this would spur or rather stifle innovation would largely depend on the interpretation and </code><br><code>application of those requirements by the authorities in charge (a potentially large amount of them at national </code><br><code>level, given the institutional framework under the Draft). There are sufficient grounds for skepticisms in this </code><br><code>respect given how similar institutional frameworks have worked out in other areas (e.g. data protection,</code><code style=""font-weight: 1000; background-color: #FF0000;"">12</code><code> </code><br><code>consumer protection, anti-money laundering).  </code><br><code>If formalistic interpretations/application prevail, this would likely result in:  </code><br><code>(i) </code><br><code>The AI regulation turning into a </code><code>de facto</code><code> barrier to entry/expansion for smaller AI providers (e.g. </code><br><code>start-ups); </code><br><code>(ii) </code><br><code>Partnerships between start-ups and bigger, more resourceful organizations (most of which are </code><br><code>US-based corporations) in development of AI; </code><br><code>(iii) </code><br><code>Industry consolidation (similar trends have been observed in similarly regulated industries</code><code style=""font-weight: 1000; background-color: #FF0000;"">13</code><code>). </code><br><code>With this hindsight, consistent, sensible and pragmatic interpretation of and guidance on the requirements </code><br><code>under the Artificial Intelligence Act would be vital for promoting the development of AI in the EU. The </code><br><code>European Artificial Intelligence Board envisaged under Art. 56 of the Draft must be solely responsible for </code><br><code>providing guidance on the interpretation and implementation of the Artificial Intelligence Act. The national </code><br><code>authorities should play a limited role (as part of enforcement) in those areas as this would inevitably result </code><br><code>in divergence and legal uncertainty.    </code><br><code> </code><br><code>6. Self-assessment  </code><br><code>Self-assessment emerges as the main form of control mechanism under the Draft. While this allows for a </code><br><code>light-touch, less onerous regulation for the industry, similar attempts in other areas (e.g. antitrust, data </code><br><code>protection, anti-money laundering) have shown that this approach may result in suboptimal outcomes for </code><br><code>businesses (ultimately, more legal uncertainty, especially when coupled with high fines, as is the case under </code><br><code>the Draft) and society (general ethical and compliance race to the bottom).  </code>",POSITIVE
fitz_2665649_35,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665649.pdf,39,35,2665649,attachments/2665649.pdf#page=35,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>c) Respond  to  gaps  in  regulation  with  respect  to </code><br><code>economic  and  environmental  impact,  structural  forms  of </code><br><code>inequality and AI and migration control, law enforcement and </code><br><code>worker surveillance, mass surveillance, and exports of highrisk or prohibited AI outside the EU.</code><br><code>d) Remove loopholes in articles 2(4), and 83 leaving out of scope </code><br><code>of  the  AIA  AI  systems  used  as  part  of  international </code><br><code>agreements on law enforcement and large scale IT systems in </code><br><code>the migration control context. </code><br><code>e) Remove the broad exemption to forgo the duty to conduct a </code><br><code>conformity assessment on grounds of public security in article </code><br><code>47.  </code><br><code>f) Remove the exemption to the principle of purpose limitation </code><br><code>contained  in  article  54(1)(a)  for  ‚Äòinnovative  AI‚Äô  within  the </code><br><code>regulatory sandbox provisions for uses in the criminal justice </code><br><code>context. </code><br><code>3</code><br><code>Ensure responsibility to those subjected to AI systems </code><br><code>with enhanced obligations on users of all AI systems</code><br><code>In the current AIA framework, the majority of the requirements fall on providers  </code><br><code>to implement a series of technical measures designed to mitigate harm in the  </code><br><code>deployment  of  systems.  However,  many  of  these  harms  are  likely  to  be  </code><br><code>contextual  and  are  best  evaluated  and  addressed  by  the  user,  who  has </code><br><code>ultimate responsibilty to those subjected to the AI system. To ensure the use of </code><br><code>AI  systems  is  accountable  to  and  compliant  with  fundamental  rights,  we </code><br><code>recommend  that  the  requirements  on  providers  are  complemented  with  </code><br><code>obligations on users geared toward greater responsibility to those subjected to </code><br><code>AI systems.</code><br><code>a) Mandate  users  to  conduct  and  publish  an  ex  ante human </code><br><code>rights impact assessment before putting a high risk AI system </code><br><code>into use, clearly outlining the stated purpose for which the </code><br><code>system will be implemented:</code><br><code>i.</code><br><code>The impact assessment must be published on registration of use of </code><br><code>the system in the public database under article 60;</code><br><code>ii. This  impact  assessment  must  involve  prior  consultation  with </code><br><code>relevant national authorities, including equality bodies, consumer </code><br><code>protection agencies, and data protection agencies. If other impact </code><br><code>assessments are also required, these impact assessments must be </code><br><code>published together;</code><br><code>35</code>",NO_FOOTNOTES_ON_PAGE
fitz_2660159_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2660159.pdf,10,2,2660159,attachments/2660159.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>June 2021 </code><br><code>Page 2 of 10 </code><br><code>Instead, DGB calls for AI systems to be generally classified as high-risk if personal information in the </code><br><code>employment relationship is affected. This concerns both the realm of human resources administration </code><br><code>(such as the initiation of employment relationships), including the involvement of social security systems </code><br><code>and, in particular, the interaction of employees with AI systems in the work process (e.g. embodied </code><br><code>intelligence). It is crucial to prevent delimitation issues that primarily relate to new forms of humanmachine interaction or human-robot collaboration or imply algorithmic forms of control. It is necessary </code><br><code>to clarify whether this can be dealt with by the machinery regulation which is appropriate for the sector. </code><br><code> </code><br><code>DGB also calls for the legal exclusion of analysis procedures in the area of HR that turn employees into </code><br><code>objects by collecting information that cannot be deliberately controlled (‚Äúunacceptable risk‚Äù category). </code><br><code> </code><br><code>DGB further calls for a legal regulation according to which the use of personal information in the </code><br><code>employment context, in the case of AI use, requires not only individual consent but also an additional </code><br><code>agreement under collective rights that includes a transparent objective, access and usage regulations </code><br><code>and their limitations. If there is no works council or no collective bargaining agreement, the approval of </code><br><code>an authority could be obtained specifically or the ‚Äúapproval under collective rights‚Äù could be granted on </code><br><code>the basis of standard examples formulated by the supervisory authority. </code><br><code> </code><br><code> </code><br><code>3)</code><code> </code><code>DGB expressly criticises the fact that the EU Commission‚Äôs proposal does not include any process </code><br><code>requirements for participation and co-determination options for the operational use of AI systems. This </code><br><code>concerns the participation of social partners, and co-determination as well as the participation of </code><br><code>affected employees. The EU Commission already determined in the White Paper for the 2020 regulatory </code><br><code>proposal that ‚Äúinvolvement of social partners [...] is a crucial factor in ensuring a human-centred </code><br><code>approach to AI at work‚Äù. In the draft that is now currently being submitted, the participation of social </code><br><code>partners is no longer mentioned. </code><br><code> </code><br><code>DGB therefore calls for procedural regulation on operational use to enable preventive, nondiscriminatory, gender-sensitive and holistic work design, including, in particular, an operational impact </code><br><code>assessment (risk management system), the intended testing procedures (Article 7 (5)), the quality </code><br><code>management system (Article 17) for sufficient transparency and traceability and continuous evaluation </code><br><code>of the learning systems in the organisation and intervention options. Consideration of collective </code><br><code>agreements can and should be integrated in a manner that is analogous to the GDPR (Article 88). The </code><br><code>impact on operational work processes (employment prospects, profile changes, occupational health and </code><br><code>safety, etc.) must be explicitly considered in the ‚Äòrisk management system‚Äô required for high-risk </code><br><code>applications. The opportunities for employees and their representatives to participate in shaping the </code><br><code>process must urgently be reinforced and be binding and process-oriented in order to resolve conflicts in </code><br><code>objectives in a socially acceptable manner and to prevent unintended side effects in working life that </code><br><code>contradict European values. It should be clearly stipulated that company use of such technology can only </code><br><code>take place with mandatory participation of employee representatives, e.g. through the conclusion of </code><br><code>collective agreements. This must apply to the rollout of the technology and to its operational </code><br><code>implementation. It must be ensured that employee representatives have access to the relevant </code><br><code>information throughout the entire AI ‚Äúsupply chain‚Äù, that is, all the way up to the ‚Äúprovider‚Äù.  </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665514_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665514.pdf,2,1,2665514,attachments/2665514.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Feedback on public consultation on</code><br><code>‚ÄúArtificial intelligence ‚Äì ethical and legal requirements‚Äù</code><br><code>Artificial intelligence association of Lithuania supports the European Commission‚Äôs proposal</code><br><code>for the EU Artificial Intelligence Act (AIA).</code><br><code>Firstly we support the need for transparency for non-high-risk systems, for example chatbots</code><br><code>which lead a user to believe they are interacting with a human being. Second, we support</code><br><code>the effort to follow the guidelines to ensure AI will be created by ensuring the European</code><br><code>Union values, fundamental rights and principles. Having said this, we express a few areas to</code><br><code>consider.</code><br><code>‚óè</code><br><code>Provide more details on regulators. Since the fines are presented in the AIA. We are</code><br><code>concerned that there is missing information about the regulators and guidelines on</code><br><code>how and what will be responsible in each country to enforce article 5 and 10. Also,</code><br><code>which institutions will be the ones to help for all stakeholders for the consulting on</code><br><code>whether the application falls within high-risk or even prohibited applications. Without</code><br><code>the self assessment protocols, self standardisation, the current AIA might become an</code><br><code>insurmountable obstacle for SME and startups, to follow and ensure all guidelines</code><br><code>mentioned in AIA.</code><br><code>‚óè</code><br><code>Expand definition of 'internal control'. Common practice is that AI solutions are used</code><br><code>not only in border controls, airports, but also by migration departments, foreign</code><br><code>offices, criminal law enforcements, shops, self checkout shops and other commercial</code><br><code>uses. Current exception for  'internal control' is limiting all other applications, which</code><br><code>should fall under the mentioned definition.</code><br><code>‚óè</code><br><code>Improve statement (33) ""Technical inaccuracies of AI systems intended for the</code><br><code>remote biometric identification of natural persons can lead to biased results and</code><br><code>entail discriminatory effects. This is particularly relevant when it comes to age,</code><br><code>ethnicity, sex or disabilities. Therefore, ‚Äòreal-time‚Äô and ‚Äòpost‚Äô remote biometric</code><br><code>identification systems should be classified as high-risk. In view of the risks that they</code><br><code>pose, both types of remote biometric identification systems should be subject to</code><br><code>specific requirements on logging capabilities and human oversight.""</code><br><code>Is technically false and should be revised. For example the fingerprint recognition,</code><br><code>which can be run by touchless photo scanner, can produce the accuracy significantly</code><br><code>higher than any human. However such a system would fall under remote recognition</code><br><code>if it is touchless, and biometric since fingerprint is biometric data. We recommend,</code><br><code>additional statement ""Technical inaccuracies of AI systems intended for the remote</code><br><code>biometric identification of natural persons can lead to biased results and entail</code><br><code>discriminatory effects, if not certified otherwise (ex. NIST evaluations)"".</code><br><code>https://www.nist.gov/itl/iad/image-group/minutiae-interoperability-exchange-minex-iii</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665440_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665440.pdf,8,2,2665440,attachments/2665440.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>  </code><br><code> </code><br><code> </code><br><code>www.homodigitalis.gr</code><code>, ŒëœáŒΩŒπŒ¨Œ¥œâŒΩ 17-19, 118 54,  ŒëŒ∏ŒÆŒΩŒ±-Œ°ŒøœÖœÜ, ŒëœÑœÑŒπŒ∫ŒÆ, </code><code>info@homodigitalis.gr</code><code> </code><br><code> </code><br><code> </code><br><code>ŒïŒπœÉŒ±Œ≥œâŒ≥ŒÆ </code><br><code> </code><br><code>H </code><code>Homo Digitalis</code><code> Œ±œÄŒøœÑŒµŒªŒµŒØ ŒëœÉœÑŒπŒ∫ŒÆ ŒúŒ∑ ŒöŒµœÅŒ¥ŒøœÉŒ∫ŒøœÄŒπŒ∫ŒÆ ŒïœÑŒ±ŒπœÅŒØŒ± ŒºŒµ Œ≠Œ¥œÅŒ± œÑŒ∑ŒΩ ŒëŒ∏ŒÆŒΩŒ± Œ∫Œ±Œπ </code><br><code>œÉŒ∫ŒøœÄœå œÑŒ∑ œÄœÅŒøœÉœÑŒ±œÉŒØŒ± œÑœâŒΩ ŒîŒπŒ∫Œ±ŒπœâŒºŒ¨œÑœâŒΩ œÑŒøœÖ ŒëŒΩŒ∏œÅœéœÄŒøœÖ œÉœÑŒ∑ œÉœçŒ≥œáœÅŒøŒΩŒ∑ Œ∫ŒøŒπŒΩœâŒΩŒØŒ± œÑŒ∑œÇ </code><br><code>œàŒ∑œÜŒπŒ±Œ∫ŒÆœÇ œÄŒªŒ∑œÅŒøœÜŒøœÅŒØŒ±œÇ. ŒúŒµ œÑŒ∑ œÄŒ±œÅŒøœçœÉŒ± œÖœÄŒøŒ≤ŒøŒªŒÆ ŒµœÄŒπŒ∏œÖŒºŒøœçŒºŒµ ŒΩŒ± œÉœÖŒºŒºŒµœÑŒ≠œáŒøœÖŒºŒµ œÉœÑŒ∑ŒΩ </code><br><code>Œ±ŒΩŒøŒπœáœÑŒÆ Œ¥ŒπŒ±Œ≤ŒøœçŒªŒµœÖœÉŒ∑ œÑŒ∑œÇ ŒïœÖœÅœâœÄŒ±œäŒ∫ŒÆœÇ ŒïœÄŒπœÑœÅŒøœÄŒÆœÇ œÉœÑŒø œÄŒªŒ±ŒØœÉŒπŒø œÑœâŒΩ œÄœÅŒøœÑŒµŒπŒΩœåŒºŒµŒΩœâŒΩ </code><br><code>Œ¥ŒπŒ±œÑŒ¨ŒæŒµœâŒΩ Œ≥ŒπŒ± œÑŒ∑ŒΩ œÖŒπŒøŒ∏Œ≠œÑŒ∑œÉŒ∑ ŒµŒΩœåœÇ Œ∫Œ±ŒΩŒøŒΩŒπœÉŒºŒøœç œÉœÑŒøŒΩ œÑŒøŒºŒ≠Œ± œÑŒ∑œÇ Œ§ŒµœáŒΩŒ∑œÑŒÆœÇ ŒùŒøŒ∑ŒºŒøœÉœçŒΩŒ∑œÇ </code><br><code>(Œ§Œù). ŒòŒ± Œ∏Œ≠ŒªŒ±ŒºŒµ ŒΩŒ± ŒµœÖœáŒ±œÅŒπœÉœÑŒÆœÉŒøœÖŒºŒµ œÑŒ∑ŒΩ ŒïœÄŒπœÑœÅŒøœÄŒÆ Œ≥ŒπŒ± œÑŒ∑ Œ¥œÖŒΩŒ±œÑœåœÑŒ∑œÑŒ± œÄŒøœÖ ŒºŒ±œÇ œÄŒ±œÅŒ≠œáŒµŒπ </code><br><code>ŒΩŒ± ŒµŒ∫œÜœÅŒ¨œÉŒøœÖŒºŒµ œÑŒπœÇ Œ±œÄœåœàŒµŒπœÇ Œ∫Œ±Œπ œÄŒ±œÅŒ±œÑŒ∑œÅŒÆœÉŒµŒπœÇ ŒºŒ±œÇ.  </code><br><code> </code><br><code>ŒïœÄŒπœÄœÅŒøœÉŒ∏Œ≠œÑœâœÇ, œÉŒµ œÉœÖŒΩŒ≠œáŒµŒπŒ± œÑœâŒΩ œÄŒ±œÅŒ±œÑŒ∑œÅŒÆœÉŒµœâŒΩ œÄŒøœÖ œÖœÄŒøŒ≤Œ¨ŒªŒøœÖŒºŒµ ŒµŒΩœéœÄŒπŒøŒΩ œÑŒ∑œÇ </code><br><code>ŒïœÄŒπœÑœÅŒøœÄŒÆœÇ, Œ∏Œ± ŒµœÄŒπŒ∏œÖŒºŒøœçœÉŒ±ŒºŒµ ŒΩŒ± œÖœÄŒøœÉœÑŒ∑œÅŒØŒæŒøœÖŒºŒµ ŒµœÄŒØœÉŒ∑ŒºŒ± Œ∫Œ±Œπ œÑŒπœÇ Œ∏Œ≠œÉŒµŒπœÇ œÑŒøœÖ Œ¥ŒπŒ∫œÑœçŒøœÖ </code><br><code>European Digital Rights (EDRi), œåœÄœâœÇ Œ±œÖœÑŒ≠œÇ Œ≠œáŒøœÖŒΩ Œ±ŒΩŒ±œÅœÑŒ∑Œ∏ŒµŒØ œÉœÑŒ∑ŒΩ ŒπœÉœÑŒøœÉŒµŒªŒØŒ¥Œ± œÑŒ∑œÇ </code><br><code>Œ¥Œ∑ŒºœåœÉŒπŒ±œÇ Œ¥ŒπŒ±Œ≤ŒøœçŒªŒµœÖœÉŒ∑œÇ</code><code>.  </code><br><code> </code><br><code>ŒëŒ∫ŒøŒªŒøœÖŒ∏ŒµŒØ Œ∑ Œ±ŒΩŒ¨ŒªœÖœÉŒ∑ œÑŒ∑œÇ œÉœÖŒΩœÑŒ±Œ∫œÑŒπŒ∫ŒÆœÇ ŒøŒºŒ¨Œ¥Œ±œÇ œÑŒ∑œÇ Homo Digitalis, Œ∑ ŒøœÄŒøŒØŒ± ŒµœÉœÑŒπŒ¨Œ∂ŒµŒπ </code><br><code>œÑŒø ŒµŒΩŒ¥ŒπŒ±œÜŒ≠œÅŒøŒΩ œÑŒ∑œÇ œÉŒµ ŒºŒØŒ± œÉŒµŒπœÅŒ¨ Œ±œÄœå Œ¨œÅŒ∏œÅŒ± œÑŒøœÖ œÄœÅŒøœÑŒµŒπŒΩœåŒºŒµŒΩŒøœÖ Œ∫Œ±ŒΩŒøŒΩŒπœÉŒºŒøœç, ŒºŒµœÑŒ±Œæœç œÑœâŒΩ </code><br><code>ŒøœÄŒøŒØœâŒΩ œÅœÖŒ∏ŒºŒØŒ∂ŒøŒΩœÑŒ±Œπ Œø ŒøœÅŒπœÉŒºœåœÇ œÑŒ∑œÇ Œ§Œù, ŒøŒπ ŒµœÜŒ±œÅŒºŒøŒ≥Œ≠œÇ Œ∫Œ±Œπ œáœÅŒÆœÉŒµŒπœÇ Œ§Œù œÄŒøœÖ œÑœÖŒ≥œáŒ¨ŒΩŒøœÖŒΩ </code><br><code>Œ±œÄŒ±Œ≥œåœÅŒµœÖœÉŒ∑œÇ Œ∫Œ±Œπ ŒøŒπ ŒµŒæŒ±ŒπœÅŒ≠œÉŒµŒπœÇ ŒµœÄŒØ Œ±œÖœÑœéŒΩ, œÑŒ± œÉœÖœÉœÑŒÆŒºŒ±œÑŒ± Œ§Œù œÖœàŒ∑ŒªŒøœç Œ∫ŒπŒΩŒ¥œçŒΩŒøœÖ, Œ∑ </code><br><code>ŒªŒµŒπœÑŒøœÖœÅŒ≥ŒØŒ± Œ∫Œ±Œπ œÉœçœÉœÑŒ±œÉŒ∑ œÑŒøœÖ ŒïœÖœÅœâœÄŒ±œäŒ∫Œøœç Œ£œÖŒºŒ≤ŒøœÖŒªŒØŒøœÖ Œ§Œù, ŒøŒπ Œ∫œéŒ¥ŒπŒ∫ŒµœÇ Œ¥ŒµŒøŒΩœÑŒøŒªŒøŒ≥ŒØŒ±œÇ, </code><br><code>Œ∫Œ±Œπ ŒøŒπ œÉœáŒµœÑŒπŒ∫Œ≠œÇ Œ∫œÖœÅœéœÉŒµŒπœÇ œÄŒøœÖ œÄœÅŒøŒ≤ŒªŒ≠œÄŒøŒΩœÑŒ±Œπ œÉŒµ œÄŒµœÅŒØœÄœÑœâœÉŒ∑ œÄŒ±œÅŒ±Œ≤ŒØŒ±œÉŒ∑œÇ œÑœâŒΩ </code><br><code>œÄœÅŒøœÑŒµŒπŒΩœåŒºŒµŒΩœâŒΩ Œ¥ŒπŒ±œÑŒ¨ŒæŒµœâŒΩ, Œ∫.Œ±. </code><br><code>ŒïŒΩœåœÑŒ∑œÑŒ± 1: Œ†Œ±œÅŒ±œÑŒ∑œÅŒÆœÉŒµŒπœÇ ŒµœÄŒØ œÑŒøœÖ œÄœÅŒøœÑŒµŒπŒΩœåŒºŒµŒΩŒøœÖ ŒøœÅŒπœÉŒºŒøœç Œ≥ŒπŒ± </code><br><code>œÑŒ∑ŒΩ Œ§ŒµœáŒΩŒ∑œÑŒÆ ŒùŒøŒ∑ŒºŒøœÉœçŒΩŒ∑  </code><br><code> </code><br><code>ŒëœÄœå œÑŒ± œÄœÅœéœÑŒ± Œ¨œÅŒ∏œÅŒ± œÑŒ∑œÇ œÄœÅœåœÑŒ±œÉŒ∑œÇ œÑŒøœÖ Œ∫Œ±ŒΩŒøŒΩŒπœÉŒºŒøœç ŒµŒΩœÑŒøœÄŒØŒ∂ŒµœÑŒ±Œπ Œø Œ±ŒΩŒ∏œÅœâœÄŒøŒ∫ŒµŒΩœÑœÅŒπŒ∫œåœÇ </code><br><code>œÉŒ∫ŒøœÄœåœÇ Œ±œÖœÑŒøœç, Œ∑ ŒµŒ≥Œ≥œçŒ∑œÉŒ∑ œÑœâŒΩ Œ±ŒΩŒ∏œÅœâœÄŒØŒΩœâŒΩ Œ¥ŒπŒ∫Œ±ŒπœâŒºŒ¨œÑœâŒΩ Œ∫Œ±Œ∏œéœÇ Œ∫Œ±Œπ Œ∑ Œ±œÄŒøŒ¥ŒøœáŒÆ œÑŒ∑œÇ Œ§Œù </code><br><code>œÉœÑŒøŒΩ ŒµœÄŒπœáŒµŒπœÅŒ∑ŒºŒ±œÑŒπŒ∫œå Œ∫Œ±Œπ ŒµœÄŒµŒΩŒ¥œÖœÑŒπŒ∫œå œáœéœÅŒø. Œ£œÑœåœáŒøœÇ ŒµŒØŒΩŒ±Œπ Œø ŒΩŒ≠ŒøœÇ Œ∫Œ±ŒΩŒøŒΩŒπœÉŒºœåœÇ ŒΩŒ± Œ≠œáŒµŒπ </code><br><code>Œ¨ŒºŒµœÉŒ∑ ŒµœÜŒ±œÅŒºŒøŒ≥ŒÆ ŒºŒµ œÑŒøŒΩ ŒØŒ¥ŒπŒø œÑœÅœåœÄŒø œÉŒµ œåŒªŒ± œÑŒ± KœÅŒ¨œÑŒ∑ MŒ≠ŒªŒ∑ Œ≤Œ¨œÉŒµŒπ ŒµŒΩœåœÇ Œ¥ŒπŒ±œáœÅŒøŒΩŒπŒ∫Œøœç </code><br><code>ŒøœÅŒπœÉŒºŒøœç Œ≥ŒπŒ± œÑŒ∑ŒΩ Œ§Œù.  </code><br><code> </code><br><code>ŒåŒºœâœÇ, Œø ŒøœÅŒπœÉŒºœåœÇ œÄŒøœÖ œÄœÅŒøœÑŒµŒØŒΩŒµœÑŒ±Œπ Œ≥ŒπŒ± œÑŒ∑ŒΩ Œ§Œù ( ŒÜœÅŒ∏œÅŒø 2 œÄŒµœÅ.1), œÄŒ±œÅŒ¨ œÑŒø Œ≥ŒµŒ≥ŒøŒΩœåœÇ œåœÑŒπ </code><br><code>Œ±œÄŒøœÑŒµŒªŒµŒØ ŒºŒπŒ± œÄœÅœéœÑŒ∑ Œ±œÄœåœÄŒµŒπœÅŒ± ŒøœÅŒπœÉŒºŒøœç œÉŒµ ŒµœÄŒØœÄŒµŒ¥Œø ŒΩŒøŒºŒøŒ∏ŒµœÑŒπŒ∫ŒÆœÇ œÄŒªŒ≠ŒøŒΩ œÄœÅœâœÑŒøŒ≤ŒøœÖŒªŒØŒ±œÇ </code><br><code>ŒµŒΩœÑœåœÇ œÑŒ∑œÇ ŒïŒï,  Œ¥ŒµŒΩ Œ∫Œ±ŒªœçœÄœÑŒµŒπ œÄŒªŒÆœÅœâœÇ œÑŒ∑ŒΩ Œ≠ŒΩŒΩŒøŒπŒ± Œ∫Œ±Œπ œÑŒ± œáŒ±œÅŒ±Œ∫œÑŒ∑œÅŒπœÉœÑŒπŒ∫Œ¨ </code><br><code>Œ±œÖœÑŒÆœÇ.  Œ£œÖŒ≥Œ∫ŒµŒ∫œÅŒπŒºŒ≠ŒΩŒ±, Œø ŒµŒΩ ŒªœåŒ≥œâ ŒºŒ±Œ∫œÅŒøœÉŒ∫ŒµŒªŒÆœÇ ŒøœÅŒπœÉŒºœåœÇ Œ¥ŒµŒΩ œÄŒµœÅŒπŒªŒ±ŒºŒ≤Œ¨ŒΩŒµŒπ œÉœÑŒøŒπœáŒµŒØŒ± </code><br><code>ŒøœÅŒπœÉŒºœéŒΩ œÄŒøœÖ Œ∫Œ±œÑŒ¨ œÑŒø œÄŒ±œÅŒµŒªŒ∏œåŒΩ Œ≠œáŒøœÖŒΩ Œ¥ŒøŒ∏ŒµŒØ œÄœÅŒøŒ∫ŒµŒπŒºŒ≠ŒΩŒøœÖ ŒΩŒ± ŒøœÅŒπœÉœÑŒµŒØ œÑŒø œÑŒπ ŒµŒØŒΩŒ±Œπ Œ§Œù Œ±œÄœå </code><br><code>Œ¨ŒªŒªŒøœÖœÇ œÜŒøœÅŒµŒØœÇ Œ¥ŒπŒµŒ∏ŒΩœéœÇ, œÉœÖŒºœÄŒµœÅŒπŒªŒ±ŒºŒ≤Œ±ŒΩŒøŒºŒ≠ŒΩœâŒΩ œÉœáŒµœÑŒπŒ∫œéŒΩ ŒøŒºŒ¨Œ¥œâŒΩ ŒµœÅŒ≥Œ±œÉŒØŒ±œÇ Œ∫Œ±Œπ ŒµŒΩœÑœåœÇ </code><br><code>œÑŒ∑œÇ ŒïŒï. ŒöŒøŒπŒΩœå œáŒ±œÅŒ±Œ∫œÑŒ∑œÅŒπœÉœÑŒπŒ∫œå œÑœâŒΩ ŒøœÅŒπœÉŒºœéŒΩ Œ±œÖœÑœéŒΩ ŒµŒØŒΩŒ±Œπ œåœÑŒπ ŒµŒØŒΩŒ±Œπ Œ±ŒΩŒ∏ŒµŒ∫œÑŒπŒ∫ŒøŒØ œÉŒµ </code><br><code>œÑŒµœáŒΩŒøŒªŒøŒ≥ŒπŒ∫Œ≠œÇ ŒºŒµœÑŒ±Œ≤ŒøŒªŒ≠œÇ, Œ∫Œ±Œ∏œéœÇ Œ±Œ∫ŒøŒªŒøœÖŒ∏ŒøœçŒΩ ŒºŒØŒ± ŒøœÖŒ¥Œ≠œÑŒµœÅŒ∑ œÄœÅŒøœÉŒ≠Œ≥Œ≥ŒπœÉŒ∑, ŒºŒµ Œ≤Œ¨œÉŒ∑ œÑŒ∑ŒΩ </code><br><code>ŒøœÄŒøŒØŒ± Œø ŒøœÅŒπœÉŒºœåœÇ œÑŒ∑œÇ Œ§Œù Œ¥ŒµŒΩ œÄœÅŒ≠œÄŒµŒπ ŒΩŒ± Œ±œÄŒøœÑŒµŒªŒµŒØœÑŒ±Œπ Œ±œÄœå ŒºŒπŒ± œÄŒµœÅŒπŒ≥œÅŒ±œÜŒÆ œÑœâŒΩ œÑŒµœáŒΩŒøŒªŒøŒ≥ŒπœéŒΩ </code><br><code>œÄŒøœÖ Œ±œÖœÑŒÆ Œ±œÄŒ±œÅœÑŒØŒ∂ŒµŒπ œÄœÅŒøŒ∫ŒµŒπŒºŒ≠ŒΩŒøœÖ ŒΩŒ± Œ±œÄŒøœÜŒµœçŒ≥ŒµœÑŒ±Œπ Œ∑ œÉœÖŒΩŒµœáŒÆœÇ ŒµŒΩŒ∑ŒºŒ≠œÅœâœÉŒ∑ Œ±œÖœÑŒøœç Œ∫Œ¨Œ∏Œµ </code><br><code>œÜŒøœÅŒ¨ œÄŒøœÖ ŒºŒµœÑŒ±Œ≤Œ¨ŒªŒªŒøŒΩœÑŒ±Œπ ŒøŒπ ŒΩŒ≠ŒµœÇ œÑŒµœáŒΩŒøŒªŒøŒ≥ŒØŒµœÇ.  </code><br><code> </code><br><code>ŒïœÄŒπœÄŒªŒ≠ŒøŒΩ, Œø ŒøœÅŒπœÉŒºœåœÇ Œ±œÖœÑœåœÇ Œ≠œáŒµŒπ œáŒ±œÅŒ±Œ∫œÑŒÆœÅŒ± Œ∫Œ±Œ∏Œ±œÅŒ¨ Œ±œåœÅŒπœÉœÑŒø ŒºŒµ ŒµœÄŒπœÅœÅŒøŒ≠œÇ Œ±œÄœå </code><br><code>Œ±ŒΩŒ∏œÅœéœÄŒøœÖœÇ œÑŒøœÖ ŒµœÄŒπœáŒµŒπœÅŒ∑ŒºŒ±œÑŒπŒ∫Œøœç & ŒµœÄŒµŒΩŒ¥œÖœÑŒπŒ∫Œøœç Œ∫œåœÉŒºŒøœÖ. ŒüŒπ Œ±œÉŒ¨œÜŒµŒπŒµœÇ œÄŒøœÖ </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662802_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662802.pdf,4,2,2662802,attachments/2662802.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>2/4 </code><br><code> </code><br><code> </code><br><code>Scope (article 2) </code><br><code> </code><br><code>As a reminder, the manufacturer of a machine must carry out a risk assessment, according to the general principles </code><br><code>and safety integration principles (paragraph 1.1.2 of Annex I of the Machinery Directive) and determine which essential requirements are applicable. Subsequently, he can use technical references, specifically harmonised standards, in order to implement solutions complying with the state of the art. The cornerstone of the New Approach is </code><br><code>to ensure technology neutrality, i.e. the legislator is seeking results without imposing the means. </code><br><code> </code><br><code>Artificial intelligence is a technical tool among others, allowing to improve the functioning of a machinery and to </code><br><code>make available new functionalities to the user. As a technology, it does not intrinsically create a new hazard. </code><br><code>FIM proposal </code><br><code>Considering these elements and those presented in the introduction, we strongly believe that AI systems integrated </code><br><code>in products already in the scope of the New Approach regulation and used in the workplace should be excluded from </code><br><code>this Regulation‚Äôs scope. </code><br><code>Definitions (article 3) </code><br><code>AI Definition </code><br><code>FIM draws attention to the fact that, despite European and international past and present work, no given definition </code><br><code>of artificial intelligence has reached a consensus, due mainly to the fact that there is no objective criterion to distinguish the concept of artificial intelligence from a conventional algorithm. </code><br><code>We also note a gap between the definition given in the ""Ethics Guidelines for Trustworthy AI"" and the one adopted </code><br><code>in this draft Regulation. </code><br><code>Finally, the definition of artificial intelligence (Article 3.1) is too broad and includes software applications already </code><br><code>widely used in the industry. This is the case, for example, for logic-based approaches.  </code><br><code>Substantial modification definition  </code><br><code>Furthermore, the introduction of substantial modification concept in the Artificial intelligence draft Regulation is a </code><br><code>source of legal insecurity, as it is in the draft revision of the Machinery Directive (</code><code>please see FIM position on this </code><br><code>matter</code><code>). </code><br><code>FIM proposal  </code><br><code>In this context, we propose the following amendment:  </code><br><code> </code><br><code>‚ÄúSubstantial modification‚Äô means a change to the AI system following its placing on the market or putting </code><br><code>into service, </code><code>not foreseen by the provider</code><code>, which </code><code>may</code><code> affect the compliance of the AI system with the requirements set out in Title III, Chapter 2 of this Regulation or </code><code>and</code><code> result in a modification to the intended </code><br><code>purpose for which the AI system has been assessed;‚Äù </code><br><code>Requirements and obligations of Chapters II et III of Title III </code><br><code>The different provisions of Articles 8 to 15 seem to be disproportionate to the objective of the draft Regulation. </code><br><code> </code><br><code>Moreover, the risks these provisions address have already been taken into account by the essential requirements of </code><br><code>the various regulations listed in Annex II, Section A, in particular by the Directive 2006/42/EC. Only, the legal logic </code><br><code>adopted by these regulations is different from the one in the AI draft: they allow manufacturers to implement agile </code><br><code>and proportionate solutions to achieve health and safety general objective.  </code><br><code> </code><br><code>Article 9 introduces the issue of the risk management system, which consists of a continuous iterative process run </code><br><code>throughout the life cycle, thus requiring regular systematic updating. </code><br><code> </code><br><code>This requirement is contrary to the current provisions of the NLF. In the case of a machinery incorporating a ""high </code><br><code>risk"" AI system, the New Legislative Framework states that it must remain safe throughout its life cycle. While it is </code><br><code>likely that machineries may continue to learn (supervised or unsupervised learning), after they have been put into </code><br><code>service, this functionality can only be the result of a deliberate intention by the manufacturer, for example to improve </code><br><code>the performance of the machine. In this context, the manufacturer must, by design, control the learning capability, </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665345_11,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665345.pdf,17,11,2665345,attachments/2665345.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>ÔÇß</code><code> </code><br><code>Pflichtcheck vor dem Griff nach biometrischen Daten</code><code>: Vor jedem Einsatz biometrischer Daten sollten Datenschutzbeh√∂rden angesichts des hohen Risiko- und Schadenspotenzials pr√ºfen, ob die Verarbeitung biometrischer Daten notwendig und sinnvoll ist. </code><br><code>ÔÇß</code><code> </code><br><code>Onlinebanking und andere Anwendungen ohne bleibende biometrische Daten</code><code>: Es darf zu </code><br><code>keiner dauerhaften Speicherung von biometrischen Daten kommen, um das Risiko von Identit√§tsdiebstahl zu minimieren.  </code><br><code>ÔÇß</code><code> </code><br><code>Gesichtsfotos als sensible Daten</code><code>: Onlinefotos werden bereits in unz√§hligen F√§llen f√ºr die </code><br><code>Identifikation von Personen durch Gesichtserkennung genutzt.  </code><br><code>Rechtlich ist offen, inwieweit diese Daten als biometrisch gelten. Hier besteht dringend Bedarf, </code><br><code>Portr√§ts vor versteckter biometrischer Auswertung zu sch√ºtzen. </code><br><code> </code><br><code>Klassifizierung von KI-Systemen als hoch-riskant (Art 6) </code><br><code> </code><br><code>Hochriskant sind KI-Systeme nur dann, wenn sie als Sicherheitskomponente oder -produkt nach den in </code><br><code>Anhang II angef√ºhrten Harmonisierungsrechtsvorschriften gelten. Zus√§tzlich muss die Sicherheitskomponente bzw das Sicherheitsprodukt einer Konformit√§tsbewertung durch Dritte wiederum nach den in </code><br><code>Anhang II angef√ºhrten Harmonisierungsrechtsvorschriften unterzogen werden.</code><code> </code><br><code>KI w√§re nur dann hochriskant, wenn sie mit einer externen Zertifizierung nach den ‚ÄûNew Approach‚Äú RL </code><br><code>√ºber die technische Produktkonformit√§t verbunden ist. F√ºr die Qualifizierung eines KI-Sicherheitsproduktes als hochriskant kann aber nicht ernsthaft ausschlaggebend sein, ob es einer New Approach RL </code><br><code>unterliegt und nach dieser extern zu zertifizieren ist (was im √úbrigen selten der Fall ist). Dieser Ansatz </code><br><code>ist verfehlt und muss durch sachgerechte Kriterien ersetzt werden. </code><br><code>Als hochriskant gelten zudem die im Annex III aufgez√§hlten Anwendungen. Diese Liste sollte nur deskriptiv sein, denn wichtige Bereiche finden gar keine Erw√§hnung (zB KI, die sensible Gesundheitsdaten </code><br><code>benutzt, Betrugs- und Missbrauchserkennung aufgrund des Kundenverhaltens, werblich-manipulative </code><br><code>Beeinflussung des Nutzerverhaltens, Produktempfehlungen, Nachrichtenselektion uvm).  </code><br><code>Die EU-Kommission kann zwar den Annex III erg√§nzen, allerdings nur die bereits angelegten Kategorien </code><br><code>um weitere Beispiele erweitern. Neue Kategorien sind ausgeschlossen. Damit k√∂nnen wichtige, verbraucherrelevante Bereiche nicht erfasst werden. Zudem muss von weiteren Beispielen ein hohes Risiko in Form von Sch√§den an Gesundheit oder Sicherheit oder eine negative Beeintr√§chtigung von </code><br><code>Grundrechten ausgehen. Wirtschaftliche Sch√§den sind nicht erw√§hnt.  </code><br><code> </code><br><code>Anmerkungen zum Annex:  </code><br><code> </code><br><code>ÔÇß</code><code> </code><br><code>Die Erfassung von ‚Äû</code><code>KI-Systemen, die f√ºr die biometrische Echtzeit-Fernidentifizierung‚Äú</code><code> </code><br><code>verwendet werden sollen ist zu eng. Biometrische KI-Systeme sind auch dort im Vormarsch, bei </code><br><code>denen keine ‚ÄûFern‚Äú-Identifikation stattfindet (Onlinebanking, Ger√§teentsperrung etc). Aufgrund </code><br><code>der hohen Missbrauchsgefahr und den Fehlerraten sollten auch diese Anwendungen mitreguliert werden.  </code><br><code>ÔÇß</code><code> </code><br><code>Bei der ‚Äû</code><code>Zug√§nglichkeit und Inanspruchnahme grundlegender privater und √∂ffentlicher </code><br><code>Dienste und Leistungen</code><code>‚Äú bedarf es erl√§uternder Beispiele, was darunterf√§llt. </code><br><code>ÔÇß</code><code> </code><br><code>Die ‚Äû</code><code>Kleinanbieter-Ausnahme f√ºr den Eigengebrauch</code><code>‚Äú in Bezug auf ‚ÄûKI-Systeme, die f√ºr die </code><br><code>Kreditw√ºrdigkeitspr√ºfung und Kreditpunktebewertung‚Äú</code><code> verwendet werden sollen sollte kritisch hinterfragt werden. Risiken bestehen unabh√§ngig von der Unternehmensgr√∂√üe.  </code><br><code>ÔÇß</code><code> </code><br><code>‚Äû</code><code>KI-Systeme, die von Strafverfolgungsbeh√∂rden f√ºr individuelle Risikobewertungen</code><code> nat√ºrlicher Personen verwendet werden sollen, um das Risiko abzusch√§tzen, dass eine nat√ºrliche </code><br><code>Person Straftaten begeht oder erneut begeht‚Ä¶‚Äú sollten zu den absolut verbotenen Praktiken </code><br><code>z√§hlen. Verletzung der Menschenw√ºrde, hohe Fehlerraten, diskriminierende Bias uvm sind nur </code><br><code>einige der Gr√ºnde, warum f√ºr derartige Anwendungen grunds√§tzlich kein Platz sein sollte.  </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665524_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665524.pdf,3,2,2665524,attachments/2665524.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>  </code><br><code>syst√®mes d‚Äôidentification biom√©trique √† distance fond√©s sur l‚ÄôIA</code><code>,</code><code> sont consid√©r√©s √† haut risque </code><br><code>et la Commission rappelle l'interdiction de leur utilisation dans l'espace public et en temps r√©el </code><br><code>""aux fins du maintien de l‚Äôordre"" en dehors de cas sp√©cifiques encadr√©s judiciairement. Dans </code><br><code>les secteurs o√π les risques sont limit√©s, la Commission recommande une transparence </code><br><code>d‚Äôinformation sur la pr√©sence de l‚ÄôIA. Enfin, dans d‚Äôautres domaines d‚Äôapplication comme les </code><br><code>jeux vid√©o ou les filtres antispam, les risques li√©s √† la pr√©sence de l‚ÄôIA sont consid√©r√©s comme </code><br><code>minimes et ne n√©cessitent pas d'encadrement sp√©cifique.   </code><br><code>   </code><br><code>Il est important de souligner le tournant strat√©gique que constitue cette r√©glementation.  Apr√®s </code><br><code>une longue p√©riode de r√©gulation prudente des outils et applications num√©riques, except√© sous </code><br><code>l‚Äôangle de la protection des donn√©es ou de la lutte contre la cybercriminalit√©, la Commission </code><br><code>entend acc√©l√©rer le pas. Cette √©volution touchant l‚ÄôIA est l‚Äôune des composantes d‚Äôune strat√©gie </code><br><code>clairement √©nonc√©e dans les lignes politiques 2019-2024 de la Commission et √† laquelle vont </code><br><code>contribuer d‚Äôautres textes parmi lesquels le Digital Market Act, le Digital Service Act, le Data </code><br><code>Governance Act ou encore l‚ÄôOpen Data Directive. L‚ÄôEurope compte √† juste titre s‚Äôimposer en </code><br><code>gardienne des valeurs dans le nouvel environnement num√©rique et jouer pleinement de son </code><br><code>exp√©rience de r√©gulateur pour aligner les op√©rations de ses concurrents directs (am√©ricains et </code><br><code>asiatiques) sur son rythme.   </code><br><code>   </code><br><code>2.</code><code> </code><code>Le secteur de la Justice hautement sensible pour les citoyens europ√©ens : le texte m√©rite </code><br><code>d‚Äô√™tre approfondi avec davantage de garanties pour la </code><code>¬´ justice predictive</code><code> ¬ª  </code><br><code> </code><br><code>Ces ambitions conduisent la Commission √† proposer une approche originale en termes de </code><br><code>risques en distinguant 4 types d‚Äôapplications, avec des contraintes √† l‚Äôintensit√© d√©croissant. Ce </code><br><code>faisant, elle n‚Äô√©chappe toutefois pas √† une certaine complexit√© pour la mise en conformit√© des </code><br><code>applications pr√©sentant le plus de risque de dommage sur les individus. C‚Äôest particuli√®rement </code><br><code>vrai dans le domaine de la justice. La classification des syst√®mes d√©bouchera n√©cessairement </code><br><code>sur des interpr√©tations et des d√©bats, des op√©rateurs pouvant √™tre tent√©s d‚Äô√©viter la r√©gulation </code><br><code>contraignante pour des applications √† la limite du haut risque (est-ce qu‚Äôun potentiel syst√®me de </code><br><code>gestion des audiences dans les tribunaux repr√©sente un ¬´ haut risque ¬ª par exemple ?). Le </code><br><code>prononc√© de sanctions au titre des √©ventuels manquement devra √©galement prendre en compte </code><br><code>la concurrence possible entre diff√©rents ordres de juridictions (p√©nales et administratives), </code><br><code>probl√©matique d√©j√† connue entre les autorit√©s de protection des donn√©es et les juridictions </code><br><code>p√©nales notamment.    </code><br><code>   </code><br><code>Par ailleurs, si l‚Äôon peut se r√©jouir de la mention de certains probl√®mes sous-estim√©s (comme les </code><br><code>biais d‚Äôautomatisation ‚Äì art. 14, 4, b), on comprend que des questions de principe n‚Äôont pas √©t√© </code><br><code>approfondies, notamment celle de l‚Äôopportunit√© du recours √† des algorithmes dans certaines </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665616_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665616.pdf,3,1,2665616,attachments/2665616.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>FNA   9‚Äì11 Avenue Michelet          F‚Äì93583  Saint-Ouen Cedex          aliou.sow@fna.fr        </code><code>ÔÄß</code><code> (+33) 1 40 11 99 04  -  (+33) 6 62 79 62 12</code><code> </code><code> </code><br><code>Federation of Craft Businesses in the automotive sector and in mobility services (FNA) </code><code>would like to thank the authors of the Proposal </code><br><code>of European Regulation laying down harmonized rules on Artificial Intelligence (AI) to inform citizens and stakeholders about the </code><br><code>Commission's work in order to allow them to provide feedback on the intended initiative and to participate effectively in future consultation </code><br><code>activities. FNA representatives also express their thanks for being given the opportunity of once again making submissions on the </code><br><code>Consultation. They make the following comments. </code><br><code>Access to Automotive data and information on AI </code><br><code>Findings of new undue hardship on advanced technologies </code><br><code> </code><br><code>- Electric vehicles</code><code> </code><br><code>1. Advanced technologies win the automotive industry: on the one hand, the electric car revolutionizes maintenance methods; on the other </code><br><code>hand, the increasing connectivity of new </code><code>""smart""</code><code> vehicles tends to channel diagnostic, maintenance and repair services through the </code><br><code>manufacturer's network. </code><br><code>2. While the classic vehicle has thousands of moving parts in the engine, the electric car is less than ten: maintenance is therefore simpler </code><br><code>and 30 to 40% cheaper. The battery, which is managed by the on-board electronics, warns the driver of the level of his capacity being </code><br><code>affected by the charge and discharge cycles. In order to carry out the maintenance of the vehicle, it must be temporarily deactivated via a </code><br><code>safety connector. The specificity of the maintenance relies precisely on the safety instructions and data to be applied: the work is done under </code><br><code>tension, the electrical intensities are strong. </code><br><code>3. Difficulties in access to the maintenance and repair market, which have already been identified for the combustion engine, are focused </code><br><code>on the electrical system: many safety data and the needed equipment. While the new technique is designed to reduce the cost of </code><br><code>maintenance and repair, vehicle manufacturers tend to reserve the operation to their network, at the expense of consumers‚Äô choice. </code><br><code>3.1. For example, Mrs Johanne Berner Hansen, lawyer of Dansk Bilbrancher√•d (DBR) reported that TESLA representatives are able to </code><br><code>‚Äúbypass‚Äù the car. They can make it ‚Äúunsupported‚Äù which means that TESLA can remove the capability of fast charge. This makes the car </code><br><code>useless in many ways. TESLA has an intern notification that states why a car is being ‚Äúunsupported‚Äù. There is no opportunity to reverse, i.e. </code><br><code>the car cannot be supported again. This is a major issue for consumers as TESLA is in this way keeping competition out. </code><br><code>3.2. In the same way TESLA removes the entire warranty of the car if electric components (for instance tail-or front-gate) have been fitted </code><br><code>together by others than TESLA network. In order for the car owner to get the warranty back, he needs to get the entire wiring harness in the </code><br><code>car displaced ‚Äì of course at TESLA and of course by charge.  </code><br><code>3.3. TESLA and their way of doing business is a major issue all around Europe and right know, Danish DBR and French FNA fear that </code><br><code>TESLA way of business will spread out to other car manufacturers - only because </code><code>""it works"".</code><code>  </code><br><code>As a result, it is difficult for car owners to make a choice regarding repairer, since no independent repairer is entitled to repair TESLA cars.  </code><br><code> </code><br><code>- Connected vehicles </code><br><code>4. The same observation can be made for the connected vehicle: data exchanges are supposed to offer more comfort and to be a source of </code><br><code>savings for the consumer: remote prognosis of the vehicle condition, use, support services, smart electric vehicle recharging, traffic </code><br><code>management, connection to infrastructure and other vehicles for cooperative and highly automated driving. But the exclusive and permanent </code><br><code>connection of manufacturers with motorists entails the risk of favoring their network. </code><br><code>5. Moreover, this risk has been highlighted by the European Commission, after the study carried out by the Cooperative Intelligent Transport </code><br><code>Systems (C-ITS) platform it has set up. Members warned the European Parliament and the Council of Ministers that these new challenges </code><br><code>have raised concerns about the potential exclusion of independent operators and the monitoring of their activities by manufacturers that are </code><br><code>in competition with them. The role of European legislation is fundamental to ensure that key conditions are fulfilled, in particular those </code><br><code>identified by the C-ITS platform: </code><br><code>-a prior consent of the person concerned (driver or vehicle owner); </code><br><code>-a fair and undistorted competition; </code><br><code>-data protection and privacy; </code><br><code>-non-falsifiable access and liability; </code><br><code>-data saving. </code><br><code>These conditions should be met in light of the need to protect the potential intellectual property</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>. </code><br><code> </code><br><code> </code><br><code> </code><br><code>1</code><code>European Commission - Report from the Commission to the European Parliament and the Council on the functioning of the system for access to information </code><br><code>on repair and maintenance of vehicles established by Regulation (EC) No 715/2007 - COM (2016) 782 Final of 9 December 2016, paragraph 5.6 page 11. </code><br><code>Chapter 8 of the report of the Cooperative Intelligent Transport Systems (C-ITS) platform on access to embedded data and resources: </code><br><code>http://ec.europa.eu/transport/themes/its/doc/c-its-platform-final-report-january-2016.pdf</code><code>. </code>",POSITIVE
fitz_2665252_2,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2665252.pdf,2,2,2665252,attachments/2665252.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>resources, not private funds. The increasing degree of digitalization and AI in education have made </code><br><code>education an interesting market for companies, who want to make a profit. This poses a real danger </code><br><code>for privatization, commercialization and monopolization, all of which endanger the equal access to </code><br><code>education. The EC must ensure that local governments do not disregard their duties and take up their </code><br><code>role in providing funds so that education remains a public good that can‚Äôt be dictated by EdTech </code><br><code>companies. In providing a framework, governments can ensure that there is a level playing field, which </code><br><code>should help against monopolization. The government needs to help to ensure that AI and the </code><br><code>companies behind it work to the benefit of the schools, students and teachers, so that AI adapts to </code><br><code>the educational system, instead of education catering to AI and Edtech companies. AI should become </code><br><code>a useful tool for schools, students and teachers; they cannot become a money-tool for AI and the </code><br><code>companies behind it. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665432_4,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665432.pdf,7,4,2665432,attachments/2665432.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>4 </code><br><code>will be excluded from the scope. This is for example the case of online profiling and </code><br><code>personalisation techniques, content recommender systems that select what people see in </code><br><code>their social media feeds. Also, a significant number of connected devices embedded with </code><br><code>AI (e.g. smart meters, connected toys, virtual assistants) are not classified as high-risk </code><br><code>AI.</code><code style=""font-weight: 1000; background-color: #FF0000;"">13</code><code> </code><br><code> </code><br><code>The risks created and potential for harm stemming from the myriad of AI systems that are </code><br><code>and will increasingly be present in consumers‚Äô lives are not properly addressed by the </code><br><code>regulation. Even if there is a basic requirement of transparency in Article 52, the scope of </code><br><code>this provision is limited to certain AI systems and transparency alone is not sufficient. </code><br><code> </code><br><code>Secondly, the ‚Äòhigh-risk‚Äô category is too narrowly defined as well.</code><code style=""font-weight: 1000; background-color: #FF0000;"">14</code><code> The risks taken into </code><br><code>consideration are limited to those of health and safety and the protection of fundamental </code><br><code>rights</code><code style=""font-weight: 1000; background-color: #FF0000;"">15</code><code>, leaving out basic consumer rights, societal effects, impact on democracy, rule of </code><br><code>law or environmental impact, as well as the potential for economic harm. For example, AI </code><br><code>used to assess the eligibility of someone for a health or car insurance, or the cost of such </code><br><code>insurance, would not be considered as ‚Äòhigh-risk‚Äô AI.  </code><br><code> </code><br><code>The Commission has competence to update the list of high-risk AI falling under Annex III</code><code>.</code><code> </code><br><code>However, several strict conditions need to be fulfilled, making it very difficult to make use </code><br><code>of such possibility in practice.</code><code style=""font-weight: 1000; background-color: #FF0000;"">16</code><code> For example, it limits the possibility of expanding the scope </code><br><code>to the areas already listed in Annex III. </code><br><code> </code><br><code>BEUC in contrast supports a ‚Äòrisk-based approach‚Äô where all AI (and not only high-risk) are </code><br><code>subject to a minimum set of rules (starting with basic principles of transparency, fairness, </code><br><code>accountability, non-discrimination, security, etc.). Then, the higher the risk, the stricter </code><br><code>the specific requirements should become. A broader, more inclusive approach is necessary </code><br><code>in the proposed Regulation. </code><br><code>3.</code><code> </code><code>For high risk AI applications, a conformity assessment by third parties </code><br><code>should be the rule, not the exception </code><br><code>The proposal provides for far too much reliance on industry self-assessing that it complies </code><br><code>with the rules.</code><code style=""font-weight: 1000; background-color: #FF0000;"">17</code><code> This approach is not adequate as it does not take in consideration the </code><br><code>complexity of the risks posed by AI and is likely not to be effective to protect consumers.  </code><br><code> </code><br><code>First, there is an evident conflict of interest: the entity assessing whether a certain product </code><br><code>is in compliance with the rules is the same company who has an interest in placing the AI </code><br><code>on the EU market as quickly as possible.  </code><br><code> </code><br><code>Second, a survey</code><code style=""font-weight: 1000; background-color: #FF0000;"">18</code><code> about independent third-party testing shows that the compliance and </code><br><code>safety of independently-checked products can be considerably higher than for products </code><br><code>that rely simply on manufacturer‚Äôs self-declaration of conformity. </code><br><code> </code><br><code> </code><br><code>13</code><code> According to Article 44 (3) of the proposal, the manufacturer of these devices may need to apply AI standards </code><br><code>under certain conditions. </code><br><code>14</code><code> See Article 6 of the proposal; </code><br><code>15</code><code> See Article 7 (1) b) of the proposal; </code><br><code>16</code><code> See Article 7 of the proposal; </code><br><code>17</code><code> See Article 43 of the proposal; </code><br><code>18</code><code> </code><br><code>http://www.ifia-federation.org/content/wp-content/uploads/2016/11/Consumer-Products-Safety-Study-</code><br><code>2016.pdf</code><code>   </code>",POSITIVE
fitz_2662109_3,other,../24212003_requirements_for_artificial_intelligence/attachments/2662109.pdf,3,3,2662109,attachments/2662109.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>HOPE POSITION</code><code> </code><br><code>                                                                                                      </code><code>May 2021</code><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>HOPE ‚Äì European Hospital and Healthcare Federation </code><br><code>www.hope.be</code><code>  </code><br><code> 3 | </code><code>P a g e</code><code>  </code><br><code>During the evaluation and review report of the EU data protection legislation, the European </code><br><code>Commission should specifically evaluate the need to establish rules on: anonymization techniques of </code><br><code>health data; data access and control when it comes to use of data coming from multiply sources; and </code><br><code>quality and safety standards for all information systems where health data is processed.  </code><br><code> </code><br><code>The EU should establish a legal framework for AI integrating the specifics of healthcare. </code><br><code> </code><br><code>Finally, financing such innovation and in particular adapting the hospital financing to its development </code><br><code>will have to be considered carefully by Member states. </code><br><code> </code><br><code> </code><br><code> </code><br><code>******* </code><br><code>HOPE, the European Hospital and Healthcare Federation, is a European non-profit organisation, </code><br><code>created in 1966. HOPE represents national public and private hospitals associations and hospitals </code><br><code>owners either federations of local and regional authorities or national health services. Today, HOPE is </code><br><code>made up of 36 organisations coming from the 27 Member States of the European Union, as well as </code><br><code>from the United Kingdom, Switzerland and Serbia as observer members. HOPE mission is to promote </code><br><code>improvements in the health of citizens throughout Europe, high standard of hospital care and to foster </code><br><code>efficiency with humanity in the organisation and operation of hospital and healthcare services. </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665433_3,company,../24212003_requirements_for_artificial_intelligence/attachments/2665433.pdf,5,3,2665433,attachments/2665433.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>3 </code><br><code> </code><br><code>Second, we would recommend adding a definition that distinguishes more clearly between </code><code>users as </code><br><code>deployers</code><code> (which is often the case in a B2B context) and </code><code>users as individual end users</code><code>, which would </code><br><code>be a fundamental step in ensuring that all entities involved in the AI deployment phase receive clear </code><br><code>and proportionate obligations. </code><br><code>Finally, we would suggest adding an additional category in Article 28 to address instances in which </code><br><code>an AI user or another third party uses or modifies an AI system in a manner that would render it a </code><br><code>high-risk AI system. In such circumstances, the user or the other third-party should be considered </code><br><code>the AI provider under the Act. </code><br><code>Responsibilities of AI providers </code><br><code>AI providers are however aware of their own responsibilities. They are best placed to explain how </code><br><code>the AI tool works and how it was developed (e.g. which part of the solution uses AI; which AI </code><br><code>technique listed under Annex I was used), so that the users can assess if it is appropriate for them </code><br><code>and use it responsibly in the workplace.  </code><br><code>AI providers can also make AI systems as transparent as possible for users and explain their intended </code><br><code>outcome. We agree with the formulation of Article 13(1) (‚Äúsufficiently transparent to enable users to </code><br><code>interpret the system‚Äôs output and use it appropriately‚Äù) but believe any attempts to open up AI </code><br><code>algorithms would provide little transparency to users.  </code><br><code> </code><br><code>3.</code><code> </code><code>Requirements for high-risk AI should be principle-based and achievable  </code><br><code>Requirements should be principle-based and easily applicable for developers and/or users of AI </code><br><code>solutions. Compliance with some of the requirements proposed for high-risk AI looks very difficult to </code><br><code>achieve, especially </code><code>Article 10 on Data and Data Governance </code><code>and </code><code>Article 14 on Human Oversight.</code><code>  </code><br><code>Data Governance </code><br><code>Article 10(3) provides that ‚ÄúTraining, validation and testing data sets shall be relevant, </code><br><code>representative, free of errors and complete.‚Äù Aiming for ‚Äúfree of errors‚Äù and ‚Äúcomplete‚Äù datasets is </code><br><code>difficult to achieve, so we would propose a more practical formulation (‚Äúwhenever possible‚Äù or </code><br><code>‚Äúdeploy reasonable efforts to‚Ä¶‚Äù). </code><br><code> </code><br><code>It will also be difficult to guarantee ‚Äúrepresentative‚Äù datasets without a precise definition of what </code><br><code>this means in concrete terms. A possible definition could be datasets ‚Äúthat represent real-world </code><br><code>distribution‚Äù.   </code><br><code> </code><br><code>The wording of Article 10(3) is also probably too simplistic, and we would treat training/validation </code><br><code>and testing separately. For testing, we believe it is important to use actual, real-world datasets, e.g. </code><br><code>incomplete datasets (even with errors). This is a way to assess how decisions can be biased by the </code><br><code>use of imperfect datasets.  </code><br><code> </code><br><code>On a final note, AI providers have no access to or control over the datasets used by their customers </code><br><code>to train AI models, and would not be in a position to meet the requirements laid out in Article 10.  </code><br><code> </code><br><code>Human oversight </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663339_16,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663339.pdf,19,16,2663339,attachments/2663339.pdf#page=16,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>www.acea.auto </code><br><code>15</code><code> </code><br><code>o</code><code> The notions of monitor and control need to be clarified/defined: these could </code><br><code>be at stake if the high-risk system is used outside the EU, where there may </code><br><code>be no possibility to control the system.   </code><br><code> </code><br><code>1) High-risk AI systems shall be designed and developed in such a way, including with </code><br><code>appropriate human-machine interface tools, that they can be effectively overseen </code><br><code>by natural persons during the period in which the AI system is in use. </code><code> </code><br><code>o</code><code> Article 14(1): What constitutes ‚Äòeffectively overseen‚Äô and how does that </code><br><code>combine with control of complex machines, e.g. in manufacturing?  </code><br><code>o</code><code> For manufacturing, this oversight is more relevant for quality inspection </code><br><code>(perception of how the system works) than for machine control (functioning </code><br><code>of the system itself).  </code><br><code>o</code><code> In the case of automated driving, it would mean that wheel and pedals could </code><br><code>never be removed from the vehicle, which is not consistent with L4 shuttle </code><br><code>services or robotaxis and nor with Implementing Regulation currently being </code><br><code>drafted by DG GROW in the Motor Vehicle Working Group. </code><br><code> </code><br><code>4)(e) be able to intervene on the operation of the high-risk AI system or interrupt the </code><br><code>system through a ‚Äústop‚Äù button or a similar procedure. </code><br><code>o</code><code> A stop button might not be appropriate to handle situations where it would </code><br><code>be even more critical if the operation just stops. The requirement should be </code><br><code>rephrased by emphasising that humans can intervene in the operation of an </code><br><code>AI system by putting it in a safe position or situation. </code><br><code> </code><br><code>‚Ä¢ </code><br><code>Article 15</code><code> ‚Äì Accuracy, robustness and cybersecurity </code><br><code>o</code><code> General comment in regard to motor vehicles: the new cybersecurity </code><br><code>Regulation UNECE R155 already covers automotive products (e.g. </code><br><code>automated vehicles) and should be effectively enforced so to avoid </code><br><code>duplication of cybersecurity requirements only for AI. </code><br><code>o</code><code> What is the </code><code>‚Äòappropriate level of robustness‚Äô</code><code> and how can this be defined? </code><br><code>As there are different robustness metrics, clearer criteria should be </code><br><code>established here.   </code><br><code> </code><br><code>QUALITY MANAGEMENT SYSTEM </code><br><code>‚Ä¢ </code><br><code>Article 17 </code><code>‚Äì Quality management system </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665501_2,other,../24212003_requirements_for_artificial_intelligence/attachments/2665501.pdf,10,2,2665501,attachments/2665501.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>2 </code><br><code>cooperation with the different regional AI hubs, including </code><code>Kenniscentrum Data & Maatschappij</code><code>, </code><br><code>CRIDS/NADI</code><code> and the </code><code>AI Institute for the Common Good</code><code> (FARI). During and after the workshop, </code><br><code>feedback was gathered on the proposed AI Regulation‚Äôs strengths and weaknesses from the </code><br><code>perspective of the Belgian AI ecosystem. On this basis, a first draft note was prepared, which was </code><br><code>subsequently circulated within the AI4Belgium community in order to give all members an </code><br><code>opportunity to consult the document and provide further input. The end result comprises the </code><br><code>consolidated feedback of the AI4Belgium members on the proposed AI Regulation.  </code><br><code>In what follows, the key points of feedback that were raised by the AI community are described, </code><br><code>with suggestions on how the proposal could be further improved. Given the rich diversity of </code><br><code>AI4Belgium‚Äôs membership, there are certain aspects of the proposal on which members </code><br><code>disagreed. In those instances, we have reflected this variety of opinions in this document, in order </code><br><code>to provide a comprehensive overview.  </code><br><code>Overview of AI4Belgium‚Äôs feedback</code><code> </code><br><code>1) Overall, the proposed regulatory framework is much appreciated and welcomed  </code><br><code>AI4Belgium commends the Commission‚Äôs overall approach to the regulatory framework for AI that </code><br><code>is being put forward in the proposal. The debate on an adequate ethical and legal framework for </code><br><code>Artificial Intelligence has been ongoing for several years now, and the time was more than ripe to </code><br><code>propose a number of binding rules to ensure that individuals and organizations can trust AI </code><br><code>systems that are deployed across the EU through verifiable procedures rather than voluntary </code><br><code>guidelines. These procedures to enable trust are not only important to ensure compliance with </code><br><code>fundamental rights, but also to stimulate the adoption (and intra-EU trade) of AI, and to capture </code><br><code>the benefits that this technology can generate. It can be hoped that the EU model will be adopted </code><br><code>beyond the Union‚Äôs borders, and provisions about the extraterritorial effect of the proposed </code><br><code>regulation are hence also welcome. </code><br><code>The emphasis on regulating the use of AI rather than the technology itself is an advantage of the </code><br><code>proposal. At the same time, given that AI systems can be repurposed for various uses, the </code><br><code>Commission‚Äôs proposed design, development and deployment requirements (in terms of riskmanagement, data governance, technical documentation, transparency, human oversight, and </code><br><code>accuracy, robustness and cybersecurity) are essential, and will need to be translated into practice </code><br><code>through various methods, which can also be adapted to the specific sectors in which the systems </code><br><code>are used.  </code><br><code>2) The ‚Äúlist-based‚Äù approach of the proposal risks being incomplete, and it requires </code><br><code>periodic assessments </code><br><code>AI4Belgium members particularly appreciated the risk-based approach of the proposed AI </code><br><code>regulation, and the important signposting of risk levels by way of the chapters‚Äô headings (namely </code><br><code>a set of prohibited AI practices, a set of high-risk AI practices, a set of AI practices that require </code><br><code>further transparency, and other AI applications).  </code><br><code>Some members, however, rightfully remarked that this list-based approach risks being </code><br><code>incomplete. Others pointed towards the risk of the list to become overly-inclusive. It is </code><br><code>acknowledged that it is difficult to ensure the comprehensiveness of such lists from the outset. It </code><br><code>is therefore crucial that the said lists are updated periodically and in a speedy manner, without </code><br><code>the need to revisit the entire regulatory framework.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662381_3,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2662381.pdf,4,3,2662381,attachments/2662381.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>  </code><br><code> </code><br><code> </code><br><code>  </code><br><code> </code><br><code>Pagina 3 van 4 </code><br><code>  </code><br><code>keuze voor het niveau van certificering vindt onder de CSA plaats door de </code><br><code>producent. Wij vinden het wenselijk dat in de AI Act goed wordt </code><br><code>vastgelegd hoe de CSA assurance levels passen bij High Risk definitie.  </code><br><code> </code><br><code>3.</code><code> </code><code>Definities </code><br><code>De uitleg van definities heeft verstrekkende gevolgen voor de toepasselijkheid van </code><br><code>wetgeving en bevoegdheid van de toezichthouders. Toezichthouders zijn bij </code><br><code>heldere definities meer voorspelbaar en aanbieders en gebruikers ontlenen er </code><br><code>zekerheid aan. Het is belangrijk meer aandacht te besteden aan het aanscherpen </code><br><code>van de definities of het nader duiden daarvan. We noemen drie voorbeelden: </code><br><code>‚Ä¢</code><code> </code><br><code>eIDAS: Het gebruik van de term ‚Äòremote biometric identification system‚Äô </code><br><code>in het AI framework lijkt een andere betekenis te hebben ten opzichte van </code><br><code>andere toepassingen (o.a. identificatiesystemen voor eID en AML Anti </code><br><code>money laundering toepassingen) waar gesproken wordt van ‚Äòremote </code><br><code>identification‚Äô in de eIDAS verordening; </code><br><code>‚Ä¢</code><code> </code><br><code>Productregelgeving: Het begrip ‚Äúprovider‚Äù sluit niet aan bij </code><br><code>productregelgeving. Net zomin als ‚Äúsmall-scale provider‚Äù of ‚Äúuser‚Äù. Deze </code><br><code>verwijzen eerder naar diensten dan producten.   </code><br><code>‚Ä¢</code><code> </code><br><code>‚ÄúProducenten van (High Risk) AI-systemen‚Äù: Voor de vraag wie de </code><br><code>producent is van een AI-systeem moet duidelijk zijn of een CE-markering </code><br><code>gekoppeld wordt aan het doel van het product. Wie is bijvoorbeeld de </code><br><code>producent van het AI-systeem in een slimme speaker?  </code><br><code> </code><br><code>4.</code><code> </code><code>Standaardisatie en conformiteit </code><br><code>Standaardisatie draagt bij aan harmonisatie van de markttoegang door onder </code><br><code>andere producten en diensten veilig, compatibel en uitwisselbaar te maken, </code><br><code>waarmee het maatschappelijk belang wordt gediend. Tevens heeft standaardisatie </code><br><code>en normering een belangrijke rol bij het uitoefenen van de handhaving- en </code><br><code>toezichtstaak op nationaal niveau. De systematische en getrapte indeling van </code><br><code>geharmoniseerde standaarden -en bij het ontbreken van deze standaarden- het </code><br><code>kunnen stellen van aanvullende vereisten is in de concept Verordening Artifici√´le </code><br><code>Intelligentie helder verwoord. </code><br><code>Voor een consistente en systematische opzet van mogelijk aanvullende vereisten </code><br><code>op het terrein van AI -en in bredere zin digitale veiligheid-is het goed dat de </code><br><code>Commissie vroegtijdig de Europese Standaardisatie Organisaties (ESOs) en ENISA </code><br><code>actief heeft betrokken. De ESO‚Äôs hebben de kennis en expertise om een </code><br><code>belangrijke rol te kunnen vervullen bij de verdere ontwikkeling van standaarden </code><br><code>en voor ENISA geldt dit bij de identificatie van relevante ontwikkelingen bij AI </code><br><code>binnen de CSA certificering als ook bij de inrichting van het systeem van </code><br><code>conformiteitsbeoordeling. Wij adviseren daarom de rol van de ESOs bij de </code><br><code>ontwikkeling van standaarden te borgen in de verordening.  </code><br><code> </code><br><code>5.</code><code> </code><code>Bevoegdheid (nationaal en internationaal) </code><br><code>Het is onduidelijk welke ‚Äòcompetent authority‚Äô wanneer kan/mag/moet acteren. </code><br><code>‚Ä¢</code><code> </code><br><code>Een AI-systeem dat op de markt is gebracht kan door meerdere </code><br><code>gebruikers in meerdere lidstaten gebruikt worden. Hoe voorziet de EC een </code><br><code>co√∂rdinatie van toezichtsactiviteiten en een effici√´nte aanpak van de </code><br><code>handhaving als meer dan √©√©n sectorale toezichthouder bevoegd is, en </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665527_5,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665527.pdf,5,5,2665527,attachments/2665527.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>to</code><br><code>verify</code><br><code>whether the AI systems comply with the requirements contained in the</code><br><code>European Commission‚Äôs Proposal.</code><br><code>III.</code><br><code>The European Commission‚Äôs Proposal will have its impact not only on the EU and its</code><br><code>Member States but also on other countries and will shape the rights of citizens in the</code><br><code>context of the use of artificial intelligence for decades. Due to this fact, we consider it</code><br><code>extremely important to ensure proper regulation of issues related to AI systems used in</code><br><code>any way by public institutions.</code><br><code>4</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665590_1,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665590.pdf,1,1,2665590,attachments/2665590.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Putting startups at the heart of AI innovation - CroAI‚Äôs opinion on the European</code><br><code>Commission‚Äôs Artificial Intelligence Act</code><br><code>In April 2021, the European Commission (EC) published its much-awaited Artificial Intelligence</code><br><code>Act (AIA), the first global attempt to establish a legal framework for a technology that, as the AIA</code><br><code>states, carries both benefits and risks to humans and society. The Croatian AI Association</code><br><code>(CroAI) welcomes the EC‚Äôs efforts to set its own approach to AI, as it previously did with privacy.</code><br><code>However, our main concern is that the AIA does not adequately address the needs of start-ups,</code><br><code>who are the main drivers of innovation.</code><br><code>CroAI believes that the AIA must be an enabler of AI innovation and strongly stand behind</code><br><code>startu-ps, especially during prototyping and testing while pursuing a product-market fit. We</code><br><code>therefore advocate for the AIA to include unequivocal support for innovators by mandating the</code><br><code>following measures:</code><br><code>1.</code><br><code>Startups are allowed to create their own sandboxes on a case-by-case basis rather than</code><br><code>a one-size-fits all approach, due to the unique nature of each test.</code><br><code>2.</code><br><code>Start-ups will follow a Code of conduct. The Code of conduct will help them mitigate risks</code><br><code>while testing through tools such as limiting the number of test users, human oversight,</code><br><code>purchasing insurance, transparency, and accountability.</code><br><code>3.</code><br><code>While in their sandboxes, startups do not need to involve supervisory authorities. Still,</code><br><code>they are accountable for complying with the Code of conduct.</code><br><code>4.</code><br><code>When leaving their sandboxes, which means that they have found a product-market fit,</code><br><code>startups need to invest in fully complying with AIA rules and regulations, which will make</code><br><code>much more sense at that time in a product‚Äôs development cycle.</code><br><code>The principal concern founders and investors have when thinking about doing AI in Europe is</code><br><code>the cost and unpredictability of complying with the AIA. They see it as an unnecessary risk and</code><br><code>a burden that they can easily avoid by moving a start-up to some other innovation hub in the</code><br><code>world. CroAI believes that by integrating these measures into the AIA, the EC will address most</code><br><code>of those concerns and make the EU an excellent choice for AI innovation.</code><br><code>CroAI is at the European Commission‚Äôs disposal to elaborate more on the reasoning behind this</code><br><code>proposal and how to get it to life.</code>",NO_FOOTNOTES_ON_PAGE
fitz_2665221_3,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665221.pdf,5,3,2665221,attachments/2665221.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Oxford Commission on AI & Good Governance, 10pp.  </code><br><code>Retrieved from: </code><code>https://oxcaigg.oii.ox.ac.uk</code><code>  </code><br><code>Engagement with private technology companies to influence and promote the responsible </code><br><code>development and use of data and new technologies: </code><br><code>Research from the Oxford Commission on AI & Good Governance suggests that data sharing </code><br><code>between different government entities and across government and industry is pivotal in </code><br><code>determining the success of data-driven projects within local governments with the example of </code><br><code>the UK. OxCAIGG recommends a formal mechanisms for collaboration across all local </code><br><code>authorities and with industry; and a platform to complete all relevant information about </code><br><code>information technology projects in local authorities.  </code><br><code>Thomas Vogl (2021). Artificial Intelligence in Local Government. Working paper 2021.2 </code><br><code>Oxford, UK: Oxford Commission on AI & Good Governance. 18pp.  </code><br><code>Retrieved from: </code><code>https://oxcaigg.oii.ox.ac.uk</code><code> . </code><br><code>How can the EU build resilience in civil society, with Government and business against the </code><br><code>threats posed by abuses of new technologies by state and non-state actors?  </code><br><code>Research from the Oxford Commission on AI & Good Governance has developed four </code><br><code>overarching principles for the use and implementation of artificial intelligence within </code><br><code>government and public service that may offer useful guidance here.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Inclusive Design: issues around discrimination and bias of AI in relation to </code><br><code>inadequate data sets, exclusion of minorities and under-represented groups, and the </code><br><code>lack of diversity in design.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Informed Procurement: issues around the acquisition and development in relation to </code><br><code>due diligence, design and usability specifications and the assessment of risks and </code><br><code>benefits.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Purposeful Implementation: issues around the use of AI in relation to interoperability, </code><br><code>training needs for public servants, and integration with decision-making processes.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Persistent Accountability: issues around the accountability and transparency of AI in </code><br><code>relation to ‚Äòblack box‚Äô algorithms, the interpretability and explainability of systems, </code><br><code>monitoring </code><br><code>and auditing.  </code><br><code>Lisa-Maria Neudert & Philip N. Howard (2020). Four Principles for Integrating AI & Good </code><br><code>Governance. Working paper 2020.1 Oxford, UK: Oxford Commission on AI & Good </code><br><code>Governance. 15 pp.  </code><br><code>Retrieved from: </code><code>https://oxcaigg.oii.ox.ac.uk</code><code>  </code><br><code>Interplay between regulation and digital technical standards </code><br><code>Digital technical standards have risen up the political agenda in democracies, following the </code><br><code>recognition of China‚Äôs strategic use of technical standardisation to embed values inimical to </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663405_8,company,../24212003_requirements_for_artificial_intelligence/attachments/2663405.pdf,10,8,2663405,attachments/2663405.pdf#page=8,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>8/10</code><br><code>Position Paper on the EU AI Draft Regulation  | IDEMIA </code><code> </code><br><code>Date | #ref                          </code><br><code> </code><br><code> </code><br><code>compliance monitoring of high risk AI system. IP protection is crucial to provide an incentive that drives </code><br><code>investment. </code><code>This obligation is excessive as it affects trade secrets and know-how, it creates </code><br><code>vulnerability for IT security. </code><br><code> </code><br><code>Overall, transparency and access to documentation as described by the draft Regulation is too intrusive as it </code><br><code>requires providers of AI system to divulge their trade secrets and know-how. Providers would give access to </code><br><code>their datasets to evaluation bodies, including through APIs, which represents a significant IT security </code><br><code>vulnerability. This contradicts both best practices in security of R&D operations, and contractual agreements </code><br><code>with customers when protecting sensitive technologies.  </code><br><code>Finally, in addition to affecting intellectual property, these requirements significantly increase costs for </code><br><code>providers and affect the competitiveness of the EU industry. </code><br><code>   </code><br><code> </code><br><code> </code><br><code>Recommendation 6</code><code>: IP information of European Providers should only be shared with independent public </code><br><code>authorities; disclosure of highly sensitive information to private third-party auditors raises IP protection </code><br><code>concerns to the industry and could undermine the EU sovereignty. </code><br><code> </code><br><code> </code><code>Certification and test approach </code><code> </code><br><code>Testing should be carried out on a black box mode, in the sense that a detailed knowledge of the inner workings </code><br><code>of the product should not be required to assess whether or not the system is performing to standard.  </code><br><code> </code><br><code>A European NIST is the best approach in our opinion (test databases, confidential and sequestrated data). </code><br><code>Such a tool is definitely within Europe‚Äôs grasp:  Europe is fortunate to have a vibrant, state-of-the-art </code><br><code>academic ecosystem which could form the backbone and expertise of this network</code><code>. As a key industry </code><br><code>player, IDEMIA would be happy to support the development of such a capability in the EU with our own </code><br><code>expertise and industry experience, just as we supported the development of the NIST capability throughout </code><br><code>2000s and 2010s. </code><br><code>We think that this capability will be key in ensuring Europe‚Äôs sovereignty. Europe will need to make sure that </code><br><code>AI is tested according to its own values, following requirements derived from European use cases, and above </code><br><code>all, using data which is relevant to use cases and the operational situation in Europe. </code><br><code> </code><br><code> </code><br><code> </code><br><code>Recommendation 7:</code><code> The evaluation of the performance of the product should be done </code><code>ex post</code><code> in a ‚Äúblack box </code><br><code>mode‚Äù by a European standardisation body, as it is the only approach to ensure that the final product complies </code><br><code>with the requirements. </code><br><code> </code><br><code> </code><br><code>CONCLUSION </code><br><code>Like the GDPR, this text it is far-reaching and ambitious; it has the potential to become a fundamental text and </code><br><code>a reference for AI regulation and to be replicated outside Europe. </code><code> </code><br><code> </code><br><code>IDEMIA supports a clear legal framework that defines rules for both providers and users of AI system in order </code><br><code>to create trust and transparency in the use of such a system while preserving both the European position in </code><br><code>the AI technology landscape and the fundamental rights and freedoms of individuals. </code><br><code> </code><br><code>IDEMIA, as a global leader based in Europe, is keen to work with the institutions to provide more in-depth </code><br><code>insights to support policy making and believes that a dialogue with all stakeholders is necessary to ensure the </code><br><code>best regulatory outcome. </code><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662611_5,other,../24212003_requirements_for_artificial_intelligence/attachments/2662611.pdf,7,5,2662611,attachments/2662611.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>The proposal for an EU AI Act of 21 April 2021 </code><br><code>Hildebrandt commentary 19 July 2021 </code><br><code> </code><br><code>related AI systems), the same goes for AI systems deployed in the context of </code><code>insurance</code><code> (life, health, </code><br><code>real estate etc.), in the context of energy usage (energy usage data can be used to infer profession, </code><br><code>religion, or to predict defaulting on payment; this is not about safety components in critical </code><br><code>infrastructure as under point 2), and </code><code>housing</code><code> (think of discrimination based on ethnicity or gender).  </code><br><code>Furthermore, under point 8, which concerns ‚ÄòAdministration of justice and democratic processes‚Äô, </code><br><code>the current proposal only lists AI systems used in the context of the judiciary. Considering the </code><code>major </code><br><code>impact of the use of these systems</code><code> (notably for legal search, dispute resolution, automation of </code><br><code>decisions made by public administration, and legal advice and representation by attorneys) </code><code>on the </code><br><code>administration of justice, the nature of legal protection and the system of checks and balances of </code><br><code>the rule of law</code><code>,</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> all AI systems whose intended purpose is </code><code>to research, apply or decide positive law</code><code> </code><br><code>should be qualified as high risk. This will ensure that such systems are scientifically validated, verified </code><br><code>and tested before being integrated in legal practice in the broad sense of that term.  </code><br><code>Art. 10 Data and Data Governance</code><code> </code><br><code>- how will the use of synthetic data fit the requirements? </code><br><code>- ‚Äòfree of errors and complete‚Äô sounds over the top </code><br><code>- 10(4) is crucial, especially taking into account that the world is in flux, the presumption of art. 42 is </code><br><code>therefor outrageous, going against the grain of all that is required in art. 10 </code><br><code>- 10(5) seems to provide an additional exception to the prohibition of processing of art. 9 GDPR data, </code><br><code>though recital (41) seems to deny this </code><br><code>- paragraph 6 is incomprehensible, though admittedly the Regulation pays too little attention to </code><br><code>other stages of machine learning (notably the construction of hypothesis space, choice of language, </code><br><code>type of ML etc.), it is not clear whether this is what paragraph 6 refers to </code><br><code>Art. 14 Human oversight</code><code> </code><br><code>- excellent article but paragraph 4(a) seems to require the impossible ‚Äòfully understand the capacities </code><br><code>and limitations of the high-risk AI system‚Äô. What if even the developers cannot assert this (in the </code><br><code>case of deep learning systems)? And what about the fact that ‚Äòerrors, faults or inconsistencies [‚Ä¶] </code><br><code>may occur within the system or the environment in which the system operates, in particular due to </code><br><code>their interaction with natural persons or other systems‚Äô (art. 15.3). Although art. 15 rightly requires </code><br><code>resilience and reliability, this does not necessarily mean that those tasked with human oversight will </code><br><code>fully understand both the capacities and the limitations. I would propose to rephrase as ‚Äòhas </code><br><code>relevant understanding</code><code> of the capacities and limitations of the high-risk AI system‚Äô. </code><br><code>Art. 15 Accuracy, robustness, and cybersecurity</code><code> </code><br><code>- excellent requirements for high risks systems. </code><br><code>- the performance metric for these systems </code><code>should not be accuracy only</code><code>. On the contrary, the </code><br><code>metrics should include precision and recall, which are far more relevant for affected natural persons, </code><br><code>see footnote 1.  </code><br><code>Art. 43 Conformity assessment</code><code> </code><br><code>- high risk AI systems of Annex III should all come under the obligation to involve independent </code><br><code> </code><br><code>3</code><code> See on the dangers of integration of such software e.g. Masha Medvedeva, Martijn Wieling and Michel Vols, </code><br><code>‚ÄòThe Danger of Reverse-Engineering of Automated Judicial Decision-Making Systems‚Äô [2020] arXiv:2012.10301 </code><br><code>[cs] <http://arxiv.org/abs/2012.10301> accessed 5 May 2021.  </code>",POSITIVE
fitz_2665558_33,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665558.pdf,61,33,2665558,attachments/2665558.pdf#page=33,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Forthcoming in Computer Law & Security Review </code><br><code>- 33 - </code><br><code> </code><br><code>serious penalties arising from unlawful processing</code><code style=""font-weight: 1000; background-color: #FF0000;"">130</code><code>, mean the safest approach for </code><br><code>providers may be to treat all input data as special category data. Though this is potentially </code><br><code>quite burdensome for providers, the need for this precautionary approach arises directly </code><br><code>from the nature of turn-key AI services and the practices of providers themselves ‚Äì as we </code><br><code>argue above, their supplementary processing positions them as controllers for certain </code><br><code>stages of the AIaaS chain. They do not, however, require their own legal basis for processing </code><br><code>(and therefore do not need to take such a precautionary approach) where they do not </code><br><code>engage in supplementary processing. In that case, following our preceding analysis, they are </code><br><code>processors acting under the instruction of their controller customers. Providers should </code><br><code>therefore balance any benefits they realise through supplementary processing against the </code><br><code>legal risks they may incur as a result. </code><br><code> </code><br><code>Where special category data is in fact processed in the AIaaS chain, customers and providers </code><br><code>as controllers will usually need to have the explicit consent of data subjects for the purposes </code><br><code>for which the AIaaS processing chain is executing</code><code style=""font-weight: 1000; background-color: #FF0000;"">131</code><code>. This is because the restrictive nature of </code><br><code>the ‚Äòsubstantial public interest‚Äô exemption means that it will generally be inapplicable to </code><br><code>AIaaS. For that exemption to apply, the processing must be </code><code>necessary</code><code> (there must be no </code><br><code>alternative or more privacy-preserving means of achieving the same outcome</code><code style=""font-weight: 1000; background-color: #FF0000;"">132</code><code>), and it </code><br><code>must be based on an appropriate EU or domestic law. In the UK, for instance, this law is the </code><br><code>Data Protection Act 2018, which sets out 22 conditions in which ‚Äòsubstantial public interest‚Äô </code><br><code>might apply</code><code style=""font-weight: 1000; background-color: #FF0000;"">133</code><code>. These conditions are narrow and generally require controllers to meet </code><br><code>certain criteria (and many, such as ‚Äòadministration of justice and parliamentary purposes‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">134</code><code>, </code><br><code>‚Äòpolitical parties‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">135</code><code>, and ‚Äòelected representatives responding to requests‚Äô</code><code style=""font-weight: 1000; background-color: #FF0000;"">136</code><code>, are unlikely to </code><br><code>be relevant). Thirteen of those conditions require controllers to justify not obtaining explicit </code><br><code> </code><br><code>130</code><code> GDPR art 58, art 83. </code><br><code>131</code><code> This is not to say that explicit consent is the only ground on which providers can rely for any of their </code><br><code>processing; for ‚Äòordinary‚Äô personal data, they may be able to use consent, contract, and legitimate interests. </code><br><code>We do, though, argue that, due to the difficulties in distinguishing special category data, this would involve </code><br><code>considerable risk for providers. </code><br><code>132</code><code> European Data Protection Supervisor, ‚ÄòAssessing the necessity of measures that limit the fundamental right </code><br><code>to the protection of personal data: A Toolkit‚Äô (2017) <h</code><code>https://edps.europa.eu/sites/edp/files/publication/1704-11_necessity_toolkit_en_0.pdf</code><code>> accessed 13 November 2020. </code><br><code>133</code><code> Data Protection Act 2018 (‚ÄòDPA 2018‚Äô), sch 1, pt 2. </code><br><code>134</code><code> DPA 2018, sch 1, pt 2, para 7. </code><br><code>135</code><code> DPA 2018, sch 1, pt 2, para 22. </code><br><code>136</code><code> DPA 2018, sch 1, pt 2, para 23. </code>",POSITIVE
fitz_2663366_5,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663366.pdf,13,5,2663366,attachments/2663366.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>29 July 2021 </code><br><code> </code><br><code> </code><br><code> </code><br><code>5</code><code> </code><br><code> </code><br><code>o</code><code> </code><code>The element on output reversibility (point g)  should be reconsidered in light of the fact that the </code><br><code>right to erasure under the GDPR may serve as a mechanism that allows individuals to reverse </code><br><code>the outcome produced by the AI system; and </code><br><code> </code><br><code>o</code><code> </code><code>Regarding impact on data protection rights (point h), the remedies provided for by the GDPR </code><br><code>should be regarded as effective measures of redress, including the possibility to lodge a </code><br><code>complaint with a data protection authority (DPA).  </code><br><code> </code><br><code>4.</code><code> </code><br><code>Remote biometric identification  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Definition </code><code>- The definitions of biometric identification are not sufficiently clear in the AI Act. In </code><br><code>particular, it is unclear whether some biometric AI systems would not be considered high-risk biometric </code><br><code>identification systems. Because the risks and opportunities are so great, it is important to provide </code><br><code>clarity. For instance, AI technologies can have significant innovative and positive impacts such as </code><br><code>improving healthcare delivery and outcomes, monitoring health and safety, enabling people with </code><br><code>disabilities to better navigate the physical world, or could be used for safety and security purposes. </code><br><code>The use of biometric data for such purposes, however, is currently a concern within the proposed AI </code><br><code>Act.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Biometric authentication ‚Äì </code><code>The Act should specifically distinguish biometric authentication from </code><br><code>biometric identification, and confirm that biometric authentication falls outside of the scope of the AI </code><br><code>Act.6 Biometric identification requires comparing an individual‚Äôs biometric data to the biometric data </code><br><code>of many other individuals stored in a database to identify said individual (i.e. one-to-many matching). </code><br><code>On the other hand, biometric authentication entails less risk, given that it consists of comparing two </code><br><code>biometric templates usually assumed to belong to the same individual (i.e. one-to-one matching) and </code><br><code>no link with the actual identity is established. Biometric authentication is highly beneficial and is often </code><br><code>relied upon to ensure verified access to personal data or confidential information.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>ID verification</code><code> </code><code>and ADM</code><code> - CIPL underlines that remote biometric identification should not </code><br><code>automatically be qualified as ‚Äúhigh-risk AI.‚Äù This would avoid capturing uses such as TouchID on </code><br><code>phones, fingerprint access to an office, or online ID verification as part of setting up a bank account. </code><br><code>While the underlying ID verification technology could carry risks if it has not been developed fairly, in </code><br><code>most scenarios, there will always be a fallback option if the biometric identification fails. The risks </code><br><code>would be high only where the failure of biometric ID verification would lead to a significant negative </code><br><code>impact on the individual or where there is no fallback option. At the same time, ID verification falls </code><br><code>under the definition of automated decision-making under Article 22 of the GDPR and, as such, is </code><br><code>                                                 </code><br><code>6</code><code> This is consistent with the Commission‚Äôs February 2020 AI White Paper (in footnote 52), which specified that </code><br><code>‚Äúremote biometric identification should be distinguished from biometric authentication (the latter is a security </code><br><code>process that relies on the unique biological characteristics of an individual to verify that he/she is who he/she says </code><br><code>he/she is).‚Äù </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663064_1,trade_union,../24212003_requirements_for_artificial_intelligence/attachments/2663064.pdf,11,1,2663064,attachments/2663064.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>1 </code><br><code> </code><br><code> </code><br><code> </code><br><code>Contribution de Numeum au </code><code>Artificial Intelligence Act</code><code> (AI Act) </code><br><code> </code><br><code>Contexte </code><br><code> </code><br><code>Dans la continuit√© du livre blanc sur l‚Äôintelligence artificielle (IA), la Commission europ√©enne a pr√©sent√©, </code><br><code>le mercredi 21 avril, une proposition de r√®glement pour l‚Äô√©tablissement de r√®gles harmonis√©es en </code><br><code>mati√®re d‚ÄôIA, aussi intitul√©e Artificial Intelligence Act. Avec cette initiative, la Commission propose de </code><br><code>nouvelles r√®gles visant √† faire de l'Europe </code><code>¬´ la place mondiale de l‚ÄôIA de confiance ¬ª</code><code>. La proposition </code><br><code>pr√©voit notamment : </code><br><code>‚ñ™</code><code> </code><br><code>Des exigences relatives aux utilisations de l'IA √† haut risque ; </code><br><code>‚ñ™</code><code> </code><br><code>L‚Äôinterdiction de certaines utilisations de l‚ÄôIA ; </code><br><code>‚ñ™</code><code> </code><br><code>L‚Äôautor√©gulation sur les IA dites ¬´ √† faible risque ¬ª ; </code><br><code>‚ñ™</code><code> </code><br><code>La cr√©ation d‚Äôun Conseil de l‚ÄôIA anim√© par les 27 Etats membres.</code><code> </code><code> </code><br><code> </code><br><code>Numeum soutient la Commission europ√©enne dans son ambition de stimuler le d√©veloppement et </code><br><code>l'adoption de l'IA et des nouvelles technologies, </code><code>tout en veillant √† ce que les risques potentiels soient </code><br><code>trait√©s de mani√®re ad√©quate.  </code><br><code>Ainsi, la volont√© de la Commission de cr√©er un syst√®me europ√©en √† m√™me de garantir la confiance des </code><br><code>citoyens et stimuler l‚Äôadoption des usages IA, tout en assurant celle des entreprises dans le d√©ploiement </code><br><code>de leurs produits et applications IA et la capacit√© d‚Äôinnovation en Europe, nous appara√Æt une strat√©gie </code><br><code>ambitieuse et adapt√©e aux enjeux de d√©veloppement du potentiel de l‚ÄôIA en Europe.  </code><br><code>L‚ÄôUE a les moyens de devenir un acteur mondial de l‚ÄôIA. Cela suppose une am√©lioration de l'acc√®s aux </code><br><code>donn√©es et la collaboration entre les entreprises, ce qui contribuera √† la transformation num√©rique de </code><br><code>l'Europe. L‚ÄôUE et les Etats membres doivent pleinement s‚Äôengager dans cette transition, par des </code><br><code>investissements massifs dans les technologies et les infrastructures qui permettront √† l'Europe de </code><br><code>d√©velopper ses atouts en mati√®re d‚ÄôIA, notamment pour la formation des chercheurs et ing√©nieurs et </code><br><code>l‚Äôaccompagnement de l‚Äô√©volution des m√©tiers concern√©s. </code><code>Numeum appelle √† continuer √† fa√ßonner </code><br><code>une approche europ√©enne de l‚ÄôIA ouverte et inclusive qui favorise l‚Äôinnovation tout en assurant </code><br><code>la sauvegarde des droits fondamentaux. En somme, doter l‚ÄôIA d‚Äôune triple dimension qui soit </code><br><code>culturelle, √©thique et juridique.</code><code> </code><br><code>Compte tenu de la diversit√© des applications et des technologies de l'IA, nous saluons le fait que la </code><br><code>Commission adopte une approche cibl√©e et fond√©e sur les risques. Une telle approche devrait √™tre </code><br><code>bas√©e sur </code><code>des d√©finitions claires</code><code> </code><code>et prendre en compte le risque pos√© par le d√©ploiement d'un </code><br><code>syst√®me d'IA, le domaine d'application, le type de d√©ploiement et la nature des risques.</code><code>  </code><br><code>Dans cette optique, Numeum et ses adh√©rents souhaitent r√©it√©rer ici l‚Äôattention port√©e √† l‚Äôadoption d‚Äôune </code><br><code>approche pragmatique et √©quilibr√©e dans la d√©finition du nouveau cadre r√®glementaire par la </code><br><code>Commission europ√©enne.</code><code> </code><code>Afin de garantir la praticabilit√© de certaines dispositions du r√®glement, </code><br><code>l‚Äô√©valuation des risques pos√©s par un syst√®me d‚ÄôIA doit n√©cessairement √™tre limit√©e aux risques connus </code><br><code>et pr√©visibles associ√©s √† chaque syst√®me d‚ÄôIA √† haut risque.  </code><br><code> </code><br><code>Enfin, notre organisation souhaite poursuivre son implication vis-√†-vis des travaux et des r√©flexions </code><br><code>conduits par la Commission europ√©enne, le Parlement et le Conseil dans le cadre de cette initiative.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2662473_1,other,../24212003_requirements_for_artificial_intelligence/attachments/2662473.pdf,2,1,2662473,attachments/2662473.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Fon +32 2 282 05-50 </code><br><code>info@dsv-europa.de </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>www.dsv-europa.de </code><br><code>Transparenzregister </code><br><code>Nr. 917393784-31 </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>Deutsche Sozialversicherung </code><br><code>Europavertretung </code><br><code>Rue d‚ÄôArlon 50 </code><br><code>B-1000 Bruxelles </code><br><code> </code><br><code>Die √∂ffentliche Konsultation der EUKommission zum Vorschlag eines ‚ÄûGesetzes √ºber K√ºnstliche Intelligenz‚Äú, COM </code><br><code>(2021)206 final  </code><br><code>Stellungnahme der Deutschen Sozialversicherung vom </code><br><code>14.07.2021 </code><br><code>Die Deutsche Rentenversicherung Bund (DRV Bund), die Deutsche Gesetzliche </code><br><code>Unfallversicherung (DGUV), der GKV-Spitzenverband und die Verb√§nde der ge-</code><br><code>setzlichen Kranken- und Pflegekassen auf Bundesebene und die Sozialversiche-</code><br><code>rung f√ºr Landwirtschaft, Forsten und Gartenbau (SVLFG) haben sich mit Blick auf </code><br><code>ihre gemeinsamen europapolitischen Interessen zur ‚ÄûDeutschen Sozialversiche-</code><br><code>rung Arbeitsgemeinschaft Europa e. V.‚Äú zusammengeschlossen.  </code><br><code>Der Verein vertritt die Interessen seiner Mitglieder gegen√ºber den Organen der </code><br><code>Europ√§ischen Union (EU) sowie anderen europ√§ischen Institutionen und ber√§t die </code><br><code>relevanten Akteure im Rahmen aktueller Gesetzgebungsvorhaben und Initiativen.  </code><br><code>Die Kranken- und Pflegeversicherung, die Rentenversicherung und die Unfallver-</code><br><code>sicherung bieten als Teil eines gesetzlichen Versicherungssystems wirksamen </code><br><code>Schutz vor den Folgen gro√üer Lebensrisiken. </code><br><code>Stellungnahme </code><br><code>Die Deutsche Sozialversicherung begr√º√üt den Entwurf eines Gesetzes √ºber </code><br><code>K√ºnstliche Intelligenz (KI). Er hat weitreichende Auswirkungen auf die Entwicklung </code><br><code>und den Einsatz von KI, nicht zuletzt auch im Bereich der √∂ffentlichen Verwaltung. </code><br><code>Die Deutsche Sozialversicherung sieht m√∂gliche mitgliedschafts-, beitrags- und </code><br><code>leistungsrechtliche Bez√ºge und ist sich ihrer Verantwortung im Umgang mit KI be-</code><br><code>wusst. Sie begr√º√üt daher eine Kl√§rung unter anderem der ethischen und haftungs-</code><br><code>rechtlichen Fragen. Dabei wird im Rahmen der weiteren Verhandlungen des Ver-</code><br><code>ordnungsentwurfs und dar√ºber hinaus zu kl√§ren sein, bis zu welcher Detail- und </code><br><code>Entscheidungstiefe ein europ√§isches Handeln erforderlich ist.    </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665463_4,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665463.pdf,8,4,2665463,attachments/2665463.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>DIHK | Deutscher Industrie- und Handelskammertag e.V. </code><br><code>Address for visitors: Breite Strasse 29 | 10178 Berlin-Mitte | Postal address: DIHK | 11052 Berlin </code><br><code>Tel. +49 30-20308-0 | Fax +49 30-20308-1000 | Internet: </code><code>www.dihk.de</code><code> </code><br><code>- 4 - </code><br><code> </code><br><code>The definition of AI should definitely be made more specific and differentiated so that it reflects the </code><br><code>wide range of possible AI applications and the significant technological differences in the complexity </code><br><code>of the underlying algorithms. The unfiltered inclusion of all systems that include techniques such as </code><br><code>machine learning, logic-based concepts and, in particular, statistical approaches, search and optimisation methods clearly goes too far. This also applies against the background of the impact of AI </code><br><code>Regulation on other EU requirements, such as those from the Machinery Regulation. The degree of </code><br><code>independence of the system or decision-making autonomy appears to be suitable as a guideline </code><br><code>and yardstick. Similarly, risk-relevant properties such as the training and learning ability of systems </code><br><code>should be included. For example, a distinction has to be made between AI that is no longer changing and a solution that is constantly evolving in customer use. </code><br><code>This distinction even at the level of the field of application is urgently required, as it is disproportionate to demand extensive testing and compliance obligations even for low-threshold, dependent applications. In order for the EU to be able to establish a pioneering role in the field of AI, companies </code><br><code>that are already using or developing AI today must be protected accordingly and be given incentives </code><br><code>to continue to focus on it in the future.  </code><br><code>We therefore advocate a differentiated definition of AI that is suitable for practical application over </code><br><code>the long term and which finds a benchmark that enables a clear distinction to be made between the </code><br><code>different stages of technological development.  </code><br><code>Further Development of Risk Classes </code><br><code>The risk-based approach selected by the EU Commission is to be supported as a matter of principle. A distinction is made between AI systems that represent an ‚Äúunacceptable risk‚Äù, a ‚Äúhigh risk‚Äù </code><br><code>and a ‚Äúlow‚Äù or ‚Äúminimal risk‚Äù. This differentiation enables a proportionate regulatory intervention to </code><br><code>be implemented where this is meaningful in safety-relevant areas. However, the yardstick for this </code><br><code>should only be the specific risks that arise from the intended use of the AI systems themselves. This </code><br><code>is important from a company‚Äôs point of view, as the transfer of all general risks to the individual application will lead to a more reticent approach to the technology.  </code><br><code>Consideration of the risk areas and their sometimes high demands on AI systems makes this clear. </code><br><code>The obligations that are associated with the development and operation of high-risk AI in particular </code><br><code>must also be justified in individual cases. For this purpose, it is not only the criticality of the field of </code><br><code>application, but also the degree of development and the associated autonomy or decision-making </code><br><code>autonomy of the AI system that also has to be taken into consideration. In this way it is possible to </code><br><code>minimise the additional burden without failing to take account of any risks. </code><br><code>This should be taken into account to the extent that the high-risk areas listed in Annex III are not </code><br><code>subject to a general suspicion of being hazardous. The consequence of such a deterrent effect that </code><br><code>emanates from such a suspicion, particularly in sectors such as education, human resources or services, is that AI might be disproportionately restricted in its development. Against this background, a </code><br><code>comparison of existing practice, EU objectives and possible risks has to be carried out. It needs to </code><br><code>be determined to what extent leeway for innovative companies, for example in the dual use of data </code><br><code>or the application of AI in non-critical areas, is unjustifiably restricted here.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665431_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665431.pdf,5,2,2665431,attachments/2665431.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>We connect Polish Business and Science with the EU</code><code> </code><br><code> </code><br><code>Page 2 of 4</code><code> </code><br><code> </code><br><code>EU Transparency Register Number: 548212735276-89</code><code> </code><br><code> </code><br><code> </code><br><code>In particular, we would like to highlight the following aspects of the AI regulation: </code><br><code>1/ High-risk AI systems  </code><br><code>In the discussed regulation proposal, it is recognized that the application of a risk-based </code><br><code>framework is a better solution than the general regulation of all AI systems. Risks  </code><br><code>and hazards should be determined on a per-sector and case-by-case basis. The risk measures </code><br><code>should also give the consideration to the legal and safety implications.  </code><br><code>In our opinion, the category of high-risk algorithms is too broad. Deeming all </code><br><code>algorithms applied to the critical infrastructure as high-risk (Annex III) will paralyze  </code><br><code>the development of the domain in the currently least digitized industry, such as energy and </code><br><code>petrol sector. Algorithms allowing for internal optimization of raw material consumption, </code><br><code>predicting hydrocarbon quality as well as allocation of means of transport for fuel delivery to </code><br><code>terminals or stations should not be the subject of these regulations. They are an element of </code><br><code>the improvement of operational excellence in enterprises. High-risk algorithms used in the </code><br><code>energy and petrol sector do not concern the end customer and do not cause the risk of </code><br><code>discrimination on human rights. We therefore strongly recommend that this category be </code><br><code>excluded from the definition of ""high-risk"" algorithms. </code><br><code> </code><br><code>2/ Record-keeping of the high-risk algorithms  </code><br><code>We would like to lay down another solution than an automatic registration of all high-risk </code><br><code>algorithms. In our opinion, the reverse mechanism should be used ‚Äì i.e. allowing users  </code><br><code>to request conformity assessment and verification by audit bodies through raised objections </code><br><code>if needed. We find that enabling to use only the ""certified"" algorithms will significantly slow </code><br><code>down the development of AI in the EU and at the same time it will cause uncontrolled </code><br><code>migration of algorithm development centers outside the EU, in particular to the US and </code><br><code>China, and consequently will not solve the problem posed by regulations.  </code><br><code>The proposed provisions impose an excessive administrative burden on the suppliers of the </code><br><code>algorithms and in consequence it will delay the technological development of many </code><br><code>industries, thus cause suboptimal resource management by enterprises. </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665168_5,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2665168.pdf,5,5,2665168,attachments/2665168.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Second, the proposed definition risks to capture in the regulation also traditional software </code><br><code>systems that process data and take decisions. These systems are already rigorously tested </code><br><code>and covered by current legislation.  </code><br><code>Finally, there is no definition of ‚Äúdata‚Äù.  This means that an every-day interpretation of the </code><br><code>word needs to be used. </code><br><code> </code><br><code> </code><br><code> </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665558_7,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665558.pdf,61,7,2665558,attachments/2665558.pdf#page=7,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Forthcoming in Computer Law & Security Review </code><br><code>- 7 - </code><br><code> </code><br><code>data</code><code style=""font-weight: 1000; background-color: #FF0000;"">14</code><code>. These models can be used to make classifications, predictions, decisions, and so on </code><br><code>with new data. To illustrate</code><code style=""font-weight: 1000; background-color: #FF0000;"">15</code><code>, a model might be developed to recognise certain objects from </code><br><code>images. This model will be trained on a dataset containing many different images to ‚Äòlearn‚Äô </code><br><code>(statistically recognise) aspects that characterise particular objects. Once trained, the model </code><br><code>can be presented with new images, and work to classify objects in those images. </code><br><code> </code><br><code>Models and their outputs are </code><code>probabilistic</code><code>: as ML involves deriving a statistical model </code><br><code>representing the training data provided, there will be some degree of error or uncertainty </code><br><code>regarding their representation and outputs. The performance and operation of an ML model </code><br><code>is determined by how it is engineered ‚Äì relevant factors include the specifics of the data </code><br><code>used for training, testing, and analysis; how the data is selected, cleaned, and processed; the </code><br><code>machine learning methods, configurations, and parameters used to build the statistical </code><br><code>model representing the data; any post-processing (corrections and adjustments of model </code><br><code>outputs); and so on. ML is therefore often described as differing from traditional software </code><br><code>engineering, where the functionality and outcomes are explicitly programmed for. </code><br><code> </code><br><code>2.2.1.</code><code> </code><code>AIaaS in practice </code><br><code> </code><br><code>AIaaS is a subset of cloud services that can encompass: </code><code>(i)</code><code> providing technical environments </code><br><code>and resources to facilitate customers in undertaking their own ML (sometimes called </code><br><code>‚ÄòMachine Learning as a Service‚Äô); or </code><code>(ii)</code><code> providing access to pre-built models that customers </code><br><code>can essentially ‚Äòplug‚Äô into their applications. There are a range of possible AIaaS </code><br><code>arrangements; for instance, some services may represent a hybrid of the above, such as </code><br><code>those involving prebuilt or partially trained models that can be customised by the customer </code><br><code>through additional ML operations. This paper focuses specifically on </code><code>(ii)</code><code>, the most prominent </code><br><code>form of AIaaS</code><code>,</code><code> though aspects of our discussion may also be relevant for other AIaaS </code><br><code>variations. As such, we use ‚ÄúAIaaS‚Äù to refer to </code><code>commercial offerings that allow access to </code><br><code> </code><br><code>14</code><code> Junfei Qiu, Qihui Wu, Guoru Ding, Yuhua Xu, and Shuo Feng, ‚ÄòA Survey of Machine Learning for Big Data </code><br><code>Processing‚Äô [2016] </code><code>Eurasip Journal on Advances in Signal Processing</code><code> 1.  </code><br><code>15</code><code> This illustration details a </code><code>supervised</code><code> machine learning system. That is, where the system learns from input </code><br><code>data that is labelled with the desired output. Most AI services involve supervised systems. Some services ‚Äì </code><br><code>such as anomaly detection ‚Äì involve </code><code>unsupervised</code><code> machine learning. That is, where data is not labelled, but the </code><br><code>system itself is tasked with finding patterns and correlations in the data. </code>",POSITIVE
fitz_2662176_4,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2662176.pdf,5,4,2662176,attachments/2662176.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>Finans Danmark</code><code>  </code><code>|</code><code>  Amaliegade 7  </code><code>|</code><code>  1256 K√∏benhavn K  </code><code>|</code><code>  www.finansdanmark.dk </code><br><code>4 </code><br><code>Notat </code><br><code> </code><br><code> </code><br><code>13. juli 2021 </code><br><code>Dok. nr. FIDA-151247800-703844-v1</code><code> </code><br><code> </code><br><code> </code><br><code>-</code><code> </code><br><code>The proposal and general regulation do not clearly distinguish between artifi-</code><br><code>cial intelligence used to support human decisions and when the artificial in-</code><br><code>telligence works autonomously. Therefore, Finance Denmark suggest apply-</code><br><code>ing a risk-based approach that should lead to lighter regulatory requirements </code><br><code>for artificial intelligences that are used to support human decisions. Whereas </code><br><code>autonomous artificial intelligences should be regulated in more detail and </code><br><code>thereby adding a proportionality consideration into the regulatory setup.  </code><br><code> </code><br><code>-</code><code> </code><br><code>For example: If a specific AI-system is used to autonomously decide whether </code><br><code>a consumer should be granted credit or not - we agree that this should be </code><br><code>classified as high-risk. But if the AI-systems only work as a guideline and will be </code><br><code>followed by a human assessment, we suggest that a risk-based proportional-</code><br><code>ity is added to the narrow and binary high and low risk approach in the pro-</code><br><code>posal. </code><br><code> </code><br><code>-</code><code> </code><br><code>We strongly suggest that expert systems are exempt from the very broad AI </code><br><code>definition in annex I. The very nature of expert systems makes its contradict-</code><br><code>ing to include them in this regulation, because they have no elements that </code><br><code>makes them in any way ‚Äòintelligent‚Äô. An expert system will always render de-</code><br><code>terministic results whereas AI systems will render probabilistic results.  </code><br><code> </code><br><code>-</code><code> </code><br><code>As an alternative to a broad exemption of expert systems from the AI defini-</code><br><code>tion, we suggest that a narrower AI definition will be applied with regard to </code><br><code>future AI regulation of the financial sector where expert systems are not in-</code><br><code>cluded. We suggest that an AI definition for the financial sector will be based </code><br><code>on the current work and experience that e.g.  Danish and German FSA have </code><br><code>done on AI. This will ensure that the general regulations of the financial sector </code><br><code>and the future AI regulation will work well together.  </code><br><code> </code><br><code>-</code><code> </code><br><code>An exemption will also support a more risk-based approach in regard to the </code><br><code>financial sector which will ensure greater coherency with existing regulation </code><br><code>as stated above. Today most banks use expert system to help support the in-</code><br><code>dividual credit decisions. By supplementing such expert systems with simple </code><br><code>statistical features does not make them probabilistic AI systems. If more statis-</code><br><code>tical and probabilistic elements are added, then the expert systems will </code><br><code>eventually become an AI-system. To ensure that such gradual development </code><br><code>is regulated in a coherent way, a risk-based approach to AI-regulation is </code><br><code>beneficial. If a risk-based approach is not adopted with regard to the finan-</code><br><code>cial sector we fear that the proposed high-risk low / low risk setup will create </code>",NO_FOOTNOTES_ON_PAGE
fitz_2663310_3,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663310.pdf,13,3,2663310,attachments/2663310.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>CAIDP Statement </code><br><code> </code><br><code>EU AI Regulation </code><br><code>28 July 2021 </code><br><code> </code><br><code>European Commission </code><br><code>3 </code><br><code>‚Ä¢</code><code> </code><code>It is also not clear from the wording who makes the determination that a practice is </code><br><code>‚Äòsubliminal‚Äô, ‚Äòmaterially distorting‚Äô, ‚Äòlikely‚Äô to cause harm. This provision should be </code><br><code>clarified.</code><code> </code><br><code> </code><br><code>‚Ä¢</code><code> </code><code>The wording of the prohibited use case with such narrowed scope to define makes it </code><br><code>practically impossible to effectively ban any practice or protect any individual from </code><br><code>exploitation of vulnerabilities. Examples should be provided to help make clear </code><br><code>which practices will be prohibited.</code><code> </code><br><code> </code><br><code> </code><br><code>The UN Convention on the Rights of Persons with Disabilities</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> states that ‚ÄòPersons with </code><br><code>disabilities include those who have long-term physical, mental, intellectual or sensory </code><br><code>impairments which in interaction with various barriers may hinder their full and effective </code><br><code>participation in society on an equal basis with others.‚Äô The UN Convention recommends </code><br><code>""Universal design"" - design of products, environments, programmes and services to be usable </code><br><code>by all people, to the greatest extent possible, without the need for adaptation or specialized </code><br><code>design. Same definition has been adopted by the CJEU in the Z case.</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code> </code><br><code> </code><br><code> </code><br><code>CAIDP recommends that the right to accessibility should apply to everyone, as various </code><br><code>barriers may hinder full and effective participation in society on an equal basis with others. </code><br><code> </code><br><code>Use of ‚Äòreal-time‚Äô remote biometric identification systems in publicly accessible spaces for the </code><br><code>purpose of law enforcement  </code><br><code> </code><br><code>‚Ä¢</code><code> </code><code>The wording of the prohibited use case does not cover the use of a system by </code><br><code>private companies in public spaces; nor does it prohibit non-real time biometric </code><br><code>identification systems.  Even when not done in real-time, the collection of data to be </code><br><code>analyzed at a later stage and ‚Äúany‚Äù biometric identification in public spaces can </code><br><code>negatively impact a person‚Äôs expectation of being anonymous and the fundamental </code><br><code>rights to express one‚Äôs self and freedom of association and freedom of movement. </code><br><code> </code><br><code>‚Ä¢</code><code> </code><code>Such narrow scope of the definition of prohibited practice makes it useless in </code><br><code>practice. We support the intent of the Article. We are concerned that the text does </code><br><code>not fulfill the purpose of the Article. </code><br><code> </code><br><code> </code><br><code>CAIDP recommends a ban on biometric recognition systems used for mass surveillance </code><br><code>purposes. CAIDP also made this recommendations in our report</code><code> Artificial Intelligence and </code><br><code> </code><br><code>6</code><code> UN Convention on the Rights of Persons with Disabilities (2007), </code><br><code>https://www.ohchr.org/EN/HRBodies/CRPD/Pages/ConventionRightsPersonsWithDisabilities.as</code><br><code>px#2</code><code> </code><br><code>7</code><code> CJEU Decision on Z Case (2014), </code><code>https://eur-lex.europa.eu/legalcontent/en/TXT/?uri=CELEX:62012CJ0363</code><code>  </code>",POSITIVE
fitz_2665221_1,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665221.pdf,5,1,2665221,attachments/2665221.pdf#page=1,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>Submission by the Oxford Commission on AI and Good Governance (OxCAIGG) The </code><br><code>Oxford Internet Institute, University of Oxford. Contact: Professor Philip Howard: </code><br><code>Philip.howard@oii.ox.ac.uk</code><code> </code><br><code> </code><br><code>Summary: A list of questions for consideration and reflections on views on AI in EU MS. A </code><br><code>look at the interplay between regulation and digital technical standards. An overview of</code><code> </code><code>skills </code><br><code>and capacity for regulators and participants in standards. Reflections on the proposed extraterritorial effect for the draft AI Regulation ‚Äì risks and benefits. </code><br><code>Questions to be asked: </code><br><code>  </code><br><code>ÔÇ∑</code><code> </code><code>What types of bodies or public agencies are needed? Are existing bodies fit to </code><br><code>regulate AI?  </code><br><code>ÔÇ∑</code><code> </code><code>At what level(s) should AI be regulated? How should power be balanced between the </code><br><code>EU Institutions and the Member States? </code><br><code>ÔÇ∑</code><code> </code><code>Could there be some type of resource agency that brings together knowledge and </code><br><code>expertise to assist governments in adopting AI?  </code><br><code>ÔÇ∑</code><code> </code><code>What bodies will provide oversight into regulation? </code><br><code>ÔÇ∑</code><code> </code><code>What bodies will be the final arbiter on violations of the regulations? </code><br><code>o</code><code> </code><code>An arbitration board with appeal‚Äôs mechanisms? </code><br><code>ÔÇ∑</code><code> </code><code>How will the regulatory body maintain its independence? </code><br><code>ÔÇ∑</code><code> </code><code>How will all the elements be funded? </code><br><code>ÔÇ∑</code><code> </code><code>How will the EU collaborate with industry? </code><br><code>o</code><code> </code><code>Should be independent of funding.  </code><br><code>ÔÇ∑</code><code> </code><code>What kind of data protection principles will there be? </code><br><code>ÔÇ∑</code><code> </code><code>How can good governance norms be correctly implemented? </code><br><code>ÔÇ∑</code><code> </code><code>How will the EU keep up with the pace of development so that regulation remains </code><br><code>relevant? How will it anticipate and deal with loopholes? </code><br><code>o</code><code> </code><code>Regulatory sandboxes might be an approach  </code><br><code>ÔÇ∑</code><code> </code><code>How will ethics considerations be incorporated into the regulator? </code><br><code>ÔÇ∑</code><code> </code><code>How will the skills set be maintained and developed to run regulation? </code><br><code>ÔÇ∑</code><code> </code><code>How will the EU engage with other key players in particular those disconnected from </code><br><code>the mainstream discourse such as China? How will it share best practice and lessons </code><br><code>learned? </code><br><code> </code><br><code>AI and the EU </code><br><code>Our research at the Oxford Commission on AI & Good Governance shows that opinions </code><br><code>regarding Artificial Intelligence in the EU are divided. Using survey data from a sample of </code><br><code>154,195 respondents in 142 countries, we analyze basic indicators of public perceptions </code><br><code>about the potential harms and opportunities of involving AI in our personal affairs and public </code><br><code>life.  </code><br><code>1.</code><code> </code><code>There are regional and East-West divides in public attitudes towards AI-driven </code><br><code>automated decision making, with worries that AI will be harmful running highest in </code><br><code>North America (47%) and Latin America (49%), and notably smaller proportions of </code><br><code>respondents in Southeast Asia (25%) and East Asia (11%) concerned that AI will be </code><br><code>harmful.  </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665514_2,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665514.pdf,2,2,2665514,attachments/2665514.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>‚óè</code><br><code>Ensure that no fragmentation between SME and Large Enterprises will be</code><br><code>encouraged. Firstly the responsibilities and infrastructure burden should be</code><br><code>proportional. The SME or even a startup, might end up near a steep wall of</code><br><code>regulations, before being able to create the solution and attract the funding, if the</code><br><code>regulations will involve non-only final product services, but also the process of</code><br><code>prototype and research stages. Secondary, the creators of general-use AI systems,</code><br><code>have rarely are able to control the usage of technology in final stages of application,</code><br><code>for example, the technology of object detection and service to learn to detect objects</code><br><code>in images without the programming leads to unlimited number of applications from</code><br><code>detection cancer cells in tumor, to detecting guns/war equipment in satellite images.</code><br><code>And the creator of object detection technology, can not control on which data the</code><br><code>technology will be used in the final application. In such we support the separation of</code><br><code>the creator of technology, and the final integrator of application.</code><br><code>‚óè</code><br><code>Incorporate the self testing capabilities or general guidelines to follow. We agree, that</code><br><code>some guidelines for standardization and testing should come as insurance tool, but</code><br><code>Hold providers, deployers and users to feasible standards. As currently phrased,</code><br><code>certain mandatory requirements of the regulation will be extremely difficult or</code><br><code>impossible to meet in practice (e.g., the Art 10(3) requirement that datasets be ‚Äúfree</code><br><code>of errors and complete‚Äù demands a level of perfection that is not technically feasible).</code><br><code>Especially, when for example in the medical diagnosis field, the AI solutions with</code><br><code>80-90% accuracy might be significantly better than expert doctors. We suggest</code><br><code>following the widely known independent training/validation/testing pipeline, which</code><br><code>should be followed by best practices used in the AI community.</code><br><code>‚óè</code><br><code>The clear explanation of definitions ‚Äúsafety components‚Äù and ‚Äúsignificant changes‚Äù</code><br><code>and similar should come to place. There may be multiple collisions and fragmentation</code><br><code>between the countries implementations of AI systems testing and tracking products, if</code><br><code>there are no clear definitions. This is critical when talking about the scope of</code><br><code>requirements for high-risk AI systems.</code>",POSITIVE
fitz_2665448_3,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665448.pdf,5,3,2665448,attachments/2665448.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>3 </code><br><code>Article 15 ‚Äì</code><code> </code><code>Accuracy, Robustness and Cybersecurity </code><br><code> </code><br><code> </code><br><code>Europe TPC notes that the Proposal appropriately considers ‚Äúhealth‚Äù and ‚Äúsafety‚Äù </code><br><code>matters to which the use of AI and autonomous systems may give rise. It does so, however, </code><br><code>only in the most general and colloquial senses. Europe TPC strongly recommends that the </code><br><code>Commission carefully study the established discipline of Safety Assurance with the express </code><br><code>objective of ‚Äúimporting‚Äù such concepts, terminology, and methodologies from it (and its </code><br><code>many present applications) as may be appropriate and useful in addressing AI systems of all </code><br><code>kinds and their potential impacts.</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code>  </code><br><code> </code><br><code>Terms such as ‚Äúrobustness‚Äù and ‚Äúaccuracy‚Äù as employed in Article 15 also must be </code><br><code>carefully defined, rather than left to self-definition by system providers. Rather, the Commission may wish to adopt or endorse accepted concepts and terms used to define security </code><br><code>levels in an established standard, </code><code>e.g.</code><code>, ISA/IEC 62443.</code><code style=""font-weight: 1000; background-color: #FF0000;"">6</code><code> </code><br><code> </code><br><code> </code><br><code>In addition, Europe TPC encourages the Commission to modify Article 15 (and poten-</code><br><code>tially other germane parts of the Proposal)</code><code style=""font-weight: 1000; background-color: #FF0000;"">7</code><code> to effectively regulate the use of AI systems which </code><br><code>‚Äì although initially tested, verified, and certified ‚Äì by design will continue to ‚Äúlearn‚Äù and </code><br><code>modify themselves after their initial approval and deployment. There is an ongoing risk, </code><br><code>therefore, that the performance of such AI systems may degrade over time or be otherwise </code><br><code>volatile. Malicious action, intended to produce such maladaptive modifications with the </code><br><code>intent to produce harm, is of particular concern. The Proposal thus should directly address </code><br><code>mechanisms for detecting and remedying such ‚Äúoperational divergence‚Äù in previously </code><br><code>approved self-modifying AI systems.    </code><br><code> </code><br><code> </code><br><code>Europe TPC notes that once a particular system is approved, it also will be essential to </code><br><code>regulate the process of evaluating and approving updates to the software that controls it </code><br><code>and, per above, detecting potentially undesirable or harmful software-generated operational </code><br><code>divergence. Such processes must be as rigorous as those established for initial system approval, inspection, testing and/or certification.  </code><br><code> </code><br><code>Similarly, periodic security updates to autonomous control system software also are </code><br><code>certain to be required. Because such ‚Äúfixes‚Äù may be urgent and matter of life and safety, </code><br><code>expedited but still rigorous valuation and approval processes must be designed for them. </code><br><code>While these issues may arise most conspicuously with respect to autonomous vehicles, they </code><br><code>also will need to be addressed in the context of many other less visible AI applications. </code><br><code> </code><br><code>Article 40 ‚Äì Harmonised Standards </code><br><code> </code><br><code> </code><br><code>Europe TPC agrees that devising standards for AI systems and publishing them along-</code><br><code>side those of other ‚Äúvertical‚Äù sectors (</code><code>e.g.</code><code>, chemical substances, construction products, cosmetics) in the Official Journal of the European Union is appropriate and desirable. It suggests, however, that the creation and publication of standards ‚Äúacross‚Äù the AI sector also </code><br><code>may be productive. Such ‚Äúhorizontal‚Äù standards categories might include (but need not be </code><br><code>limited to) ‚ÄúAI algorithms,‚Äù ‚Äúcontroller software,‚Äù and ‚Äútraining data.‚Äù </code><br><code> </code><br><code> </code><br><code>5</code><code> The automated driving industry has formalized these principles in standards ISO 26262-1:2018, ‚ÄúRoad </code><br><code>vehicles  - Functional safety‚Äù and draft ISO/DIS 21448, ‚ÄúRoad vehicles - Safety of the intended functionality.‚Äù   </code><br><code> </code><br><code>6</code><code> </code><code>See</code><code>, </code><code>https://www.isa.org/intech-home/2018/september-october/departments/new-standard-specifiessecurity-capabilities-for-c</code><code> </code><br><code> </code><br><code>7</code><code> The Commission may wish to expressly address responsibility for detecting operational divergence in Article </code><br><code>16 concerning human oversight of self-modifying AI systems. </code>",POSITIVE
fitz_2663366_11,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2663366.pdf,13,11,2663366,attachments/2663366.pdf#page=11,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code>29 July 2021 </code><br><code> </code><br><code> </code><br><code> </code><br><code>11</code><code> </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Promote responsible AI practices - </code><code>The AI Act should also consider how to promote, incentivise and </code><br><code>reward industry best practices and responsible approaches to AI development and use. Such </code><br><code>‚Äúincentives‚Äù could include for instance recognising self-regulatory commitments of organisations that </code><br><code>publicly define the AI values and principles they implement along with progress against benchmarks, </code><br><code>using demonstrated accountability as a ‚Äúlicence to operate‚Äù by allowing accountable and/or certified </code><br><code>organisations greater opportunities to use and share data responsibly or using demonstrated AI </code><br><code>accountability as a criterion for public procurement projects.</code><code style=""font-weight: 1000; background-color: #FF0000;"">11</code><code>  </code><br><code> </code><br><code>11.</code><code> </code><code>Reporting obligations  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Reporting of serious incident and malfunctioning</code><code> ‚Äì Article 62 would require the reporting to market </code><br><code>surveillance authorities of any serious incident or any malfunctioning that constitutes a breach of </code><br><code>obligations under EU law. As it stands, the provision is excessively broad. While the concept of ‚Äúserious </code><br><code>incident‚Äù is sufficiently limited in scope, that of ‚Äúmalfunctioning‚Äù is not. This term lacks qualifiers and </code><br><code>could be given broad meaning to apply to any circumstance in which the AI system does not perform </code><br><code>as intended. This threshold seems unreasonably low and would create large administrative costs for </code><br><code>AI providers that would have to closely monitor and report all cases of malfunctioning. This low </code><br><code>threshold would also create huge workloads for market surveillance authorities that would have to </code><br><code>assess and decide on appropriate measures in such cases. Therefore CIL recommends limiting the </code><br><code>reporting obligation to serious incidents and to instances of serious malfunctioning that breach EU law.  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Risk of double reporting obligations</code><code> ‚Äì CIPL also flags the potential risk of overlapping reporting </code><br><code>obligations under different EU laws. It is possible that a serious incident involving an AI system could </code><br><code>give rise to reporting obligations under the AI Act and under the NIS Directive or the GDPR, for instance. </code><br><code>However, each of these instruments specify different reporting times and require that notification be </code><br><code>made to different regulators, who will be tasked with assessing the incident and deciding on </code><br><code>appropriate measures, including investigations. CIPL recommends that Article 62 be revisited to </code><br><code>account for the duplication of reporting duties, and, where overlapping obligations exist, clarify which </code><br><code>reporting system applies under which circumstances. </code><br><code> </code><br><code>12.</code><code> </code><code>Interaction of the AI Act with the GDPR  </code><br><code> </code><br><code>ÔÇ∑</code><code> </code><code>Relation between the AI Act and the GDPR</code><code> - The AI Act is largely an instrument of product law, but </code><br><code>makes multiple references to, and impacts, fundamental rights, including data protection. While it </code><br><code>appears that there is no intent to construct the AI Act as </code><code>lex specialis</code><code> to the GDPR, CIPL underlines the </code><br><code>necessity to ensure that the AI Act does not have the unintended effect of creating additional or </code><br><code>conflicting rules with respect to personal data, including cases where AI applications do not interfere </code><br><code>with data protection while bringing benefits to society.   </code><br><code>                                                 </code><br><code>11</code><code> See note 3 at page 8 for more examples of possible incentives.  </code>",POSITIVE
fitz_2665430_8,other,../24212003_requirements_for_artificial_intelligence/attachments/2665430.pdf,8,8,2665430,attachments/2665430.pdf#page=8,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>from the statistical, or from both</code><code>.</code><code style=""font-weight: 1000; background-color: #FF0000;"">50</code><code> Otherwise, AI providers would not understand what the </code><br><code>appropriate way is to prevent or minimize bias.  </code><br><code> </code><br><code>More information:  </code><br><code> </code><br><code> </code><br><code>These comments are based on the author‚Äôs post ‚Äò</code><code>MAKING AI‚ÄôS TRANSPARENCY </code><br><code>TRANSPARENT: notes on the EU Proposal for the AI Act</code><code>‚Äô published on the </code><code>European Law Blog</code><code>, </code><br><code>article ‚ÄòAI as a Medical Device: Between the Medical Devices Framework and the General AI </code><br><code>Regulation‚Äô accepted for the conference and for the relevant publication ‚Äò</code><code>Time to Re-shape the </code><br><code>Digital Society</code><code>‚Äô,</code><code style=""font-weight: 1000; background-color: #FF0000;"">51</code><code> and the article in progress on legal implications of biases in AI‚Äôs clinical </code><br><code>genetics. It is recommended to read the published article for more detailed explanations of the main </code><br><code>points provided herein.  </code><br><code> </code><br><code> </code><br><code> </code><br><code>  </code><br><code> </code><br><code>50</code><code> Actually, the AI Act in its recital 44 makes the correlation between statistical and societal views: ‚ÄòHigh data quality is essential </code><br><code>for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to </code><br><code>ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited </code><br><code>by Union law.‚Äô</code><code> </code><br><code>51</code><code> International Conference celebrating the 40th (+1) Anniversary of CRIDS to be held on 18th and 19th of November 2021, Namur, </code><br><code>Belgium </code>",POSITIVE
fitz_2665649_33,ngo,../24212003_requirements_for_artificial_intelligence/attachments/2665649.pdf,39,33,2665649,attachments/2665649.pdf#page=33,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>special or protected categories (e.g. by combining eye </code><br><code>and hair colour to predict ethnicity or using the wearing of a </code><br><code>headscarf to predict religion). In the event that the self-learning </code><br><code>nature of some AI systems makes it difficult to know whether </code><br><code>people  are  being  assigned  to  categories  that  could  lead  to </code><br><code>discrimination,  the  precautionary  principle  dictates  that  such </code><br><code>uses should also be prohibited;</code><br><code>‚ñ™</code><code> As  already  explained  above,  the  definition  of  biometric  data </code><br><code>(article 3.33) must be complemented with a definition of human </code><br><code>features to ensure that it includes all data relevant to biometric </code><br><code>or  other  human feature  categorisation  (without  loopholes  for </code><br><code>types of data or methods of processing that don‚Äôt meet the </code><br><code>current threshold);</code><code style=""font-weight: 1000; background-color: #FF0000;"">64</code><br><code> </code><br><code>v. Emotion recognition, which is scientifically invalid and can unduly </code><br><code>infringe on all human rights, in particular human dignity and free </code><br><code>expression,  by  comprehensively  prohibiting  the  placing  on  the </code><br><code>market, putting into service or use of AI to infer, predict, analyse or </code><br><code>assess  a  person‚Äôs  emotions,  feelings,  emotional  state,  beliefs, </code><br><code>preferences, intentions or otherwise inner thoughts, as well as to </code><br><code>use human features, behaviours or expressions to predict future </code><br><code>actions or behaviours;</code><br><code>vi. Uses of AI that constitute mass surveillance should be prohibited. </code><br><code>Mass  surveillance  means  the  surveillance  of,  or  potential  for </code><br><code>surveillance  of,  whole  or  part  populations  (including  specific </code><br><code>groups), and is thus inherently unnecessary and disproportionate.</code><br><code>Note that outside of the proposed</code><code> prohibitions of biometric mass surveillance</code><code>,  </code><code>biometric categorisation on the basis of special or protected </code><br><code>categories</code><code> and </code><code>emotion recognition </code><code>in general, there are additional AI applications which use human features and which can pose a high risk to fundamental rights. Therefore we additionally recommend that:</code><br><code>1. Heading 1 of Annex III is changed to ‚ÄúPhysiological, behavioural, biomet-</code><br><code>ric and neurological authentication, identification and categorisation‚Äù. If </code><br><code>human features are defined in article 3 according to our recommendation, then an alternative heading could be ‚ÄúAuthentication, identification </code><br><code>and categorisation of human features‚Äù;</code><br><code>2. Under heading 1, the following use cases are added in addition to the ex-</code><br><code>isting use case (1.a), but are not necessarily exhaustive at this point:</code><br><code>i.</code><br><code>Physiological, behavioural or biometric authentication, identification or </code><br><code>categorisation (i.e. of human features) for law enforcement purposes;</code><br><code>64 See Access Now‚Äôs Submission to the Consultation on the AI White Paper for a deeper </code><br><code>engagement with issues of the definition of (biometric) categorisation.</code><br><code>33</code>",POSITIVE
fitz_2660134_3,public_authority,../24212003_requirements_for_artificial_intelligence/attachments/2660134.pdf,4,3,2660134,attachments/2660134.pdf#page=3,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>y con ello ser√≠a perfectamente identificable el titular y responsable.  Cabe recordar asimismo </code><br><code>que el Registro Mercantil ser√≠a f√°cilmente consultable en toda la Uni√≥n Europea a trav√©s del </code><br><code>sistema europeo de interconexi√≥n de los registros mercantiles (BRIS)</code><code>. Esta soluci√≥n podr√≠a </code><br><code>funcionar a nivel europeo mediante la red de interconexi√≥n de Registros Mercantiles europeos </code><br><code>BRIS ya que, desde junio de 2017, los registros mercantiles de todos los pa√≠ses de la UE est√°n </code><br><code>interconectados. </code><br><code>Esto supone que se puede buscar informaci√≥n sobre las sociedades registradas en cualquier pa√≠s </code><br><code>de la UE, Islandia, Liechtenstein o Noruega adem√°s de que los registros pueden compartir </code><br><code>informaci√≥n sobre sucursales extranjeras y fusiones transfronterizas de empresas. </code><br><code>Este sistema - sistema de interconexi√≥n de los registros empresariales (BRIS) - constituye un </code><br><code>esfuerzo conjunto por parte de los gobiernos de la UE y de la Comisi√≥n Europea que permitir√≠a </code><br><code>conectar el registro de los sistemas de inteligencia artificial est√° previsto tanto en la regulaci√≥n </code><br><code>nacional (art√≠culo 17.5 CCo</code><code style=""font-weight: 1000; background-color: #FF0000;"">1</code><code>) como comunitaria (Directiva 2017/1132/UE</code><code style=""font-weight: 1000; background-color: #FF0000;"">2</code><code> y Reglamento de </code><br><code>Ejecuci√≥n (UE) 2020/2244</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code>) y permitir√≠a conectar las inscripciones en el Registros de Bienes </code><br><code>Muebles no s√≥lo con las sociedades espa√±olas sino con otras entidades europeas. </code><br><code>Si, como sugieren algunos autores y hemos mencionado anteriormente, llegamos al extremo de  </code><br><code>atribuir una personalidad jur√≠dica a los sistemas de inteligencia artificial la inscripci√≥n en el </code><br><code>Registro de Bienes Muebles nacional en los t√©rminos mencionados puede configurarse, como el </code><br><code>momento determinante, en su caso, del </code><code>nacimiento de dicha personalidad electr√≥nica</code><code>, de </code><br><code>modo an√°logo a lo que ya ocurre con el nacimiento de la personalidad jur√≠dica de las sociedades </code><br><code>y su inscripci√≥n en el Registro Mercantil, conforme a lo dispuesto en los art√≠culos 33 del Real </code><br><code>Decreto Legislativo 1/2010, de 2 de junio, por el que se aprueba el texto refundido de la Ley de </code><br><code>Sociedades de Capital. </code><br><code>Sin perjuicio de su ulterior desarrollo, los </code><code>requisitos para su inscripci√≥n</code><code> deber√≠an contener los </code><br><code>siguientes elementos: clase de sistema de inteligencia artificial, algoritmo,‚Ä¶ (identificado con un </code><br><code>hash SHA-256, su marca, nombre comercial, patente de invenci√≥n, etc.); t√≠tulo o denominaci√≥n, </code><br><code>si la tuviere; explotaci√≥n a que est√© destinada, en su caso; fecha y n√∫mero de inscripci√≥n, </code><br><code>renovaci√≥n, rehabilitaci√≥n o pr√≥rroga en la base de datos especial o administrativa </code><br><code>correspondiente y tipo de derecho real que se adquiere, propiedad, derecho real de garant√≠a o </code><br><code>cualquier otro de la misma especie. </code><br><code> </code><br><code>1</code><code> Art. 17.5 CCo. ¬´El Registro Mercantil asegurar√° la interconexi√≥n con la plataforma central europea en la </code><br><code>forma que se determine por las normas de la Uni√≥n Europea y las normas reglamentarias que las </code><br><code>desarrollen. El intercambio de informaci√≥n a trav√©s del sistema de interconexi√≥n facilitar√° a los </code><br><code>interesados la obtenci√≥n de informaci√≥n sobre las indicaciones referentes al nombre y forma jur√≠dica de </code><br><code>la sociedad, su domicilio social, el Estado miembro en el que estuviera registrada y su n√∫mero de registro.¬ª </code><br><code>2</code><code> El sistema de interconexi√≥n de registros mercantiles se regula en los art√≠culos 22 y 23 de la DIRECTIVA </code><br><code>(UE) 2017/1132 DEL PARLAMENTO EUROPEO Y DEL CONSEJO de 14 de junio de 2017 sobre determinados </code><br><code>aspectos del Derecho de sociedades. </code><br><code>Disponible en https://eur-lex.europa.eu/legal-content/ES/LSU/?uri=CELEX:32017L1132 </code><br><code>3</code><code> REGLAMENTO DE EJECUCI√ìN (UE) 2020/2244 DE LA COMISI√ìN de 17 de diciembre de 2020 por el que </code><br><code>se establecen disposiciones de aplicaci√≥n de la Directiva (UE) 2017/1132 del Parlamento Europeo y del </code><br><code>Consejo en lo que respecta a las especificaciones y los procedimientos t√©cnicos necesarios para el sistema </code><br><code>de interconexi√≥n de registros, y por el que se deroga el Reglamento de Ejecuci√≥n (UE) 2015/884 de la </code><br><code>Comisi√≥n.  </code><br><code>Disponible en https://eur-lex.europa.eu/legal-content/ES/TXT/HTML/?uri=CELEX:32020R2244 </code>",POSITIVE
fitz_2663367_2,business_association,../24212003_requirements_for_artificial_intelligence/attachments/2663367.pdf,3,2,2663367,attachments/2663367.pdf#page=2,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code>implementation process. Modifications to the regulation and in particular to the scope would </code><br><code>compromise legal certainty and predictability for the stakeholders, as well as their application of the </code><br><code>regulation. Therefore, modifications to the regulation would require adequate consideration and </code><br><code>consultation, which should be duly specified in the draft. It is vital that industry stakeholders are </code><br><code>involved in the process of modification in order to provide legal clarity and bring trust.  </code><br><code> </code><br><code>Third, the </code><code>timeline</code><code> will be important for the providers, for the application of both the regulation in its </code><br><code>initial version and the modifications made afterwards.  </code><br><code> </code><br><code>The proposal specifies that AI systems should </code><code>comply with the regulation</code><code> when ‚Äúplaced on the </code><br><code>market‚Äù or ‚Äúput into service‚Äù. This concept is already well-established and documented in existing </code><br><code>legislation relating to physical products, but for digital products this concept is quite new. As digital </code><br><code>systems often undergo multiple iterations and developments before they are considered to be fully </code><br><code>implemented and ready to release, clarification is needed on when the AI system should be </code><br><code>compliant. If, for example, compliance should already be ensured when a proof of concept is </code><br><code>delivered to a client, this would significantly increase the costs of developing an AI business case </code><br><code>while at that point there is no certainty the application will actually be purchased by the client. </code><br><code> </code><br><code>The </code><code>application of modifications</code><code> to the regulation is equally important and even more delicate. In that </code><br><code>case, the modifications will impact AI systems that are already on the market and are being used, </code><br><code>requiring sufficient time for providers to implement these rules. In case of changes, a transition period </code><br><code>of minimum two years should be foreseen to ensure that industry has time to ensure their solutions </code><br><code>are compliant with the amended regulation. </code><br><code> </code><br><code>Fourth, Agoria suggests focusing on the </code><code>intended uses</code><code> of the AI applications, </code><code>as</code><code> </code><code>determined by the </code><br><code>providers</code><code>. The concept of ‚Äúreasonably foreseeable misuse‚Äù will undoubtedly make the application of </code><br><code>the regulation more difficult for providers and result in discussions between the stakeholders. Also, </code><br><code>the providers should not be liable in case of the users‚Äô misuse of the AI application, especially if such </code><br><code>misuse is intentional or if the provider has no means of avoiding this risk.  </code><br><code> </code><br><code>Fifth, as regards the </code><code>support for innovation</code><code>, Agoria notes that the proposal recognized the </code><br><code>administrative and financial impact it has on providers of AI systems. Agoria expects that the </code><br><code>proposed support mechanisms </code><code>will not suffice</code><code> to ensure that our European companies can continue </code><br><code>to innovate. In general, most SMEs believe that this regulation will put them at a huge disadvantage, </code><br><code>as their size and limited resources will make it difficult to comply with this regulation. This matter </code><br><code>should get more attention, as often it is the small-scale providers that drive AI innovation and build the </code><br><code>applications needed to improve the quality of our lives. </code><br><code> </code><br><code>Regarding </code><code>regulatory sandboxes</code><code>, it is positive that the Commission urges the Member States to take </code><br><code>action, although this should be further formalized. Agoria considers that regulatory sandboxes can </code><br><code>only foster innovation if they are supported by adequate incentives and budgets.  </code><br><code> </code><br><code>On the </code><code>measures for small scale providers and users</code><code>, Agoria supports the proposal although we </code><br><code>consider that these measures need to be extended. While these small-scale providers and users have </code><br><code>priority access to legal sandboxes, participating in them will require a huge time investment from their </code><br><code>side and Agoria is of the opinion they should be financially encouraged to participate. It is positive that </code><br><code>there will be a reduction of the cost on third-party conformity assessment, although this should be </code><br><code>further extended to specific funding for companies that undergo a self-assessment procedure. In </code><br><code>Belgium we see that start-ups, scale-ups and SME‚Äôs develop many of the most innovative AI </code><br><code>applications. We think that this regulation will put them at a competitive disadvantage compared to </code><br><code>bigger companies that already have a lot of experience with compliance mechanisms. </code><br><code> </code><br><code>As a final point, the European Union should consider that this regulation will have a considerable </code><br><code>impact on its place in the AI world</code><code>. A negative impact on AI applications that could have a positive </code>",NO_FOOTNOTES_ON_PAGE
fitz_2665425_10,consumer_organisation,../24212003_requirements_for_artificial_intelligence/attachments/2665425.pdf,27,10,2665425,attachments/2665425.pdf#page=10,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code>Artificial intelligence needs real world regulation </code><br><code>10</code><code> l 27 </code><br><code>Verbraucherzentrale Bundesverband e.V.</code><code> </code><br><code>life insurance</code><code style=""font-weight: 1000; background-color: #FF0000;"">11</code><code>, AI-based claims handling for car-</code><code style=""font-weight: 1000; background-color: #FF0000;"">12</code><code>, liability and household content insurances</code><code>13</code><code>. Insurers can already use individual behavioural-based bonus programs as </code><br><code>vehicle to price particular consumer groups out of the market, thereby undermining the </code><br><code>principle of solidarity.</code><code style=""font-weight: 1000; background-color: #FF0000;"">14</code><code> Thereby AI can lead to significant unjustified treatment, discrimination and financial harm for individuals or groups of consumers.</code><code>15</code><code> </code><br><code>Another area in which AI systems can obviously lead to significant financial harm for </code><br><code>consumers are consumer-facing AI applications intended to be used automated financial investment or portfolio management.</code><code>16</code><code>  </code><br><code>AI-driven payment and debt collection services are another area of concern. The online </code><br><code>shopping boom during the corona pandemic led to an increasing number of consumer </code><br><code>complaints that revealed underlying problems of these services</code><code style=""font-weight: 1000; background-color: #FF0000;"">17</code><code>: A large provider </code><br><code>raised vzbv‚Äôs attention as he rejects consumers‚Äô money transfers allegedly, because </code><br><code>the stated purpose for the transaction does not exactly correspond to its specifications. </code><br><code>The reason for the rejection is presumably a fully automated process. Consequently, </code><br><code>the provider uses a debt collection agency to handle the case and charges the consumer for the additional costs. The rejection of payments and the corresponding additional costs are obviously unjustified, as the provider can nonetheless assign the payment to the right consumers: he informs them that their transfer was rejected. </code><br><code>vzbv, in line with the German Data Ethics Commission (DEK) and the academic community, points out that it is not sufficient to simply refer to GDPR when it comes to the </code><br><code>protection of personal data in the context of AI: the GDPR‚Äôs scope is limited and does </code><br><code>for example not regulate profiling or scoring per se or the automated preparation of human decisions. One core function of AI applications in consumer markets is the classification and prediction of user behaviour based on profiles/scores in order to prepare or </code><br><code>make/prepare decisions about consumers. Furthermore, GDPR only covers personal </code><br><code>data. However, AI applications increasingly rely on non-personal data, also when preparing or taking decisions about consumers, which leaves consumers unprotected. It </code><br><code>___________________________________________________________________________________________</code><code> </code><br><code>11</code><code> Generali Vitality GmbH (see FN. 9). </code><br><code>12</code><code> In the car insurance sector, AI is employed to examine photos of damages and to decide on the coverage of the dam-</code><br><code>age or repair costs.The ‚ÄúAllianz Schaden Express App‚Äù in Austria also automatically decides on cases but usually with </code><br><code>a human in the loop Frankfurter Allgemeine Zeitung GmbH: Geld in 30 Sekunden?: Der vollautomatische KfzSachverst√§ndige (2018), URL: https://www.faz.net/aktuell/finanzen/meine-finanzen/versichern-undschuetzen/kuenstliche-intelligenz-in-der-kfz-versicherung-eine-revolution-15374987.html [Access: 20.07.2021]. See </code><br><code>also: SVRV - Advisory Council for Consumer Affairs: Consumer-friendly scoring. Report of the Advisory Council for </code><br><code>Consumer Affairs (2018), URL: http://www.svr-verbraucherfragen.de/en/ [Access: 20.07.2021]; Insurance Journal: </code><br><code>Tokio Marine Uses Tractable's Artificial Intelligence Solution for Auto Claims in Japan (2020), URL: </code><br><code>https://www.insurancejournal.com/news/international/2020/05/11/568090.htm [Access: 23.07.2021]  </code><br><code>13</code><code> Lemonade: How Lemonade's Tech-Powered Claims Work | Lemonade Insurance (2021), URL: </code><br><code>https://www.lemonade.com/de/en/claims [Access: 23.07.2021]. </code><br><code>14</code><code> ‚ÄûAnother subject for discussion is whether bonus programmes are used as a vehicle for indirect risk selectivity if those </code><br><code>who are healthy anyway and those who are health-conscious are the main beneficiaries. The survey shows that some </code><br><code>health insurance funds deliberately set out to appeal to health-conscious individuals. This may be interpreted as an </code><br><code>attempt to recruit and retain the youngest and healthiest possible clientele.‚Äù, SVRV - Advisory Council for Consumer </code><br><code>Affairs (2018) (see FN. 12), p. 86. </code><br><code>15</code><code> The report of the Advisory Council for Consumer Affairs at Germany‚Äôs Federal Ministry of Justice and Consumer </code><br><code>Protection cites an insurer warning: ‚ÄúBehavioural tariffs may result in individual groups of insured persons exploiting </code><br><code>them at the expense of people whose illnesses are not lifestyle-related. We therefore take a very critical view of these </code><br><code>tariffs.‚Äù ebd. p. 86 </code><br><code>16</code><code> Compare: Frankenfield, Jake: What Is a Robo-Advisor? in: Investopedia (2021), URL: </code><br><code>https://www.investopedia.com/terms/r/roboadvisor-roboadviser.asp [Access: 20.07.2021]. </code><br><code>17</code><code> Verbraucherzentrale Bundesverband: Beschwerden zu digitalen Bezahldiensten nehmen zu (2021), URL: </code><br><code>https://www.vzbv.de/pressemitteilungen/beschwerden-zu-digitalen-bezahldiensten-nehmen-zu [Access: 04.08.2021]. </code>",FALSE_NEGATIVE
fitz_2663324_4,company,../24212003_requirements_for_artificial_intelligence/attachments/2663324.pdf,7,4,2663324,attachments/2663324.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code>  </code><br><code> </code><br><code>  </code><br><code>Page </code><code>4</code><code> of 7</code><code> </code><br><code> </code><br><code>Establish a unified standard for data exchange </code><br><code>Incentives for organizations/vendors that align </code><br><code>with common data standards </code><br><code>Promote data interoperability and exchange </code><br><code>protocol, based on the experience of ERNs </code><br><code>(Collaboration between several member states on </code><br><code>the same topic) </code><br><code> </code><br><code>Allow transfer of data between local centres </code><br><code>Centralise data collection from regional data sets </code><br><code> </code><br><code> </code><br><code>REGULATORY CHALLENGES </code><br><code>Validation process and Reimbursement </code><br><code>Member States are slowly but surely adopting national legislatures allowing for digital health solutions to </code><br><code>apply for </code><code>reimbursement</code><code>. While it is far from being general practice, the trend is growing, and regulatory </code><br><code>agencies or Health Technology Assessment (HTA) bodies are adapting their pathways to include the </code><br><code>assessment of digital health solutions, including AI-based solutions. </code><br><code>While AI providing health data based on continuous monitoring (both passively and actively) of daily </code><br><code>activities has a greater potential for reimbursement, the solutions utilizing AI without immediate health </code><br><code>benefits might find a more challenging process. </code><br><code>The data set used by the AI applications or used as secondary data (for instance, when an AI-based tool </code><br><code>provides information on an internal decision point) in the application dossier for HTA, reimbursement </code><br><code>process or post-marketing studies must be of the </code><code>highest standards of quality</code><code>. It is crucial to </code><code>demonstrate </code><br><code>the relevance</code><code> of the data in a lifecycle approach: in the context of approval and, at a later stage, </code><br><code>reimbursement. Should the regulator dispute the quality and/or the validity of the data used by the algorithm, </code><br><code>the entire process could be severely delayed, henceforth, delaying </code><code>access to patients</code><code>. </code><br><code>While one can understand the challenge in adapting to an extremely changing world, as well as to the </code><br><code>massive disruption AI is about to bring to healthcare, it is indisputable that regulators need to issue </code><code>clear </code><br><code>guidance</code><code> on quality requirements for validation processes, reimbursement criteria, and post-marketing </code><br><code>studies</code><code style=""font-weight: 1000; background-color: #FF0000;"">12</code><code>. We also call for clearer visibility on the possible evolution of the regulatory framework, to take </code><br><code>into account the coming development of technology (e.g.: adaptive AI). </code><br><code>                                                      </code><br><code>12</code><code> </code><code>Be it for surveillance or studies in the context of real-world evidence generation.</code><code> </code>",POSITIVE
fitz_2665397_5,academic_research_institution,../24212003_requirements_for_artificial_intelligence/attachments/2665397.pdf,15,5,2665397,attachments/2665397.pdf#page=5,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>4 of 14</code><code> </code><br><code> </code><br><code>-</code><code> </code><br><code>Providers established in the Union that create and offer AI systems, independently </code><br><code>from where they place the system on the market or into service.</code><code style=""font-weight: 1000; background-color: #FF0000;"">3</code><code> This extension would </code><br><code>avoid the situation where European providers develop systems that would be </code><br><code>prohibited or classified as high risk in the EU and make them available in third </code><br><code>countries (which do not offer an equivalent level of protection of fundamental rights) </code><br><code>without the guarantees established in the Draft AI Act.</code><code style=""font-weight: 1000; background-color: #FF0000;"">4</code><code> It will also cover the situations </code><br><code>of testing/quality assurance processes that happen in-house, i.e. before the </code><br><code>placement of the AI system on the market, as the intended purpose may differ. </code><code> </code><code>  </code><br><code>-</code><code> </code><br><code>Providers and users of AI systems located in a third country if the outcome of the AI </code><br><code>systems is used outside the EU but with an impact on persons located in the EU. This </code><br><code>suggestion would be in line with the territorial scope of application of the GDPR, which </code><br><code>applies extra-EU, where the controller targets citizens located in the Union (Article </code><br><code>3(2) GDPR). We are in this regard suggesting to centre the territorial application not </code><br><code>only around the EU single market but also around EU citizens. In our opinion, </code><br><code>connecting the application of the AI Act to EU citizens would lead to an equal footing </code><br><code>for providers and users in the EU Member States and third countries. This level playing </code><br><code>field would serve as an incentive for innovation for providers established in the EU.</code><code> </code><br><code>  </code><br><code> </code><br><code>3.</code><code> </code><code>The risk-based approach </code><br><code> </code><br><code>The Draft AI Act intends to differentiate rules and obligations based on the level of risk </code><br><code>entailed by the AI system. It distinguishes between: </code><br><code>-</code><code> </code><br><code>Prohibited AI practices involving a level of risk considered ‚Äúunacceptable‚Äù</code><code style=""font-weight: 1000; background-color: #FF0000;"">5</code><code> or contrary </code><br><code>to EU values; </code><br><code>-</code><code> </code><br><code>High-risk AI systems, i.e. systems that pose a significant risk to the health, safety, or </code><br><code>fundamental rights of natural persons; </code><br><code>-</code><code> </code><br><code>All other cases. </code><br><code> </code><br><code>3.1.</code><code> </code><br><code>Prohibited AI practices </code><br><code> </code><br><code>Four different practices (listed in Article 5) are prohibited by default because the Commission </code><br><code>retained that they pose an unacceptable risk to the health and safety or fundamental rights </code><br><code>of natural persons, or they are considered contrary to European values. This attempt to </code><br><code>exclude certain systems is commendable, but the formulation is not entirely satisfying. These </code><br><code>prohibitions are too narrowly designed and might be difficult to enforce in practice. </code><br><code> </code><br><code>3</code><code> In the current Draft, the definitions of ‚Äúplacing on the market‚Äù and ‚Äúputting into service‚Äù refer </code><br><code>exclusively to the Union market. See, Article 3(9)-(11). </code><br><code>4</code><code> On this risk, see also Veale and Borgesius (n. 2), p. 8. </code><br><code>5</code><code> See point 5.2.2, Explanatory Memorandum, Draft AI Act. </code>",POSITIVE
fitz_2665502_4,other,../24212003_requirements_for_artificial_intelligence/attachments/2665502.pdf,10,4,2665502,attachments/2665502.pdf#page=4,Footnotes in doc + all footnotes highlighted: üëç; something other than footnote or not all footnotes highlighted: üëé; no footnote in doc: üëå,"<code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code> </code><br><code>4 </code><br><code>the natural person developing the system. They stress that, in fact, some AI tools often have no </code><br><code>broader purpose beyond serving as building blocks for various user-designed applications, which </code><br><code>in turn serve more specific user-generated intended purpose; such general-purpose tools are not </code><br><code>in and of themselves AI systems, but rather serve as components or precursors of AI systems. </code><br><code>The text of the Regulation should hence be more explicit regarding the allocation of </code><br><code>responsibilities when it comes to general purpose tools, as ‚Äì in that case ‚Äì it is the user who often </code><br><code>ultimately decides on the intended use of the AI system rather than the provider. </code><br><code>Furthermore, the proposed definition also seems to focus on AI as a software exclusively. This </code><br><code>can cause some concerns from the scientific community around the exclusion of AI as a hardware </code><br><code>(e.g. in robotic devices). If the aim is to secure a broader definition of AI (which includes robotics), </code><br><code>this should ideally be clarified in the text or through other guidance documents. Similarly, it would </code><br><code>be beneficial to provide more clarity on whether and to which extent AI research is seen in scope </code><br><code>of the AI Act, given that it is not directly related to putting an AI product in service or on the market. </code><br><code>Finally, attention can be drawn to sectorial legislation that already legislates AI systems, in </code><br><code>particular the medical device and in-vitro diagnostic regulation. The potential legislative and </code><br><code>standardization overlap brings a risk of conflicts and could increase costs for AI providers, which </code><br><code>‚Äì in the area of AI in healthcare, for instance ‚Äì may ultimately be transposed onto the healthcare </code><br><code>system or the patient. Great care should hence be taken to align the definitions and requirements </code><br><code>of the proposed regulation and existing sectorial legislation. </code><br><code>4) Further clarifications are needed as regards certain terms  </code><br><code>Putting on the Market/Into service</code><code>: </code><code>The proposal specifies that AI systems should comply with the </code><br><code>regulation when ‚Äúplaced on the market‚Äù or ‚Äúput into service‚Äù. This concept is already wellestablished and documented in existing legislation relating to physical products, but for digital </code><br><code>products this concept is quite new. As digital systems often undergo multiple iterations and </code><br><code>developments before they are considered fully implemented and ready to release, clarification is </code><br><code>needed on when the AI system should be compliant. Some members pointed out that if, for </code><br><code>example, compliance should already be ensured when a proof of concept is delivered to a client, </code><br><code>this would significantly increase the costs of developing an AI business case while at that point </code><br><code>there is no certainty the application will actually be purchased by the client.</code><code> </code><br><code>Citizen Scoring</code><code>: Public services already assign scores to citizens in numerous contexts (from </code><br><code>verifying eligibility to social benefits, to the risk of recidivism, and from the need for intervention to </code><br><code>protect / place children to the chances of easily finding a job). Further clarification about the </code><br><code>practices that fall under the prohibition on general citizen scoring would hence be welcomed, as </code><br><code>the language that is currently used in the proposed regulation, and the examples provided by the </code><br><code>European Commission in its presentations, are not always clear. </code><br><code>Remote biometric identification system:</code><code> The current definition is not entirely clear. Some may </code><br><code>argue that ‚Äúremote‚Äù is referring to cloud services, while others will consider this as referring to </code><br><code>no-physical-contact identification systems (such as cameras). For reasons of legal certainty, it </code><br><code>would be good to clarify this. </code><br><code>Task allocation:</code><code> Annex III bullet 4(b) considers AI systems as high-risk if these are used for task </code><br><code>allocation in the context of employment, workers management and access to self-employment. </code><br><code>Already today, a lot of software is used to assign tasks in production plants, call-centers, </code>",NO_FOOTNOTES_ON_PAGE
